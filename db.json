{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/favicon.ico","path":"favicon.ico","modified":0,"renderable":0},{"_id":"source/images/Webserver_memory_graph.jpg","path":"images/Webserver_memory_graph.jpg","modified":0,"renderable":0},{"_id":"source/images/academic_faces3.png","path":"images/academic_faces3.png","modified":0,"renderable":0},{"_id":"source/images/anchr_1.jpg","path":"images/anchr_1.jpg","modified":0,"renderable":0},{"_id":"source/images/anchr_2.jpg","path":"images/anchr_2.jpg","modified":0,"renderable":0},{"_id":"source/images/angular2_logo.png","path":"images/angular2_logo.png","modified":0,"renderable":0},{"_id":"source/images/apple-touch-icon.png","path":"images/apple-touch-icon.png","modified":0,"renderable":0},{"_id":"source/images/benchmarks.svg","path":"images/benchmarks.svg","modified":0,"renderable":0},{"_id":"source/images/benchmarks2.svg","path":"images/benchmarks2.svg","modified":0,"renderable":0},{"_id":"source/images/cartpole1.jpg","path":"images/cartpole1.jpg","modified":0,"renderable":0},{"_id":"source/images/cartpole2.png","path":"images/cartpole2.png","modified":0,"renderable":0},{"_id":"source/images/cartpole3.png","path":"images/cartpole3.png","modified":0,"renderable":0},{"_id":"source/images/cartpole4.png","path":"images/cartpole4.png","modified":0,"renderable":0},{"_id":"source/images/cert.png","path":"images/cert.png","modified":0,"renderable":0},{"_id":"source/images/crawlbuddy2.png","path":"images/crawlbuddy2.png","modified":0,"renderable":0},{"_id":"source/images/doodlerbot_icon.png","path":"images/doodlerbot_icon.png","modified":0,"renderable":0},{"_id":"source/images/dqn1.png","path":"images/dqn1.png","modified":0,"renderable":0},{"_id":"source/images/dqn2.png","path":"images/dqn2.png","modified":0,"renderable":0},{"_id":"source/images/dqn3.png","path":"images/dqn3.png","modified":0,"renderable":0},{"_id":"source/images/dqn4.png","path":"images/dqn4.png","modified":0,"renderable":0},{"_id":"source/images/expensebot_icon.png","path":"images/expensebot_icon.png","modified":0,"renderable":0},{"_id":"source/images/gh_ka_world.png","path":"images/gh_ka_world.png","modified":0,"renderable":0},{"_id":"source/images/gh_location_langs.png","path":"images/gh_location_langs.png","modified":0,"renderable":0},{"_id":"source/images/gh_locations.png","path":"images/gh_locations.png","modified":0,"renderable":0},{"_id":"source/images/gh_popular_lang.png","path":"images/gh_popular_lang.png","modified":0,"renderable":0},{"_id":"source/images/halite_langs.png","path":"images/halite_langs.png","modified":0,"renderable":0},{"_id":"source/images/middleman.png","path":"images/middleman.png","modified":0,"renderable":0},{"_id":"source/images/middleman2.png","path":"images/middleman2.png","modified":0,"renderable":0},{"_id":"source/images/mqtt_bench_2.png","path":"images/mqtt_bench_2.png","modified":0,"renderable":0},{"_id":"source/images/push_screenshot1.png","path":"images/push_screenshot1.png","modified":0,"renderable":0},{"_id":"source/images/push_screenshot2.png","path":"images/push_screenshot2.png","modified":0,"renderable":0},{"_id":"source/images/qn1.png","path":"images/qn1.png","modified":0,"renderable":0},{"_id":"source/images/qn2.png","path":"images/qn2.png","modified":0,"renderable":0},{"_id":"source/images/qn3.png","path":"images/qn3.png","modified":0,"renderable":0},{"_id":"source/images/qn4.png","path":"images/qn4.png","modified":0,"renderable":0},{"_id":"source/images/qn_icon.png","path":"images/qn_icon.png","modified":0,"renderable":0},{"_id":"source/images/qn_feature.png","path":"images/qn_feature.png","modified":0,"renderable":0},{"_id":"source/images/raid01.png","path":"images/raid01.png","modified":0,"renderable":0},{"_id":"source/images/raid10.png","path":"images/raid10.png","modified":0,"renderable":0},{"_id":"source/images/statista.png","path":"images/statista.png","modified":0,"renderable":0},{"_id":"source/images/svhn_cropping.png","path":"images/svhn_cropping.png","modified":0,"renderable":0},{"_id":"source/images/svhn_steps.png","path":"images/svhn_steps.png","modified":0,"renderable":0},{"_id":"source/images/svhn_labels.png","path":"images/svhn_labels.png","modified":0,"renderable":0},{"_id":"source/images/trivia.jpg","path":"images/trivia.jpg","modified":0,"renderable":0},{"_id":"source/images/unhosted.jpg","path":"images/unhosted.jpg","modified":0,"renderable":0},{"_id":"source/images/webdev_techstack.png","path":"images/webdev_techstack.png","modified":0,"renderable":0},{"_id":"source/images/webdevlist.jpg","path":"images/webdevlist.jpg","modified":0,"renderable":0},{"_id":"source/images/webserver_performance.png","path":"images/webserver_performance.png","modified":0,"renderable":0},{"_id":"source/images/dns1.png","path":"images/dns1.png","modified":0,"renderable":0},{"_id":"source/images/do.png","path":"images/do.png","modified":0,"renderable":0},{"_id":"source/images/gh_eer.png","path":"images/gh_eer.png","modified":0,"renderable":0},{"_id":"source/images/graphql_github.png","path":"images/graphql_github.png","modified":0,"renderable":0},{"_id":"source/images/tello1.jpg","path":"images/tello1.jpg","modified":0,"renderable":0},{"_id":"source/images/thesis_mockup.png","path":"images/thesis_mockup.png","modified":0,"renderable":0},{"_id":"source/images/thesis_stack.png","path":"images/thesis_stack.png","modified":0,"renderable":0},{"_id":"source/images/webdev_techstack_large.png","path":"images/webdev_techstack_large.png","modified":0,"renderable":0},{"_id":"source/images/webservers.png","path":"images/webservers.png","modified":0,"renderable":0},{"_id":"source/images/academic_faces2.png","path":"images/academic_faces2.png","modified":0,"renderable":0},{"_id":"source/images/scorecard.jpg","path":"images/scorecard.jpg","modified":0,"renderable":0},{"_id":"source/images/svhn_cropped_images.png","path":"images/svhn_cropped_images.png","modified":0,"renderable":0},{"_id":"source/images/tello2.png","path":"images/tello2.png","modified":0,"renderable":0},{"_id":"themes/cactus-dark/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/images/favicon-192x192.png","path":"images/favicon-192x192.png","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/images/favicon.ico","path":"images/favicon.ico","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/justified-gallery/jquery.justifiedGallery.min.js","path":"lib/justified-gallery/jquery.justifiedGallery.min.js","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/justified-gallery/justifiedGallery.min.css","path":"lib/justified-gallery/justifiedGallery.min.css","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/styles.css","path":"lib/meslo-LG/styles.css","modified":0,"renderable":1},{"_id":"source/images/dns2.png","path":"images/dns2.png","modified":0,"renderable":0},{"_id":"themes/cactus-dark/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/jquery/jquery.min.js","path":"lib/jquery/jquery.min.js","modified":0,"renderable":1},{"_id":"source/images/academic_faces1.png","path":"images/academic_faces1.png","modified":0,"renderable":0},{"_id":"themes/cactus-dark/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"source/images/halite_game.png","path":"images/halite_game.png","modified":0,"renderable":0},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-Italic.ttf","path":"lib/meslo-LG/fonts/MesloLGM-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-Italic.ttf","path":"lib/meslo-LG/fonts/MesloLGS-Italic.ttf","modified":0,"renderable":1},{"_id":"source/images/svhn_labelimg.png","path":"images/svhn_labelimg.png","modified":0,"renderable":0},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-Italic.ttf","path":"lib/meslo-LG/fonts/MesloLGL-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-BoldItalic.ttf","path":"lib/meslo-LG/fonts/MesloLGM-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-Bold.ttf","path":"lib/meslo-LG/fonts/MesloLGM-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-Regular.ttf","path":"lib/meslo-LG/fonts/MesloLGM-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-Bold.ttf","path":"lib/meslo-LG/fonts/MesloLGS-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-BoldItalic.ttf","path":"lib/meslo-LG/fonts/MesloLGS-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-Regular.ttf","path":"lib/meslo-LG/fonts/MesloLGS-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-BoldItalic.ttf","path":"lib/meslo-LG/fonts/MesloLGL-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-Regular.ttf","path":"lib/meslo-LG/fonts/MesloLGL-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-Bold.ttf","path":"lib/meslo-LG/fonts/MesloLGL-Bold.ttf","modified":0,"renderable":1},{"_id":"source/images/graphql_cover.png","path":"images/graphql_cover.png","modified":1,"renderable":0},{"_id":"source/images/graphql_screenshots1.png","path":"images/graphql_screenshots1.png","modified":1,"renderable":0},{"_id":"source/images/graphql_screencast1.gif","path":"images/graphql_screencast1.gif","modified":1,"renderable":0},{"_id":"source/images/graphql_screencast2.gif","path":"images/graphql_screencast2.gif","modified":1,"renderable":0}],"Cache":[{"_id":"themes/cactus-dark/LICENSE","hash":"4d5f5f360a18c68f0fd1897bdb1eb1210c2893e3","modified":1591456473488},{"_id":"themes/cactus-dark/README.md","hash":"f38b2f4771eeccc0ae0959ac3e3c485a9d159d4a","modified":1591456473488},{"_id":"themes/cactus-dark/_config.yml","hash":"d2d20c8a2c5100b0b4b55fac6887b5acce95d0ff","modified":1591456473488},{"_id":"source/favicon.ico","hash":"20ed69c9944365966aa23c9c8c7f9c4a4211dfde","modified":1591456473461},{"_id":"source/_data/badges.json","hash":"846311b9d702e919a0ac958670fe9a975bfc964d","modified":1591456473456},{"_id":"source/_data/projects.json","hash":"a36e2bdfdfefa66614cfb7f18bbf74ba458c4298","modified":1591456473457},{"_id":"source/about/index.md","hash":"ffd396904773cdbfd7ae2acb36b43eaedc24b89e","modified":1591456473460},{"_id":"source/_posts/anchr-io-image-uploads-bookmarks-and-shortlink-service.md","hash":"789d49d89b31f482f80e792ffdc627d7c245fe3c","modified":1591456473457},{"_id":"source/_posts/basic-benchmarks-of-5-different-mqtt-brokers.md","hash":"f828a3c60d65df5403768ce4055776c07937333a","modified":1591456473457},{"_id":"source/_posts/building-a-cloud-native-web-scraper-using-8-different-aws-services.md","hash":"fb3f0589820ca5c053d45a99c43d1dfad024cdd0","modified":1591456473457},{"_id":"source/_posts/caddy-a-modern-web-server-vs-nginx.md","hash":"7bb4735076f9613bb629eb98e81fd984c8e3cac7","modified":1591456473457},{"_id":"source/_posts/cartpole-with-a-deep-q-network.md","hash":"56ac89d42c5fa70fef93d97aa70a36c5a73f2de1","modified":1591456473457},{"_id":"source/_posts/cartpole-with-qlearning-first-experiences-with-openai-gym.md","hash":"a5fe2bcea7c7ede171406f2aabd158c5e4d6c67e","modified":1591456473457},{"_id":"source/_posts/design-of-a-linked-dataenabled-microservice-platform-for-the-industrial-internet-of-things.md","hash":"c1c3aab0d95a2987e58fe9189598f9c36a18ccc9","modified":1591456473457},{"_id":"source/_posts/detecting-academics-major-from-facial-images.md","hash":"fed4f8961158a9bd5ab799e61bec245e049ba74b","modified":1591456473457},{"_id":"source/_posts/digitalocean-my-preferred-cloud-hosting-provider.md","hash":"7d6b7e6bed189f180fb58f52f0ea971dae96ab45","modified":1591456473458},{"_id":"source/_posts/exploratory-analysis-on-github-data.md","hash":"b54714eb9cf86a05c4527546aab412935100f6ca","modified":1591456473458},{"_id":"source/_posts/flying-a-dji-tello-drone-with-go.md","hash":"14b334c7bd5d61ad5d6dbd2f0b29318e58fc9513","modified":1591456473458},{"_id":"source/_posts/halite-a-rule-based-ai-bot.md","hash":"3de3fbf224dea1cfdeba2abb6be99d982ae108cf","modified":1591456473458},{"_id":"source/_posts/how-to-enable-dns-over-tls-on-ubuntu-using-coredns.md","hash":"fc2e1d4da688dfb5627fce9794ca83702ef91fbc","modified":1591456473458},{"_id":"source/_posts/how-to-load-svg-into-imageview-by-url-in-android.md","hash":"d1fef41e625b21066a61704358b14055c7dff96b","modified":1591456473458},{"_id":"source/_posts/how-to-load-yago-into-apache-jena-fuseki.md","hash":"47a6356bc8cfce54d352a58373a76ac38f3009ea","modified":1591456473458},{"_id":"source/_posts/how-to-make-telegram-bots.md","hash":"f8301172c8718c9f1dc2536f7f32779483b6dfeb","modified":1591456473458},{"_id":"source/_posts/how-to-receive-sharing-intents-in-flutter.md","hash":"f9726102fe6df4f3cd68c3056197b329ab492a0c","modified":1591456473458},{"_id":"source/_posts/http-performance-java-jersey-vs-go-vs-nodejs.md","hash":"1275fd0526b96f4945ba0b321e12c0bc7bd14183","modified":1591456473458},{"_id":"source/_posts/http20-server-push-proxy.md","hash":"e5a83031a445f7686f8c2ec319d516bbeaf0b4e7","modified":1591456473458},{"_id":"source/_posts/innovation-in-germany-not.md","hash":"31d27dc67d2c57fac4a28ac15ca1ebdbaca57fef","modified":1591456473458},{"_id":"source/_posts/instant-messenger-security-encryption-overview.md","hash":"7b203b55c65ac10680bf38a0ba68f5d7d7642988","modified":1591456473458},{"_id":"source/_posts/learning-angular2-what-is-new.md","hash":"942d033405f0192315d26efedbe8b8bbce20631a","modified":1591456473459},{"_id":"source/_posts/linkeddata-trivia-game.md","hash":"31af0a6f06a35ece0bae4cdfb846a95f99bec744","modified":1591456473459},{"_id":"source/_posts/linux-cache-information-bash-script.md","hash":"9b5526414d08dc4102119d7c31996c9961288fbb","modified":1591456473459},{"_id":"source/_posts/migrate-maildir-to-new-server-using-imapsync.md","hash":"15ae57c2734c5e3514532806234212184f340166","modified":1591456473459},{"_id":"source/_posts/ml-telegram-chat-message-classification.md","hash":"4a5b3f5230185168fd13a2520a95956a0a716157","modified":1591456473459},{"_id":"source/_posts/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.md","hash":"077068e1f85b7c2c2236168ec1fbfab3f3c447fa","modified":1591603319253},{"_id":"source/_posts/my-teck-stack-if-i-had-to-build-an-app-today.md","hash":"173cd62d9eccceda4387127935c58d132aff6774","modified":1591456473459},{"_id":"source/_posts/overgrive-not-starting-on-ubuntu-18-04.md","hash":"d93db4eaa9f0d98fa99438cb21e919be3b3fbb8b","modified":1591456473459},{"_id":"source/_posts/quiznerd-my-experiences-with-the-android-developer-nanodegree.md","hash":"9d3959a7d576c8fde1461c3263ba9d04dfb82065","modified":1591456473459},{"_id":"source/_posts/telegram-bot-example-code-in-nodejs.md","hash":"239d417e5bb97d5dc2997ef95b103cd9949472b4","modified":1591456473459},{"_id":"source/_posts/telegram-expensebot-doodlerbot.md","hash":"30f5598d3774028f543c17dc63a4e32ae9d3cbdc","modified":1591456473459},{"_id":"source/_posts/telegram-middleman-bot-push-notifications-as-easy-as-post.md","hash":"306b6f5022ae9e5466951c67863bdc9e9c76de58","modified":1591456473459},{"_id":"source/_posts/transfer-learning-for-multi-digit-recognition-using-tensorflow-object-detection-and-svhn-classifier.md","hash":"927d820a00813bb4252929de11247c80b255f732","modified":1591456473459},{"_id":"source/_posts/unhostedorg-applications-with-remotestorageio-and-webfingernet.md","hash":"213eca3c8df289c14d237fbcec4038bfe6994c04","modified":1591456473460},{"_id":"source/_posts/web-development-technology-stack.md","hash":"3196d6ac39cf01f233d0919f4d4405e768dfa565","modified":1591456473460},{"_id":"source/_posts/webdevlistnet-the-developers-resource-collection.md","hash":"25371b06f834fc1274ade224d3e8dc072b9b4e79","modified":1591456473460},{"_id":"source/_posts/what-i-like-about-developing-apps-with-flutter.md","hash":"e966434a8a7e403a7cddd2931e4a60a8a8be3cbb","modified":1591456473460},{"_id":"source/_posts/why-raid-10-is-better-than-raid-01.md","hash":"a6eafba6baa3e10da17b3d93281bd3860cd0893d","modified":1591456473460},{"_id":"source/articles/index.md","hash":"2265dc41dd7b9083d5ba7a2a4a78ae6109ef27b3","modified":1591456473460},{"_id":"source/images/Webserver_memory_graph.jpg","hash":"aeaf52174560a2d3a7ff32fda21a26b96c907cc9","modified":1591456473461},{"_id":"source/images/academic_faces3.png","hash":"34b476eb62b5b5e7f2a14e13dbd91df0336ed53c","modified":1591456473463},{"_id":"source/images/anchr_1.jpg","hash":"0d50b24329bdcddea234f0c3ade3f2846daae148","modified":1591456473464},{"_id":"source/images/anchr_2.jpg","hash":"d07dfedb2fc549983f63bf7251f3d011b96188aa","modified":1591456473464},{"_id":"source/images/angular2_logo.png","hash":"189713e0c0de88477c6726fc59b4cd1cfb16b05e","modified":1591456473464},{"_id":"source/images/apple-touch-icon.png","hash":"07059d8fe92f685176d96045edb70c01deb8c4d0","modified":1591456473464},{"_id":"source/images/benchmarks.svg","hash":"e9fbfdf69d7ac1c9890d541fb07d69f844fbb99a","modified":1591456473464},{"_id":"source/images/benchmarks2.svg","hash":"c4bd56f99f3a8810fd50a3caf7a99213b407a0d5","modified":1591456473464},{"_id":"source/images/cartpole1.jpg","hash":"3e237ad7b3bb78ada0d507c8f644a791797b194e","modified":1591456473464},{"_id":"source/images/cartpole2.png","hash":"bb292c589bd25b8b59c75989377163232d29b1c8","modified":1591456473464},{"_id":"source/images/cartpole3.png","hash":"03883366ab1f16b607653ef239b99ede1ac56efc","modified":1591456473464},{"_id":"source/images/cartpole4.png","hash":"d92a7e37c8e88986a51e9a7e6b0124093b5a7425","modified":1591456473464},{"_id":"source/images/cert.png","hash":"e4d28e0c8650b56a92b7ae739f51a92e6e3b83f6","modified":1591456473465},{"_id":"source/images/crawlbuddy2.png","hash":"1325a3db38972d5ec796edeb3658c16110ce7703","modified":1591456473465},{"_id":"source/images/doodlerbot_icon.png","hash":"7e548feeabd180f1e5af390365ce88fe70fe619e","modified":1591456473468},{"_id":"source/images/dqn1.png","hash":"500531da445ad95337e7d60b0834ce15ed235b41","modified":1591456473469},{"_id":"source/images/dqn2.png","hash":"07d2f6e25d0328c8986d173efab87bc59e63f944","modified":1591456473469},{"_id":"source/images/dqn3.png","hash":"664f10963a00e2d66fe7dde7e78848d26dbf78cd","modified":1591456473469},{"_id":"source/images/dqn4.png","hash":"e564dd5d0e8258145f93d65eb1a600a9d5489826","modified":1591456473469},{"_id":"source/images/expensebot_icon.png","hash":"479cb80aa0620db49e0a353ac7f550f7cc9f205d","modified":1591456473469},{"_id":"source/images/gh_ka_world.png","hash":"a0255eeaed9a38c0e836366cf95a3cee3f0f78ab","modified":1591456473470},{"_id":"source/images/gh_location_langs.png","hash":"4009dd4fdd7ef840a05e72015bd27e33304e9f70","modified":1591456473470},{"_id":"source/images/gh_locations.png","hash":"facdac2dd039f0a0ed316bcd565c7fa00b8671b6","modified":1591456473470},{"_id":"source/images/gh_popular_lang.png","hash":"b01bc1aeb03d98939236a1a248e0a49ecbeeeb8b","modified":1591456473471},{"_id":"source/images/halite_langs.png","hash":"68561eee10ca68ffafaadcbe07056dc039d3be23","modified":1591456473475},{"_id":"source/images/middleman.png","hash":"f09ddfd0c8c0973d16567a1122981e6b14db615b","modified":1591456473475},{"_id":"source/images/middleman2.png","hash":"456e72dd3824a95589cc264627b81596eb589f96","modified":1591456473475},{"_id":"source/images/mqtt_bench_2.png","hash":"6c06e0cf8b8fa8f1c8f515109f1f2421125797fc","modified":1591456473475},{"_id":"source/images/push_screenshot1.png","hash":"91d5fa3e4c5a3a3d0f369a8d90055d029e8ef28e","modified":1591456473475},{"_id":"source/images/push_screenshot2.png","hash":"144ca3caed3e5b9285f7f8edeba41917266ed529","modified":1591456473475},{"_id":"source/images/qn1.png","hash":"17a5644a157e5ea4bc14e59803e864e2fc16cc14","modified":1591456473476},{"_id":"source/images/qn2.png","hash":"54be91ee889ba922dfc8c45c2a74c443dd78d71f","modified":1591456473476},{"_id":"source/images/qn3.png","hash":"a739db3163e768538b63016ccef5aa2b350e7379","modified":1591456473476},{"_id":"source/images/qn4.png","hash":"78c55641b41fc2adb3741d296bc3dad6ecea52b3","modified":1591456473476},{"_id":"source/images/qn_icon.png","hash":"ed89d828e187737ab741237cb46fa15420601199","modified":1591456473477},{"_id":"source/images/qn_feature.png","hash":"cfb596148c2709567f2ab2677a8a057f27a34be8","modified":1591456473477},{"_id":"source/images/raid01.png","hash":"15da9f10e5a4d3f452287daf4420da8582d3ad31","modified":1591456473477},{"_id":"source/images/raid10.png","hash":"ae8cb17a2ae2eba7d238a8d56136f60823172242","modified":1591456473477},{"_id":"source/images/statista.png","hash":"4385242cac93067b8ca8dd02c01a1778bf1c6102","modified":1591456473478},{"_id":"source/images/svhn_cropping.png","hash":"018a3065fce6d4e2ff3ff5af3e057d5026e14aee","modified":1591456473480},{"_id":"source/images/svhn_steps.png","hash":"5c98dfb047be7e5ee72f5eca4163709af2c5491e","modified":1591456473483},{"_id":"source/images/svhn_labels.png","hash":"0607b91a6fe26c164c2b179c99111b7a6bf7c57c","modified":1591456473483},{"_id":"source/images/trivia.jpg","hash":"8e7715fa940d79f43fd1e2a9ff8c2149cf0b11a8","modified":1591456473485},{"_id":"source/images/unhosted.jpg","hash":"6e0bde1ad7bfb3ba8d9b634f6518299e2d9d1f1b","modified":1591456473485},{"_id":"source/images/webdev_techstack.png","hash":"e3f4b8cdac4233ebcb5d7e20ee42703c9ffad6ba","modified":1591456473486},{"_id":"source/images/webdevlist.jpg","hash":"cfa9e038e301cdcd577c7833fc8b220f6c0d6a98","modified":1591456473487},{"_id":"source/images/webserver_performance.png","hash":"737f74276f3832a342464110566fdab49593de94","modified":1591456473487},{"_id":"source/imprint/index.md","hash":"2c2cd06c84a2548e6d1883d62dbbe16e4ff61e8f","modified":1591456473488},{"_id":"themes/cactus-dark/layout/archive.ejs","hash":"ab9798bf534485a4fed4d3089011421858afdd26","modified":1591456473489},{"_id":"themes/cactus-dark/layout/index.ejs","hash":"2954d576adf4bfd9b08163cdc30f37bde9fee2a3","modified":1591456473489},{"_id":"themes/cactus-dark/layout/layout.ejs","hash":"8484532ad7c4da22f46fc1394bb2fd9ded34be1f","modified":1591456473489},{"_id":"themes/cactus-dark/layout/page.ejs","hash":"b6b7b1e6dc856a0e62f35da0151f67ba41143e04","modified":1591456473489},{"_id":"themes/cactus-dark/layout/post.ejs","hash":"d314910bfc152ed403026254fd9d1094e313ae2e","modified":1591456473489},{"_id":"themes/cactus-dark/scripts/meta.js","hash":"fa6055a39851c9953d033e70c1614547b94dce60","modified":1591456473489},{"_id":"themes/cactus-dark/scripts/thumbnail.js","hash":"df8829fd8c3119650037eba5ec11bdce06acff9d","modified":1591456473489},{"_id":"source/images/dns1.png","hash":"433d04ba49041ffb476a9078ee718b406c9da509","modified":1591456473466},{"_id":"source/images/do.png","hash":"dd43ce0ffde6885ad251e6e4edff64b36ddecd02","modified":1591456473468},{"_id":"source/images/gh_eer.png","hash":"b665077e6cc582c5f80dc60050de443c5162fef9","modified":1591456473470},{"_id":"source/images/graphql_github.png","hash":"09adc322fa961c5987b96f9490b0f828e9fe8499","modified":1591456473471},{"_id":"source/images/tello1.jpg","hash":"de1755a8066e6d7708664d42c4d0cc272dca7b7a","modified":1591456473483},{"_id":"source/images/thesis_mockup.png","hash":"a5ec4a8949edd6a168c866c675ab6c4e2bc00d33","modified":1591456473484},{"_id":"source/images/thesis_stack.png","hash":"5a410fcb932c552dbc0096f59fda594be334e9cd","modified":1591456473485},{"_id":"source/images/webdev_techstack_large.png","hash":"76169ecb9523c245c6588156fcff24c851b5c43b","modified":1591456473486},{"_id":"source/images/webservers.png","hash":"c1550f9bcb7350c9ebef5090484960877d8dda55","modified":1591456473487},{"_id":"source/images/academic_faces2.png","hash":"2b720cc693e62ea98de454ce2ed44e63cece1848","modified":1591456473463},{"_id":"source/images/scorecard.jpg","hash":"e4b56c4f825d42890e6c735c17d6a29a80a7f075","modified":1591456473478},{"_id":"source/images/svhn_cropped_images.png","hash":"c952784933038f0569e0b3dba353433481afc00d","modified":1591456473479},{"_id":"source/images/tello2.png","hash":"74c9b351d3cb971d31c5a91a9f6633b3c2a0312a","modified":1591456473484},{"_id":"themes/cactus-dark/layout/_partial/comments.ejs","hash":"853a4500da515ef3facc51a055886eaf8efd080d","modified":1591456473488},{"_id":"themes/cactus-dark/layout/_partial/footer.ejs","hash":"7f6b3f126a58e6734b658ab57bc6b41822bc9342","modified":1591456473488},{"_id":"themes/cactus-dark/layout/_partial/head.ejs","hash":"7f4c7a4efe22f0735d7bfdd2045283bde447b6b7","modified":1591456473488},{"_id":"themes/cactus-dark/layout/_partial/header.ejs","hash":"889fe54bbfd1fb3357e8c0614d57a437a72f782a","modified":1591456473488},{"_id":"themes/cactus-dark/layout/_partial/pagination.ejs","hash":"ca660c59aec56daa4a7b41715b97434d4a24c37e","modified":1591456473488},{"_id":"themes/cactus-dark/layout/_partial/scripts.ejs","hash":"ffdf85e347233b6dc3b12296cd3d25cd1d0bd8e6","modified":1591456473488},{"_id":"themes/cactus-dark/layout/_partial/styles.ejs","hash":"e62b799d8ac369d1f1b36bd2649ecc34aec3384c","modified":1591456473488},{"_id":"themes/cactus-dark/source/css/_extend.styl","hash":"faca25132d55e8989d1c1d638e55d1e97de3c561","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_mixins.styl","hash":"c921ceb620deedddd38c9cec28190995e8764bab","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_util.styl","hash":"219187b864e44380c0fd91870ae7fa9d3e3321cd","modified":1591456473491},{"_id":"themes/cactus-dark/source/css/_variables.styl","hash":"80345f77f0e601669047cbb3c44491720c3b5c13","modified":1591456473491},{"_id":"themes/cactus-dark/source/css/style.styl","hash":"9a989e414ab2fa12f39791f2ea07c22aec00c670","modified":1591456473491},{"_id":"themes/cactus-dark/source/images/favicon-192x192.png","hash":"07059d8fe92f685176d96045edb70c01deb8c4d0","modified":1591456473491},{"_id":"themes/cactus-dark/source/images/favicon.ico","hash":"164bc240105d72d826efc048442d85dcf90d2cce","modified":1591456473491},{"_id":"themes/cactus-dark/source/js/main.js","hash":"2703a7cb4fc7056d13215b9fde675da426b9cdc4","modified":1591456473491},{"_id":"themes/cactus-dark/layout/_partial/post/actions_desktop.ejs","hash":"2319dea76f205c27dd59c994921f66350df8027a","modified":1591456473488},{"_id":"themes/cactus-dark/layout/_partial/post/actions_mobile.ejs","hash":"e7638a83e5aaa4bf5b24440ca76fec8eb563bed7","modified":1591456473488},{"_id":"themes/cactus-dark/layout/_partial/post/date.ejs","hash":"12a4a7ba6334e3e5c03d9a9601d7779a27c2e082","modified":1591456473488},{"_id":"themes/cactus-dark/layout/_partial/post/gallery.ejs","hash":"9aecd8908e8a684f33dc20c02497c0f1774137c7","modified":1591456473488},{"_id":"themes/cactus-dark/layout/_partial/post/share.ejs","hash":"25a3406f97e976ec13239f0d3f32f9e512511f50","modified":1591456473488},{"_id":"themes/cactus-dark/layout/_partial/post/tag.ejs","hash":"bfab03ef986d35ccad583f2d2b575db4a8d2789e","modified":1591456473488},{"_id":"themes/cactus-dark/layout/_partial/post/title.ejs","hash":"a060f1c6e3718494a6b1d0e1981ea0bf4e549828","modified":1591456473488},{"_id":"themes/cactus-dark/source/css/_highlight/agate.styl","hash":"601eb70448a16b918df132f6fc41e891ae053653","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/androidstudio.styl","hash":"65d09f1b0e81c6a182f549fd3de51e59823c97ae","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/arta.styl","hash":"1a5accc115f41d1b669ed708ac6a29abac876599","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-cave-dark.styl","hash":"bc647b2c1d971d7cc947aa1ed66e9fd115261921","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-dune-dark.styl","hash":"df50a85a4b14c7ca6e825d665594b91229d0e460","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-estuary-dark.styl","hash":"d84382bc8298f96730757391d3e761b7e640f406","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-forest-dark.styl","hash":"57c154c6045a038dc7df0a25927853e10bf48c4a","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-heath-dark.styl","hash":"b0cf13b2233e7bc38342032d2d7296591a4c2bcf","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-lakeside-dark.styl","hash":"bb0a8c4ad0dd8e3e7de7122ddf268fc42aa94acb","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-plateau-dark.styl","hash":"09c64f1a7052aec9070c36c0431df25216afaea1","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-savanna-dark.styl","hash":"a16c919a1ccf2f845488078fb341381bec46b1f3","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-seaside-dark.styl","hash":"ce233a101daea7124cbfcd34add43ccfe2e1e1c7","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-sulphurpool-dark.styl","hash":"414b0cfc142f70afe359c16450b651e28bf7325a","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/codepen-embed.styl","hash":"f4dcc84d8e39f9831a5efe80e51923fc3054feb0","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/dark.styl","hash":"71ce56d311cc2f3a605f6e2c495ccd7236878404","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/darkula.styl","hash":"ad0d5728d21645039c9f199e7a56814170ed3bab","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/far.styl","hash":"d9928010ffe71e80b97a5afcba1a4975efdd7372","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/hopscotch.styl","hash":"b374c6550b89b4751aedc8fbc3cf98d95bd70ead","modified":1591456473489},{"_id":"themes/cactus-dark/source/css/_highlight/hybrid.styl","hash":"ea8d7ddc258b073308746385f5cb85aabb8bfb83","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/ir-black.styl","hash":"693078bbd72a2091ed30f506cc55949600b717af","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/kimbie.styl","hash":"45dbb168f22d739d0109745d2decd66b5f94e786","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/monokai-sublime.styl","hash":"25aa2fc1dbe38593e7c7ebe525438a39574d9935","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/monokai.styl","hash":"5a4fe9f957fd7a368c21b62a818403db4270452f","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/obsidian.styl","hash":"55572bbcfee1de6c31ac54681bb00336f5ae826d","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/paraiso.styl","hash":"f1537bd868579fa018ecdbfd2eb922dcf3ba2cac","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/pojoaque.styl","hash":"77dae9dc41945359d17fe84dbd317f1b40b2ee33","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/railscasts.styl","hash":"acd620f8bb7ff0e3fe5f9a22b4433ceef93a05e6","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/rainbow.styl","hash":"ce73b858fc0aba0e57ef9fb136c083082746bc1d","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/solarized-dark.styl","hash":"702b9299a48c90124e3ac1d45f1591042f2beccc","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/sunburst.styl","hash":"a0b5b5129547a23865d400cfa562ea0ac1ee3958","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/tomorrow-night-blue.styl","hash":"8b3087d4422be6eb800935a22eb11e035341c4ba","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/tomorrow-night-bright.styl","hash":"0ac6af6ecb446b5b60d6226748e4a6532db34f57","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/tomorrow-night-eighties.styl","hash":"fa57b3bb7857a160fc856dbe319b31e30cc5d771","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/tomorrow-night.styl","hash":"19b3080d4b066b40d50d7e7f297472482b5801fd","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_highlight/zenburn.styl","hash":"fc5ec840435dad80964d04519d3f882ddc03746a","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_partial/archive.styl","hash":"18fa7f84a9783c5fb56c9f450ea93bd88408e682","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_partial/article.styl","hash":"3dbf627d9f27ebf0b10cdc4d28341e35786b3cf5","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_partial/comments.styl","hash":"11fb41241a13971d23fc3f7e6d60315c7f248396","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_partial/footer.styl","hash":"344f6877733a488f7f07f87fbaa518295948766f","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_partial/header.styl","hash":"86676f767cfacd9203477de5ed1545bd51b0169c","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_partial/index.styl","hash":"4734fb46f11a0b5425f17c9db4441bb9711d8b9e","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_partial/pagination.styl","hash":"03a1b81d60dae3dd55963b7e74a6fee83470e6bb","modified":1591456473490},{"_id":"themes/cactus-dark/source/lib/justified-gallery/jquery.justifiedGallery.min.js","hash":"b2683e7a872bc109b1756a65188a37cef7d0bd5c","modified":1591456473499},{"_id":"themes/cactus-dark/source/lib/justified-gallery/justifiedGallery.min.css","hash":"13fbcba5e97aa88b748d94d3efc4718475279907","modified":1591456473499},{"_id":"themes/cactus-dark/source/lib/meslo-LG/styles.css","hash":"eb88d0b9f1bbef99070e9627e2c96d892036bf7e","modified":1591456473526},{"_id":"source/images/dns2.png","hash":"f6d051761163a98aaf7be38b302eff9a1ab84493","modified":1591456473468},{"_id":"themes/cactus-dark/source/css/_partial/post/actions_desktop.styl","hash":"a9f9b6382d313f9ef9ff9f53bd0db11e5b36edf4","modified":1591456473490},{"_id":"themes/cactus-dark/source/css/_partial/post/actions_mobile.styl","hash":"e6a802d7ee1023c5fc5fac18bb0ba3dc03ef2ac8","modified":1591456473490},{"_id":"themes/cactus-dark/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1591456473491},{"_id":"themes/cactus-dark/source/lib/jquery/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1591456473499},{"_id":"source/images/academic_faces1.png","hash":"3f28ba07d6a0ed06102da625d36d3c7adf8968ea","modified":1591456473462},{"_id":"themes/cactus-dark/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1591456473491},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1591456473498},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1591456473499},{"_id":"source/images/halite_game.png","hash":"d67ea5a35af5f4cf91808c78eb156e259fbf50bf","modified":1591456473474},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1591456473494},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1591456473498},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1591456473493},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1591456473497},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-Italic.ttf","hash":"68700db02debd4b922304134da83b829cbfddfc9","modified":1591456473522},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-Italic.ttf","hash":"7f7cdbdcc26279c04046632e22d872f111bc9399","modified":1591456473525},{"_id":"source/images/svhn_labelimg.png","hash":"ac92c45effcfc6fb5b8671165b6365ae4ce3126e","modified":1591456473482},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-Italic.ttf","hash":"96c97a0a098ca40802f948ae56fa37aa6683d034","modified":1591456473512},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-BoldItalic.ttf","hash":"65ddb11e75ee93909e845ab912a36717c48f1c94","modified":1591456473521},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-Bold.ttf","hash":"a8a8df3393bccc365335fc5eb0a62a6b7ccd32b9","modified":1591456473517},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-Regular.ttf","hash":"5e220152adefe905b2197f873d7cee99eca50e91","modified":1591456473523},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-Bold.ttf","hash":"df202ce09cbdc70bc16b81983a13ef0f94e46f10","modified":1591456473524},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-BoldItalic.ttf","hash":"d895a1bd25e36c58b7f463ebe14de09f186d5ab4","modified":1591456473525},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-Regular.ttf","hash":"56fa0e33a390b704afc56af93a31576ccdbbdd9e","modified":1591456473526},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-BoldItalic.ttf","hash":"a9a431fc7a6c3a67c98021d4035c12a07a4f1070","modified":1591456473508},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-Regular.ttf","hash":"2b912dd13f052f645ee19951604610bb350d50af","modified":1591456473516},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-Bold.ttf","hash":"bfa1ed9a263ed78462f06d322de13bd5bd0906b2","modified":1591456473504},{"_id":"source/sitemap.tpl.xml","hash":"e984757a75a235ba47aef199f34e6ded92452685","modified":1591457473130},{"_id":"themes/cactus-dark/sitemap.tpl.xml","hash":"e984757a75a235ba47aef199f34e6ded92452685","modified":1591457485045},{"_id":"source/_posts/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.md","hash":"4d4989e2e3b2c55312509024cb9006067a776229","modified":1591598435998},{"_id":"source/images/graphql_cover.png","hash":"c8fe5419bd25058d2895012590a3acd8f776cfd6","modified":1591603273822},{"_id":"source/images/graphql_screenshots1.png","hash":"5cd775bd60413f33042928effb6a8d456a513be0","modified":1591598436001},{"_id":"source/images/graphql_screencast1.gif","hash":"63805d525e34d18f85a528eaa44374a0d529b583","modified":1591598436002},{"_id":"source/images/graphql_screencast2.gif","hash":"a840bbef10d6871ad47b113d49550c3308aa5f4e","modified":1591598436005}],"Category":[],"Data":[{"_id":"badges","data":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"]},{"_id":"projects","data":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}],"Page":[{"title":"about","date":"2017-05-03T20:21:16.000Z","_content":"\n# About\n\n## Me\n\nHey, welcome and thank you for visiting my webpage.\n\nMy name is Ferdinand M√ºtsch, I am 26 years old, living in Karlsruhe, Germany and close to finishing my studies on [information engineering and management](https://informationswirtschaft.org) at the [Karlsruhe Institute of Technology](https://kit.edu).\n\nCurrently, I am working in my Master's thesis at [ITIV](https://itiv.kit.edu) on the subjects of _Cooperative Perception and Cellular Vehicle-2-X Communication for Autonomous Driving_. Prior to this, I got my Bachelor's degree in 2016 with a final thesis about _Microservices and Semantic Web_ at [TECO](https://teco.edu).\n\nMy interests are ‚Äì among others ‚Äì software development, especially in a web- and mobile context, as well as data science and machine leatning. I consider myself very open-minded and progressive and I am continuously interested in new technology.\n\nBesides my studies I used to work as a student employee at [Inovex](https://inovex.de) in Karlsruhe and as a self-employed Freelancer developer. In addition, I did a rewarding internship at the [Volkswagen Electronic Research Lab (ERL)](https://vwiecc.com/) in Belmont, California as part of the software platforms team.\nI participated in realizing software projects for small and mid-sized businesses in the context of Industry 4.0, the Internet of Things, big-scale data analytics, web platforms and more. \n\n## My topics\nFull-Stack Web, Mobile, DevOps, Data Science, Machine Learning\n\n## My Languages\nJava (‚òÖ‚òÖ‚òÖ‚òÖ), JavaScript (‚òÖ‚òÖ‚òÖ), Go (‚òÖ‚òÖ‚òÖ), Python (‚òÖ‚òÖ‚òÖ), Scala (‚òÖ‚òÖ), R (‚òÖ), Dart (‚òÖ)\n\n## My Social Networks\nIf you are interested in some of my projects, take a look at the [Projects](/#projects) section or review my profile on [GitHub](https://github.com/muety). Please also have a look at my [LinkedIn](https://www.linkedin.com/in/ferdinand-m%C3%BCtsch/) profile.","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-05-03 22:21:16\n---\n\n# About\n\n## Me\n\nHey, welcome and thank you for visiting my webpage.\n\nMy name is Ferdinand M√ºtsch, I am 26 years old, living in Karlsruhe, Germany and close to finishing my studies on [information engineering and management](https://informationswirtschaft.org) at the [Karlsruhe Institute of Technology](https://kit.edu).\n\nCurrently, I am working in my Master's thesis at [ITIV](https://itiv.kit.edu) on the subjects of _Cooperative Perception and Cellular Vehicle-2-X Communication for Autonomous Driving_. Prior to this, I got my Bachelor's degree in 2016 with a final thesis about _Microservices and Semantic Web_ at [TECO](https://teco.edu).\n\nMy interests are ‚Äì among others ‚Äì software development, especially in a web- and mobile context, as well as data science and machine leatning. I consider myself very open-minded and progressive and I am continuously interested in new technology.\n\nBesides my studies I used to work as a student employee at [Inovex](https://inovex.de) in Karlsruhe and as a self-employed Freelancer developer. In addition, I did a rewarding internship at the [Volkswagen Electronic Research Lab (ERL)](https://vwiecc.com/) in Belmont, California as part of the software platforms team.\nI participated in realizing software projects for small and mid-sized businesses in the context of Industry 4.0, the Internet of Things, big-scale data analytics, web platforms and more. \n\n## My topics\nFull-Stack Web, Mobile, DevOps, Data Science, Machine Learning\n\n## My Languages\nJava (‚òÖ‚òÖ‚òÖ‚òÖ), JavaScript (‚òÖ‚òÖ‚òÖ), Go (‚òÖ‚òÖ‚òÖ), Python (‚òÖ‚òÖ‚òÖ), Scala (‚òÖ‚òÖ), R (‚òÖ), Dart (‚òÖ)\n\n## My Social Networks\nIf you are interested in some of my projects, take a look at the [Projects](/#projects) section or review my profile on [GitHub](https://github.com/muety). Please also have a look at my [LinkedIn](https://www.linkedin.com/in/ferdinand-m%C3%BCtsch/) profile.","updated":"2020-06-06T15:14:33.460Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckb3slhh1000040mqigki7o87","content":"<h1>About</h1>\n<h2 id=\"Me\">Me</h2>\n<p>Hey, welcome and thank you for visiting my webpage.</p>\n<p>My name is Ferdinand M√ºtsch, I am 26 years old, living in Karlsruhe, Germany and close to finishing my studies on <a href=\"https://informationswirtschaft.org\" target=\"_blank\" rel=\"noopener\">information engineering and management</a> at the <a href=\"https://kit.edu\" target=\"_blank\" rel=\"noopener\">Karlsruhe Institute of Technology</a>.</p>\n<p>Currently, I am working in my Master‚Äôs thesis at <a href=\"https://itiv.kit.edu\" target=\"_blank\" rel=\"noopener\">ITIV</a> on the subjects of <em>Cooperative Perception and Cellular Vehicle-2-X Communication for Autonomous Driving</em>. Prior to this, I got my Bachelor‚Äôs degree in 2016 with a final thesis about <em>Microservices and Semantic Web</em> at <a href=\"https://teco.edu\" target=\"_blank\" rel=\"noopener\">TECO</a>.</p>\n<p>My interests are ‚Äì among others ‚Äì software development, especially in a web- and mobile context, as well as data science and machine leatning. I consider myself very open-minded and progressive and I am continuously interested in new technology.</p>\n<p>Besides my studies I used to work as a student employee at <a href=\"https://inovex.de\" target=\"_blank\" rel=\"noopener\">Inovex</a> in Karlsruhe and as a self-employed Freelancer developer. In addition, I did a rewarding internship at the <a href=\"https://vwiecc.com/\" target=\"_blank\" rel=\"noopener\">Volkswagen Electronic Research Lab (ERL)</a> in Belmont, California as part of the software platforms team.<br>\nI participated in realizing software projects for small and mid-sized businesses in the context of Industry 4.0, the Internet of Things, big-scale data analytics, web platforms and more.</p>\n<h2 id=\"My-topics\">My topics</h2>\n<p>Full-Stack Web, Mobile, DevOps, Data Science, Machine Learning</p>\n<h2 id=\"My-Languages\">My Languages</h2>\n<p>Java (‚òÖ‚òÖ‚òÖ‚òÖ), JavaScript (‚òÖ‚òÖ‚òÖ), Go (‚òÖ‚òÖ‚òÖ), Python (‚òÖ‚òÖ‚òÖ), Scala (‚òÖ‚òÖ), R (‚òÖ), Dart (‚òÖ)</p>\n<h2 id=\"My-Social-Networks\">My Social Networks</h2>\n<p>If you are interested in some of my projects, take a look at the <a href=\"/#projects\">Projects</a> section or review my profile on <a href=\"https://github.com/muety\" target=\"_blank\" rel=\"noopener\">GitHub</a>. Please also have a look at my <a href=\"https://www.linkedin.com/in/ferdinand-m%C3%BCtsch/\" target=\"_blank\" rel=\"noopener\">LinkedIn</a> profile.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1>About</h1>\n<h2 id=\"Me\">Me</h2>\n<p>Hey, welcome and thank you for visiting my webpage.</p>\n<p>My name is Ferdinand M√ºtsch, I am 26 years old, living in Karlsruhe, Germany and close to finishing my studies on <a href=\"https://informationswirtschaft.org\" target=\"_blank\" rel=\"noopener\">information engineering and management</a> at the <a href=\"https://kit.edu\" target=\"_blank\" rel=\"noopener\">Karlsruhe Institute of Technology</a>.</p>\n<p>Currently, I am working in my Master‚Äôs thesis at <a href=\"https://itiv.kit.edu\" target=\"_blank\" rel=\"noopener\">ITIV</a> on the subjects of <em>Cooperative Perception and Cellular Vehicle-2-X Communication for Autonomous Driving</em>. Prior to this, I got my Bachelor‚Äôs degree in 2016 with a final thesis about <em>Microservices and Semantic Web</em> at <a href=\"https://teco.edu\" target=\"_blank\" rel=\"noopener\">TECO</a>.</p>\n<p>My interests are ‚Äì among others ‚Äì software development, especially in a web- and mobile context, as well as data science and machine leatning. I consider myself very open-minded and progressive and I am continuously interested in new technology.</p>\n<p>Besides my studies I used to work as a student employee at <a href=\"https://inovex.de\" target=\"_blank\" rel=\"noopener\">Inovex</a> in Karlsruhe and as a self-employed Freelancer developer. In addition, I did a rewarding internship at the <a href=\"https://vwiecc.com/\" target=\"_blank\" rel=\"noopener\">Volkswagen Electronic Research Lab (ERL)</a> in Belmont, California as part of the software platforms team.<br>\nI participated in realizing software projects for small and mid-sized businesses in the context of Industry 4.0, the Internet of Things, big-scale data analytics, web platforms and more.</p>\n<h2 id=\"My-topics\">My topics</h2>\n<p>Full-Stack Web, Mobile, DevOps, Data Science, Machine Learning</p>\n<h2 id=\"My-Languages\">My Languages</h2>\n<p>Java (‚òÖ‚òÖ‚òÖ‚òÖ), JavaScript (‚òÖ‚òÖ‚òÖ), Go (‚òÖ‚òÖ‚òÖ), Python (‚òÖ‚òÖ‚òÖ), Scala (‚òÖ‚òÖ), R (‚òÖ), Dart (‚òÖ)</p>\n<h2 id=\"My-Social-Networks\">My Social Networks</h2>\n<p>If you are interested in some of my projects, take a look at the <a href=\"/#projects\">Projects</a> section or review my profile on <a href=\"https://github.com/muety\" target=\"_blank\" rel=\"noopener\">GitHub</a>. Please also have a look at my <a href=\"https://www.linkedin.com/in/ferdinand-m%C3%BCtsch/\" target=\"_blank\" rel=\"noopener\">LinkedIn</a> profile.</p>\n"},{"title":"Articles","date":"2017-09-13T20:43:52.000Z","_content":"\n# Articles\n\nThis is a collection of articles, blog posts, scientific papers and other resources I found on the internet, which I consider interesting, helpful, well-written or something to think about. They are not in any particular order.\n\n ## General\n * [Are We Living in a Computer Simulation?](https://doi.org/10.1111/1467-9213.00309)\n * [Please do learn to code](https://medium.freecodecamp.org/please-do-learn-to-code-233597dd141c)\n * [My Google Internship](http://www.goldsborough.me/google/internship/2016/11/18/01-57-54-my_google_internship_/)\n * [What are 10 things that you should definitely do before turning 30?](https://www.quora.com/What-are-10-things-that-you-should-definitely-do-before-turning-30/answer/K-John-25?srid=uBHCu)\n * [From Coding To Management To Leadership](https://dev.to/lpasqualis/from-coding-to-management-to-leadership)\n\n## Development\n * [Scala & Design Patterns](https://www.scala-lang.org/old/sites/default/files/FrederikThesis.pdf)\n * [Pragmatic Programming Techniques](http://horicky.blogspot.com/2009/11/nosql-patterns.html)\n * [Aiohttp vs Multithreaded Flask for High I/O Applications](http://y.tsutsumi.io/aiohttp-vs-multithreaded-flask-for-high-io-applications.html)\n * [What is Java String Pool?](https://www.journaldev.com/797/what-is-java-string-pool)\n * [What Are Reactive Streams in Java?](https://dzone.com/articles/what-are-reactive-streams-in-java)\n * [The State of Developer Ecosystem in 2017](https://www.jetbrains.com/research/devecosystem-2017/)\n * [Stack Overflow Developer Survey 2017](http://stackoverflow.com/insights/survey/2017/#technology-most-loved-dreaded-and-wanted-languages)\n * [Web Developer Security Checklist](https://simplesecurity.sensedeep.com/web-developer-security-checklist-f2e4f43c9c56)\n * [Websocket Shootout: Clojure, C++, Elixir, Go, NodeJS, and Ruby](https://hashrocket.com/blog/posts/websocket-shootout)\n * [Understanding the NodeJS event loop](http://blog.mixu.net/2011/02/01/understanding-the-node-js-event-loop/)\n * [5 Reasons to Use Protocol Buffers Instead of JSON](http://blog.codeclimate.com/blog/2014/06/05/choose-protocol-buffers/)\n * [Understanding Reactive Programming and RxJS](https://auth0.com/blog/understanding-reactive-programming-and-rxjs)\n * [How We Moved Our API From Ruby to Go and Saved Our Sanity](http://blog.parse.com/learn/how-we-moved-our-api-from-ruby-to-go-and-saved-our-sanity/)\n * [10 things I learned making the fastest site in the world](https://hackernoon.com/10-things-i-learned-making-the-fastest-site-in-the-world-18a0e1cdf4a7#.syz3poebn)\n\n## DevOps & Other\n * [A Guide to the Kubernetes Networking Model](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/)\n * [Linux Applications Performance](https://unixism.net/2019/04/linux-applications-performance-introduction/)\n * [Inside the Linux boot process](https://www.ibm.com/developerworks/library/l-linuxboot/)\n * [Why aren‚Äôt we using SSH for everything?](https://medium.com/@shazow/ssh-how-does-it-even-9e43586e4ffc)\n * [what-happens-when](https://github.com/alex/what-happens-when/blob/master/README.rst#the-g-key-is-pressed)\n\n## Machine Learning & Data Science\n * [Introduction to GANs](https://www.kaggle.com/jesucristo/gan-introduction)\n * [The State of Self-Driving Cars for Everybodyüö∂üöò](https://towardsdatascience.com/the-state-of-self-driving-cars-for-everybody-29446c1c2e2c)\n * [Should you teach Python or R for data science?](https://www.dataschool.io/python-or-r-for-data-science/)\n * [How to build a simple artificial neural network with Go](https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/)\n * [Introduction to Deep Learning 2016](https://blog.algorithmia.com/introduction-to-deep-learning-2016/)\n * [Instagram photos reveal predictive markers of depression](https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-017-0110-z)\n * [Deep Q-Learning with Keras and Gym](https://keon.io/deep-q-learning/)","source":"articles/index.md","raw":"---\ntitle: Articles\ndate: 2017-09-13 22:43:52\n---\n\n# Articles\n\nThis is a collection of articles, blog posts, scientific papers and other resources I found on the internet, which I consider interesting, helpful, well-written or something to think about. They are not in any particular order.\n\n ## General\n * [Are We Living in a Computer Simulation?](https://doi.org/10.1111/1467-9213.00309)\n * [Please do learn to code](https://medium.freecodecamp.org/please-do-learn-to-code-233597dd141c)\n * [My Google Internship](http://www.goldsborough.me/google/internship/2016/11/18/01-57-54-my_google_internship_/)\n * [What are 10 things that you should definitely do before turning 30?](https://www.quora.com/What-are-10-things-that-you-should-definitely-do-before-turning-30/answer/K-John-25?srid=uBHCu)\n * [From Coding To Management To Leadership](https://dev.to/lpasqualis/from-coding-to-management-to-leadership)\n\n## Development\n * [Scala & Design Patterns](https://www.scala-lang.org/old/sites/default/files/FrederikThesis.pdf)\n * [Pragmatic Programming Techniques](http://horicky.blogspot.com/2009/11/nosql-patterns.html)\n * [Aiohttp vs Multithreaded Flask for High I/O Applications](http://y.tsutsumi.io/aiohttp-vs-multithreaded-flask-for-high-io-applications.html)\n * [What is Java String Pool?](https://www.journaldev.com/797/what-is-java-string-pool)\n * [What Are Reactive Streams in Java?](https://dzone.com/articles/what-are-reactive-streams-in-java)\n * [The State of Developer Ecosystem in 2017](https://www.jetbrains.com/research/devecosystem-2017/)\n * [Stack Overflow Developer Survey 2017](http://stackoverflow.com/insights/survey/2017/#technology-most-loved-dreaded-and-wanted-languages)\n * [Web Developer Security Checklist](https://simplesecurity.sensedeep.com/web-developer-security-checklist-f2e4f43c9c56)\n * [Websocket Shootout: Clojure, C++, Elixir, Go, NodeJS, and Ruby](https://hashrocket.com/blog/posts/websocket-shootout)\n * [Understanding the NodeJS event loop](http://blog.mixu.net/2011/02/01/understanding-the-node-js-event-loop/)\n * [5 Reasons to Use Protocol Buffers Instead of JSON](http://blog.codeclimate.com/blog/2014/06/05/choose-protocol-buffers/)\n * [Understanding Reactive Programming and RxJS](https://auth0.com/blog/understanding-reactive-programming-and-rxjs)\n * [How We Moved Our API From Ruby to Go and Saved Our Sanity](http://blog.parse.com/learn/how-we-moved-our-api-from-ruby-to-go-and-saved-our-sanity/)\n * [10 things I learned making the fastest site in the world](https://hackernoon.com/10-things-i-learned-making-the-fastest-site-in-the-world-18a0e1cdf4a7#.syz3poebn)\n\n## DevOps & Other\n * [A Guide to the Kubernetes Networking Model](https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/)\n * [Linux Applications Performance](https://unixism.net/2019/04/linux-applications-performance-introduction/)\n * [Inside the Linux boot process](https://www.ibm.com/developerworks/library/l-linuxboot/)\n * [Why aren‚Äôt we using SSH for everything?](https://medium.com/@shazow/ssh-how-does-it-even-9e43586e4ffc)\n * [what-happens-when](https://github.com/alex/what-happens-when/blob/master/README.rst#the-g-key-is-pressed)\n\n## Machine Learning & Data Science\n * [Introduction to GANs](https://www.kaggle.com/jesucristo/gan-introduction)\n * [The State of Self-Driving Cars for Everybodyüö∂üöò](https://towardsdatascience.com/the-state-of-self-driving-cars-for-everybody-29446c1c2e2c)\n * [Should you teach Python or R for data science?](https://www.dataschool.io/python-or-r-for-data-science/)\n * [How to build a simple artificial neural network with Go](https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/)\n * [Introduction to Deep Learning 2016](https://blog.algorithmia.com/introduction-to-deep-learning-2016/)\n * [Instagram photos reveal predictive markers of depression](https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-017-0110-z)\n * [Deep Q-Learning with Keras and Gym](https://keon.io/deep-q-learning/)","updated":"2020-06-06T15:14:33.460Z","path":"articles/index.html","comments":1,"layout":"page","_id":"ckb3slhha000240mqn5pdgvky","content":"<h1>Articles</h1>\n<p>This is a collection of articles, blog posts, scientific papers and other resources I found on the internet, which I consider interesting, helpful, well-written or something to think about. They are not in any particular order.</p>\n<h2 id=\"General\">General</h2>\n<ul>\n<li><a href=\"https://doi.org/10.1111/1467-9213.00309\" target=\"_blank\" rel=\"noopener\">Are We Living in a Computer Simulation?</a></li>\n<li><a href=\"https://medium.freecodecamp.org/please-do-learn-to-code-233597dd141c\" target=\"_blank\" rel=\"noopener\">Please do learn to code</a></li>\n<li><a href=\"http://www.goldsborough.me/google/internship/2016/11/18/01-57-54-my_google_internship_/\" target=\"_blank\" rel=\"noopener\">My Google Internship</a></li>\n<li><a href=\"https://www.quora.com/What-are-10-things-that-you-should-definitely-do-before-turning-30/answer/K-John-25?srid=uBHCu\" target=\"_blank\" rel=\"noopener\">What are 10 things that you should definitely do before turning 30?</a></li>\n<li><a href=\"https://dev.to/lpasqualis/from-coding-to-management-to-leadership\" target=\"_blank\" rel=\"noopener\">From Coding To Management To Leadership</a></li>\n</ul>\n<h2 id=\"Development\">Development</h2>\n<ul>\n<li><a href=\"https://www.scala-lang.org/old/sites/default/files/FrederikThesis.pdf\" target=\"_blank\" rel=\"noopener\">Scala &amp; Design Patterns</a></li>\n<li><a href=\"http://horicky.blogspot.com/2009/11/nosql-patterns.html\" target=\"_blank\" rel=\"noopener\">Pragmatic Programming Techniques</a></li>\n<li><a href=\"http://y.tsutsumi.io/aiohttp-vs-multithreaded-flask-for-high-io-applications.html\" target=\"_blank\" rel=\"noopener\">Aiohttp vs Multithreaded Flask for High I/O Applications</a></li>\n<li><a href=\"https://www.journaldev.com/797/what-is-java-string-pool\" target=\"_blank\" rel=\"noopener\">What is Java String Pool?</a></li>\n<li><a href=\"https://dzone.com/articles/what-are-reactive-streams-in-java\" target=\"_blank\" rel=\"noopener\">What Are Reactive Streams in Java?</a></li>\n<li><a href=\"https://www.jetbrains.com/research/devecosystem-2017/\" target=\"_blank\" rel=\"noopener\">The State of Developer Ecosystem in 2017</a></li>\n<li><a href=\"http://stackoverflow.com/insights/survey/2017/#technology-most-loved-dreaded-and-wanted-languages\" target=\"_blank\" rel=\"noopener\">Stack Overflow Developer Survey 2017</a></li>\n<li><a href=\"https://simplesecurity.sensedeep.com/web-developer-security-checklist-f2e4f43c9c56\" target=\"_blank\" rel=\"noopener\">Web Developer Security Checklist</a></li>\n<li><a href=\"https://hashrocket.com/blog/posts/websocket-shootout\" target=\"_blank\" rel=\"noopener\">Websocket Shootout: Clojure, C++, Elixir, Go, NodeJS, and Ruby</a></li>\n<li><a href=\"http://blog.mixu.net/2011/02/01/understanding-the-node-js-event-loop/\" target=\"_blank\" rel=\"noopener\">Understanding the NodeJS event loop</a></li>\n<li><a href=\"http://blog.codeclimate.com/blog/2014/06/05/choose-protocol-buffers/\" target=\"_blank\" rel=\"noopener\">5 Reasons to Use Protocol Buffers Instead of JSON</a></li>\n<li><a href=\"https://auth0.com/blog/understanding-reactive-programming-and-rxjs\" target=\"_blank\" rel=\"noopener\">Understanding Reactive Programming and RxJS</a></li>\n<li><a href=\"http://blog.parse.com/learn/how-we-moved-our-api-from-ruby-to-go-and-saved-our-sanity/\" target=\"_blank\" rel=\"noopener\">How We Moved Our API From Ruby to Go and Saved Our Sanity</a></li>\n<li><a href=\"https://hackernoon.com/10-things-i-learned-making-the-fastest-site-in-the-world-18a0e1cdf4a7#.syz3poebn\" target=\"_blank\" rel=\"noopener\">10 things I learned making the fastest site in the world</a></li>\n</ul>\n<h2 id=\"DevOps-Other\">DevOps &amp; Other</h2>\n<ul>\n<li><a href=\"https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/\" target=\"_blank\" rel=\"noopener\">A Guide to the Kubernetes Networking Model</a></li>\n<li><a href=\"https://unixism.net/2019/04/linux-applications-performance-introduction/\" target=\"_blank\" rel=\"noopener\">Linux Applications Performance</a></li>\n<li><a href=\"https://www.ibm.com/developerworks/library/l-linuxboot/\" target=\"_blank\" rel=\"noopener\">Inside the Linux boot process</a></li>\n<li><a href=\"https://medium.com/@shazow/ssh-how-does-it-even-9e43586e4ffc\" target=\"_blank\" rel=\"noopener\">Why aren‚Äôt we using SSH for everything?</a></li>\n<li><a href=\"https://github.com/alex/what-happens-when/blob/master/README.rst#the-g-key-is-pressed\" target=\"_blank\" rel=\"noopener\">what-happens-when</a></li>\n</ul>\n<h2 id=\"Machine-Learning-Data-Science\">Machine Learning &amp; Data Science</h2>\n<ul>\n<li><a href=\"https://www.kaggle.com/jesucristo/gan-introduction\" target=\"_blank\" rel=\"noopener\">Introduction to GANs</a></li>\n<li><a href=\"https://towardsdatascience.com/the-state-of-self-driving-cars-for-everybody-29446c1c2e2c\" target=\"_blank\" rel=\"noopener\">The State of Self-Driving Cars for Everybodyüö∂üöò</a></li>\n<li><a href=\"https://www.dataschool.io/python-or-r-for-data-science/\" target=\"_blank\" rel=\"noopener\">Should you teach Python or R for data science?</a></li>\n<li><a href=\"https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/\" target=\"_blank\" rel=\"noopener\">How to build a simple artificial neural network with Go</a></li>\n<li><a href=\"https://blog.algorithmia.com/introduction-to-deep-learning-2016/\" target=\"_blank\" rel=\"noopener\">Introduction to Deep Learning 2016</a></li>\n<li><a href=\"https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-017-0110-z\" target=\"_blank\" rel=\"noopener\">Instagram photos reveal predictive markers of depression</a></li>\n<li><a href=\"https://keon.io/deep-q-learning/\" target=\"_blank\" rel=\"noopener\">Deep Q-Learning with Keras and Gym</a></li>\n</ul>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1>Articles</h1>\n<p>This is a collection of articles, blog posts, scientific papers and other resources I found on the internet, which I consider interesting, helpful, well-written or something to think about. They are not in any particular order.</p>\n<h2 id=\"General\">General</h2>\n<ul>\n<li><a href=\"https://doi.org/10.1111/1467-9213.00309\" target=\"_blank\" rel=\"noopener\">Are We Living in a Computer Simulation?</a></li>\n<li><a href=\"https://medium.freecodecamp.org/please-do-learn-to-code-233597dd141c\" target=\"_blank\" rel=\"noopener\">Please do learn to code</a></li>\n<li><a href=\"http://www.goldsborough.me/google/internship/2016/11/18/01-57-54-my_google_internship_/\" target=\"_blank\" rel=\"noopener\">My Google Internship</a></li>\n<li><a href=\"https://www.quora.com/What-are-10-things-that-you-should-definitely-do-before-turning-30/answer/K-John-25?srid=uBHCu\" target=\"_blank\" rel=\"noopener\">What are 10 things that you should definitely do before turning 30?</a></li>\n<li><a href=\"https://dev.to/lpasqualis/from-coding-to-management-to-leadership\" target=\"_blank\" rel=\"noopener\">From Coding To Management To Leadership</a></li>\n</ul>\n<h2 id=\"Development\">Development</h2>\n<ul>\n<li><a href=\"https://www.scala-lang.org/old/sites/default/files/FrederikThesis.pdf\" target=\"_blank\" rel=\"noopener\">Scala &amp; Design Patterns</a></li>\n<li><a href=\"http://horicky.blogspot.com/2009/11/nosql-patterns.html\" target=\"_blank\" rel=\"noopener\">Pragmatic Programming Techniques</a></li>\n<li><a href=\"http://y.tsutsumi.io/aiohttp-vs-multithreaded-flask-for-high-io-applications.html\" target=\"_blank\" rel=\"noopener\">Aiohttp vs Multithreaded Flask for High I/O Applications</a></li>\n<li><a href=\"https://www.journaldev.com/797/what-is-java-string-pool\" target=\"_blank\" rel=\"noopener\">What is Java String Pool?</a></li>\n<li><a href=\"https://dzone.com/articles/what-are-reactive-streams-in-java\" target=\"_blank\" rel=\"noopener\">What Are Reactive Streams in Java?</a></li>\n<li><a href=\"https://www.jetbrains.com/research/devecosystem-2017/\" target=\"_blank\" rel=\"noopener\">The State of Developer Ecosystem in 2017</a></li>\n<li><a href=\"http://stackoverflow.com/insights/survey/2017/#technology-most-loved-dreaded-and-wanted-languages\" target=\"_blank\" rel=\"noopener\">Stack Overflow Developer Survey 2017</a></li>\n<li><a href=\"https://simplesecurity.sensedeep.com/web-developer-security-checklist-f2e4f43c9c56\" target=\"_blank\" rel=\"noopener\">Web Developer Security Checklist</a></li>\n<li><a href=\"https://hashrocket.com/blog/posts/websocket-shootout\" target=\"_blank\" rel=\"noopener\">Websocket Shootout: Clojure, C++, Elixir, Go, NodeJS, and Ruby</a></li>\n<li><a href=\"http://blog.mixu.net/2011/02/01/understanding-the-node-js-event-loop/\" target=\"_blank\" rel=\"noopener\">Understanding the NodeJS event loop</a></li>\n<li><a href=\"http://blog.codeclimate.com/blog/2014/06/05/choose-protocol-buffers/\" target=\"_blank\" rel=\"noopener\">5 Reasons to Use Protocol Buffers Instead of JSON</a></li>\n<li><a href=\"https://auth0.com/blog/understanding-reactive-programming-and-rxjs\" target=\"_blank\" rel=\"noopener\">Understanding Reactive Programming and RxJS</a></li>\n<li><a href=\"http://blog.parse.com/learn/how-we-moved-our-api-from-ruby-to-go-and-saved-our-sanity/\" target=\"_blank\" rel=\"noopener\">How We Moved Our API From Ruby to Go and Saved Our Sanity</a></li>\n<li><a href=\"https://hackernoon.com/10-things-i-learned-making-the-fastest-site-in-the-world-18a0e1cdf4a7#.syz3poebn\" target=\"_blank\" rel=\"noopener\">10 things I learned making the fastest site in the world</a></li>\n</ul>\n<h2 id=\"DevOps-Other\">DevOps &amp; Other</h2>\n<ul>\n<li><a href=\"https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/\" target=\"_blank\" rel=\"noopener\">A Guide to the Kubernetes Networking Model</a></li>\n<li><a href=\"https://unixism.net/2019/04/linux-applications-performance-introduction/\" target=\"_blank\" rel=\"noopener\">Linux Applications Performance</a></li>\n<li><a href=\"https://www.ibm.com/developerworks/library/l-linuxboot/\" target=\"_blank\" rel=\"noopener\">Inside the Linux boot process</a></li>\n<li><a href=\"https://medium.com/@shazow/ssh-how-does-it-even-9e43586e4ffc\" target=\"_blank\" rel=\"noopener\">Why aren‚Äôt we using SSH for everything?</a></li>\n<li><a href=\"https://github.com/alex/what-happens-when/blob/master/README.rst#the-g-key-is-pressed\" target=\"_blank\" rel=\"noopener\">what-happens-when</a></li>\n</ul>\n<h2 id=\"Machine-Learning-Data-Science\">Machine Learning &amp; Data Science</h2>\n<ul>\n<li><a href=\"https://www.kaggle.com/jesucristo/gan-introduction\" target=\"_blank\" rel=\"noopener\">Introduction to GANs</a></li>\n<li><a href=\"https://towardsdatascience.com/the-state-of-self-driving-cars-for-everybody-29446c1c2e2c\" target=\"_blank\" rel=\"noopener\">The State of Self-Driving Cars for Everybodyüö∂üöò</a></li>\n<li><a href=\"https://www.dataschool.io/python-or-r-for-data-science/\" target=\"_blank\" rel=\"noopener\">Should you teach Python or R for data science?</a></li>\n<li><a href=\"https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/\" target=\"_blank\" rel=\"noopener\">How to build a simple artificial neural network with Go</a></li>\n<li><a href=\"https://blog.algorithmia.com/introduction-to-deep-learning-2016/\" target=\"_blank\" rel=\"noopener\">Introduction to Deep Learning 2016</a></li>\n<li><a href=\"https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-017-0110-z\" target=\"_blank\" rel=\"noopener\">Instagram photos reveal predictive markers of depression</a></li>\n<li><a href=\"https://keon.io/deep-q-learning/\" target=\"_blank\" rel=\"noopener\">Deep Q-Learning with Keras and Gym</a></li>\n</ul>\n"},{"title":"imprint","date":"2019-01-02T07:46:32.000Z","_content":"\n# Imprint\n\n### Legal Disclosure & Privacy Statement\nInformation in accordance with section 5 German TMG\n\n```\nFerdinand M√ºtsch\nVorholzstra√üe 11 \n76137 Karlsruhe\nGermany\n```\n\n### Contact\n```\nTelephone: 0Ô∏è‚É£1Ô∏è‚É£7Ô∏è‚É£6Ô∏è‚É£4Ô∏è‚É£5Ô∏è‚É£6Ô∏è‚É£4Ô∏è‚É£1Ô∏è‚É£9Ô∏è‚É£7Ô∏è‚É£4Ô∏è‚É£\nE-Mail: ferdinand@muetsch.io\nWeb: www.muetsch.io\n```\n\n### Disclaimer\n#### Accountability for content\nThe contents of our pages have been created with the utmost care. However, we cannot guarantee the contents' accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this context, please note that we are accordingly not obliged to monitor merely the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per ¬ß¬ß 8 to 10 of the Telemedia Act (TMG).\n\n#### Accountability for links\nResponsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately. \n\n#### Copyright\nOur web pages and their contents are subject to German copyright law. Unless expressly permitted by law (¬ß 44a et seq. of the copyright law), every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are allowed only for private use, so must not serve either directly or indirectly for earnings. Unauthorized utilization of copyrighted works is punishable (¬ß 106 of the copyright law).\n\n#### General\nYour personal data (e.g. title, name, house address, e-mail address, phone number, bank details, credit card number) are processed by us only in accordance with the provisions of German data privacy laws. The following provisions describe the type, scope and purpose of collecting, processing and utilizing personal data. This data privacy policy applies only to our web pages. If links on our pages route you to other pages, please inquire there about how your data are handled in such cases.\n\n#### Inventory data\n(1) Your personal data, insofar as these are necessary for this contractual relationship (inventory data) in terms of its establishment, organization of content and modifications, are used exclusively for fulfilling the contract. For goods to be delivered, for instance, your name and address must be relayed to the supplier of the goods. \n(2) Without your explicit consent or a legal basis, your personal data are not passed on to third parties outside the scope of fulfilling this contract. After completion of the contract, your data are blocked against further use. After expiry of deadlines as per tax-related and commercial regulations, these data are deleted unless you have expressly consented to their further use.\n\n#### Information about cookies\n(1) To optimize our web presence, we use cookies. These are small text files stored in your computer's main memory. These cookies are deleted after you close the browser. Other cookies remain on your computer (long-term cookies) and permit its recognition on your next visit. This allows us to improve your access to our site.\n(2) You can prevent storage of cookies by choosing a \"disable cookies\" option in your browser settings. But this can limit the functionality of our Internet offers as a result.\n\n#### Disclosure\nAccording to the Federal Data Protection Act, you have a right to free-of-charge information about your stored data, and possibly entitlement to correction, blocking or deletion of such data. Inquiries can be directed to the following e-mail addresses: ( [ferdinand@muetsch.io](mailto:ferdinand@muetsch.io) )\n\nSource: [twiggs translations](http://www.twigg.de/)\n\n### German Privacy Policy (Datenschutzerkl√§rung)\nDiese Datenschutzerkl√§rung kl√§rt Sie √ºber die Art, den Umfang und Zweck der Verarbeitung von personenbezogenen Daten (nachfolgend kurz ‚ÄûDaten‚Äú) innerhalb unseres Onlineangebotes und der mit ihm verbundenen Webseiten, Funktionen und Inhalte sowie externen Onlinepr√§senzen, wie z.B. unser Social Media Profile auf. (nachfolgend gemeinsam bezeichnet als ‚ÄûOnlineangebot‚Äú). Im Hinblick auf die verwendeten Begrifflichkeiten, wie z.B. ‚ÄûVerarbeitung‚Äú oder ‚ÄûVerantwortlicher‚Äú verweisen wir auf die Definitionen im Art. 4 der Datenschutzgrundverordnung (DSGVO).\n\n#### Arten der verarbeiteten Daten:\n- Bestandsdaten (z.B., Namen, Adressen).\n- Kontaktdaten (z.B., E-Mail, Telefonnummern).\n- Inhaltsdaten (z.B., Texteingaben, Fotografien, Videos).\n- Nutzungsdaten (z.B., besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten).\n- Meta-/Kommunikationsdaten (z.B., Ger√§te-Informationen, IP-Adressen).\n\n#### Kategorien betroffener Personen\nBesucher und Nutzer des Onlineangebotes (Nachfolgend bezeichnen wir die betroffenen Personen zusammenfassend auch als ‚ÄûNutzer‚Äú).\n\n#### Zweck der Verarbeitung\n- Zurverf√ºgungstellung des Onlineangebotes, seiner Funktionen und Inhalte.\n- Beantwortung von Kontaktanfragen und Kommunikation mit Nutzern.\n- Sicherheitsma√ünahmen.\n- Reichweitenmessung/Marketing\n\n#### Verwendete Begrifflichkeiten \n‚ÄûPersonenbezogene Daten‚Äú sind alle Informationen, die sich auf eine identifizierte oder identifizierbare nat√ºrliche Person (im Folgenden ‚Äûbetroffene Person‚Äú) beziehen; als identifizierbar wird eine nat√ºrliche Person angesehen, die direkt oder indirekt, insbesondere mittels Zuordnung zu einer Kennung wie einem Namen, zu einer Kennnummer, zu Standortdaten, zu einer Online-Kennung (z.B. Cookie) oder zu einem oder mehreren besonderen Merkmalen identifiziert werden kann, die Ausdruck der physischen, physiologischen, genetischen, psychischen, wirtschaftlichen, kulturellen oder sozialen Identit√§t dieser nat√ºrlichen Person sind.\n\n‚ÄûVerarbeitung‚Äú ist jeder mit oder ohne Hilfe automatisierter Verfahren ausgef√ºhrten Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten. Der Begriff reicht weit und umfasst praktisch jeden Umgang mit Daten.\n\nAls ‚ÄûVerantwortlicher‚Äú wird die nat√ºrliche oder juristische Person, Beh√∂rde, Einrichtung oder andere Stelle, die allein oder gemeinsam mit anderen √ºber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten entscheidet, bezeichnet.\n\n#### Ma√ügebliche Rechtsgrundlagen\nNach Ma√ügabe des Art. 13 DSGVO teilen wir Ihnen die Rechtsgrundlagen unserer Datenverarbeitungen mit. Sofern die Rechtsgrundlage in der Datenschutzerkl√§rung nicht genannt wird, gilt Folgendes: Die Rechtsgrundlage f√ºr die Einholung von Einwilligungen ist Art. 6 Abs. 1 lit. a und Art. 7 DSGVO, die Rechtsgrundlage f√ºr die Verarbeitung zur Erf√ºllung unserer Leistungen und Durchf√ºhrung vertraglicher Ma√ünahmen sowie Beantwortung von Anfragen ist Art. 6 Abs. 1 lit. b DSGVO, die Rechtsgrundlage f√ºr die Verarbeitung zur Erf√ºllung unserer rechtlichen Verpflichtungen ist Art. 6 Abs. 1 lit. c DSGVO, und die Rechtsgrundlage f√ºr die Verarbeitung zur Wahrung unserer berechtigten Interessen ist Art. 6 Abs. 1 lit. f DSGVO. F√ºr den Fall, dass lebenswichtige Interessen der betroffenen Person oder einer anderen nat√ºrlichen Person eine Verarbeitung personenbezogener Daten erforderlich machen, dient Art. 6 Abs. 1 lit. d DSGVO als Rechtsgrundlage.\n\n#### Zusammenarbeit mit Auftragsverarbeitern und Dritten\nSofern wir im Rahmen unserer Verarbeitung Daten gegen√ºber anderen Personen und Unternehmen (Auftragsverarbeitern oder Dritten) offenbaren, sie an diese √ºbermitteln oder ihnen sonst Zugriff auf die Daten gew√§hren, erfolgt dies nur auf Grundlage einer gesetzlichen Erlaubnis (z.B. wenn eine √úbermittlung der Daten an Dritte, wie an Zahlungsdienstleister, gem. Art. 6 Abs. 1 lit. b DSGVO zur Vertragserf√ºllung erforderlich ist), Sie eingewilligt haben, eine rechtliche Verpflichtung dies vorsieht oder auf Grundlage unserer berechtigten Interessen (z.B. beim Einsatz von Beauftragten, Webhostern, etc.).\n\nSofern wir Dritte mit der Verarbeitung von Daten auf Grundlage eines sog. ‚ÄûAuftragsverarbeitungsvertrages‚Äú beauftragen, geschieht dies auf Grundlage des Art. 28 DSGVO.\n\n#### √úbermittlungen in Drittl√§nder\nSofern wir Daten in einem Drittland (d.h. au√üerhalb der Europ√§ischen Union (EU) oder des Europ√§ischen Wirtschaftsraums (EWR)) verarbeiten oder dies im Rahmen der Inanspruchnahme von Diensten Dritter oder Offenlegung, bzw. √úbermittlung von Daten an Dritte geschieht, erfolgt dies nur, wenn es zur Erf√ºllung unserer (vor)vertraglichen Pflichten, auf Grundlage Ihrer Einwilligung, aufgrund einer rechtlichen Verpflichtung oder auf Grundlage unserer berechtigten Interessen geschieht. Vorbehaltlich gesetzlicher oder vertraglicher Erlaubnisse, verarbeiten oder lassen wir die Daten in einem Drittland nur beim Vorliegen der besonderen Voraussetzungen der Art. 44 ff. DSGVO verarbeiten. D.h. die Verarbeitung erfolgt z.B. auf Grundlage besonderer Garantien, wie der offiziell anerkannten Feststellung eines der EU entsprechenden Datenschutzniveaus (z.B. f√ºr die USA durch das ‚ÄûPrivacy Shield‚Äú) oder Beachtung offiziell anerkannter spezieller vertraglicher Verpflichtungen (so genannte ‚ÄûStandardvertragsklauseln‚Äú).\n\n#### Rechte der betroffenen Personen\nSie haben das Recht, eine Best√§tigung dar√ºber zu verlangen, ob betreffende Daten verarbeitet werden und auf Auskunft √ºber diese Daten sowie auf weitere Informationen und Kopie der Daten entsprechend Art. 15 DSGVO.\n\nSie haben entsprechend. Art. 16 DSGVO das Recht, die Vervollst√§ndigung der Sie betreffenden Daten oder die Berichtigung der Sie betreffenden unrichtigen Daten zu verlangen.\n\nSie haben nach Ma√ügabe des Art. 17 DSGVO das Recht zu verlangen, dass betreffende Daten unverz√ºglich gel√∂scht werden, bzw. alternativ nach Ma√ügabe des Art. 18 DSGVO eine Einschr√§nkung der Verarbeitung der Daten zu verlangen.\n\nSie haben das Recht zu verlangen, dass die Sie betreffenden Daten, die Sie uns bereitgestellt haben nach Ma√ügabe des Art. 20 DSGVO zu erhalten und deren √úbermittlung an andere Verantwortliche zu fordern.\n\nSie haben ferner gem. Art. 77 DSGVO das Recht, eine Beschwerde bei der zust√§ndigen Aufsichtsbeh√∂rde einzureichen.\n\n#### Widerrufsrecht\nSie haben das Recht, erteilte Einwilligungen gem. Art. 7 Abs. 3 DSGVO mit Wirkung f√ºr die Zukunft zu widerrufen\n\n#### Widerspruchsrecht\nSie k√∂nnen der k√ºnftigen Verarbeitung der Sie betreffenden Daten nach Ma√ügabe des Art. 21 DSGVO jederzeit widersprechen. Der Widerspruch kann insbesondere gegen die Verarbeitung f√ºr Zwecke der Direktwerbung erfolgen.\n\n#### Cookies und Widerspruchsrecht bei Direktwerbung\nAls ‚ÄûCookies‚Äú werden kleine Dateien bezeichnet, die auf Rechnern der Nutzer gespeichert werden. Innerhalb der Cookies k√∂nnen unterschiedliche Angaben gespeichert werden. Ein Cookie dient prim√§r dazu, die Angaben zu einem Nutzer (bzw. dem Ger√§t auf dem das Cookie gespeichert ist) w√§hrend oder auch nach seinem Besuch innerhalb eines Onlineangebotes zu speichern. Als tempor√§re Cookies, bzw. ‚ÄûSession-Cookies‚Äú oder ‚Äûtransiente Cookies‚Äú, werden Cookies bezeichnet, die gel√∂scht werden, nachdem ein Nutzer ein Onlineangebot verl√§sst und seinen Browser schlie√üt. In einem solchen Cookie kann z.B. der Inhalt eines Warenkorbs in einem Onlineshop oder ein Login-Staus gespeichert werden. Als ‚Äûpermanent‚Äú oder ‚Äûpersistent‚Äú werden Cookies bezeichnet, die auch nach dem Schlie√üen des Browsers gespeichert bleiben. So kann z.B. der Login-Status gespeichert werden, wenn die Nutzer diese nach mehreren Tagen aufsuchen. Ebenso k√∂nnen in einem solchen Cookie die Interessen der Nutzer gespeichert werden, die f√ºr Reichweitenmessung oder Marketingzwecke verwendet werden. Als ‚ÄûThird-Party-Cookie‚Äú werden Cookies bezeichnet, die von anderen Anbietern als dem Verantwortlichen, der das Onlineangebot betreibt, angeboten werden (andernfalls, wenn es nur dessen Cookies sind spricht man von ‚ÄûFirst-Party Cookies‚Äú).\n\nWir k√∂nnen tempor√§re und permanente Cookies einsetzen und kl√§ren hier√ºber im Rahmen unserer Datenschutzerkl√§rung auf.\n\nFalls die Nutzer nicht m√∂chten, dass Cookies auf ihrem Rechner gespeichert werden, werden sie gebeten die entsprechende Option in den Systemeinstellungen ihres Browsers zu deaktivieren. Gespeicherte Cookies k√∂nnen in den Systemeinstellungen des Browsers gel√∂scht werden. Der Ausschluss von Cookies kann zu Funktionseinschr√§nkungen dieses Onlineangebotes f√ºhren.\n\nEin genereller Widerspruch gegen den Einsatz der zu Zwecken des Onlinemarketing eingesetzten Cookies kann bei einer Vielzahl der Dienste, vor allem im Fall des Trackings, √ºber die US-amerikanische Seite http://www.aboutads.info/choices/ oder die EU-Seite http://www.youronlinechoices.com/ erkl√§rt werden. Des Weiteren kann die Speicherung von Cookies mittels deren Abschaltung in den Einstellungen des Browsers erreicht werden. Bitte beachten Sie, dass dann gegebenenfalls nicht alle Funktionen dieses Onlineangebotes genutzt werden k√∂nnen.\n\n#### L√∂schung von Daten\nDie von uns verarbeiteten Daten werden nach Ma√ügabe der Art. 17 und 18 DSGVO gel√∂scht oder in ihrer Verarbeitung eingeschr√§nkt. Sofern nicht im Rahmen dieser Datenschutzerkl√§rung ausdr√ºcklich angegeben, werden die bei uns gespeicherten Daten gel√∂scht, sobald sie f√ºr ihre Zweckbestimmung nicht mehr erforderlich sind und der L√∂schung keine gesetzlichen Aufbewahrungspflichten entgegenstehen. Sofern die Daten nicht gel√∂scht werden, weil sie f√ºr andere und gesetzlich zul√§ssige Zwecke erforderlich sind, wird deren Verarbeitung eingeschr√§nkt. D.h. die Daten werden gesperrt und nicht f√ºr andere Zwecke verarbeitet. Das gilt z.B. f√ºr Daten, die aus handels- oder steuerrechtlichen Gr√ºnden aufbewahrt werden m√ºssen.\n\nNach gesetzlichen Vorgaben in Deutschland erfolgt die Aufbewahrung insbesondere f√ºr 6 Jahre gem√§√ü ¬ß 257 Abs. 1 HGB (Handelsb√ºcher, Inventare, Er√∂ffnungsbilanzen, Jahresabschl√ºsse, Handelsbriefe, Buchungsbelege, etc.) sowie f√ºr 10 Jahre gem√§√ü ¬ß 147 Abs. 1 AO (B√ºcher, Aufzeichnungen, Lageberichte, Buchungsbelege, Handels- und Gesch√§ftsbriefe, F√ºr Besteuerung relevante Unterlagen, etc.).\n\nNach gesetzlichen Vorgaben in √ñsterreich erfolgt die Aufbewahrung insbesondere f√ºr 7 J gem√§√ü ¬ß 132 Abs. 1 BAO (Buchhaltungsunterlagen, Belege/Rechnungen, Konten, Belege, Gesch√§ftspapiere, Aufstellung der Einnahmen und Ausgaben, etc.), f√ºr 22 Jahre im Zusammenhang mit Grundst√ºcken und f√ºr 10 Jahre bei Unterlagen im Zusammenhang mit elektronisch erbrachten Leistungen, Telekommunikations-, Rundfunk- und Fernsehleistungen, die an Nichtunternehmer in EU-Mitgliedstaaten erbracht werden und f√ºr die der Mini-One-Stop-Shop (MOSS) in Anspruch genommen wird.\n\n#### Hosting\nDie von uns in Anspruch genommenen Hosting-Leistungen dienen der Zurverf√ºgungstellung der folgenden Leistungen: Infrastruktur- und Plattformdienstleistungen, Rechenkapazit√§t, Speicherplatz und Datenbankdienste, Sicherheitsleistungen sowie technische Wartungsleistungen, die wir zum Zwecke des Betriebs dieses Onlineangebotes einsetzen.\n\nHierbei verarbeiten wir, bzw. unser Hostinganbieter Bestandsdaten, Kontaktdaten, Inhaltsdaten, Vertragsdaten, Nutzungsdaten, Meta- und Kommunikationsdaten von Kunden, Interessenten und Besuchern dieses Onlineangebotes auf Grundlage unserer berechtigten Interessen an einer effizienten und sicheren Zurverf√ºgungstellung dieses Onlineangebotes gem. Art. 6 Abs. 1 lit. f DSGVO i.V.m. Art. 28 DSGVO (Abschluss Auftragsverarbeitungsvertrag).\n\n#### Erhebung von Zugriffsdaten und Logfiles\nWir, bzw. unser Hostinganbieter, erhebt auf Grundlage unserer berechtigten Interessen im Sinne des Art. 6 Abs. 1 lit. f. DSGVO Daten √ºber jeden Zugriff auf den Server, auf dem sich dieser Dienst befindet (sogenannte Serverlogfiles). Zu den Zugriffsdaten geh√∂ren Name der abgerufenen Webseite, Datei, Datum und Uhrzeit des Abrufs, √ºbertragene Datenmenge, Meldung √ºber erfolgreichen Abruf, Browsertyp nebst Version, das Betriebssystem des Nutzers, Referrer URL (die zuvor besuchte Seite), IP-Adresse und der anfragende Provider.\n\nLogfile-Informationen werden aus Sicherheitsgr√ºnden (z.B. zur Aufkl√§rung von Missbrauchs- oder Betrugshandlungen) f√ºr die Dauer von maximal 7 Tagen gespeichert und danach gel√∂scht. Daten, deren weitere Aufbewahrung zu Beweiszwecken erforderlich ist, sind bis zur endg√ºltigen Kl√§rung des jeweiligen Vorfalls von der L√∂schung ausgenommen.\n\n#### Google Universal Analytics\nWir setzen auf Grundlage unserer berechtigten Interessen (d.h. Interesse an der Analyse, Optimierung und wirtschaftlichem Betrieb unseres Onlineangebotes im Sinne des Art. 6 Abs. 1 lit. f. DSGVO) Google Analytics, einen Webanalysedienst der Google LLC (‚ÄûGoogle‚Äú) ein. Google verwendet Cookies. Die durch das Cookie erzeugten Informationen √ºber Benutzung des Onlineangebotes durch die Nutzer werden in der Regel an einen Server von Google in den USA √ºbertragen und dort gespeichert.\n\nGoogle ist unter dem Privacy-Shield-Abkommen zertifiziert und bietet hierdurch eine Garantie, das europ√§ische Datenschutzrecht einzuhalten (https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active).\n\nGoogle wird diese Informationen in unserem Auftrag benutzen, um die Nutzung unseres Onlineangebotes durch die Nutzer auszuwerten, um Reports √ºber die Aktivit√§ten innerhalb dieses Onlineangebotes zusammenzustellen und um weitere, mit der Nutzung dieses Onlineangebotes und der Internetnutzung verbundene Dienstleistungen, uns gegen√ºber zu erbringen. Dabei k√∂nnen aus den verarbeiteten Daten pseudonyme Nutzungsprofile der Nutzer erstellt werden.\n\nWir setzen Google Analytics in der Ausgestaltung als ‚ÄûUniversal-Analytics‚Äú ein. ‚ÄûUniversal Analytics‚Äú bezeichnet ein Verfahren von Google Analytics, bei dem die Nutzeranalyse auf Grundlage einer pseudonymen Nutzer-ID erfolgt und damit ein pseudonymes Profil des Nutzers mit Informationen aus der Nutzung verschiedener Ger√§ten erstellt wird (sog. ‚ÄûCross-Device-Tracking‚Äú).\n\nWir setzen Google Analytics nur mit aktivierter IP-Anonymisierung ein. Das bedeutet, die IP-Adresse der Nutzer wird von Google innerhalb von Mitgliedstaaten der Europ√§ischen Union oder in anderen Vertragsstaaten des Abkommens √ºber den Europ√§ischen Wirtschaftsraum gek√ºrzt. Nur in Ausnahmef√§llen wird die volle IP-Adresse an einen Server von Google in den USA √ºbertragen und dort gek√ºrzt.\n\nDie von dem Browser des Nutzers √ºbermittelte IP-Adresse wird nicht mit anderen Daten von Google zusammengef√ºhrt. Die Nutzer k√∂nnen die Speicherung der Cookies durch eine entsprechende Einstellung ihrer Browser-Software verhindern; die Nutzer k√∂nnen dar√ºber hinaus die Erfassung der durch das Cookie erzeugten und auf ihre Nutzung des Onlineangebotes bezogenen Daten an Google sowie die Verarbeitung dieser Daten durch Google verhindern, indem sie das unter folgendem Link verf√ºgbare Browser-Plugin herunterladen und installieren: http://tools.google.com/dlpage/gaoptout?hl=de.\n\nWeitere Informationen zur Datennutzung durch Google, Einstellungs- und Widerspruchsm√∂glichkeiten erfahren Sie auf den Webseiten von Google: https://www.google.com/intl/de/policies/privacy/partners (‚ÄûDatennutzung durch Google bei Ihrer Nutzung von Websites oder Apps unserer Partner‚Äú), http://www.google.com/policies/technologies/ads (‚ÄûDatennutzung zu Werbezwecken‚Äú), http://www.google.de/settings/ads (‚ÄûInformationen verwalten, die Google verwendet, um Ihnen Werbung einzublenden‚Äú).\n\n#### Youtube\nWir binden die Videos der Plattform ‚ÄúYouTube‚Äù des Anbieters Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA, ein. Datenschutzerkl√§rung: https://www.google.com/policies/privacy/, Opt-Out: https://adssettings.google.com/authenticated.\n\n[Erstellt mit Datenschutz-Generator.de von RA Dr. Thomas Schwenke](https://datenschutz-generator.de/)","source":"imprint/index.md","raw":"---\ntitle: imprint\ndate: 2019-01-02 08:46:32\n---\n\n# Imprint\n\n### Legal Disclosure & Privacy Statement\nInformation in accordance with section 5 German TMG\n\n```\nFerdinand M√ºtsch\nVorholzstra√üe 11 \n76137 Karlsruhe\nGermany\n```\n\n### Contact\n```\nTelephone: 0Ô∏è‚É£1Ô∏è‚É£7Ô∏è‚É£6Ô∏è‚É£4Ô∏è‚É£5Ô∏è‚É£6Ô∏è‚É£4Ô∏è‚É£1Ô∏è‚É£9Ô∏è‚É£7Ô∏è‚É£4Ô∏è‚É£\nE-Mail: ferdinand@muetsch.io\nWeb: www.muetsch.io\n```\n\n### Disclaimer\n#### Accountability for content\nThe contents of our pages have been created with the utmost care. However, we cannot guarantee the contents' accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this context, please note that we are accordingly not obliged to monitor merely the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per ¬ß¬ß 8 to 10 of the Telemedia Act (TMG).\n\n#### Accountability for links\nResponsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately. \n\n#### Copyright\nOur web pages and their contents are subject to German copyright law. Unless expressly permitted by law (¬ß 44a et seq. of the copyright law), every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are allowed only for private use, so must not serve either directly or indirectly for earnings. Unauthorized utilization of copyrighted works is punishable (¬ß 106 of the copyright law).\n\n#### General\nYour personal data (e.g. title, name, house address, e-mail address, phone number, bank details, credit card number) are processed by us only in accordance with the provisions of German data privacy laws. The following provisions describe the type, scope and purpose of collecting, processing and utilizing personal data. This data privacy policy applies only to our web pages. If links on our pages route you to other pages, please inquire there about how your data are handled in such cases.\n\n#### Inventory data\n(1) Your personal data, insofar as these are necessary for this contractual relationship (inventory data) in terms of its establishment, organization of content and modifications, are used exclusively for fulfilling the contract. For goods to be delivered, for instance, your name and address must be relayed to the supplier of the goods. \n(2) Without your explicit consent or a legal basis, your personal data are not passed on to third parties outside the scope of fulfilling this contract. After completion of the contract, your data are blocked against further use. After expiry of deadlines as per tax-related and commercial regulations, these data are deleted unless you have expressly consented to their further use.\n\n#### Information about cookies\n(1) To optimize our web presence, we use cookies. These are small text files stored in your computer's main memory. These cookies are deleted after you close the browser. Other cookies remain on your computer (long-term cookies) and permit its recognition on your next visit. This allows us to improve your access to our site.\n(2) You can prevent storage of cookies by choosing a \"disable cookies\" option in your browser settings. But this can limit the functionality of our Internet offers as a result.\n\n#### Disclosure\nAccording to the Federal Data Protection Act, you have a right to free-of-charge information about your stored data, and possibly entitlement to correction, blocking or deletion of such data. Inquiries can be directed to the following e-mail addresses: ( [ferdinand@muetsch.io](mailto:ferdinand@muetsch.io) )\n\nSource: [twiggs translations](http://www.twigg.de/)\n\n### German Privacy Policy (Datenschutzerkl√§rung)\nDiese Datenschutzerkl√§rung kl√§rt Sie √ºber die Art, den Umfang und Zweck der Verarbeitung von personenbezogenen Daten (nachfolgend kurz ‚ÄûDaten‚Äú) innerhalb unseres Onlineangebotes und der mit ihm verbundenen Webseiten, Funktionen und Inhalte sowie externen Onlinepr√§senzen, wie z.B. unser Social Media Profile auf. (nachfolgend gemeinsam bezeichnet als ‚ÄûOnlineangebot‚Äú). Im Hinblick auf die verwendeten Begrifflichkeiten, wie z.B. ‚ÄûVerarbeitung‚Äú oder ‚ÄûVerantwortlicher‚Äú verweisen wir auf die Definitionen im Art. 4 der Datenschutzgrundverordnung (DSGVO).\n\n#### Arten der verarbeiteten Daten:\n- Bestandsdaten (z.B., Namen, Adressen).\n- Kontaktdaten (z.B., E-Mail, Telefonnummern).\n- Inhaltsdaten (z.B., Texteingaben, Fotografien, Videos).\n- Nutzungsdaten (z.B., besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten).\n- Meta-/Kommunikationsdaten (z.B., Ger√§te-Informationen, IP-Adressen).\n\n#### Kategorien betroffener Personen\nBesucher und Nutzer des Onlineangebotes (Nachfolgend bezeichnen wir die betroffenen Personen zusammenfassend auch als ‚ÄûNutzer‚Äú).\n\n#### Zweck der Verarbeitung\n- Zurverf√ºgungstellung des Onlineangebotes, seiner Funktionen und Inhalte.\n- Beantwortung von Kontaktanfragen und Kommunikation mit Nutzern.\n- Sicherheitsma√ünahmen.\n- Reichweitenmessung/Marketing\n\n#### Verwendete Begrifflichkeiten \n‚ÄûPersonenbezogene Daten‚Äú sind alle Informationen, die sich auf eine identifizierte oder identifizierbare nat√ºrliche Person (im Folgenden ‚Äûbetroffene Person‚Äú) beziehen; als identifizierbar wird eine nat√ºrliche Person angesehen, die direkt oder indirekt, insbesondere mittels Zuordnung zu einer Kennung wie einem Namen, zu einer Kennnummer, zu Standortdaten, zu einer Online-Kennung (z.B. Cookie) oder zu einem oder mehreren besonderen Merkmalen identifiziert werden kann, die Ausdruck der physischen, physiologischen, genetischen, psychischen, wirtschaftlichen, kulturellen oder sozialen Identit√§t dieser nat√ºrlichen Person sind.\n\n‚ÄûVerarbeitung‚Äú ist jeder mit oder ohne Hilfe automatisierter Verfahren ausgef√ºhrten Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten. Der Begriff reicht weit und umfasst praktisch jeden Umgang mit Daten.\n\nAls ‚ÄûVerantwortlicher‚Äú wird die nat√ºrliche oder juristische Person, Beh√∂rde, Einrichtung oder andere Stelle, die allein oder gemeinsam mit anderen √ºber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten entscheidet, bezeichnet.\n\n#### Ma√ügebliche Rechtsgrundlagen\nNach Ma√ügabe des Art. 13 DSGVO teilen wir Ihnen die Rechtsgrundlagen unserer Datenverarbeitungen mit. Sofern die Rechtsgrundlage in der Datenschutzerkl√§rung nicht genannt wird, gilt Folgendes: Die Rechtsgrundlage f√ºr die Einholung von Einwilligungen ist Art. 6 Abs. 1 lit. a und Art. 7 DSGVO, die Rechtsgrundlage f√ºr die Verarbeitung zur Erf√ºllung unserer Leistungen und Durchf√ºhrung vertraglicher Ma√ünahmen sowie Beantwortung von Anfragen ist Art. 6 Abs. 1 lit. b DSGVO, die Rechtsgrundlage f√ºr die Verarbeitung zur Erf√ºllung unserer rechtlichen Verpflichtungen ist Art. 6 Abs. 1 lit. c DSGVO, und die Rechtsgrundlage f√ºr die Verarbeitung zur Wahrung unserer berechtigten Interessen ist Art. 6 Abs. 1 lit. f DSGVO. F√ºr den Fall, dass lebenswichtige Interessen der betroffenen Person oder einer anderen nat√ºrlichen Person eine Verarbeitung personenbezogener Daten erforderlich machen, dient Art. 6 Abs. 1 lit. d DSGVO als Rechtsgrundlage.\n\n#### Zusammenarbeit mit Auftragsverarbeitern und Dritten\nSofern wir im Rahmen unserer Verarbeitung Daten gegen√ºber anderen Personen und Unternehmen (Auftragsverarbeitern oder Dritten) offenbaren, sie an diese √ºbermitteln oder ihnen sonst Zugriff auf die Daten gew√§hren, erfolgt dies nur auf Grundlage einer gesetzlichen Erlaubnis (z.B. wenn eine √úbermittlung der Daten an Dritte, wie an Zahlungsdienstleister, gem. Art. 6 Abs. 1 lit. b DSGVO zur Vertragserf√ºllung erforderlich ist), Sie eingewilligt haben, eine rechtliche Verpflichtung dies vorsieht oder auf Grundlage unserer berechtigten Interessen (z.B. beim Einsatz von Beauftragten, Webhostern, etc.).\n\nSofern wir Dritte mit der Verarbeitung von Daten auf Grundlage eines sog. ‚ÄûAuftragsverarbeitungsvertrages‚Äú beauftragen, geschieht dies auf Grundlage des Art. 28 DSGVO.\n\n#### √úbermittlungen in Drittl√§nder\nSofern wir Daten in einem Drittland (d.h. au√üerhalb der Europ√§ischen Union (EU) oder des Europ√§ischen Wirtschaftsraums (EWR)) verarbeiten oder dies im Rahmen der Inanspruchnahme von Diensten Dritter oder Offenlegung, bzw. √úbermittlung von Daten an Dritte geschieht, erfolgt dies nur, wenn es zur Erf√ºllung unserer (vor)vertraglichen Pflichten, auf Grundlage Ihrer Einwilligung, aufgrund einer rechtlichen Verpflichtung oder auf Grundlage unserer berechtigten Interessen geschieht. Vorbehaltlich gesetzlicher oder vertraglicher Erlaubnisse, verarbeiten oder lassen wir die Daten in einem Drittland nur beim Vorliegen der besonderen Voraussetzungen der Art. 44 ff. DSGVO verarbeiten. D.h. die Verarbeitung erfolgt z.B. auf Grundlage besonderer Garantien, wie der offiziell anerkannten Feststellung eines der EU entsprechenden Datenschutzniveaus (z.B. f√ºr die USA durch das ‚ÄûPrivacy Shield‚Äú) oder Beachtung offiziell anerkannter spezieller vertraglicher Verpflichtungen (so genannte ‚ÄûStandardvertragsklauseln‚Äú).\n\n#### Rechte der betroffenen Personen\nSie haben das Recht, eine Best√§tigung dar√ºber zu verlangen, ob betreffende Daten verarbeitet werden und auf Auskunft √ºber diese Daten sowie auf weitere Informationen und Kopie der Daten entsprechend Art. 15 DSGVO.\n\nSie haben entsprechend. Art. 16 DSGVO das Recht, die Vervollst√§ndigung der Sie betreffenden Daten oder die Berichtigung der Sie betreffenden unrichtigen Daten zu verlangen.\n\nSie haben nach Ma√ügabe des Art. 17 DSGVO das Recht zu verlangen, dass betreffende Daten unverz√ºglich gel√∂scht werden, bzw. alternativ nach Ma√ügabe des Art. 18 DSGVO eine Einschr√§nkung der Verarbeitung der Daten zu verlangen.\n\nSie haben das Recht zu verlangen, dass die Sie betreffenden Daten, die Sie uns bereitgestellt haben nach Ma√ügabe des Art. 20 DSGVO zu erhalten und deren √úbermittlung an andere Verantwortliche zu fordern.\n\nSie haben ferner gem. Art. 77 DSGVO das Recht, eine Beschwerde bei der zust√§ndigen Aufsichtsbeh√∂rde einzureichen.\n\n#### Widerrufsrecht\nSie haben das Recht, erteilte Einwilligungen gem. Art. 7 Abs. 3 DSGVO mit Wirkung f√ºr die Zukunft zu widerrufen\n\n#### Widerspruchsrecht\nSie k√∂nnen der k√ºnftigen Verarbeitung der Sie betreffenden Daten nach Ma√ügabe des Art. 21 DSGVO jederzeit widersprechen. Der Widerspruch kann insbesondere gegen die Verarbeitung f√ºr Zwecke der Direktwerbung erfolgen.\n\n#### Cookies und Widerspruchsrecht bei Direktwerbung\nAls ‚ÄûCookies‚Äú werden kleine Dateien bezeichnet, die auf Rechnern der Nutzer gespeichert werden. Innerhalb der Cookies k√∂nnen unterschiedliche Angaben gespeichert werden. Ein Cookie dient prim√§r dazu, die Angaben zu einem Nutzer (bzw. dem Ger√§t auf dem das Cookie gespeichert ist) w√§hrend oder auch nach seinem Besuch innerhalb eines Onlineangebotes zu speichern. Als tempor√§re Cookies, bzw. ‚ÄûSession-Cookies‚Äú oder ‚Äûtransiente Cookies‚Äú, werden Cookies bezeichnet, die gel√∂scht werden, nachdem ein Nutzer ein Onlineangebot verl√§sst und seinen Browser schlie√üt. In einem solchen Cookie kann z.B. der Inhalt eines Warenkorbs in einem Onlineshop oder ein Login-Staus gespeichert werden. Als ‚Äûpermanent‚Äú oder ‚Äûpersistent‚Äú werden Cookies bezeichnet, die auch nach dem Schlie√üen des Browsers gespeichert bleiben. So kann z.B. der Login-Status gespeichert werden, wenn die Nutzer diese nach mehreren Tagen aufsuchen. Ebenso k√∂nnen in einem solchen Cookie die Interessen der Nutzer gespeichert werden, die f√ºr Reichweitenmessung oder Marketingzwecke verwendet werden. Als ‚ÄûThird-Party-Cookie‚Äú werden Cookies bezeichnet, die von anderen Anbietern als dem Verantwortlichen, der das Onlineangebot betreibt, angeboten werden (andernfalls, wenn es nur dessen Cookies sind spricht man von ‚ÄûFirst-Party Cookies‚Äú).\n\nWir k√∂nnen tempor√§re und permanente Cookies einsetzen und kl√§ren hier√ºber im Rahmen unserer Datenschutzerkl√§rung auf.\n\nFalls die Nutzer nicht m√∂chten, dass Cookies auf ihrem Rechner gespeichert werden, werden sie gebeten die entsprechende Option in den Systemeinstellungen ihres Browsers zu deaktivieren. Gespeicherte Cookies k√∂nnen in den Systemeinstellungen des Browsers gel√∂scht werden. Der Ausschluss von Cookies kann zu Funktionseinschr√§nkungen dieses Onlineangebotes f√ºhren.\n\nEin genereller Widerspruch gegen den Einsatz der zu Zwecken des Onlinemarketing eingesetzten Cookies kann bei einer Vielzahl der Dienste, vor allem im Fall des Trackings, √ºber die US-amerikanische Seite http://www.aboutads.info/choices/ oder die EU-Seite http://www.youronlinechoices.com/ erkl√§rt werden. Des Weiteren kann die Speicherung von Cookies mittels deren Abschaltung in den Einstellungen des Browsers erreicht werden. Bitte beachten Sie, dass dann gegebenenfalls nicht alle Funktionen dieses Onlineangebotes genutzt werden k√∂nnen.\n\n#### L√∂schung von Daten\nDie von uns verarbeiteten Daten werden nach Ma√ügabe der Art. 17 und 18 DSGVO gel√∂scht oder in ihrer Verarbeitung eingeschr√§nkt. Sofern nicht im Rahmen dieser Datenschutzerkl√§rung ausdr√ºcklich angegeben, werden die bei uns gespeicherten Daten gel√∂scht, sobald sie f√ºr ihre Zweckbestimmung nicht mehr erforderlich sind und der L√∂schung keine gesetzlichen Aufbewahrungspflichten entgegenstehen. Sofern die Daten nicht gel√∂scht werden, weil sie f√ºr andere und gesetzlich zul√§ssige Zwecke erforderlich sind, wird deren Verarbeitung eingeschr√§nkt. D.h. die Daten werden gesperrt und nicht f√ºr andere Zwecke verarbeitet. Das gilt z.B. f√ºr Daten, die aus handels- oder steuerrechtlichen Gr√ºnden aufbewahrt werden m√ºssen.\n\nNach gesetzlichen Vorgaben in Deutschland erfolgt die Aufbewahrung insbesondere f√ºr 6 Jahre gem√§√ü ¬ß 257 Abs. 1 HGB (Handelsb√ºcher, Inventare, Er√∂ffnungsbilanzen, Jahresabschl√ºsse, Handelsbriefe, Buchungsbelege, etc.) sowie f√ºr 10 Jahre gem√§√ü ¬ß 147 Abs. 1 AO (B√ºcher, Aufzeichnungen, Lageberichte, Buchungsbelege, Handels- und Gesch√§ftsbriefe, F√ºr Besteuerung relevante Unterlagen, etc.).\n\nNach gesetzlichen Vorgaben in √ñsterreich erfolgt die Aufbewahrung insbesondere f√ºr 7 J gem√§√ü ¬ß 132 Abs. 1 BAO (Buchhaltungsunterlagen, Belege/Rechnungen, Konten, Belege, Gesch√§ftspapiere, Aufstellung der Einnahmen und Ausgaben, etc.), f√ºr 22 Jahre im Zusammenhang mit Grundst√ºcken und f√ºr 10 Jahre bei Unterlagen im Zusammenhang mit elektronisch erbrachten Leistungen, Telekommunikations-, Rundfunk- und Fernsehleistungen, die an Nichtunternehmer in EU-Mitgliedstaaten erbracht werden und f√ºr die der Mini-One-Stop-Shop (MOSS) in Anspruch genommen wird.\n\n#### Hosting\nDie von uns in Anspruch genommenen Hosting-Leistungen dienen der Zurverf√ºgungstellung der folgenden Leistungen: Infrastruktur- und Plattformdienstleistungen, Rechenkapazit√§t, Speicherplatz und Datenbankdienste, Sicherheitsleistungen sowie technische Wartungsleistungen, die wir zum Zwecke des Betriebs dieses Onlineangebotes einsetzen.\n\nHierbei verarbeiten wir, bzw. unser Hostinganbieter Bestandsdaten, Kontaktdaten, Inhaltsdaten, Vertragsdaten, Nutzungsdaten, Meta- und Kommunikationsdaten von Kunden, Interessenten und Besuchern dieses Onlineangebotes auf Grundlage unserer berechtigten Interessen an einer effizienten und sicheren Zurverf√ºgungstellung dieses Onlineangebotes gem. Art. 6 Abs. 1 lit. f DSGVO i.V.m. Art. 28 DSGVO (Abschluss Auftragsverarbeitungsvertrag).\n\n#### Erhebung von Zugriffsdaten und Logfiles\nWir, bzw. unser Hostinganbieter, erhebt auf Grundlage unserer berechtigten Interessen im Sinne des Art. 6 Abs. 1 lit. f. DSGVO Daten √ºber jeden Zugriff auf den Server, auf dem sich dieser Dienst befindet (sogenannte Serverlogfiles). Zu den Zugriffsdaten geh√∂ren Name der abgerufenen Webseite, Datei, Datum und Uhrzeit des Abrufs, √ºbertragene Datenmenge, Meldung √ºber erfolgreichen Abruf, Browsertyp nebst Version, das Betriebssystem des Nutzers, Referrer URL (die zuvor besuchte Seite), IP-Adresse und der anfragende Provider.\n\nLogfile-Informationen werden aus Sicherheitsgr√ºnden (z.B. zur Aufkl√§rung von Missbrauchs- oder Betrugshandlungen) f√ºr die Dauer von maximal 7 Tagen gespeichert und danach gel√∂scht. Daten, deren weitere Aufbewahrung zu Beweiszwecken erforderlich ist, sind bis zur endg√ºltigen Kl√§rung des jeweiligen Vorfalls von der L√∂schung ausgenommen.\n\n#### Google Universal Analytics\nWir setzen auf Grundlage unserer berechtigten Interessen (d.h. Interesse an der Analyse, Optimierung und wirtschaftlichem Betrieb unseres Onlineangebotes im Sinne des Art. 6 Abs. 1 lit. f. DSGVO) Google Analytics, einen Webanalysedienst der Google LLC (‚ÄûGoogle‚Äú) ein. Google verwendet Cookies. Die durch das Cookie erzeugten Informationen √ºber Benutzung des Onlineangebotes durch die Nutzer werden in der Regel an einen Server von Google in den USA √ºbertragen und dort gespeichert.\n\nGoogle ist unter dem Privacy-Shield-Abkommen zertifiziert und bietet hierdurch eine Garantie, das europ√§ische Datenschutzrecht einzuhalten (https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active).\n\nGoogle wird diese Informationen in unserem Auftrag benutzen, um die Nutzung unseres Onlineangebotes durch die Nutzer auszuwerten, um Reports √ºber die Aktivit√§ten innerhalb dieses Onlineangebotes zusammenzustellen und um weitere, mit der Nutzung dieses Onlineangebotes und der Internetnutzung verbundene Dienstleistungen, uns gegen√ºber zu erbringen. Dabei k√∂nnen aus den verarbeiteten Daten pseudonyme Nutzungsprofile der Nutzer erstellt werden.\n\nWir setzen Google Analytics in der Ausgestaltung als ‚ÄûUniversal-Analytics‚Äú ein. ‚ÄûUniversal Analytics‚Äú bezeichnet ein Verfahren von Google Analytics, bei dem die Nutzeranalyse auf Grundlage einer pseudonymen Nutzer-ID erfolgt und damit ein pseudonymes Profil des Nutzers mit Informationen aus der Nutzung verschiedener Ger√§ten erstellt wird (sog. ‚ÄûCross-Device-Tracking‚Äú).\n\nWir setzen Google Analytics nur mit aktivierter IP-Anonymisierung ein. Das bedeutet, die IP-Adresse der Nutzer wird von Google innerhalb von Mitgliedstaaten der Europ√§ischen Union oder in anderen Vertragsstaaten des Abkommens √ºber den Europ√§ischen Wirtschaftsraum gek√ºrzt. Nur in Ausnahmef√§llen wird die volle IP-Adresse an einen Server von Google in den USA √ºbertragen und dort gek√ºrzt.\n\nDie von dem Browser des Nutzers √ºbermittelte IP-Adresse wird nicht mit anderen Daten von Google zusammengef√ºhrt. Die Nutzer k√∂nnen die Speicherung der Cookies durch eine entsprechende Einstellung ihrer Browser-Software verhindern; die Nutzer k√∂nnen dar√ºber hinaus die Erfassung der durch das Cookie erzeugten und auf ihre Nutzung des Onlineangebotes bezogenen Daten an Google sowie die Verarbeitung dieser Daten durch Google verhindern, indem sie das unter folgendem Link verf√ºgbare Browser-Plugin herunterladen und installieren: http://tools.google.com/dlpage/gaoptout?hl=de.\n\nWeitere Informationen zur Datennutzung durch Google, Einstellungs- und Widerspruchsm√∂glichkeiten erfahren Sie auf den Webseiten von Google: https://www.google.com/intl/de/policies/privacy/partners (‚ÄûDatennutzung durch Google bei Ihrer Nutzung von Websites oder Apps unserer Partner‚Äú), http://www.google.com/policies/technologies/ads (‚ÄûDatennutzung zu Werbezwecken‚Äú), http://www.google.de/settings/ads (‚ÄûInformationen verwalten, die Google verwendet, um Ihnen Werbung einzublenden‚Äú).\n\n#### Youtube\nWir binden die Videos der Plattform ‚ÄúYouTube‚Äù des Anbieters Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA, ein. Datenschutzerkl√§rung: https://www.google.com/policies/privacy/, Opt-Out: https://adssettings.google.com/authenticated.\n\n[Erstellt mit Datenschutz-Generator.de von RA Dr. Thomas Schwenke](https://datenschutz-generator.de/)","updated":"2020-06-06T15:14:33.488Z","path":"imprint/index.html","comments":1,"layout":"page","_id":"ckb3slhhe000440mq7e6kbx1o","content":"<h1>Imprint</h1>\n<h3 id=\"Legal-Disclosure-Privacy-Statement\">Legal Disclosure &amp; Privacy Statement</h3>\n<p>Information in accordance with section 5 German TMG</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Ferdinand M√ºtsch</span><br><span class=\"line\">Vorholzstra√üe 11 </span><br><span class=\"line\">76137 Karlsruhe</span><br><span class=\"line\">Germany</span><br></pre></td></tr></table></figure>\n<h3 id=\"Contact\">Contact</h3>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Telephone: 0Ô∏è‚É£1Ô∏è‚É£7Ô∏è‚É£6Ô∏è‚É£4Ô∏è‚É£5Ô∏è‚É£6Ô∏è‚É£4Ô∏è‚É£1Ô∏è‚É£9Ô∏è‚É£7Ô∏è‚É£4Ô∏è‚É£</span><br><span class=\"line\">E-Mail: ferdinand@muetsch.io</span><br><span class=\"line\">Web: www.muetsch.io</span><br></pre></td></tr></table></figure>\n<h3 id=\"Disclaimer\">Disclaimer</h3>\n<h4 id=\"Accountability-for-content\">Accountability for content</h4>\n<p>The contents of our pages have been created with the utmost care. However, we cannot guarantee the contents‚Äô accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this context, please note that we are accordingly not obliged to monitor merely the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per ¬ß¬ß 8 to 10 of the Telemedia Act (TMG).</p>\n<h4 id=\"Accountability-for-links\">Accountability for links</h4>\n<p>Responsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately.</p>\n<h4 id=\"Copyright\">Copyright</h4>\n<p>Our web pages and their contents are subject to German copyright law. Unless expressly permitted by law (¬ß 44a et seq. of the copyright law), every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are allowed only for private use, so must not serve either directly or indirectly for earnings. Unauthorized utilization of copyrighted works is punishable (¬ß 106 of the copyright law).</p>\n<h4 id=\"General\">General</h4>\n<p>Your personal data (e.g. title, name, house address, e-mail address, phone number, bank details, credit card number) are processed by us only in accordance with the provisions of German data privacy laws. The following provisions describe the type, scope and purpose of collecting, processing and utilizing personal data. This data privacy policy applies only to our web pages. If links on our pages route you to other pages, please inquire there about how your data are handled in such cases.</p>\n<h4 id=\"Inventory-data\">Inventory data</h4>\n<p>(1) Your personal data, insofar as these are necessary for this contractual relationship (inventory data) in terms of its establishment, organization of content and modifications, are used exclusively for fulfilling the contract. For goods to be delivered, for instance, your name and address must be relayed to the supplier of the goods.<br>\n(2) Without your explicit consent or a legal basis, your personal data are not passed on to third parties outside the scope of fulfilling this contract. After completion of the contract, your data are blocked against further use. After expiry of deadlines as per tax-related and commercial regulations, these data are deleted unless you have expressly consented to their further use.</p>\n<h4 id=\"Information-about-cookies\">Information about cookies</h4>\n<p>(1) To optimize our web presence, we use cookies. These are small text files stored in your computer‚Äôs main memory. These cookies are deleted after you close the browser. Other cookies remain on your computer (long-term cookies) and permit its recognition on your next visit. This allows us to improve your access to our site.<br>\n(2) You can prevent storage of cookies by choosing a ‚Äúdisable cookies‚Äù option in your browser settings. But this can limit the functionality of our Internet offers as a result.</p>\n<h4 id=\"Disclosure\">Disclosure</h4>\n<p>According to the Federal Data Protection Act, you have a right to free-of-charge information about your stored data, and possibly entitlement to correction, blocking or deletion of such data. Inquiries can be directed to the following e-mail addresses: ( <a href=\"mailto:ferdinand@muetsch.io\">ferdinand@muetsch.io</a> )</p>\n<p>Source: <a href=\"http://www.twigg.de/\" target=\"_blank\" rel=\"noopener\">twiggs translations</a></p>\n<h3 id=\"German-Privacy-Policy-Datenschutzerklarung\">German Privacy Policy (Datenschutzerkl√§rung)</h3>\n<p>Diese Datenschutzerkl√§rung kl√§rt Sie √ºber die Art, den Umfang und Zweck der Verarbeitung von personenbezogenen Daten (nachfolgend kurz ‚ÄûDaten‚Äú) innerhalb unseres Onlineangebotes und der mit ihm verbundenen Webseiten, Funktionen und Inhalte sowie externen Onlinepr√§senzen, wie z.B. unser Social Media Profile auf. (nachfolgend gemeinsam bezeichnet als ‚ÄûOnlineangebot‚Äú). Im Hinblick auf die verwendeten Begrifflichkeiten, wie z.B. ‚ÄûVerarbeitung‚Äú oder ‚ÄûVerantwortlicher‚Äú verweisen wir auf die Definitionen im Art. 4 der Datenschutzgrundverordnung (DSGVO).</p>\n<h4 id=\"Arten-der-verarbeiteten-Daten\">Arten der verarbeiteten Daten:</h4>\n<ul>\n<li>Bestandsdaten (z.B., Namen, Adressen).</li>\n<li>Kontaktdaten (z.B., E-Mail, Telefonnummern).</li>\n<li>Inhaltsdaten (z.B., Texteingaben, Fotografien, Videos).</li>\n<li>Nutzungsdaten (z.B., besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten).</li>\n<li>Meta-/Kommunikationsdaten (z.B., Ger√§te-Informationen, IP-Adressen).</li>\n</ul>\n<h4 id=\"Kategorien-betroffener-Personen\">Kategorien betroffener Personen</h4>\n<p>Besucher und Nutzer des Onlineangebotes (Nachfolgend bezeichnen wir die betroffenen Personen zusammenfassend auch als ‚ÄûNutzer‚Äú).</p>\n<h4 id=\"Zweck-der-Verarbeitung\">Zweck der Verarbeitung</h4>\n<ul>\n<li>Zurverf√ºgungstellung des Onlineangebotes, seiner Funktionen und Inhalte.</li>\n<li>Beantwortung von Kontaktanfragen und Kommunikation mit Nutzern.</li>\n<li>Sicherheitsma√ünahmen.</li>\n<li>Reichweitenmessung/Marketing</li>\n</ul>\n<h4 id=\"Verwendete-Begrifflichkeiten\">Verwendete Begrifflichkeiten</h4>\n<p>‚ÄûPersonenbezogene Daten‚Äú sind alle Informationen, die sich auf eine identifizierte oder identifizierbare nat√ºrliche Person (im Folgenden ‚Äûbetroffene Person‚Äú) beziehen; als identifizierbar wird eine nat√ºrliche Person angesehen, die direkt oder indirekt, insbesondere mittels Zuordnung zu einer Kennung wie einem Namen, zu einer Kennnummer, zu Standortdaten, zu einer Online-Kennung (z.B. Cookie) oder zu einem oder mehreren besonderen Merkmalen identifiziert werden kann, die Ausdruck der physischen, physiologischen, genetischen, psychischen, wirtschaftlichen, kulturellen oder sozialen Identit√§t dieser nat√ºrlichen Person sind.</p>\n<p>‚ÄûVerarbeitung‚Äú ist jeder mit oder ohne Hilfe automatisierter Verfahren ausgef√ºhrten Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten. Der Begriff reicht weit und umfasst praktisch jeden Umgang mit Daten.</p>\n<p>Als ‚ÄûVerantwortlicher‚Äú wird die nat√ºrliche oder juristische Person, Beh√∂rde, Einrichtung oder andere Stelle, die allein oder gemeinsam mit anderen √ºber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten entscheidet, bezeichnet.</p>\n<h4 id=\"Masgebliche-Rechtsgrundlagen\">Ma√ügebliche Rechtsgrundlagen</h4>\n<p>Nach Ma√ügabe des Art. 13 DSGVO teilen wir Ihnen die Rechtsgrundlagen unserer Datenverarbeitungen mit. Sofern die Rechtsgrundlage in der Datenschutzerkl√§rung nicht genannt wird, gilt Folgendes: Die Rechtsgrundlage f√ºr die Einholung von Einwilligungen ist Art. 6 Abs. 1 lit. a und Art. 7 DSGVO, die Rechtsgrundlage f√ºr die Verarbeitung zur Erf√ºllung unserer Leistungen und Durchf√ºhrung vertraglicher Ma√ünahmen sowie Beantwortung von Anfragen ist Art. 6 Abs. 1 lit. b DSGVO, die Rechtsgrundlage f√ºr die Verarbeitung zur Erf√ºllung unserer rechtlichen Verpflichtungen ist Art. 6 Abs. 1 lit. c DSGVO, und die Rechtsgrundlage f√ºr die Verarbeitung zur Wahrung unserer berechtigten Interessen ist Art. 6 Abs. 1 lit. f DSGVO. F√ºr den Fall, dass lebenswichtige Interessen der betroffenen Person oder einer anderen nat√ºrlichen Person eine Verarbeitung personenbezogener Daten erforderlich machen, dient Art. 6 Abs. 1 lit. d DSGVO als Rechtsgrundlage.</p>\n<h4 id=\"Zusammenarbeit-mit-Auftragsverarbeitern-und-Dritten\">Zusammenarbeit mit Auftragsverarbeitern und Dritten</h4>\n<p>Sofern wir im Rahmen unserer Verarbeitung Daten gegen√ºber anderen Personen und Unternehmen (Auftragsverarbeitern oder Dritten) offenbaren, sie an diese √ºbermitteln oder ihnen sonst Zugriff auf die Daten gew√§hren, erfolgt dies nur auf Grundlage einer gesetzlichen Erlaubnis (z.B. wenn eine √úbermittlung der Daten an Dritte, wie an Zahlungsdienstleister, gem. Art. 6 Abs. 1 lit. b DSGVO zur Vertragserf√ºllung erforderlich ist), Sie eingewilligt haben, eine rechtliche Verpflichtung dies vorsieht oder auf Grundlage unserer berechtigten Interessen (z.B. beim Einsatz von Beauftragten, Webhostern, etc.).</p>\n<p>Sofern wir Dritte mit der Verarbeitung von Daten auf Grundlage eines sog. ‚ÄûAuftragsverarbeitungsvertrages‚Äú beauftragen, geschieht dies auf Grundlage des Art. 28 DSGVO.</p>\n<h4 id=\"Ubermittlungen-in-Drittlander\">√úbermittlungen in Drittl√§nder</h4>\n<p>Sofern wir Daten in einem Drittland (d.h. au√üerhalb der Europ√§ischen Union (EU) oder des Europ√§ischen Wirtschaftsraums (EWR)) verarbeiten oder dies im Rahmen der Inanspruchnahme von Diensten Dritter oder Offenlegung, bzw. √úbermittlung von Daten an Dritte geschieht, erfolgt dies nur, wenn es zur Erf√ºllung unserer (vor)vertraglichen Pflichten, auf Grundlage Ihrer Einwilligung, aufgrund einer rechtlichen Verpflichtung oder auf Grundlage unserer berechtigten Interessen geschieht. Vorbehaltlich gesetzlicher oder vertraglicher Erlaubnisse, verarbeiten oder lassen wir die Daten in einem Drittland nur beim Vorliegen der besonderen Voraussetzungen der Art. 44 ff. DSGVO verarbeiten. D.h. die Verarbeitung erfolgt z.B. auf Grundlage besonderer Garantien, wie der offiziell anerkannten Feststellung eines der EU entsprechenden Datenschutzniveaus (z.B. f√ºr die USA durch das ‚ÄûPrivacy Shield‚Äú) oder Beachtung offiziell anerkannter spezieller vertraglicher Verpflichtungen (so genannte ‚ÄûStandardvertragsklauseln‚Äú).</p>\n<h4 id=\"Rechte-der-betroffenen-Personen\">Rechte der betroffenen Personen</h4>\n<p>Sie haben das Recht, eine Best√§tigung dar√ºber zu verlangen, ob betreffende Daten verarbeitet werden und auf Auskunft √ºber diese Daten sowie auf weitere Informationen und Kopie der Daten entsprechend Art. 15 DSGVO.</p>\n<p>Sie haben entsprechend. Art. 16 DSGVO das Recht, die Vervollst√§ndigung der Sie betreffenden Daten oder die Berichtigung der Sie betreffenden unrichtigen Daten zu verlangen.</p>\n<p>Sie haben nach Ma√ügabe des Art. 17 DSGVO das Recht zu verlangen, dass betreffende Daten unverz√ºglich gel√∂scht werden, bzw. alternativ nach Ma√ügabe des Art. 18 DSGVO eine Einschr√§nkung der Verarbeitung der Daten zu verlangen.</p>\n<p>Sie haben das Recht zu verlangen, dass die Sie betreffenden Daten, die Sie uns bereitgestellt haben nach Ma√ügabe des Art. 20 DSGVO zu erhalten und deren √úbermittlung an andere Verantwortliche zu fordern.</p>\n<p>Sie haben ferner gem. Art. 77 DSGVO das Recht, eine Beschwerde bei der zust√§ndigen Aufsichtsbeh√∂rde einzureichen.</p>\n<h4 id=\"Widerrufsrecht\">Widerrufsrecht</h4>\n<p>Sie haben das Recht, erteilte Einwilligungen gem. Art. 7 Abs. 3 DSGVO mit Wirkung f√ºr die Zukunft zu widerrufen</p>\n<h4 id=\"Widerspruchsrecht\">Widerspruchsrecht</h4>\n<p>Sie k√∂nnen der k√ºnftigen Verarbeitung der Sie betreffenden Daten nach Ma√ügabe des Art. 21 DSGVO jederzeit widersprechen. Der Widerspruch kann insbesondere gegen die Verarbeitung f√ºr Zwecke der Direktwerbung erfolgen.</p>\n<h4 id=\"Cookies-und-Widerspruchsrecht-bei-Direktwerbung\">Cookies und Widerspruchsrecht bei Direktwerbung</h4>\n<p>Als ‚ÄûCookies‚Äú werden kleine Dateien bezeichnet, die auf Rechnern der Nutzer gespeichert werden. Innerhalb der Cookies k√∂nnen unterschiedliche Angaben gespeichert werden. Ein Cookie dient prim√§r dazu, die Angaben zu einem Nutzer (bzw. dem Ger√§t auf dem das Cookie gespeichert ist) w√§hrend oder auch nach seinem Besuch innerhalb eines Onlineangebotes zu speichern. Als tempor√§re Cookies, bzw. ‚ÄûSession-Cookies‚Äú oder ‚Äûtransiente Cookies‚Äú, werden Cookies bezeichnet, die gel√∂scht werden, nachdem ein Nutzer ein Onlineangebot verl√§sst und seinen Browser schlie√üt. In einem solchen Cookie kann z.B. der Inhalt eines Warenkorbs in einem Onlineshop oder ein Login-Staus gespeichert werden. Als ‚Äûpermanent‚Äú oder ‚Äûpersistent‚Äú werden Cookies bezeichnet, die auch nach dem Schlie√üen des Browsers gespeichert bleiben. So kann z.B. der Login-Status gespeichert werden, wenn die Nutzer diese nach mehreren Tagen aufsuchen. Ebenso k√∂nnen in einem solchen Cookie die Interessen der Nutzer gespeichert werden, die f√ºr Reichweitenmessung oder Marketingzwecke verwendet werden. Als ‚ÄûThird-Party-Cookie‚Äú werden Cookies bezeichnet, die von anderen Anbietern als dem Verantwortlichen, der das Onlineangebot betreibt, angeboten werden (andernfalls, wenn es nur dessen Cookies sind spricht man von ‚ÄûFirst-Party Cookies‚Äú).</p>\n<p>Wir k√∂nnen tempor√§re und permanente Cookies einsetzen und kl√§ren hier√ºber im Rahmen unserer Datenschutzerkl√§rung auf.</p>\n<p>Falls die Nutzer nicht m√∂chten, dass Cookies auf ihrem Rechner gespeichert werden, werden sie gebeten die entsprechende Option in den Systemeinstellungen ihres Browsers zu deaktivieren. Gespeicherte Cookies k√∂nnen in den Systemeinstellungen des Browsers gel√∂scht werden. Der Ausschluss von Cookies kann zu Funktionseinschr√§nkungen dieses Onlineangebotes f√ºhren.</p>\n<p>Ein genereller Widerspruch gegen den Einsatz der zu Zwecken des Onlinemarketing eingesetzten Cookies kann bei einer Vielzahl der Dienste, vor allem im Fall des Trackings, √ºber die US-amerikanische Seite <a href=\"http://www.aboutads.info/choices/\" target=\"_blank\" rel=\"noopener\">http://www.aboutads.info/choices/</a> oder die EU-Seite <a href=\"http://www.youronlinechoices.com/\" target=\"_blank\" rel=\"noopener\">http://www.youronlinechoices.com/</a> erkl√§rt werden. Des Weiteren kann die Speicherung von Cookies mittels deren Abschaltung in den Einstellungen des Browsers erreicht werden. Bitte beachten Sie, dass dann gegebenenfalls nicht alle Funktionen dieses Onlineangebotes genutzt werden k√∂nnen.</p>\n<h4 id=\"Loschung-von-Daten\">L√∂schung von Daten</h4>\n<p>Die von uns verarbeiteten Daten werden nach Ma√ügabe der Art. 17 und 18 DSGVO gel√∂scht oder in ihrer Verarbeitung eingeschr√§nkt. Sofern nicht im Rahmen dieser Datenschutzerkl√§rung ausdr√ºcklich angegeben, werden die bei uns gespeicherten Daten gel√∂scht, sobald sie f√ºr ihre Zweckbestimmung nicht mehr erforderlich sind und der L√∂schung keine gesetzlichen Aufbewahrungspflichten entgegenstehen. Sofern die Daten nicht gel√∂scht werden, weil sie f√ºr andere und gesetzlich zul√§ssige Zwecke erforderlich sind, wird deren Verarbeitung eingeschr√§nkt. D.h. die Daten werden gesperrt und nicht f√ºr andere Zwecke verarbeitet. Das gilt z.B. f√ºr Daten, die aus handels- oder steuerrechtlichen Gr√ºnden aufbewahrt werden m√ºssen.</p>\n<p>Nach gesetzlichen Vorgaben in Deutschland erfolgt die Aufbewahrung insbesondere f√ºr 6 Jahre gem√§√ü ¬ß 257 Abs. 1 HGB (Handelsb√ºcher, Inventare, Er√∂ffnungsbilanzen, Jahresabschl√ºsse, Handelsbriefe, Buchungsbelege, etc.) sowie f√ºr 10 Jahre gem√§√ü ¬ß 147 Abs. 1 AO (B√ºcher, Aufzeichnungen, Lageberichte, Buchungsbelege, Handels- und Gesch√§ftsbriefe, F√ºr Besteuerung relevante Unterlagen, etc.).</p>\n<p>Nach gesetzlichen Vorgaben in √ñsterreich erfolgt die Aufbewahrung insbesondere f√ºr 7 J gem√§√ü ¬ß 132 Abs. 1 BAO (Buchhaltungsunterlagen, Belege/Rechnungen, Konten, Belege, Gesch√§ftspapiere, Aufstellung der Einnahmen und Ausgaben, etc.), f√ºr 22 Jahre im Zusammenhang mit Grundst√ºcken und f√ºr 10 Jahre bei Unterlagen im Zusammenhang mit elektronisch erbrachten Leistungen, Telekommunikations-, Rundfunk- und Fernsehleistungen, die an Nichtunternehmer in EU-Mitgliedstaaten erbracht werden und f√ºr die der Mini-One-Stop-Shop (MOSS) in Anspruch genommen wird.</p>\n<h4 id=\"Hosting\">Hosting</h4>\n<p>Die von uns in Anspruch genommenen Hosting-Leistungen dienen der Zurverf√ºgungstellung der folgenden Leistungen: Infrastruktur- und Plattformdienstleistungen, Rechenkapazit√§t, Speicherplatz und Datenbankdienste, Sicherheitsleistungen sowie technische Wartungsleistungen, die wir zum Zwecke des Betriebs dieses Onlineangebotes einsetzen.</p>\n<p>Hierbei verarbeiten wir, bzw. unser Hostinganbieter Bestandsdaten, Kontaktdaten, Inhaltsdaten, Vertragsdaten, Nutzungsdaten, Meta- und Kommunikationsdaten von Kunden, Interessenten und Besuchern dieses Onlineangebotes auf Grundlage unserer berechtigten Interessen an einer effizienten und sicheren Zurverf√ºgungstellung dieses Onlineangebotes gem. Art. 6 Abs. 1 lit. f DSGVO i.V.m. Art. 28 DSGVO (Abschluss Auftragsverarbeitungsvertrag).</p>\n<h4 id=\"Erhebung-von-Zugriffsdaten-und-Logfiles\">Erhebung von Zugriffsdaten und Logfiles</h4>\n<p>Wir, bzw. unser Hostinganbieter, erhebt auf Grundlage unserer berechtigten Interessen im Sinne des Art. 6 Abs. 1 lit. f. DSGVO Daten √ºber jeden Zugriff auf den Server, auf dem sich dieser Dienst befindet (sogenannte Serverlogfiles). Zu den Zugriffsdaten geh√∂ren Name der abgerufenen Webseite, Datei, Datum und Uhrzeit des Abrufs, √ºbertragene Datenmenge, Meldung √ºber erfolgreichen Abruf, Browsertyp nebst Version, das Betriebssystem des Nutzers, Referrer URL (die zuvor besuchte Seite), IP-Adresse und der anfragende Provider.</p>\n<p>Logfile-Informationen werden aus Sicherheitsgr√ºnden (z.B. zur Aufkl√§rung von Missbrauchs- oder Betrugshandlungen) f√ºr die Dauer von maximal 7 Tagen gespeichert und danach gel√∂scht. Daten, deren weitere Aufbewahrung zu Beweiszwecken erforderlich ist, sind bis zur endg√ºltigen Kl√§rung des jeweiligen Vorfalls von der L√∂schung ausgenommen.</p>\n<h4 id=\"Google-Universal-Analytics\">Google Universal Analytics</h4>\n<p>Wir setzen auf Grundlage unserer berechtigten Interessen (d.h. Interesse an der Analyse, Optimierung und wirtschaftlichem Betrieb unseres Onlineangebotes im Sinne des Art. 6 Abs. 1 lit. f. DSGVO) Google Analytics, einen Webanalysedienst der Google LLC (‚ÄûGoogle‚Äú) ein. Google verwendet Cookies. Die durch das Cookie erzeugten Informationen √ºber Benutzung des Onlineangebotes durch die Nutzer werden in der Regel an einen Server von Google in den USA √ºbertragen und dort gespeichert.</p>\n<p>Google ist unter dem Privacy-Shield-Abkommen zertifiziert und bietet hierdurch eine Garantie, das europ√§ische Datenschutzrecht einzuhalten (<a href=\"https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&amp;status=Active\" target=\"_blank\" rel=\"noopener\">https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&amp;status=Active</a>).</p>\n<p>Google wird diese Informationen in unserem Auftrag benutzen, um die Nutzung unseres Onlineangebotes durch die Nutzer auszuwerten, um Reports √ºber die Aktivit√§ten innerhalb dieses Onlineangebotes zusammenzustellen und um weitere, mit der Nutzung dieses Onlineangebotes und der Internetnutzung verbundene Dienstleistungen, uns gegen√ºber zu erbringen. Dabei k√∂nnen aus den verarbeiteten Daten pseudonyme Nutzungsprofile der Nutzer erstellt werden.</p>\n<p>Wir setzen Google Analytics in der Ausgestaltung als ‚ÄûUniversal-Analytics‚Äú ein. ‚ÄûUniversal Analytics‚Äú bezeichnet ein Verfahren von Google Analytics, bei dem die Nutzeranalyse auf Grundlage einer pseudonymen Nutzer-ID erfolgt und damit ein pseudonymes Profil des Nutzers mit Informationen aus der Nutzung verschiedener Ger√§ten erstellt wird (sog. ‚ÄûCross-Device-Tracking‚Äú).</p>\n<p>Wir setzen Google Analytics nur mit aktivierter IP-Anonymisierung ein. Das bedeutet, die IP-Adresse der Nutzer wird von Google innerhalb von Mitgliedstaaten der Europ√§ischen Union oder in anderen Vertragsstaaten des Abkommens √ºber den Europ√§ischen Wirtschaftsraum gek√ºrzt. Nur in Ausnahmef√§llen wird die volle IP-Adresse an einen Server von Google in den USA √ºbertragen und dort gek√ºrzt.</p>\n<p>Die von dem Browser des Nutzers √ºbermittelte IP-Adresse wird nicht mit anderen Daten von Google zusammengef√ºhrt. Die Nutzer k√∂nnen die Speicherung der Cookies durch eine entsprechende Einstellung ihrer Browser-Software verhindern; die Nutzer k√∂nnen dar√ºber hinaus die Erfassung der durch das Cookie erzeugten und auf ihre Nutzung des Onlineangebotes bezogenen Daten an Google sowie die Verarbeitung dieser Daten durch Google verhindern, indem sie das unter folgendem Link verf√ºgbare Browser-Plugin herunterladen und installieren: <a href=\"http://tools.google.com/dlpage/gaoptout?hl=de\" target=\"_blank\" rel=\"noopener\">http://tools.google.com/dlpage/gaoptout?hl=de</a>.</p>\n<p>Weitere Informationen zur Datennutzung durch Google, Einstellungs- und Widerspruchsm√∂glichkeiten erfahren Sie auf den Webseiten von Google: <a href=\"https://www.google.com/intl/de/policies/privacy/partners\" target=\"_blank\" rel=\"noopener\">https://www.google.com/intl/de/policies/privacy/partners</a> (‚ÄûDatennutzung durch Google bei Ihrer Nutzung von Websites oder Apps unserer Partner‚Äú), <a href=\"http://www.google.com/policies/technologies/ads\" target=\"_blank\" rel=\"noopener\">http://www.google.com/policies/technologies/ads</a> (‚ÄûDatennutzung zu Werbezwecken‚Äú), <a href=\"http://www.google.de/settings/ads\" target=\"_blank\" rel=\"noopener\">http://www.google.de/settings/ads</a> (‚ÄûInformationen verwalten, die Google verwendet, um Ihnen Werbung einzublenden‚Äú).</p>\n<h4 id=\"Youtube\">Youtube</h4>\n<p>Wir binden die Videos der Plattform ‚ÄúYouTube‚Äù des Anbieters Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA, ein. Datenschutzerkl√§rung: <a href=\"https://www.google.com/policies/privacy/\" target=\"_blank\" rel=\"noopener\">https://www.google.com/policies/privacy/</a>, Opt-Out: <a href=\"https://adssettings.google.com/authenticated\" target=\"_blank\" rel=\"noopener\">https://adssettings.google.com/authenticated</a>.</p>\n<p><a href=\"https://datenschutz-generator.de/\" target=\"_blank\" rel=\"noopener\">Erstellt mit Datenschutz-Generator.de von RA Dr. Thomas Schwenke</a></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1>Imprint</h1>\n<h3 id=\"Legal-Disclosure-Privacy-Statement\">Legal Disclosure &amp; Privacy Statement</h3>\n<p>Information in accordance with section 5 German TMG</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Ferdinand M√ºtsch</span><br><span class=\"line\">Vorholzstra√üe 11 </span><br><span class=\"line\">76137 Karlsruhe</span><br><span class=\"line\">Germany</span><br></pre></td></tr></table></figure>\n<h3 id=\"Contact\">Contact</h3>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Telephone: 0Ô∏è‚É£1Ô∏è‚É£7Ô∏è‚É£6Ô∏è‚É£4Ô∏è‚É£5Ô∏è‚É£6Ô∏è‚É£4Ô∏è‚É£1Ô∏è‚É£9Ô∏è‚É£7Ô∏è‚É£4Ô∏è‚É£</span><br><span class=\"line\">E-Mail: ferdinand@muetsch.io</span><br><span class=\"line\">Web: www.muetsch.io</span><br></pre></td></tr></table></figure>\n<h3 id=\"Disclaimer\">Disclaimer</h3>\n<h4 id=\"Accountability-for-content\">Accountability for content</h4>\n<p>The contents of our pages have been created with the utmost care. However, we cannot guarantee the contents‚Äô accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this context, please note that we are accordingly not obliged to monitor merely the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per ¬ß¬ß 8 to 10 of the Telemedia Act (TMG).</p>\n<h4 id=\"Accountability-for-links\">Accountability for links</h4>\n<p>Responsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately.</p>\n<h4 id=\"Copyright\">Copyright</h4>\n<p>Our web pages and their contents are subject to German copyright law. Unless expressly permitted by law (¬ß 44a et seq. of the copyright law), every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are allowed only for private use, so must not serve either directly or indirectly for earnings. Unauthorized utilization of copyrighted works is punishable (¬ß 106 of the copyright law).</p>\n<h4 id=\"General\">General</h4>\n<p>Your personal data (e.g. title, name, house address, e-mail address, phone number, bank details, credit card number) are processed by us only in accordance with the provisions of German data privacy laws. The following provisions describe the type, scope and purpose of collecting, processing and utilizing personal data. This data privacy policy applies only to our web pages. If links on our pages route you to other pages, please inquire there about how your data are handled in such cases.</p>\n<h4 id=\"Inventory-data\">Inventory data</h4>\n<p>(1) Your personal data, insofar as these are necessary for this contractual relationship (inventory data) in terms of its establishment, organization of content and modifications, are used exclusively for fulfilling the contract. For goods to be delivered, for instance, your name and address must be relayed to the supplier of the goods.<br>\n(2) Without your explicit consent or a legal basis, your personal data are not passed on to third parties outside the scope of fulfilling this contract. After completion of the contract, your data are blocked against further use. After expiry of deadlines as per tax-related and commercial regulations, these data are deleted unless you have expressly consented to their further use.</p>\n<h4 id=\"Information-about-cookies\">Information about cookies</h4>\n<p>(1) To optimize our web presence, we use cookies. These are small text files stored in your computer‚Äôs main memory. These cookies are deleted after you close the browser. Other cookies remain on your computer (long-term cookies) and permit its recognition on your next visit. This allows us to improve your access to our site.<br>\n(2) You can prevent storage of cookies by choosing a ‚Äúdisable cookies‚Äù option in your browser settings. But this can limit the functionality of our Internet offers as a result.</p>\n<h4 id=\"Disclosure\">Disclosure</h4>\n<p>According to the Federal Data Protection Act, you have a right to free-of-charge information about your stored data, and possibly entitlement to correction, blocking or deletion of such data. Inquiries can be directed to the following e-mail addresses: ( <a href=\"mailto:ferdinand@muetsch.io\">ferdinand@muetsch.io</a> )</p>\n<p>Source: <a href=\"http://www.twigg.de/\" target=\"_blank\" rel=\"noopener\">twiggs translations</a></p>\n<h3 id=\"German-Privacy-Policy-Datenschutzerklarung\">German Privacy Policy (Datenschutzerkl√§rung)</h3>\n<p>Diese Datenschutzerkl√§rung kl√§rt Sie √ºber die Art, den Umfang und Zweck der Verarbeitung von personenbezogenen Daten (nachfolgend kurz ‚ÄûDaten‚Äú) innerhalb unseres Onlineangebotes und der mit ihm verbundenen Webseiten, Funktionen und Inhalte sowie externen Onlinepr√§senzen, wie z.B. unser Social Media Profile auf. (nachfolgend gemeinsam bezeichnet als ‚ÄûOnlineangebot‚Äú). Im Hinblick auf die verwendeten Begrifflichkeiten, wie z.B. ‚ÄûVerarbeitung‚Äú oder ‚ÄûVerantwortlicher‚Äú verweisen wir auf die Definitionen im Art. 4 der Datenschutzgrundverordnung (DSGVO).</p>\n<h4 id=\"Arten-der-verarbeiteten-Daten\">Arten der verarbeiteten Daten:</h4>\n<ul>\n<li>Bestandsdaten (z.B., Namen, Adressen).</li>\n<li>Kontaktdaten (z.B., E-Mail, Telefonnummern).</li>\n<li>Inhaltsdaten (z.B., Texteingaben, Fotografien, Videos).</li>\n<li>Nutzungsdaten (z.B., besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten).</li>\n<li>Meta-/Kommunikationsdaten (z.B., Ger√§te-Informationen, IP-Adressen).</li>\n</ul>\n<h4 id=\"Kategorien-betroffener-Personen\">Kategorien betroffener Personen</h4>\n<p>Besucher und Nutzer des Onlineangebotes (Nachfolgend bezeichnen wir die betroffenen Personen zusammenfassend auch als ‚ÄûNutzer‚Äú).</p>\n<h4 id=\"Zweck-der-Verarbeitung\">Zweck der Verarbeitung</h4>\n<ul>\n<li>Zurverf√ºgungstellung des Onlineangebotes, seiner Funktionen und Inhalte.</li>\n<li>Beantwortung von Kontaktanfragen und Kommunikation mit Nutzern.</li>\n<li>Sicherheitsma√ünahmen.</li>\n<li>Reichweitenmessung/Marketing</li>\n</ul>\n<h4 id=\"Verwendete-Begrifflichkeiten\">Verwendete Begrifflichkeiten</h4>\n<p>‚ÄûPersonenbezogene Daten‚Äú sind alle Informationen, die sich auf eine identifizierte oder identifizierbare nat√ºrliche Person (im Folgenden ‚Äûbetroffene Person‚Äú) beziehen; als identifizierbar wird eine nat√ºrliche Person angesehen, die direkt oder indirekt, insbesondere mittels Zuordnung zu einer Kennung wie einem Namen, zu einer Kennnummer, zu Standortdaten, zu einer Online-Kennung (z.B. Cookie) oder zu einem oder mehreren besonderen Merkmalen identifiziert werden kann, die Ausdruck der physischen, physiologischen, genetischen, psychischen, wirtschaftlichen, kulturellen oder sozialen Identit√§t dieser nat√ºrlichen Person sind.</p>\n<p>‚ÄûVerarbeitung‚Äú ist jeder mit oder ohne Hilfe automatisierter Verfahren ausgef√ºhrten Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten. Der Begriff reicht weit und umfasst praktisch jeden Umgang mit Daten.</p>\n<p>Als ‚ÄûVerantwortlicher‚Äú wird die nat√ºrliche oder juristische Person, Beh√∂rde, Einrichtung oder andere Stelle, die allein oder gemeinsam mit anderen √ºber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten entscheidet, bezeichnet.</p>\n<h4 id=\"Masgebliche-Rechtsgrundlagen\">Ma√ügebliche Rechtsgrundlagen</h4>\n<p>Nach Ma√ügabe des Art. 13 DSGVO teilen wir Ihnen die Rechtsgrundlagen unserer Datenverarbeitungen mit. Sofern die Rechtsgrundlage in der Datenschutzerkl√§rung nicht genannt wird, gilt Folgendes: Die Rechtsgrundlage f√ºr die Einholung von Einwilligungen ist Art. 6 Abs. 1 lit. a und Art. 7 DSGVO, die Rechtsgrundlage f√ºr die Verarbeitung zur Erf√ºllung unserer Leistungen und Durchf√ºhrung vertraglicher Ma√ünahmen sowie Beantwortung von Anfragen ist Art. 6 Abs. 1 lit. b DSGVO, die Rechtsgrundlage f√ºr die Verarbeitung zur Erf√ºllung unserer rechtlichen Verpflichtungen ist Art. 6 Abs. 1 lit. c DSGVO, und die Rechtsgrundlage f√ºr die Verarbeitung zur Wahrung unserer berechtigten Interessen ist Art. 6 Abs. 1 lit. f DSGVO. F√ºr den Fall, dass lebenswichtige Interessen der betroffenen Person oder einer anderen nat√ºrlichen Person eine Verarbeitung personenbezogener Daten erforderlich machen, dient Art. 6 Abs. 1 lit. d DSGVO als Rechtsgrundlage.</p>\n<h4 id=\"Zusammenarbeit-mit-Auftragsverarbeitern-und-Dritten\">Zusammenarbeit mit Auftragsverarbeitern und Dritten</h4>\n<p>Sofern wir im Rahmen unserer Verarbeitung Daten gegen√ºber anderen Personen und Unternehmen (Auftragsverarbeitern oder Dritten) offenbaren, sie an diese √ºbermitteln oder ihnen sonst Zugriff auf die Daten gew√§hren, erfolgt dies nur auf Grundlage einer gesetzlichen Erlaubnis (z.B. wenn eine √úbermittlung der Daten an Dritte, wie an Zahlungsdienstleister, gem. Art. 6 Abs. 1 lit. b DSGVO zur Vertragserf√ºllung erforderlich ist), Sie eingewilligt haben, eine rechtliche Verpflichtung dies vorsieht oder auf Grundlage unserer berechtigten Interessen (z.B. beim Einsatz von Beauftragten, Webhostern, etc.).</p>\n<p>Sofern wir Dritte mit der Verarbeitung von Daten auf Grundlage eines sog. ‚ÄûAuftragsverarbeitungsvertrages‚Äú beauftragen, geschieht dies auf Grundlage des Art. 28 DSGVO.</p>\n<h4 id=\"Ubermittlungen-in-Drittlander\">√úbermittlungen in Drittl√§nder</h4>\n<p>Sofern wir Daten in einem Drittland (d.h. au√üerhalb der Europ√§ischen Union (EU) oder des Europ√§ischen Wirtschaftsraums (EWR)) verarbeiten oder dies im Rahmen der Inanspruchnahme von Diensten Dritter oder Offenlegung, bzw. √úbermittlung von Daten an Dritte geschieht, erfolgt dies nur, wenn es zur Erf√ºllung unserer (vor)vertraglichen Pflichten, auf Grundlage Ihrer Einwilligung, aufgrund einer rechtlichen Verpflichtung oder auf Grundlage unserer berechtigten Interessen geschieht. Vorbehaltlich gesetzlicher oder vertraglicher Erlaubnisse, verarbeiten oder lassen wir die Daten in einem Drittland nur beim Vorliegen der besonderen Voraussetzungen der Art. 44 ff. DSGVO verarbeiten. D.h. die Verarbeitung erfolgt z.B. auf Grundlage besonderer Garantien, wie der offiziell anerkannten Feststellung eines der EU entsprechenden Datenschutzniveaus (z.B. f√ºr die USA durch das ‚ÄûPrivacy Shield‚Äú) oder Beachtung offiziell anerkannter spezieller vertraglicher Verpflichtungen (so genannte ‚ÄûStandardvertragsklauseln‚Äú).</p>\n<h4 id=\"Rechte-der-betroffenen-Personen\">Rechte der betroffenen Personen</h4>\n<p>Sie haben das Recht, eine Best√§tigung dar√ºber zu verlangen, ob betreffende Daten verarbeitet werden und auf Auskunft √ºber diese Daten sowie auf weitere Informationen und Kopie der Daten entsprechend Art. 15 DSGVO.</p>\n<p>Sie haben entsprechend. Art. 16 DSGVO das Recht, die Vervollst√§ndigung der Sie betreffenden Daten oder die Berichtigung der Sie betreffenden unrichtigen Daten zu verlangen.</p>\n<p>Sie haben nach Ma√ügabe des Art. 17 DSGVO das Recht zu verlangen, dass betreffende Daten unverz√ºglich gel√∂scht werden, bzw. alternativ nach Ma√ügabe des Art. 18 DSGVO eine Einschr√§nkung der Verarbeitung der Daten zu verlangen.</p>\n<p>Sie haben das Recht zu verlangen, dass die Sie betreffenden Daten, die Sie uns bereitgestellt haben nach Ma√ügabe des Art. 20 DSGVO zu erhalten und deren √úbermittlung an andere Verantwortliche zu fordern.</p>\n<p>Sie haben ferner gem. Art. 77 DSGVO das Recht, eine Beschwerde bei der zust√§ndigen Aufsichtsbeh√∂rde einzureichen.</p>\n<h4 id=\"Widerrufsrecht\">Widerrufsrecht</h4>\n<p>Sie haben das Recht, erteilte Einwilligungen gem. Art. 7 Abs. 3 DSGVO mit Wirkung f√ºr die Zukunft zu widerrufen</p>\n<h4 id=\"Widerspruchsrecht\">Widerspruchsrecht</h4>\n<p>Sie k√∂nnen der k√ºnftigen Verarbeitung der Sie betreffenden Daten nach Ma√ügabe des Art. 21 DSGVO jederzeit widersprechen. Der Widerspruch kann insbesondere gegen die Verarbeitung f√ºr Zwecke der Direktwerbung erfolgen.</p>\n<h4 id=\"Cookies-und-Widerspruchsrecht-bei-Direktwerbung\">Cookies und Widerspruchsrecht bei Direktwerbung</h4>\n<p>Als ‚ÄûCookies‚Äú werden kleine Dateien bezeichnet, die auf Rechnern der Nutzer gespeichert werden. Innerhalb der Cookies k√∂nnen unterschiedliche Angaben gespeichert werden. Ein Cookie dient prim√§r dazu, die Angaben zu einem Nutzer (bzw. dem Ger√§t auf dem das Cookie gespeichert ist) w√§hrend oder auch nach seinem Besuch innerhalb eines Onlineangebotes zu speichern. Als tempor√§re Cookies, bzw. ‚ÄûSession-Cookies‚Äú oder ‚Äûtransiente Cookies‚Äú, werden Cookies bezeichnet, die gel√∂scht werden, nachdem ein Nutzer ein Onlineangebot verl√§sst und seinen Browser schlie√üt. In einem solchen Cookie kann z.B. der Inhalt eines Warenkorbs in einem Onlineshop oder ein Login-Staus gespeichert werden. Als ‚Äûpermanent‚Äú oder ‚Äûpersistent‚Äú werden Cookies bezeichnet, die auch nach dem Schlie√üen des Browsers gespeichert bleiben. So kann z.B. der Login-Status gespeichert werden, wenn die Nutzer diese nach mehreren Tagen aufsuchen. Ebenso k√∂nnen in einem solchen Cookie die Interessen der Nutzer gespeichert werden, die f√ºr Reichweitenmessung oder Marketingzwecke verwendet werden. Als ‚ÄûThird-Party-Cookie‚Äú werden Cookies bezeichnet, die von anderen Anbietern als dem Verantwortlichen, der das Onlineangebot betreibt, angeboten werden (andernfalls, wenn es nur dessen Cookies sind spricht man von ‚ÄûFirst-Party Cookies‚Äú).</p>\n<p>Wir k√∂nnen tempor√§re und permanente Cookies einsetzen und kl√§ren hier√ºber im Rahmen unserer Datenschutzerkl√§rung auf.</p>\n<p>Falls die Nutzer nicht m√∂chten, dass Cookies auf ihrem Rechner gespeichert werden, werden sie gebeten die entsprechende Option in den Systemeinstellungen ihres Browsers zu deaktivieren. Gespeicherte Cookies k√∂nnen in den Systemeinstellungen des Browsers gel√∂scht werden. Der Ausschluss von Cookies kann zu Funktionseinschr√§nkungen dieses Onlineangebotes f√ºhren.</p>\n<p>Ein genereller Widerspruch gegen den Einsatz der zu Zwecken des Onlinemarketing eingesetzten Cookies kann bei einer Vielzahl der Dienste, vor allem im Fall des Trackings, √ºber die US-amerikanische Seite <a href=\"http://www.aboutads.info/choices/\" target=\"_blank\" rel=\"noopener\">http://www.aboutads.info/choices/</a> oder die EU-Seite <a href=\"http://www.youronlinechoices.com/\" target=\"_blank\" rel=\"noopener\">http://www.youronlinechoices.com/</a> erkl√§rt werden. Des Weiteren kann die Speicherung von Cookies mittels deren Abschaltung in den Einstellungen des Browsers erreicht werden. Bitte beachten Sie, dass dann gegebenenfalls nicht alle Funktionen dieses Onlineangebotes genutzt werden k√∂nnen.</p>\n<h4 id=\"Loschung-von-Daten\">L√∂schung von Daten</h4>\n<p>Die von uns verarbeiteten Daten werden nach Ma√ügabe der Art. 17 und 18 DSGVO gel√∂scht oder in ihrer Verarbeitung eingeschr√§nkt. Sofern nicht im Rahmen dieser Datenschutzerkl√§rung ausdr√ºcklich angegeben, werden die bei uns gespeicherten Daten gel√∂scht, sobald sie f√ºr ihre Zweckbestimmung nicht mehr erforderlich sind und der L√∂schung keine gesetzlichen Aufbewahrungspflichten entgegenstehen. Sofern die Daten nicht gel√∂scht werden, weil sie f√ºr andere und gesetzlich zul√§ssige Zwecke erforderlich sind, wird deren Verarbeitung eingeschr√§nkt. D.h. die Daten werden gesperrt und nicht f√ºr andere Zwecke verarbeitet. Das gilt z.B. f√ºr Daten, die aus handels- oder steuerrechtlichen Gr√ºnden aufbewahrt werden m√ºssen.</p>\n<p>Nach gesetzlichen Vorgaben in Deutschland erfolgt die Aufbewahrung insbesondere f√ºr 6 Jahre gem√§√ü ¬ß 257 Abs. 1 HGB (Handelsb√ºcher, Inventare, Er√∂ffnungsbilanzen, Jahresabschl√ºsse, Handelsbriefe, Buchungsbelege, etc.) sowie f√ºr 10 Jahre gem√§√ü ¬ß 147 Abs. 1 AO (B√ºcher, Aufzeichnungen, Lageberichte, Buchungsbelege, Handels- und Gesch√§ftsbriefe, F√ºr Besteuerung relevante Unterlagen, etc.).</p>\n<p>Nach gesetzlichen Vorgaben in √ñsterreich erfolgt die Aufbewahrung insbesondere f√ºr 7 J gem√§√ü ¬ß 132 Abs. 1 BAO (Buchhaltungsunterlagen, Belege/Rechnungen, Konten, Belege, Gesch√§ftspapiere, Aufstellung der Einnahmen und Ausgaben, etc.), f√ºr 22 Jahre im Zusammenhang mit Grundst√ºcken und f√ºr 10 Jahre bei Unterlagen im Zusammenhang mit elektronisch erbrachten Leistungen, Telekommunikations-, Rundfunk- und Fernsehleistungen, die an Nichtunternehmer in EU-Mitgliedstaaten erbracht werden und f√ºr die der Mini-One-Stop-Shop (MOSS) in Anspruch genommen wird.</p>\n<h4 id=\"Hosting\">Hosting</h4>\n<p>Die von uns in Anspruch genommenen Hosting-Leistungen dienen der Zurverf√ºgungstellung der folgenden Leistungen: Infrastruktur- und Plattformdienstleistungen, Rechenkapazit√§t, Speicherplatz und Datenbankdienste, Sicherheitsleistungen sowie technische Wartungsleistungen, die wir zum Zwecke des Betriebs dieses Onlineangebotes einsetzen.</p>\n<p>Hierbei verarbeiten wir, bzw. unser Hostinganbieter Bestandsdaten, Kontaktdaten, Inhaltsdaten, Vertragsdaten, Nutzungsdaten, Meta- und Kommunikationsdaten von Kunden, Interessenten und Besuchern dieses Onlineangebotes auf Grundlage unserer berechtigten Interessen an einer effizienten und sicheren Zurverf√ºgungstellung dieses Onlineangebotes gem. Art. 6 Abs. 1 lit. f DSGVO i.V.m. Art. 28 DSGVO (Abschluss Auftragsverarbeitungsvertrag).</p>\n<h4 id=\"Erhebung-von-Zugriffsdaten-und-Logfiles\">Erhebung von Zugriffsdaten und Logfiles</h4>\n<p>Wir, bzw. unser Hostinganbieter, erhebt auf Grundlage unserer berechtigten Interessen im Sinne des Art. 6 Abs. 1 lit. f. DSGVO Daten √ºber jeden Zugriff auf den Server, auf dem sich dieser Dienst befindet (sogenannte Serverlogfiles). Zu den Zugriffsdaten geh√∂ren Name der abgerufenen Webseite, Datei, Datum und Uhrzeit des Abrufs, √ºbertragene Datenmenge, Meldung √ºber erfolgreichen Abruf, Browsertyp nebst Version, das Betriebssystem des Nutzers, Referrer URL (die zuvor besuchte Seite), IP-Adresse und der anfragende Provider.</p>\n<p>Logfile-Informationen werden aus Sicherheitsgr√ºnden (z.B. zur Aufkl√§rung von Missbrauchs- oder Betrugshandlungen) f√ºr die Dauer von maximal 7 Tagen gespeichert und danach gel√∂scht. Daten, deren weitere Aufbewahrung zu Beweiszwecken erforderlich ist, sind bis zur endg√ºltigen Kl√§rung des jeweiligen Vorfalls von der L√∂schung ausgenommen.</p>\n<h4 id=\"Google-Universal-Analytics\">Google Universal Analytics</h4>\n<p>Wir setzen auf Grundlage unserer berechtigten Interessen (d.h. Interesse an der Analyse, Optimierung und wirtschaftlichem Betrieb unseres Onlineangebotes im Sinne des Art. 6 Abs. 1 lit. f. DSGVO) Google Analytics, einen Webanalysedienst der Google LLC (‚ÄûGoogle‚Äú) ein. Google verwendet Cookies. Die durch das Cookie erzeugten Informationen √ºber Benutzung des Onlineangebotes durch die Nutzer werden in der Regel an einen Server von Google in den USA √ºbertragen und dort gespeichert.</p>\n<p>Google ist unter dem Privacy-Shield-Abkommen zertifiziert und bietet hierdurch eine Garantie, das europ√§ische Datenschutzrecht einzuhalten (<a href=\"https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&amp;status=Active\" target=\"_blank\" rel=\"noopener\">https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&amp;status=Active</a>).</p>\n<p>Google wird diese Informationen in unserem Auftrag benutzen, um die Nutzung unseres Onlineangebotes durch die Nutzer auszuwerten, um Reports √ºber die Aktivit√§ten innerhalb dieses Onlineangebotes zusammenzustellen und um weitere, mit der Nutzung dieses Onlineangebotes und der Internetnutzung verbundene Dienstleistungen, uns gegen√ºber zu erbringen. Dabei k√∂nnen aus den verarbeiteten Daten pseudonyme Nutzungsprofile der Nutzer erstellt werden.</p>\n<p>Wir setzen Google Analytics in der Ausgestaltung als ‚ÄûUniversal-Analytics‚Äú ein. ‚ÄûUniversal Analytics‚Äú bezeichnet ein Verfahren von Google Analytics, bei dem die Nutzeranalyse auf Grundlage einer pseudonymen Nutzer-ID erfolgt und damit ein pseudonymes Profil des Nutzers mit Informationen aus der Nutzung verschiedener Ger√§ten erstellt wird (sog. ‚ÄûCross-Device-Tracking‚Äú).</p>\n<p>Wir setzen Google Analytics nur mit aktivierter IP-Anonymisierung ein. Das bedeutet, die IP-Adresse der Nutzer wird von Google innerhalb von Mitgliedstaaten der Europ√§ischen Union oder in anderen Vertragsstaaten des Abkommens √ºber den Europ√§ischen Wirtschaftsraum gek√ºrzt. Nur in Ausnahmef√§llen wird die volle IP-Adresse an einen Server von Google in den USA √ºbertragen und dort gek√ºrzt.</p>\n<p>Die von dem Browser des Nutzers √ºbermittelte IP-Adresse wird nicht mit anderen Daten von Google zusammengef√ºhrt. Die Nutzer k√∂nnen die Speicherung der Cookies durch eine entsprechende Einstellung ihrer Browser-Software verhindern; die Nutzer k√∂nnen dar√ºber hinaus die Erfassung der durch das Cookie erzeugten und auf ihre Nutzung des Onlineangebotes bezogenen Daten an Google sowie die Verarbeitung dieser Daten durch Google verhindern, indem sie das unter folgendem Link verf√ºgbare Browser-Plugin herunterladen und installieren: <a href=\"http://tools.google.com/dlpage/gaoptout?hl=de\" target=\"_blank\" rel=\"noopener\">http://tools.google.com/dlpage/gaoptout?hl=de</a>.</p>\n<p>Weitere Informationen zur Datennutzung durch Google, Einstellungs- und Widerspruchsm√∂glichkeiten erfahren Sie auf den Webseiten von Google: <a href=\"https://www.google.com/intl/de/policies/privacy/partners\" target=\"_blank\" rel=\"noopener\">https://www.google.com/intl/de/policies/privacy/partners</a> (‚ÄûDatennutzung durch Google bei Ihrer Nutzung von Websites oder Apps unserer Partner‚Äú), <a href=\"http://www.google.com/policies/technologies/ads\" target=\"_blank\" rel=\"noopener\">http://www.google.com/policies/technologies/ads</a> (‚ÄûDatennutzung zu Werbezwecken‚Äú), <a href=\"http://www.google.de/settings/ads\" target=\"_blank\" rel=\"noopener\">http://www.google.de/settings/ads</a> (‚ÄûInformationen verwalten, die Google verwendet, um Ihnen Werbung einzublenden‚Äú).</p>\n<h4 id=\"Youtube\">Youtube</h4>\n<p>Wir binden die Videos der Plattform ‚ÄúYouTube‚Äù des Anbieters Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA, ein. Datenschutzerkl√§rung: <a href=\"https://www.google.com/policies/privacy/\" target=\"_blank\" rel=\"noopener\">https://www.google.com/policies/privacy/</a>, Opt-Out: <a href=\"https://adssettings.google.com/authenticated\" target=\"_blank\" rel=\"noopener\">https://adssettings.google.com/authenticated</a>.</p>\n<p><a href=\"https://datenschutz-generator.de/\" target=\"_blank\" rel=\"noopener\">Erstellt mit Datenschutz-Generator.de von RA Dr. Thomas Schwenke</a></p>\n"}],"Post":[{"title":"Anchr.io ‚Äì Image uploads, bookmarks and shortlink service","date":"2015-12-01T21:47:35.000Z","_content":"\nI want to present my latest project called [Anchr.io](https://anchr.io). It claims to be a useful little helper or toolbox or the like for common tasks on the internet. The idea arised when someday I considered it useful to have a collection of web links or bookmarks ‚Äì like those you have in Chrome or Firefox ‚Äì accessible from everywhere without needing to synchronize your browser profile. Just like if you‚Äôre anywhere on an other PC, find a useful article on the internet and want to save it quickly for later at home. This is what Anchr‚Äôs **collections** feature does. It saves links ‚Äì with an optional description for easier search and separated into seperate categories / collections.\n\n![Anchr collections](images/anchr_2.jpg)\n\nThe second feature is to **upload images**. You can easily upload one or more photos from your computer oder mobile device and send them to friends or include them into forum posts or the like. Special with Anchr‚Äôs image hosting is that users are given the opportunity to client-sided **encrypt images** with a password. As a result no one without the password will ever see their photos‚Äô content.\n\n![Anchr images](/images/anchr_1.jpg)\n\nThe last feature are **shortlinks** ‚Äì actually not any different from those you know from [goo.gl](http://goo.gl) or [bit.ly](http://bit.ly). They‚Äôre useful if you have a very long web link including many query parameters, access tokens, session ids, special characters and the like and want to share them. Often special characters break the linking or your chat application has a maximum length for hyperlinks. Or you just want to keep clarity in your document or emails. In this case it can be very helpful to make the links as short as any possible ‚Äì to be precise of a length of 22 bytes with Anchr.\n\nAnchr‚Äôs focus is on ease and quickness of use ‚Äì short loading times, flat menu hierarchies, etc.\n\nAt the end just a few words about the technical aspect in addition. Anchr.io is separated clearly into backend and frontend or server- and client-application. Both are kept as modular as possible, trying to follow the MVC pattern. Interface between front- and backend is a REST API. The server is based in Node.js with a bunch of very cool new frameworks while the client is an Angular application. Concluding the development was both fun and such a good practice to me and the result will definitely make some of my daily processes a small little easier. I hope you give the app a try and leave me some feedback to dev(at)anchr.io.","source":"_posts/anchr-io-image-uploads-bookmarks-and-shortlink-service.md","raw":"---\ntitle: 'Anchr.io ‚Äì Image uploads, bookmarks and shortlink service'\ndate: 2015-12-01 22:47:35\ntags:\n---\n\nI want to present my latest project called [Anchr.io](https://anchr.io). It claims to be a useful little helper or toolbox or the like for common tasks on the internet. The idea arised when someday I considered it useful to have a collection of web links or bookmarks ‚Äì like those you have in Chrome or Firefox ‚Äì accessible from everywhere without needing to synchronize your browser profile. Just like if you‚Äôre anywhere on an other PC, find a useful article on the internet and want to save it quickly for later at home. This is what Anchr‚Äôs **collections** feature does. It saves links ‚Äì with an optional description for easier search and separated into seperate categories / collections.\n\n![Anchr collections](images/anchr_2.jpg)\n\nThe second feature is to **upload images**. You can easily upload one or more photos from your computer oder mobile device and send them to friends or include them into forum posts or the like. Special with Anchr‚Äôs image hosting is that users are given the opportunity to client-sided **encrypt images** with a password. As a result no one without the password will ever see their photos‚Äô content.\n\n![Anchr images](/images/anchr_1.jpg)\n\nThe last feature are **shortlinks** ‚Äì actually not any different from those you know from [goo.gl](http://goo.gl) or [bit.ly](http://bit.ly). They‚Äôre useful if you have a very long web link including many query parameters, access tokens, session ids, special characters and the like and want to share them. Often special characters break the linking or your chat application has a maximum length for hyperlinks. Or you just want to keep clarity in your document or emails. In this case it can be very helpful to make the links as short as any possible ‚Äì to be precise of a length of 22 bytes with Anchr.\n\nAnchr‚Äôs focus is on ease and quickness of use ‚Äì short loading times, flat menu hierarchies, etc.\n\nAt the end just a few words about the technical aspect in addition. Anchr.io is separated clearly into backend and frontend or server- and client-application. Both are kept as modular as possible, trying to follow the MVC pattern. Interface between front- and backend is a REST API. The server is based in Node.js with a bunch of very cool new frameworks while the client is an Angular application. Concluding the development was both fun and such a good practice to me and the result will definitely make some of my daily processes a small little easier. I hope you give the app a try and leave me some feedback to dev(at)anchr.io.","slug":"anchr-io-image-uploads-bookmarks-and-shortlink-service","published":1,"updated":"2020-06-06T15:14:33.457Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhh4000140mq0tgkie31","content":"<p>I want to present my latest project called <a href=\"https://anchr.io\" target=\"_blank\" rel=\"noopener\">Anchr.io</a>. It claims to be a useful little helper or toolbox or the like for common tasks on the internet. The idea arised when someday I considered it useful to have a collection of web links or bookmarks ‚Äì like those you have in Chrome or Firefox ‚Äì accessible from everywhere without needing to synchronize your browser profile. Just like if you‚Äôre anywhere on an other PC, find a useful article on the internet and want to save it quickly for later at home. This is what Anchr‚Äôs <strong>collections</strong> feature does. It saves links ‚Äì with an optional description for easier search and separated into seperate categories / collections.</p>\n<p><img src=\"images/anchr_2.jpg\" alt=\"Anchr collections\"></p>\n<p>The second feature is to <strong>upload images</strong>. You can easily upload one or more photos from your computer oder mobile device and send them to friends or include them into forum posts or the like. Special with Anchr‚Äôs image hosting is that users are given the opportunity to client-sided <strong>encrypt images</strong> with a password. As a result no one without the password will ever see their photos‚Äô content.</p>\n<p><img src=\"/images/anchr_1.jpg\" alt=\"Anchr images\"></p>\n<p>The last feature are <strong>shortlinks</strong> ‚Äì actually not any different from those you know from <a href=\"http://goo.gl\" target=\"_blank\" rel=\"noopener\">goo.gl</a> or <a href=\"http://bit.ly\" target=\"_blank\" rel=\"noopener\">bit.ly</a>. They‚Äôre useful if you have a very long web link including many query parameters, access tokens, session ids, special characters and the like and want to share them. Often special characters break the linking or your chat application has a maximum length for hyperlinks. Or you just want to keep clarity in your document or emails. In this case it can be very helpful to make the links as short as any possible ‚Äì to be precise of a length of 22 bytes with Anchr.</p>\n<p>Anchr‚Äôs focus is on ease and quickness of use ‚Äì short loading times, flat menu hierarchies, etc.</p>\n<p>At the end just a few words about the technical aspect in addition. <a href=\"http://Anchr.io\" target=\"_blank\" rel=\"noopener\">Anchr.io</a> is separated clearly into backend and frontend or server- and client-application. Both are kept as modular as possible, trying to follow the MVC pattern. Interface between front- and backend is a REST API. The server is based in Node.js with a bunch of very cool new frameworks while the client is an Angular application. Concluding the development was both fun and such a good practice to me and the result will definitely make some of my daily processes a small little easier. I hope you give the app a try and leave me some feedback to dev(at)<a href=\"http://anchr.io\" target=\"_blank\" rel=\"noopener\">anchr.io</a>.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>I want to present my latest project called <a href=\"https://anchr.io\" target=\"_blank\" rel=\"noopener\">Anchr.io</a>. It claims to be a useful little helper or toolbox or the like for common tasks on the internet. The idea arised when someday I considered it useful to have a collection of web links or bookmarks ‚Äì like those you have in Chrome or Firefox ‚Äì accessible from everywhere without needing to synchronize your browser profile. Just like if you‚Äôre anywhere on an other PC, find a useful article on the internet and want to save it quickly for later at home. This is what Anchr‚Äôs <strong>collections</strong> feature does. It saves links ‚Äì with an optional description for easier search and separated into seperate categories / collections.</p>\n<p><img src=\"images/anchr_2.jpg\" alt=\"Anchr collections\"></p>\n<p>The second feature is to <strong>upload images</strong>. You can easily upload one or more photos from your computer oder mobile device and send them to friends or include them into forum posts or the like. Special with Anchr‚Äôs image hosting is that users are given the opportunity to client-sided <strong>encrypt images</strong> with a password. As a result no one without the password will ever see their photos‚Äô content.</p>\n<p><img src=\"/images/anchr_1.jpg\" alt=\"Anchr images\"></p>\n<p>The last feature are <strong>shortlinks</strong> ‚Äì actually not any different from those you know from <a href=\"http://goo.gl\" target=\"_blank\" rel=\"noopener\">goo.gl</a> or <a href=\"http://bit.ly\" target=\"_blank\" rel=\"noopener\">bit.ly</a>. They‚Äôre useful if you have a very long web link including many query parameters, access tokens, session ids, special characters and the like and want to share them. Often special characters break the linking or your chat application has a maximum length for hyperlinks. Or you just want to keep clarity in your document or emails. In this case it can be very helpful to make the links as short as any possible ‚Äì to be precise of a length of 22 bytes with Anchr.</p>\n<p>Anchr‚Äôs focus is on ease and quickness of use ‚Äì short loading times, flat menu hierarchies, etc.</p>\n<p>At the end just a few words about the technical aspect in addition. <a href=\"http://Anchr.io\" target=\"_blank\" rel=\"noopener\">Anchr.io</a> is separated clearly into backend and frontend or server- and client-application. Both are kept as modular as possible, trying to follow the MVC pattern. Interface between front- and backend is a REST API. The server is based in Node.js with a bunch of very cool new frameworks while the client is an Angular application. Concluding the development was both fun and such a good practice to me and the result will definitely make some of my daily processes a small little easier. I hope you give the app a try and leave me some feedback to dev(at)<a href=\"http://anchr.io\" target=\"_blank\" rel=\"noopener\">anchr.io</a>.</p>\n"},{"title":"Basic benchmarks of 5 different MQTT brokers","date":"2019-07-17T20:16:53.000Z","descriptions":"This article briefly benchmarks the performance of five different, commonly used MQTT brokers.","_content":"\nIn the context of my Master's thesis I conducted a very basic performance comparison of several different MQTT brokers and quickly wanted to share my insights. Please note that these benchmarks are quite superficial only. I did not aim to perform an in-depth evaluation, but rather get a basic idea of their performance in general. \n\n## Setup\n* To perform load tests in a _publish_ scenario, I used [takanorig/mqtt-bench](https://github.com/takanorig/mqtt-bench), an MQTT benchmarking tool written in Go.\n* All tests were run with the options `-count 10000`, `-clients 25` and `-size 4096`, which means to simulate 25 concurrent MQTT clients, each sending 10,000 messages of 4 KBytes size each.\n* Both load testing tool as well as the respective broker were run locally on a 6-core, 12-thread, 3.6 Ghz machine with Ubuntu 18.04.\n* Unless otherwise stated, the brokers were started with default configuration.\n\n## Brokers\nThe following brokers were tested.\n\n| Broker        | Written In | Version | Runtime         | Additional Info                                                                                                                                                                             |\n|---------------|------------|---------|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| hbmqtt        | Python     | 0.8     | CPython 3.6     | ‚Äì                                                                                                                                                                                           |\n| hbmqtt (PyPy) | Python     | 0.8     | PyPy 3.6 v7.1.1 | ‚Äì                                                                                                                                                                                           |\n| HiveMQ CE     | Java       | 2019.1  | Oracle JDK 12   | ‚Äì                                                                                                                                                                                           |\n| Mosca         | JavaScript | 2.8.1   | Node 4.8.0      | ‚Äì                                                                                                                                                                                           |\n| Mosquitto     | C          | 1.6.3   | ‚Äì               | ‚Äì                                                                                                                                                                                           |\n| RabbitMQ      | Erlang     | 3.7.4   | ‚Äì               | enabled_plugins=[rabbitmq_management, rabbitmq_management_agent, rabbitmq_management_visualiser, rabbitmq_shovel_management, rabbitmq_stomp, rabbitmq_mqtt,rabbitmq_web_stomp, rabbitmq_web_mqtt] |\n\n## Results\nThese are the results that I obtained. Higher is better.\n\n![MQTT benchmark results](images/mqtt_bench_2.png)","source":"_posts/basic-benchmarks-of-5-different-mqtt-brokers.md","raw":"---\ntitle: Basic benchmarks of 5 different MQTT brokers\ndate: 2019-07-17 22:16:53\ntags:\ndescriptions: This article briefly benchmarks the performance of five different, commonly used MQTT brokers.\n---\n\nIn the context of my Master's thesis I conducted a very basic performance comparison of several different MQTT brokers and quickly wanted to share my insights. Please note that these benchmarks are quite superficial only. I did not aim to perform an in-depth evaluation, but rather get a basic idea of their performance in general. \n\n## Setup\n* To perform load tests in a _publish_ scenario, I used [takanorig/mqtt-bench](https://github.com/takanorig/mqtt-bench), an MQTT benchmarking tool written in Go.\n* All tests were run with the options `-count 10000`, `-clients 25` and `-size 4096`, which means to simulate 25 concurrent MQTT clients, each sending 10,000 messages of 4 KBytes size each.\n* Both load testing tool as well as the respective broker were run locally on a 6-core, 12-thread, 3.6 Ghz machine with Ubuntu 18.04.\n* Unless otherwise stated, the brokers were started with default configuration.\n\n## Brokers\nThe following brokers were tested.\n\n| Broker        | Written In | Version | Runtime         | Additional Info                                                                                                                                                                             |\n|---------------|------------|---------|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| hbmqtt        | Python     | 0.8     | CPython 3.6     | ‚Äì                                                                                                                                                                                           |\n| hbmqtt (PyPy) | Python     | 0.8     | PyPy 3.6 v7.1.1 | ‚Äì                                                                                                                                                                                           |\n| HiveMQ CE     | Java       | 2019.1  | Oracle JDK 12   | ‚Äì                                                                                                                                                                                           |\n| Mosca         | JavaScript | 2.8.1   | Node 4.8.0      | ‚Äì                                                                                                                                                                                           |\n| Mosquitto     | C          | 1.6.3   | ‚Äì               | ‚Äì                                                                                                                                                                                           |\n| RabbitMQ      | Erlang     | 3.7.4   | ‚Äì               | enabled_plugins=[rabbitmq_management, rabbitmq_management_agent, rabbitmq_management_visualiser, rabbitmq_shovel_management, rabbitmq_stomp, rabbitmq_mqtt,rabbitmq_web_stomp, rabbitmq_web_mqtt] |\n\n## Results\nThese are the results that I obtained. Higher is better.\n\n![MQTT benchmark results](images/mqtt_bench_2.png)","slug":"basic-benchmarks-of-5-different-mqtt-brokers","published":1,"updated":"2020-06-06T15:14:33.457Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhhb000340mqvk24bpau","content":"<p>In the context of my Master‚Äôs thesis I conducted a very basic performance comparison of several different MQTT brokers and quickly wanted to share my insights. Please note that these benchmarks are quite superficial only. I did not aim to perform an in-depth evaluation, but rather get a basic idea of their performance in general.</p>\n<h2 id=\"Setup\">Setup</h2>\n<ul>\n<li>To perform load tests in a <em>publish</em> scenario, I used <a href=\"https://github.com/takanorig/mqtt-bench\" target=\"_blank\" rel=\"noopener\">takanorig/mqtt-bench</a>, an MQTT benchmarking tool written in Go.</li>\n<li>All tests were run with the options <code>-count 10000</code>, <code>-clients 25</code> and <code>-size 4096</code>, which means to simulate 25 concurrent MQTT clients, each sending 10,000 messages of 4 KBytes size each.</li>\n<li>Both load testing tool as well as the respective broker were run locally on a 6-core, 12-thread, 3.6 Ghz machine with Ubuntu 18.04.</li>\n<li>Unless otherwise stated, the brokers were started with default configuration.</li>\n</ul>\n<h2 id=\"Brokers\">Brokers</h2>\n<p>The following brokers were tested.</p>\n<table>\n<thead>\n<tr>\n<th>Broker</th>\n<th>Written In</th>\n<th>Version</th>\n<th>Runtime</th>\n<th>Additional Info</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>hbmqtt</td>\n<td>Python</td>\n<td>0.8</td>\n<td>CPython 3.6</td>\n<td>‚Äì</td>\n</tr>\n<tr>\n<td>hbmqtt (PyPy)</td>\n<td>Python</td>\n<td>0.8</td>\n<td>PyPy 3.6 v7.1.1</td>\n<td>‚Äì</td>\n</tr>\n<tr>\n<td>HiveMQ CE</td>\n<td>Java</td>\n<td>2019.1</td>\n<td>Oracle JDK 12</td>\n<td>‚Äì</td>\n</tr>\n<tr>\n<td>Mosca</td>\n<td>JavaScript</td>\n<td>2.8.1</td>\n<td>Node 4.8.0</td>\n<td>‚Äì</td>\n</tr>\n<tr>\n<td>Mosquitto</td>\n<td>C</td>\n<td>1.6.3</td>\n<td>‚Äì</td>\n<td>‚Äì</td>\n</tr>\n<tr>\n<td>RabbitMQ</td>\n<td>Erlang</td>\n<td>3.7.4</td>\n<td>‚Äì</td>\n<td>enabled_plugins=[rabbitmq_management, rabbitmq_management_agent, rabbitmq_management_visualiser, rabbitmq_shovel_management, rabbitmq_stomp, rabbitmq_mqtt,rabbitmq_web_stomp, rabbitmq_web_mqtt]</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Results\">Results</h2>\n<p>These are the results that I obtained. Higher is better.</p>\n<p><img src=\"images/mqtt_bench_2.png\" alt=\"MQTT benchmark results\"></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>In the context of my Master‚Äôs thesis I conducted a very basic performance comparison of several different MQTT brokers and quickly wanted to share my insights. Please note that these benchmarks are quite superficial only. I did not aim to perform an in-depth evaluation, but rather get a basic idea of their performance in general.</p>\n<h2 id=\"Setup\">Setup</h2>\n<ul>\n<li>To perform load tests in a <em>publish</em> scenario, I used <a href=\"https://github.com/takanorig/mqtt-bench\" target=\"_blank\" rel=\"noopener\">takanorig/mqtt-bench</a>, an MQTT benchmarking tool written in Go.</li>\n<li>All tests were run with the options <code>-count 10000</code>, <code>-clients 25</code> and <code>-size 4096</code>, which means to simulate 25 concurrent MQTT clients, each sending 10,000 messages of 4 KBytes size each.</li>\n<li>Both load testing tool as well as the respective broker were run locally on a 6-core, 12-thread, 3.6 Ghz machine with Ubuntu 18.04.</li>\n<li>Unless otherwise stated, the brokers were started with default configuration.</li>\n</ul>\n<h2 id=\"Brokers\">Brokers</h2>\n<p>The following brokers were tested.</p>\n<table>\n<thead>\n<tr>\n<th>Broker</th>\n<th>Written In</th>\n<th>Version</th>\n<th>Runtime</th>\n<th>Additional Info</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>hbmqtt</td>\n<td>Python</td>\n<td>0.8</td>\n<td>CPython 3.6</td>\n<td>‚Äì</td>\n</tr>\n<tr>\n<td>hbmqtt (PyPy)</td>\n<td>Python</td>\n<td>0.8</td>\n<td>PyPy 3.6 v7.1.1</td>\n<td>‚Äì</td>\n</tr>\n<tr>\n<td>HiveMQ CE</td>\n<td>Java</td>\n<td>2019.1</td>\n<td>Oracle JDK 12</td>\n<td>‚Äì</td>\n</tr>\n<tr>\n<td>Mosca</td>\n<td>JavaScript</td>\n<td>2.8.1</td>\n<td>Node 4.8.0</td>\n<td>‚Äì</td>\n</tr>\n<tr>\n<td>Mosquitto</td>\n<td>C</td>\n<td>1.6.3</td>\n<td>‚Äì</td>\n<td>‚Äì</td>\n</tr>\n<tr>\n<td>RabbitMQ</td>\n<td>Erlang</td>\n<td>3.7.4</td>\n<td>‚Äì</td>\n<td>enabled_plugins=[rabbitmq_management, rabbitmq_management_agent, rabbitmq_management_visualiser, rabbitmq_shovel_management, rabbitmq_stomp, rabbitmq_mqtt,rabbitmq_web_stomp, rabbitmq_web_mqtt]</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Results\">Results</h2>\n<p>These are the results that I obtained. Higher is better.</p>\n<p><img src=\"images/mqtt_bench_2.png\" alt=\"MQTT benchmark results\"></p>\n"},{"title":"Building a cloud-native web scraper using 8 different AWS services","date":"2018-12-01T13:52:28.000Z","_content":"\nSounds like overkill, right? It is. Obviously, you don't need a whole bunch of cloud services to build a simple web scraper, especially since there is already a lot of them out there. However, this describes my personal journey of exploring cloud-native development on AWS by building a simple, yet useful application.\n\n# The Goal\nWhat I wanted to build was a web scraper that runs entirely on cloud infrastructure. More precisely, I wanted to build a scraper using [Selenium WebDriver](https://www.seleniumhq.org/projects/webdriver/), because it should be able to scrape not only static HTML pages, but also dynamic, JavaScript-powered single-page apps. With this requirement in mind, a simple Python script incorporating [requests](http://docs.python-requests.org/en/master/) or [urllib](https://docs.python.org/3.7/library/urllib.html#module-urllib) is not sufficient anymore. Instead, you would need at least a headless browser (like Firefox, Chrome or the out-dated [PhantomJS](http://phantomjs.org/)).\n\n## Example Use Case\nTo get a better idea of what I had been building, imagine the following use case. You are a student and your university provides a JavaScript-based website where exam results are published as soon as they are available. To retrieve your results you need to enter your student id and select a department from a drop-down list. You are curious about your grade in the most recent exam, but since you're lazy, you do not want manually check the website every day. That's where a totally over-engineered web scraper comes to play.\n\n## Requirements \nHere are some notes on what the application was supposed to be able to do (and how) - just to get a slightly better understanding.\n* Different crawl tasks are pre-defined as WebDriver scripts in Java.\n* Users can add subscriptions for pre-defined crawling jobs. They will result in a certain crawl task being executed with certain parameters (e.g. form input field values to be filled by Selenium) at a regular interval (e.g. every 24 hours).\n* When adding a subscription for a certain task (corresponding to a certain webpage), users provide their e-mail address and are getting notified once the scraper detects a change.\n* The state of a web-site is persisted in the Dynamo item for the respective subscription and compared to the most recent state that is retrieved when the scraper runs.\n\n# Architecture\nBelow you can see a high-level overview of all components and the corresponding AWS services, as well as basic interactions between the components. Please note that the diagram is not proper UML, but it should help getting an idea of the overall architecture. And it looks kind of fancy at first sight.)\n\n![](images/crawlbuddy2.png)\n([Click to view large](images/crawlbuddy2.png))\n\n\n## AWS services\nThe cloud services used are:\n* **AWS Lambda** for Serverless NodeJS functions to perform stateless tasks\n* **AWS Fargate** as an on-demand Docker container runtime to execute longer-running, more resource-intense tasks\n* **AWS DynamoDB** as a schema-less data store to manage subscriptions and website states\n* **AWS SQS** as a asynchronous messaging channel for communication between components and to trigger Lambdas\n* **AWS S3** to host a static HTML page containing a form to be used for adding new subscriptions thorugh a UI\n* **AWS API Gateway** to provide an HTTP endpoint for adding new subscriptions. It is called by the \"frontend\"-side script and subsequently triggers a Lambda to add the new subscription to Dynamo.\n* **AWS CloudWatch** to regularly trigger the execution of the scraper on Fargate in a crontab-like fashion\n* **AWS SES** to send notification e-mails when something has changed\n\n## Components\nLet's take a very brief look at what the several components are doing.\n\n## crawling-core\nThis is essentially the core part of the whole application, the actual scraper / crawler. I implemented it as a **Java** command-line application, which has the WebDriver as a dependency to be able to interact with webpages dynamically.\n\nThe program is responsible for executing an actual crawling task itself, for detecting changes by comparing the task's result to the latest state in the database, for updating the database item and for potentially pushing a change notification message to a queue. \n\nScraping tasks are defined as Java classes extending the `AbstractTask` class. For instance, you could create sub-classes `AmazonPriceTask` and `ExamsResultsTask`. While implementing these classes, you would essentially need to define input parameters (e.g. your student ID number to be filled in to a search form on the university website later on) for the crawling script and a series of commands in the `run()` method to be executed by WebDriver. \n\n`crawling-core` is developed as a standalone Java command-line application, where the name of the task to be executed (e.g. `EXAM_RESULT_TASK`) and the input parameters (e.g. `VAR_STUDENT_ID`, `VAR_DEPARTMENT_NAME`) are provided as run arguments or environment variables.\n\nIn addition to the Java program, packaged as a simple JAR, we need a browser the WebDriver can use to browse the web. I decided to use Firefox in headless mode. Ultimately, the JAR and the Firefox binary are packaged together into a Docker images based on [selenium/standalone-firefox](https://hub.docker.com/r/selenium/standalone-firefox/) and pushed to **AWS ECR** (AWS' container registry). \n\nTo execute a scraping task, e.g. our `ExamsResultsTask`, **AWS Fargate** will pull the latest Docker image from the registry, create a new container from it, set the required input parameters as environment variables and eventually run the entrypoint, which is our JAR file. \n\n## Œª crawling-crawl\n... is a very simple Lambda function written in **NodeJS**, which is responsible for launching a crawling job. It is triggered regularly through a **CloudWatch** event. First, it fetches all crawling tasks from Dynamo. A crawling task is a unique combination of a task name and a set of input parameters. Afterwards it requests Fargate to start a new new instance of `crawling-core` for every task and passes the input parameters contained in the database item. \n\n## Œª crawling-notify\n... is another Lambda, which stands at the very end of one iteration of our crawling process. It is invoked through messages in the `crawling-changes` **SQS** queue and responsible for sending out notification e-mails to subscribers. It reads change information from the invoking event, including task name, the subscriber's e-mail address and the task's output parameters (e.g. your exam grade) and composes an e-mail message that eventually gets sent through the Simple E-Mail Service (**SES**).\n\n## Œª crawling-web-subscribe\nThe last of our three Lambdas is not directly related to the crawling itself. Instead, it is used for handling HTTP requests sent by a user who wants to add a new subscription. Initiated by a simple script on an HTML page called `subscribe.html`, a POST is sent to the `/subscriptions` endpoint in the **API Gateway**, then forwarded to the `crawling-web-subscribe` and ultimately added to the Dynamo database as a new item in the `subscriptions` table.\n\n# Okay, cool. And now?\nAs I mentioned before, this project was rather a learning playground for me than a reasonable architecture for a web scraper. Although this one should, in fact, be quite scalable, you could definitely build a scraper script with much less effort. However, I learned a lot about cloud development and AWS specifically and I really like how easy things can be and how well all these different components play together. Maybe I was able to encourage the less cloud-experienced developers among you to start playing around with AWS (or some other cloud provider) as well and I hope you liked my (very spontaneously written) article. ","source":"_posts/building-a-cloud-native-web-scraper-using-8-different-aws-services.md","raw":"---\ntitle: Building a cloud-native web scraper using 8 different AWS services\ndate: 2018-12-01 14:52:28\ntags:\n---\n\nSounds like overkill, right? It is. Obviously, you don't need a whole bunch of cloud services to build a simple web scraper, especially since there is already a lot of them out there. However, this describes my personal journey of exploring cloud-native development on AWS by building a simple, yet useful application.\n\n# The Goal\nWhat I wanted to build was a web scraper that runs entirely on cloud infrastructure. More precisely, I wanted to build a scraper using [Selenium WebDriver](https://www.seleniumhq.org/projects/webdriver/), because it should be able to scrape not only static HTML pages, but also dynamic, JavaScript-powered single-page apps. With this requirement in mind, a simple Python script incorporating [requests](http://docs.python-requests.org/en/master/) or [urllib](https://docs.python.org/3.7/library/urllib.html#module-urllib) is not sufficient anymore. Instead, you would need at least a headless browser (like Firefox, Chrome or the out-dated [PhantomJS](http://phantomjs.org/)).\n\n## Example Use Case\nTo get a better idea of what I had been building, imagine the following use case. You are a student and your university provides a JavaScript-based website where exam results are published as soon as they are available. To retrieve your results you need to enter your student id and select a department from a drop-down list. You are curious about your grade in the most recent exam, but since you're lazy, you do not want manually check the website every day. That's where a totally over-engineered web scraper comes to play.\n\n## Requirements \nHere are some notes on what the application was supposed to be able to do (and how) - just to get a slightly better understanding.\n* Different crawl tasks are pre-defined as WebDriver scripts in Java.\n* Users can add subscriptions for pre-defined crawling jobs. They will result in a certain crawl task being executed with certain parameters (e.g. form input field values to be filled by Selenium) at a regular interval (e.g. every 24 hours).\n* When adding a subscription for a certain task (corresponding to a certain webpage), users provide their e-mail address and are getting notified once the scraper detects a change.\n* The state of a web-site is persisted in the Dynamo item for the respective subscription and compared to the most recent state that is retrieved when the scraper runs.\n\n# Architecture\nBelow you can see a high-level overview of all components and the corresponding AWS services, as well as basic interactions between the components. Please note that the diagram is not proper UML, but it should help getting an idea of the overall architecture. And it looks kind of fancy at first sight.)\n\n![](images/crawlbuddy2.png)\n([Click to view large](images/crawlbuddy2.png))\n\n\n## AWS services\nThe cloud services used are:\n* **AWS Lambda** for Serverless NodeJS functions to perform stateless tasks\n* **AWS Fargate** as an on-demand Docker container runtime to execute longer-running, more resource-intense tasks\n* **AWS DynamoDB** as a schema-less data store to manage subscriptions and website states\n* **AWS SQS** as a asynchronous messaging channel for communication between components and to trigger Lambdas\n* **AWS S3** to host a static HTML page containing a form to be used for adding new subscriptions thorugh a UI\n* **AWS API Gateway** to provide an HTTP endpoint for adding new subscriptions. It is called by the \"frontend\"-side script and subsequently triggers a Lambda to add the new subscription to Dynamo.\n* **AWS CloudWatch** to regularly trigger the execution of the scraper on Fargate in a crontab-like fashion\n* **AWS SES** to send notification e-mails when something has changed\n\n## Components\nLet's take a very brief look at what the several components are doing.\n\n## crawling-core\nThis is essentially the core part of the whole application, the actual scraper / crawler. I implemented it as a **Java** command-line application, which has the WebDriver as a dependency to be able to interact with webpages dynamically.\n\nThe program is responsible for executing an actual crawling task itself, for detecting changes by comparing the task's result to the latest state in the database, for updating the database item and for potentially pushing a change notification message to a queue. \n\nScraping tasks are defined as Java classes extending the `AbstractTask` class. For instance, you could create sub-classes `AmazonPriceTask` and `ExamsResultsTask`. While implementing these classes, you would essentially need to define input parameters (e.g. your student ID number to be filled in to a search form on the university website later on) for the crawling script and a series of commands in the `run()` method to be executed by WebDriver. \n\n`crawling-core` is developed as a standalone Java command-line application, where the name of the task to be executed (e.g. `EXAM_RESULT_TASK`) and the input parameters (e.g. `VAR_STUDENT_ID`, `VAR_DEPARTMENT_NAME`) are provided as run arguments or environment variables.\n\nIn addition to the Java program, packaged as a simple JAR, we need a browser the WebDriver can use to browse the web. I decided to use Firefox in headless mode. Ultimately, the JAR and the Firefox binary are packaged together into a Docker images based on [selenium/standalone-firefox](https://hub.docker.com/r/selenium/standalone-firefox/) and pushed to **AWS ECR** (AWS' container registry). \n\nTo execute a scraping task, e.g. our `ExamsResultsTask`, **AWS Fargate** will pull the latest Docker image from the registry, create a new container from it, set the required input parameters as environment variables and eventually run the entrypoint, which is our JAR file. \n\n## Œª crawling-crawl\n... is a very simple Lambda function written in **NodeJS**, which is responsible for launching a crawling job. It is triggered regularly through a **CloudWatch** event. First, it fetches all crawling tasks from Dynamo. A crawling task is a unique combination of a task name and a set of input parameters. Afterwards it requests Fargate to start a new new instance of `crawling-core` for every task and passes the input parameters contained in the database item. \n\n## Œª crawling-notify\n... is another Lambda, which stands at the very end of one iteration of our crawling process. It is invoked through messages in the `crawling-changes` **SQS** queue and responsible for sending out notification e-mails to subscribers. It reads change information from the invoking event, including task name, the subscriber's e-mail address and the task's output parameters (e.g. your exam grade) and composes an e-mail message that eventually gets sent through the Simple E-Mail Service (**SES**).\n\n## Œª crawling-web-subscribe\nThe last of our three Lambdas is not directly related to the crawling itself. Instead, it is used for handling HTTP requests sent by a user who wants to add a new subscription. Initiated by a simple script on an HTML page called `subscribe.html`, a POST is sent to the `/subscriptions` endpoint in the **API Gateway**, then forwarded to the `crawling-web-subscribe` and ultimately added to the Dynamo database as a new item in the `subscriptions` table.\n\n# Okay, cool. And now?\nAs I mentioned before, this project was rather a learning playground for me than a reasonable architecture for a web scraper. Although this one should, in fact, be quite scalable, you could definitely build a scraper script with much less effort. However, I learned a lot about cloud development and AWS specifically and I really like how easy things can be and how well all these different components play together. Maybe I was able to encourage the less cloud-experienced developers among you to start playing around with AWS (or some other cloud provider) as well and I hope you liked my (very spontaneously written) article. ","slug":"building-a-cloud-native-web-scraper-using-8-different-aws-services","published":1,"updated":"2020-06-06T15:14:33.457Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhhg000540mq3r0o3glj","content":"<p>Sounds like overkill, right? It is. Obviously, you don‚Äôt need a whole bunch of cloud services to build a simple web scraper, especially since there is already a lot of them out there. However, this describes my personal journey of exploring cloud-native development on AWS by building a simple, yet useful application.</p>\n<h1>The Goal</h1>\n<p>What I wanted to build was a web scraper that runs entirely on cloud infrastructure. More precisely, I wanted to build a scraper using <a href=\"https://www.seleniumhq.org/projects/webdriver/\" target=\"_blank\" rel=\"noopener\">Selenium WebDriver</a>, because it should be able to scrape not only static HTML pages, but also dynamic, JavaScript-powered single-page apps. With this requirement in mind, a simple Python script incorporating <a href=\"http://docs.python-requests.org/en/master/\" target=\"_blank\" rel=\"noopener\">requests</a> or <a href=\"https://docs.python.org/3.7/library/urllib.html#module-urllib\" target=\"_blank\" rel=\"noopener\">urllib</a> is not sufficient anymore. Instead, you would need at least a headless browser (like Firefox, Chrome or the out-dated <a href=\"http://phantomjs.org/\" target=\"_blank\" rel=\"noopener\">PhantomJS</a>).</p>\n<h2 id=\"Example-Use-Case\">Example Use Case</h2>\n<p>To get a better idea of what I had been building, imagine the following use case. You are a student and your university provides a JavaScript-based website where exam results are published as soon as they are available. To retrieve your results you need to enter your student id and select a department from a drop-down list. You are curious about your grade in the most recent exam, but since you‚Äôre lazy, you do not want manually check the website every day. That‚Äôs where a totally over-engineered web scraper comes to play.</p>\n<h2 id=\"Requirements\">Requirements</h2>\n<p>Here are some notes on what the application was supposed to be able to do (and how) - just to get a slightly better understanding.</p>\n<ul>\n<li>Different crawl tasks are pre-defined as WebDriver scripts in Java.</li>\n<li>Users can add subscriptions for pre-defined crawling jobs. They will result in a certain crawl task being executed with certain parameters (e.g. form input field values to be filled by Selenium) at a regular interval (e.g. every 24 hours).</li>\n<li>When adding a subscription for a certain task (corresponding to a certain webpage), users provide their e-mail address and are getting notified once the scraper detects a change.</li>\n<li>The state of a web-site is persisted in the Dynamo item for the respective subscription and compared to the most recent state that is retrieved when the scraper runs.</li>\n</ul>\n<h1>Architecture</h1>\n<p>Below you can see a high-level overview of all components and the corresponding AWS services, as well as basic interactions between the components. Please note that the diagram is not proper UML, but it should help getting an idea of the overall architecture. And it looks kind of fancy at first sight.)</p>\n<p><img src=\"images/crawlbuddy2.png\" alt><br>\n(<a href=\"images/crawlbuddy2.png\">Click to view large</a>)</p>\n<h2 id=\"AWS-services\">AWS services</h2>\n<p>The cloud services used are:</p>\n<ul>\n<li><strong>AWS Lambda</strong> for Serverless NodeJS functions to perform stateless tasks</li>\n<li><strong>AWS Fargate</strong> as an on-demand Docker container runtime to execute longer-running, more resource-intense tasks</li>\n<li><strong>AWS DynamoDB</strong> as a schema-less data store to manage subscriptions and website states</li>\n<li><strong>AWS SQS</strong> as a asynchronous messaging channel for communication between components and to trigger Lambdas</li>\n<li><strong>AWS S3</strong> to host a static HTML page containing a form to be used for adding new subscriptions thorugh a UI</li>\n<li><strong>AWS API Gateway</strong> to provide an HTTP endpoint for adding new subscriptions. It is called by the ‚Äúfrontend‚Äù-side script and subsequently triggers a Lambda to add the new subscription to Dynamo.</li>\n<li><strong>AWS CloudWatch</strong> to regularly trigger the execution of the scraper on Fargate in a crontab-like fashion</li>\n<li><strong>AWS SES</strong> to send notification e-mails when something has changed</li>\n</ul>\n<h2 id=\"Components\">Components</h2>\n<p>Let‚Äôs take a very brief look at what the several components are doing.</p>\n<h2 id=\"crawling-core\">crawling-core</h2>\n<p>This is essentially the core part of the whole application, the actual scraper / crawler. I implemented it as a <strong>Java</strong> command-line application, which has the WebDriver as a dependency to be able to interact with webpages dynamically.</p>\n<p>The program is responsible for executing an actual crawling task itself, for detecting changes by comparing the task‚Äôs result to the latest state in the database, for updating the database item and for potentially pushing a change notification message to a queue.</p>\n<p>Scraping tasks are defined as Java classes extending the <code>AbstractTask</code> class. For instance, you could create sub-classes <code>AmazonPriceTask</code> and <code>ExamsResultsTask</code>. While implementing these classes, you would essentially need to define input parameters (e.g. your student ID number to be filled in to a search form on the university website later on) for the crawling script and a series of commands in the <code>run()</code> method to be executed by WebDriver.</p>\n<p><code>crawling-core</code> is developed as a standalone Java command-line application, where the name of the task to be executed (e.g. <code>EXAM_RESULT_TASK</code>) and the input parameters (e.g. <code>VAR_STUDENT_ID</code>, <code>VAR_DEPARTMENT_NAME</code>) are provided as run arguments or environment variables.</p>\n<p>In addition to the Java program, packaged as a simple JAR, we need a browser the WebDriver can use to browse the web. I decided to use Firefox in headless mode. Ultimately, the JAR and the Firefox binary are packaged together into a Docker images based on <a href=\"https://hub.docker.com/r/selenium/standalone-firefox/\" target=\"_blank\" rel=\"noopener\">selenium/standalone-firefox</a> and pushed to <strong>AWS ECR</strong> (AWS‚Äô container registry).</p>\n<p>To execute a scraping task, e.g. our <code>ExamsResultsTask</code>, <strong>AWS Fargate</strong> will pull the latest Docker image from the registry, create a new container from it, set the required input parameters as environment variables and eventually run the entrypoint, which is our JAR file.</p>\n<h2 id=\"Œª-crawling-crawl\">Œª crawling-crawl</h2>\n<p>‚Ä¶ is a very simple Lambda function written in <strong>NodeJS</strong>, which is responsible for launching a crawling job. It is triggered regularly through a <strong>CloudWatch</strong> event. First, it fetches all crawling tasks from Dynamo. A crawling task is a unique combination of a task name and a set of input parameters. Afterwards it requests Fargate to start a new new instance of <code>crawling-core</code> for every task and passes the input parameters contained in the database item.</p>\n<h2 id=\"Œª-crawling-notify\">Œª crawling-notify</h2>\n<p>‚Ä¶ is another Lambda, which stands at the very end of one iteration of our crawling process. It is invoked through messages in the <code>crawling-changes</code> <strong>SQS</strong> queue and responsible for sending out notification e-mails to subscribers. It reads change information from the invoking event, including task name, the subscriber‚Äôs e-mail address and the task‚Äôs output parameters (e.g. your exam grade) and composes an e-mail message that eventually gets sent through the Simple E-Mail Service (<strong>SES</strong>).</p>\n<h2 id=\"Œª-crawling-web-subscribe\">Œª crawling-web-subscribe</h2>\n<p>The last of our three Lambdas is not directly related to the crawling itself. Instead, it is used for handling HTTP requests sent by a user who wants to add a new subscription. Initiated by a simple script on an HTML page called <code>subscribe.html</code>, a POST is sent to the <code>/subscriptions</code> endpoint in the <strong>API Gateway</strong>, then forwarded to the <code>crawling-web-subscribe</code> and ultimately added to the Dynamo database as a new item in the <code>subscriptions</code> table.</p>\n<h1>Okay, cool. And now?</h1>\n<p>As I mentioned before, this project was rather a learning playground for me than a reasonable architecture for a web scraper. Although this one should, in fact, be quite scalable, you could definitely build a scraper script with much less effort. However, I learned a lot about cloud development and AWS specifically and I really like how easy things can be and how well all these different components play together. Maybe I was able to encourage the less cloud-experienced developers among you to start playing around with AWS (or some other cloud provider) as well and I hope you liked my (very spontaneously written) article.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Sounds like overkill, right? It is. Obviously, you don‚Äôt need a whole bunch of cloud services to build a simple web scraper, especially since there is already a lot of them out there. However, this describes my personal journey of exploring cloud-native development on AWS by building a simple, yet useful application.</p>\n<h1>The Goal</h1>\n<p>What I wanted to build was a web scraper that runs entirely on cloud infrastructure. More precisely, I wanted to build a scraper using <a href=\"https://www.seleniumhq.org/projects/webdriver/\" target=\"_blank\" rel=\"noopener\">Selenium WebDriver</a>, because it should be able to scrape not only static HTML pages, but also dynamic, JavaScript-powered single-page apps. With this requirement in mind, a simple Python script incorporating <a href=\"http://docs.python-requests.org/en/master/\" target=\"_blank\" rel=\"noopener\">requests</a> or <a href=\"https://docs.python.org/3.7/library/urllib.html#module-urllib\" target=\"_blank\" rel=\"noopener\">urllib</a> is not sufficient anymore. Instead, you would need at least a headless browser (like Firefox, Chrome or the out-dated <a href=\"http://phantomjs.org/\" target=\"_blank\" rel=\"noopener\">PhantomJS</a>).</p>\n<h2 id=\"Example-Use-Case\">Example Use Case</h2>\n<p>To get a better idea of what I had been building, imagine the following use case. You are a student and your university provides a JavaScript-based website where exam results are published as soon as they are available. To retrieve your results you need to enter your student id and select a department from a drop-down list. You are curious about your grade in the most recent exam, but since you‚Äôre lazy, you do not want manually check the website every day. That‚Äôs where a totally over-engineered web scraper comes to play.</p>\n<h2 id=\"Requirements\">Requirements</h2>\n<p>Here are some notes on what the application was supposed to be able to do (and how) - just to get a slightly better understanding.</p>\n<ul>\n<li>Different crawl tasks are pre-defined as WebDriver scripts in Java.</li>\n<li>Users can add subscriptions for pre-defined crawling jobs. They will result in a certain crawl task being executed with certain parameters (e.g. form input field values to be filled by Selenium) at a regular interval (e.g. every 24 hours).</li>\n<li>When adding a subscription for a certain task (corresponding to a certain webpage), users provide their e-mail address and are getting notified once the scraper detects a change.</li>\n<li>The state of a web-site is persisted in the Dynamo item for the respective subscription and compared to the most recent state that is retrieved when the scraper runs.</li>\n</ul>\n<h1>Architecture</h1>\n<p>Below you can see a high-level overview of all components and the corresponding AWS services, as well as basic interactions between the components. Please note that the diagram is not proper UML, but it should help getting an idea of the overall architecture. And it looks kind of fancy at first sight.)</p>\n<p><img src=\"images/crawlbuddy2.png\" alt><br>\n(<a href=\"images/crawlbuddy2.png\">Click to view large</a>)</p>\n<h2 id=\"AWS-services\">AWS services</h2>\n<p>The cloud services used are:</p>\n<ul>\n<li><strong>AWS Lambda</strong> for Serverless NodeJS functions to perform stateless tasks</li>\n<li><strong>AWS Fargate</strong> as an on-demand Docker container runtime to execute longer-running, more resource-intense tasks</li>\n<li><strong>AWS DynamoDB</strong> as a schema-less data store to manage subscriptions and website states</li>\n<li><strong>AWS SQS</strong> as a asynchronous messaging channel for communication between components and to trigger Lambdas</li>\n<li><strong>AWS S3</strong> to host a static HTML page containing a form to be used for adding new subscriptions thorugh a UI</li>\n<li><strong>AWS API Gateway</strong> to provide an HTTP endpoint for adding new subscriptions. It is called by the ‚Äúfrontend‚Äù-side script and subsequently triggers a Lambda to add the new subscription to Dynamo.</li>\n<li><strong>AWS CloudWatch</strong> to regularly trigger the execution of the scraper on Fargate in a crontab-like fashion</li>\n<li><strong>AWS SES</strong> to send notification e-mails when something has changed</li>\n</ul>\n<h2 id=\"Components\">Components</h2>\n<p>Let‚Äôs take a very brief look at what the several components are doing.</p>\n<h2 id=\"crawling-core\">crawling-core</h2>\n<p>This is essentially the core part of the whole application, the actual scraper / crawler. I implemented it as a <strong>Java</strong> command-line application, which has the WebDriver as a dependency to be able to interact with webpages dynamically.</p>\n<p>The program is responsible for executing an actual crawling task itself, for detecting changes by comparing the task‚Äôs result to the latest state in the database, for updating the database item and for potentially pushing a change notification message to a queue.</p>\n<p>Scraping tasks are defined as Java classes extending the <code>AbstractTask</code> class. For instance, you could create sub-classes <code>AmazonPriceTask</code> and <code>ExamsResultsTask</code>. While implementing these classes, you would essentially need to define input parameters (e.g. your student ID number to be filled in to a search form on the university website later on) for the crawling script and a series of commands in the <code>run()</code> method to be executed by WebDriver.</p>\n<p><code>crawling-core</code> is developed as a standalone Java command-line application, where the name of the task to be executed (e.g. <code>EXAM_RESULT_TASK</code>) and the input parameters (e.g. <code>VAR_STUDENT_ID</code>, <code>VAR_DEPARTMENT_NAME</code>) are provided as run arguments or environment variables.</p>\n<p>In addition to the Java program, packaged as a simple JAR, we need a browser the WebDriver can use to browse the web. I decided to use Firefox in headless mode. Ultimately, the JAR and the Firefox binary are packaged together into a Docker images based on <a href=\"https://hub.docker.com/r/selenium/standalone-firefox/\" target=\"_blank\" rel=\"noopener\">selenium/standalone-firefox</a> and pushed to <strong>AWS ECR</strong> (AWS‚Äô container registry).</p>\n<p>To execute a scraping task, e.g. our <code>ExamsResultsTask</code>, <strong>AWS Fargate</strong> will pull the latest Docker image from the registry, create a new container from it, set the required input parameters as environment variables and eventually run the entrypoint, which is our JAR file.</p>\n<h2 id=\"Œª-crawling-crawl\">Œª crawling-crawl</h2>\n<p>‚Ä¶ is a very simple Lambda function written in <strong>NodeJS</strong>, which is responsible for launching a crawling job. It is triggered regularly through a <strong>CloudWatch</strong> event. First, it fetches all crawling tasks from Dynamo. A crawling task is a unique combination of a task name and a set of input parameters. Afterwards it requests Fargate to start a new new instance of <code>crawling-core</code> for every task and passes the input parameters contained in the database item.</p>\n<h2 id=\"Œª-crawling-notify\">Œª crawling-notify</h2>\n<p>‚Ä¶ is another Lambda, which stands at the very end of one iteration of our crawling process. It is invoked through messages in the <code>crawling-changes</code> <strong>SQS</strong> queue and responsible for sending out notification e-mails to subscribers. It reads change information from the invoking event, including task name, the subscriber‚Äôs e-mail address and the task‚Äôs output parameters (e.g. your exam grade) and composes an e-mail message that eventually gets sent through the Simple E-Mail Service (<strong>SES</strong>).</p>\n<h2 id=\"Œª-crawling-web-subscribe\">Œª crawling-web-subscribe</h2>\n<p>The last of our three Lambdas is not directly related to the crawling itself. Instead, it is used for handling HTTP requests sent by a user who wants to add a new subscription. Initiated by a simple script on an HTML page called <code>subscribe.html</code>, a POST is sent to the <code>/subscriptions</code> endpoint in the <strong>API Gateway</strong>, then forwarded to the <code>crawling-web-subscribe</code> and ultimately added to the Dynamo database as a new item in the <code>subscriptions</code> table.</p>\n<h1>Okay, cool. And now?</h1>\n<p>As I mentioned before, this project was rather a learning playground for me than a reasonable architecture for a web scraper. Although this one should, in fact, be quite scalable, you could definitely build a scraper script with much less effort. However, I learned a lot about cloud development and AWS specifically and I really like how easy things can be and how well all these different components play together. Maybe I was able to encourage the less cloud-experienced developers among you to start playing around with AWS (or some other cloud provider) as well and I hope you liked my (very spontaneously written) article.</p>\n"},{"title":"Caddy - a modern web server (vs. nginx)","date":"2017-01-09T22:07:55.000Z","_content":"\n__Update:__ I'm glad to tell that this article made it to the front page of [Hacker News](https://news.ycombinator.com/news) only a few hours after publication ü§ì.\n\nAt the time of writing this article the web is effectively powered by three different major web server software packages. A web server, as covered in this article, basically has two purposes. One is to serve static (no dynamic functionality, no backend, no databse, ...) web sites, usually consisting of HTML, JavaScript and CSS plus images etc. The other is to act as a [reverse-proxy](https://en.wikipedia.org/wiki/Reverse_proxy) to web application backends. The three servers I just mentioned have a combined market share of 94.7 % (according to [this statistic](https://w3techs.com/technologies/overview/web_server/all)) and are named [Apache 2](https://httpd.apache.org/) (or _httpd_) (written in C), [nginx](https://www.nginx.com/solutions/web-server/) (say _\"engine ex\"_) (also written in C) and [Microsoft IIS](https://www.iis.net/) (written in C++). While the first two are platform independent and open-source, the latter is a proprietary, commercial, Windows-only Microsoft product and therefore more interesting at enterprise level rather than for smaller indie projects. Consequently I won't cover IIS further in the following. \n\n![Most popular web servers on the internet](images/webservers.png)\n\n_Most popular web servers on the internet ([Source](https://w3techs.com/technologies/overview/web_server/all))_\n\nnginx' first release was in 2004 and Apache2's roots even date back to 1995. Of course both projects are getting updates regularly, but their base concepts still remain the same. And at some point they might not perfectly fit today's requirements anymore. \n\nPersonally I switched from Apache2 to nginx a few months ago mainly because of two reasons. The first one was that I had really been annoyed by [Apache2's extremely high memory overhead](https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison). The second reason was that Apache2 still didn't have HTTP/2.0 support in 2016.\n\n![Apache2 vs. nginx memory usage](https://objects-us-west-1.dream.io/kbimages/images/Webserver_memory_graph.jpg)\n\n_Apache2 vs. nginx memory usage ([Source](https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison))_\n\nI was pretty happy with nginx and especially its performance as well as the large amount of documentation and forum posts on the web about every conceivable problem were great. But since I'm a developer and not a sysadmin there's one thing I didn't like. The configuration is not that intuitive and you really need to get into the syntax and concepts to get an understanding of knobs to turn in order to achieve a certain goal. It's also much more fine-grained than necessary for the average user. Personally I just want a simple config file with an intuitive syntax where I can tell my web server which static content to display or which backend to reverse-proxy for which route / domain. This, plus some additional features like handling compression, TLS encryption, authentication and maybe some basic rewrites, is fine. Looking for a more modern web server that fulfills these requirements I found [Caddy](https://caddyserver.com). As it turned out, it can even do a lot more cool things, while still being easy to use.\n\nCaddy is written is Go, open-source and pretty actively (according to commit history) developed on [GitHub](https://github.com/mholt/caddy). The goal when developing Caddy was exactly what I was looking for: easy configuration and fitness for today's web applications. It comes with HTTP/2.0 (and even QUIC) support out of the box and serves via HTTPS by default (HTTP to HTTPS redirection is also handled automatically, while you manually had to tell Apache2 or nginx to do so). It even obtains (and regularly renews!) [Let's Encrypt](https://letsencrypt.org/) certificates for every (sub)domain you specified in the config file. While enabling HTTPS for a site was really a pain some years ago, it's done completely automatically now. You don't need to run any script. You don't even need to create a Let's Encrypt account or install the _certbot_. At the center of Caddy are is the _middleware_ (or _directives_), which are added to the config as a one-liner. The [list of such](https://caddyserver.com/docs/) is long and you will find a middleware for almost everything. For instance there are middleware components for logging, gzipping, header modification, (basic or [JWT](https://jwt.io)-based) authentication and load balancing. But also more fancy things like automatically serving Markdown as HTML, a plug-and-play file browser GUI, HTML minification, IP filtering or pattern-based text replacement in HTML files are available as middlewares. Caddy also aligns well with PHP, using _php-fpm_, just as nginx does. As usual with Go applications, the entire program is shipped as a single binary (available for Windows, Mac, Linux and BSD), which includes all of its dependencies. Consequently you don't need to install any further libraries to be linked (-> no version conflicts), what really makes the installation a no-brainer. However, this introduces one little drawback in comparison to nginx modules: every middleware you want to use needs to be included into the binary and if it's not, you need to re-compile the program (which is done for you by the download script at Caddy website, actually). \n\nI migrated all of my websites and -apps from nginx to Caddy (which took me hardly more than an hour) and so far I'm happy with the setup. But what about performance?\n\nTo measure a very basic performance benchmark, I took [this script](https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh), which I used in [an earlier benchmark scenario](https://muetsch.io/http-performance-java-jersey-vs-go-vs-nodejs.html). This script uses the [h2load](https://github.com/nghttp2/nghttp2#benchmarking-tool) load test tool and I adjusted the parameters in a way that it performs a total of __100,000 requests__ against a specific route at my webserver with a number of __32 concurrent clients__ (each performing 3,125 requests) on __4 CPU threads__. I ran both servers with almost their default configuration, except that I turned on HTTP/2.0 with a self-signed certificate. The file served was a static HTML file containing 6.2 kBytes of data. Both h2load and the respective web server were executed locally on the same machine with the following specs.\n\n```\n===CPU:\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\n \n===RAM: \n              total        used        free      shared  buff/cache   available\nMem:           7,7G        1,8G        4,2G        316M        1,6G        5,3G\nSwap:           29G          0B         29G\n \n===OS: \nLinux ferdinand-ubuntu 4.8.0-32-generic #34-Ubuntu SMP Tue Dec 13 14:30:43 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nThe results look like this.\n![](images/webserver_performance.png)\n\n_Caddy vs. nginx performance comparison_\n\nAs you can clearly see, nginx still performs way better, at least in this very simple scenario. However, Caddy is much more easy to use, in my opinion. Seems like we are having a classical trade-off here. Anyway, you should really give Caddy a try (and I'm not getting paid for this üòâ). Concerning memory usage: I didn't observe that in detail, but suprisingly I found that neither the Caddy process nor the sum of nginx worker processes exceeded 10 MB of RAM usage (may I have done something wrong?).\n\nPlease note that I only measured one specific figure (concurrent req/s) in one specific scenario. One can think of other benchmark setups where results might be the complete opposite, potentially.\n\nBy the way, Apache2 was not included to this benchmark, because I wanted to use HTTP/2.0. Actually in the meantime there is a `mod_http2` for Apache2, but it's not included in the majority of the builds, yet, and to be honest, I didn't want to make an own one. If you're interested in that, you can get a rough idea of Apache2 vs. nginx performance in [this article](https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison) (spoiler: it's pretty poor).\n\nSo to conclude the discussion: Should you use Caddy in preference to nginx or Apache2? For private projects definitely yes, if you asked me. For more _serious_ projects you should probably wait until it's even a little more mature (e.g. when a 1.x.x version is out) and maybe also incorporating dynamic module loading. Until then I'd stick with nginx. Besides that I can't figure out a reason for preferring Apache2 over nginx, except for being too lazy to do the migration.\n\nPlease let me know if you liked my article and also if you don't agree with some of my arguments and insights.","source":"_posts/caddy-a-modern-web-server-vs-nginx.md","raw":"---\ntitle: Caddy - a modern web server (vs. nginx)\ndate: 2017-01-09 23:07:55\ntags:\n---\n\n__Update:__ I'm glad to tell that this article made it to the front page of [Hacker News](https://news.ycombinator.com/news) only a few hours after publication ü§ì.\n\nAt the time of writing this article the web is effectively powered by three different major web server software packages. A web server, as covered in this article, basically has two purposes. One is to serve static (no dynamic functionality, no backend, no databse, ...) web sites, usually consisting of HTML, JavaScript and CSS plus images etc. The other is to act as a [reverse-proxy](https://en.wikipedia.org/wiki/Reverse_proxy) to web application backends. The three servers I just mentioned have a combined market share of 94.7 % (according to [this statistic](https://w3techs.com/technologies/overview/web_server/all)) and are named [Apache 2](https://httpd.apache.org/) (or _httpd_) (written in C), [nginx](https://www.nginx.com/solutions/web-server/) (say _\"engine ex\"_) (also written in C) and [Microsoft IIS](https://www.iis.net/) (written in C++). While the first two are platform independent and open-source, the latter is a proprietary, commercial, Windows-only Microsoft product and therefore more interesting at enterprise level rather than for smaller indie projects. Consequently I won't cover IIS further in the following. \n\n![Most popular web servers on the internet](images/webservers.png)\n\n_Most popular web servers on the internet ([Source](https://w3techs.com/technologies/overview/web_server/all))_\n\nnginx' first release was in 2004 and Apache2's roots even date back to 1995. Of course both projects are getting updates regularly, but their base concepts still remain the same. And at some point they might not perfectly fit today's requirements anymore. \n\nPersonally I switched from Apache2 to nginx a few months ago mainly because of two reasons. The first one was that I had really been annoyed by [Apache2's extremely high memory overhead](https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison). The second reason was that Apache2 still didn't have HTTP/2.0 support in 2016.\n\n![Apache2 vs. nginx memory usage](https://objects-us-west-1.dream.io/kbimages/images/Webserver_memory_graph.jpg)\n\n_Apache2 vs. nginx memory usage ([Source](https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison))_\n\nI was pretty happy with nginx and especially its performance as well as the large amount of documentation and forum posts on the web about every conceivable problem were great. But since I'm a developer and not a sysadmin there's one thing I didn't like. The configuration is not that intuitive and you really need to get into the syntax and concepts to get an understanding of knobs to turn in order to achieve a certain goal. It's also much more fine-grained than necessary for the average user. Personally I just want a simple config file with an intuitive syntax where I can tell my web server which static content to display or which backend to reverse-proxy for which route / domain. This, plus some additional features like handling compression, TLS encryption, authentication and maybe some basic rewrites, is fine. Looking for a more modern web server that fulfills these requirements I found [Caddy](https://caddyserver.com). As it turned out, it can even do a lot more cool things, while still being easy to use.\n\nCaddy is written is Go, open-source and pretty actively (according to commit history) developed on [GitHub](https://github.com/mholt/caddy). The goal when developing Caddy was exactly what I was looking for: easy configuration and fitness for today's web applications. It comes with HTTP/2.0 (and even QUIC) support out of the box and serves via HTTPS by default (HTTP to HTTPS redirection is also handled automatically, while you manually had to tell Apache2 or nginx to do so). It even obtains (and regularly renews!) [Let's Encrypt](https://letsencrypt.org/) certificates for every (sub)domain you specified in the config file. While enabling HTTPS for a site was really a pain some years ago, it's done completely automatically now. You don't need to run any script. You don't even need to create a Let's Encrypt account or install the _certbot_. At the center of Caddy are is the _middleware_ (or _directives_), which are added to the config as a one-liner. The [list of such](https://caddyserver.com/docs/) is long and you will find a middleware for almost everything. For instance there are middleware components for logging, gzipping, header modification, (basic or [JWT](https://jwt.io)-based) authentication and load balancing. But also more fancy things like automatically serving Markdown as HTML, a plug-and-play file browser GUI, HTML minification, IP filtering or pattern-based text replacement in HTML files are available as middlewares. Caddy also aligns well with PHP, using _php-fpm_, just as nginx does. As usual with Go applications, the entire program is shipped as a single binary (available for Windows, Mac, Linux and BSD), which includes all of its dependencies. Consequently you don't need to install any further libraries to be linked (-> no version conflicts), what really makes the installation a no-brainer. However, this introduces one little drawback in comparison to nginx modules: every middleware you want to use needs to be included into the binary and if it's not, you need to re-compile the program (which is done for you by the download script at Caddy website, actually). \n\nI migrated all of my websites and -apps from nginx to Caddy (which took me hardly more than an hour) and so far I'm happy with the setup. But what about performance?\n\nTo measure a very basic performance benchmark, I took [this script](https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh), which I used in [an earlier benchmark scenario](https://muetsch.io/http-performance-java-jersey-vs-go-vs-nodejs.html). This script uses the [h2load](https://github.com/nghttp2/nghttp2#benchmarking-tool) load test tool and I adjusted the parameters in a way that it performs a total of __100,000 requests__ against a specific route at my webserver with a number of __32 concurrent clients__ (each performing 3,125 requests) on __4 CPU threads__. I ran both servers with almost their default configuration, except that I turned on HTTP/2.0 with a self-signed certificate. The file served was a static HTML file containing 6.2 kBytes of data. Both h2load and the respective web server were executed locally on the same machine with the following specs.\n\n```\n===CPU:\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\n \n===RAM: \n              total        used        free      shared  buff/cache   available\nMem:           7,7G        1,8G        4,2G        316M        1,6G        5,3G\nSwap:           29G          0B         29G\n \n===OS: \nLinux ferdinand-ubuntu 4.8.0-32-generic #34-Ubuntu SMP Tue Dec 13 14:30:43 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nThe results look like this.\n![](images/webserver_performance.png)\n\n_Caddy vs. nginx performance comparison_\n\nAs you can clearly see, nginx still performs way better, at least in this very simple scenario. However, Caddy is much more easy to use, in my opinion. Seems like we are having a classical trade-off here. Anyway, you should really give Caddy a try (and I'm not getting paid for this üòâ). Concerning memory usage: I didn't observe that in detail, but suprisingly I found that neither the Caddy process nor the sum of nginx worker processes exceeded 10 MB of RAM usage (may I have done something wrong?).\n\nPlease note that I only measured one specific figure (concurrent req/s) in one specific scenario. One can think of other benchmark setups where results might be the complete opposite, potentially.\n\nBy the way, Apache2 was not included to this benchmark, because I wanted to use HTTP/2.0. Actually in the meantime there is a `mod_http2` for Apache2, but it's not included in the majority of the builds, yet, and to be honest, I didn't want to make an own one. If you're interested in that, you can get a rough idea of Apache2 vs. nginx performance in [this article](https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison) (spoiler: it's pretty poor).\n\nSo to conclude the discussion: Should you use Caddy in preference to nginx or Apache2? For private projects definitely yes, if you asked me. For more _serious_ projects you should probably wait until it's even a little more mature (e.g. when a 1.x.x version is out) and maybe also incorporating dynamic module loading. Until then I'd stick with nginx. Besides that I can't figure out a reason for preferring Apache2 over nginx, except for being too lazy to do the migration.\n\nPlease let me know if you liked my article and also if you don't agree with some of my arguments and insights.","slug":"caddy-a-modern-web-server-vs-nginx","published":1,"updated":"2020-06-06T15:14:33.457Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhhi000640mqj9ini2go","content":"<p><strong>Update:</strong> I‚Äôm glad to tell that this article made it to the front page of <a href=\"https://news.ycombinator.com/news\" target=\"_blank\" rel=\"noopener\">Hacker News</a> only a few hours after publication ü§ì.</p>\n<p>At the time of writing this article the web is effectively powered by three different major web server software packages. A web server, as covered in this article, basically has two purposes. One is to serve static (no dynamic functionality, no backend, no databse, ‚Ä¶) web sites, usually consisting of HTML, JavaScript and CSS plus images etc. The other is to act as a <a href=\"https://en.wikipedia.org/wiki/Reverse_proxy\" target=\"_blank\" rel=\"noopener\">reverse-proxy</a> to web application backends. The three servers I just mentioned have a combined market share of 94.7 % (according to <a href=\"https://w3techs.com/technologies/overview/web_server/all\" target=\"_blank\" rel=\"noopener\">this statistic</a>) and are named <a href=\"https://httpd.apache.org/\" target=\"_blank\" rel=\"noopener\">Apache 2</a> (or <em>httpd</em>) (written in C), <a href=\"https://www.nginx.com/solutions/web-server/\" target=\"_blank\" rel=\"noopener\">nginx</a> (say <em>‚Äúengine ex‚Äù</em>) (also written in C) and <a href=\"https://www.iis.net/\" target=\"_blank\" rel=\"noopener\">Microsoft IIS</a> (written in C++). While the first two are platform independent and open-source, the latter is a proprietary, commercial, Windows-only Microsoft product and therefore more interesting at enterprise level rather than for smaller indie projects. Consequently I won‚Äôt cover IIS further in the following.</p>\n<p><img src=\"images/webservers.png\" alt=\"Most popular web servers on the internet\"></p>\n<p><em>Most popular web servers on the internet (<a href=\"https://w3techs.com/technologies/overview/web_server/all\" target=\"_blank\" rel=\"noopener\">Source</a>)</em></p>\n<p>nginx‚Äô first release was in 2004 and Apache2‚Äôs roots even date back to 1995. Of course both projects are getting updates regularly, but their base concepts still remain the same. And at some point they might not perfectly fit today‚Äôs requirements anymore.</p>\n<p>Personally I switched from Apache2 to nginx a few months ago mainly because of two reasons. The first one was that I had really been annoyed by <a href=\"https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison\" target=\"_blank\" rel=\"noopener\">Apache2‚Äôs extremely high memory overhead</a>. The second reason was that Apache2 still didn‚Äôt have HTTP/2.0 support in 2016.</p>\n<p><img src=\"https://objects-us-west-1.dream.io/kbimages/images/Webserver_memory_graph.jpg\" alt=\"Apache2 vs. nginx memory usage\"></p>\n<p><em>Apache2 vs. nginx memory usage (<a href=\"https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison\" target=\"_blank\" rel=\"noopener\">Source</a>)</em></p>\n<p>I was pretty happy with nginx and especially its performance as well as the large amount of documentation and forum posts on the web about every conceivable problem were great. But since I‚Äôm a developer and not a sysadmin there‚Äôs one thing I didn‚Äôt like. The configuration is not that intuitive and you really need to get into the syntax and concepts to get an understanding of knobs to turn in order to achieve a certain goal. It‚Äôs also much more fine-grained than necessary for the average user. Personally I just want a simple config file with an intuitive syntax where I can tell my web server which static content to display or which backend to reverse-proxy for which route / domain. This, plus some additional features like handling compression, TLS encryption, authentication and maybe some basic rewrites, is fine. Looking for a more modern web server that fulfills these requirements I found <a href=\"https://caddyserver.com\" target=\"_blank\" rel=\"noopener\">Caddy</a>. As it turned out, it can even do a lot more cool things, while still being easy to use.</p>\n<p>Caddy is written is Go, open-source and pretty actively (according to commit history) developed on <a href=\"https://github.com/mholt/caddy\" target=\"_blank\" rel=\"noopener\">GitHub</a>. The goal when developing Caddy was exactly what I was looking for: easy configuration and fitness for today‚Äôs web applications. It comes with HTTP/2.0 (and even QUIC) support out of the box and serves via HTTPS by default (HTTP to HTTPS redirection is also handled automatically, while you manually had to tell Apache2 or nginx to do so). It even obtains (and regularly renews!) <a href=\"https://letsencrypt.org/\" target=\"_blank\" rel=\"noopener\">Let‚Äôs Encrypt</a> certificates for every (sub)domain you specified in the config file. While enabling HTTPS for a site was really a pain some years ago, it‚Äôs done completely automatically now. You don‚Äôt need to run any script. You don‚Äôt even need to create a Let‚Äôs Encrypt account or install the <em>certbot</em>. At the center of Caddy are is the <em>middleware</em> (or <em>directives</em>), which are added to the config as a one-liner. The <a href=\"https://caddyserver.com/docs/\" target=\"_blank\" rel=\"noopener\">list of such</a> is long and you will find a middleware for almost everything. For instance there are middleware components for logging, gzipping, header modification, (basic or <a href=\"https://jwt.io\" target=\"_blank\" rel=\"noopener\">JWT</a>-based) authentication and load balancing. But also more fancy things like automatically serving Markdown as HTML, a plug-and-play file browser GUI, HTML minification, IP filtering or pattern-based text replacement in HTML files are available as middlewares. Caddy also aligns well with PHP, using <em>php-fpm</em>, just as nginx does. As usual with Go applications, the entire program is shipped as a single binary (available for Windows, Mac, Linux and BSD), which includes all of its dependencies. Consequently you don‚Äôt need to install any further libraries to be linked (-&gt; no version conflicts), what really makes the installation a no-brainer. However, this introduces one little drawback in comparison to nginx modules: every middleware you want to use needs to be included into the binary and if it‚Äôs not, you need to re-compile the program (which is done for you by the download script at Caddy website, actually).</p>\n<p>I migrated all of my websites and -apps from nginx to Caddy (which took me hardly more than an hour) and so far I‚Äôm happy with the setup. But what about performance?</p>\n<p>To measure a very basic performance benchmark, I took <a href=\"https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh\" target=\"_blank\" rel=\"noopener\">this script</a>, which I used in <a href=\"https://muetsch.io/http-performance-java-jersey-vs-go-vs-nodejs.html\">an earlier benchmark scenario</a>. This script uses the <a href=\"https://github.com/nghttp2/nghttp2#benchmarking-tool\" target=\"_blank\" rel=\"noopener\">h2load</a> load test tool and I adjusted the parameters in a way that it performs a total of <strong>100,000 requests</strong> against a specific route at my webserver with a number of <strong>32 concurrent clients</strong> (each performing 3,125 requests) on <strong>4 CPU threads</strong>. I ran both servers with almost their default configuration, except that I turned on HTTP/2.0 with a self-signed certificate. The file served was a static HTML file containing 6.2 kBytes of data. Both h2load and the respective web server were executed locally on the same machine with the following specs.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">===CPU:</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\"> </span><br><span class=\"line\">===RAM: </span><br><span class=\"line\">              total        used        free      shared  buff/cache   available</span><br><span class=\"line\">Mem:           7,7G        1,8G        4,2G        316M        1,6G        5,3G</span><br><span class=\"line\">Swap:           29G          0B         29G</span><br><span class=\"line\"> </span><br><span class=\"line\">===OS: </span><br><span class=\"line\">Linux ferdinand-ubuntu 4.8.0-32-generic #34-Ubuntu SMP Tue Dec 13 14:30:43 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure>\n<p>The results look like this.<br>\n<img src=\"images/webserver_performance.png\" alt></p>\n<p><em>Caddy vs. nginx performance comparison</em></p>\n<p>As you can clearly see, nginx still performs way better, at least in this very simple scenario. However, Caddy is much more easy to use, in my opinion. Seems like we are having a classical trade-off here. Anyway, you should really give Caddy a try (and I‚Äôm not getting paid for this üòâ). Concerning memory usage: I didn‚Äôt observe that in detail, but suprisingly I found that neither the Caddy process nor the sum of nginx worker processes exceeded 10 MB of RAM usage (may I have done something wrong?).</p>\n<p>Please note that I only measured one specific figure (concurrent req/s) in one specific scenario. One can think of other benchmark setups where results might be the complete opposite, potentially.</p>\n<p>By the way, Apache2 was not included to this benchmark, because I wanted to use HTTP/2.0. Actually in the meantime there is a <code>mod_http2</code> for Apache2, but it‚Äôs not included in the majority of the builds, yet, and to be honest, I didn‚Äôt want to make an own one. If you‚Äôre interested in that, you can get a rough idea of Apache2 vs. nginx performance in <a href=\"https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison\" target=\"_blank\" rel=\"noopener\">this article</a> (spoiler: it‚Äôs pretty poor).</p>\n<p>So to conclude the discussion: Should you use Caddy in preference to nginx or Apache2? For private projects definitely yes, if you asked me. For more <em>serious</em> projects you should probably wait until it‚Äôs even a little more mature (e.g. when a 1.x.x version is out) and maybe also incorporating dynamic module loading. Until then I‚Äôd stick with nginx. Besides that I can‚Äôt figure out a reason for preferring Apache2 over nginx, except for being too lazy to do the migration.</p>\n<p>Please let me know if you liked my article and also if you don‚Äôt agree with some of my arguments and insights.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><strong>Update:</strong> I‚Äôm glad to tell that this article made it to the front page of <a href=\"https://news.ycombinator.com/news\" target=\"_blank\" rel=\"noopener\">Hacker News</a> only a few hours after publication ü§ì.</p>\n<p>At the time of writing this article the web is effectively powered by three different major web server software packages. A web server, as covered in this article, basically has two purposes. One is to serve static (no dynamic functionality, no backend, no databse, ‚Ä¶) web sites, usually consisting of HTML, JavaScript and CSS plus images etc. The other is to act as a <a href=\"https://en.wikipedia.org/wiki/Reverse_proxy\" target=\"_blank\" rel=\"noopener\">reverse-proxy</a> to web application backends. The three servers I just mentioned have a combined market share of 94.7 % (according to <a href=\"https://w3techs.com/technologies/overview/web_server/all\" target=\"_blank\" rel=\"noopener\">this statistic</a>) and are named <a href=\"https://httpd.apache.org/\" target=\"_blank\" rel=\"noopener\">Apache 2</a> (or <em>httpd</em>) (written in C), <a href=\"https://www.nginx.com/solutions/web-server/\" target=\"_blank\" rel=\"noopener\">nginx</a> (say <em>‚Äúengine ex‚Äù</em>) (also written in C) and <a href=\"https://www.iis.net/\" target=\"_blank\" rel=\"noopener\">Microsoft IIS</a> (written in C++). While the first two are platform independent and open-source, the latter is a proprietary, commercial, Windows-only Microsoft product and therefore more interesting at enterprise level rather than for smaller indie projects. Consequently I won‚Äôt cover IIS further in the following.</p>\n<p><img src=\"images/webservers.png\" alt=\"Most popular web servers on the internet\"></p>\n<p><em>Most popular web servers on the internet (<a href=\"https://w3techs.com/technologies/overview/web_server/all\" target=\"_blank\" rel=\"noopener\">Source</a>)</em></p>\n<p>nginx‚Äô first release was in 2004 and Apache2‚Äôs roots even date back to 1995. Of course both projects are getting updates regularly, but their base concepts still remain the same. And at some point they might not perfectly fit today‚Äôs requirements anymore.</p>\n<p>Personally I switched from Apache2 to nginx a few months ago mainly because of two reasons. The first one was that I had really been annoyed by <a href=\"https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison\" target=\"_blank\" rel=\"noopener\">Apache2‚Äôs extremely high memory overhead</a>. The second reason was that Apache2 still didn‚Äôt have HTTP/2.0 support in 2016.</p>\n<p><img src=\"https://objects-us-west-1.dream.io/kbimages/images/Webserver_memory_graph.jpg\" alt=\"Apache2 vs. nginx memory usage\"></p>\n<p><em>Apache2 vs. nginx memory usage (<a href=\"https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison\" target=\"_blank\" rel=\"noopener\">Source</a>)</em></p>\n<p>I was pretty happy with nginx and especially its performance as well as the large amount of documentation and forum posts on the web about every conceivable problem were great. But since I‚Äôm a developer and not a sysadmin there‚Äôs one thing I didn‚Äôt like. The configuration is not that intuitive and you really need to get into the syntax and concepts to get an understanding of knobs to turn in order to achieve a certain goal. It‚Äôs also much more fine-grained than necessary for the average user. Personally I just want a simple config file with an intuitive syntax where I can tell my web server which static content to display or which backend to reverse-proxy for which route / domain. This, plus some additional features like handling compression, TLS encryption, authentication and maybe some basic rewrites, is fine. Looking for a more modern web server that fulfills these requirements I found <a href=\"https://caddyserver.com\" target=\"_blank\" rel=\"noopener\">Caddy</a>. As it turned out, it can even do a lot more cool things, while still being easy to use.</p>\n<p>Caddy is written is Go, open-source and pretty actively (according to commit history) developed on <a href=\"https://github.com/mholt/caddy\" target=\"_blank\" rel=\"noopener\">GitHub</a>. The goal when developing Caddy was exactly what I was looking for: easy configuration and fitness for today‚Äôs web applications. It comes with HTTP/2.0 (and even QUIC) support out of the box and serves via HTTPS by default (HTTP to HTTPS redirection is also handled automatically, while you manually had to tell Apache2 or nginx to do so). It even obtains (and regularly renews!) <a href=\"https://letsencrypt.org/\" target=\"_blank\" rel=\"noopener\">Let‚Äôs Encrypt</a> certificates for every (sub)domain you specified in the config file. While enabling HTTPS for a site was really a pain some years ago, it‚Äôs done completely automatically now. You don‚Äôt need to run any script. You don‚Äôt even need to create a Let‚Äôs Encrypt account or install the <em>certbot</em>. At the center of Caddy are is the <em>middleware</em> (or <em>directives</em>), which are added to the config as a one-liner. The <a href=\"https://caddyserver.com/docs/\" target=\"_blank\" rel=\"noopener\">list of such</a> is long and you will find a middleware for almost everything. For instance there are middleware components for logging, gzipping, header modification, (basic or <a href=\"https://jwt.io\" target=\"_blank\" rel=\"noopener\">JWT</a>-based) authentication and load balancing. But also more fancy things like automatically serving Markdown as HTML, a plug-and-play file browser GUI, HTML minification, IP filtering or pattern-based text replacement in HTML files are available as middlewares. Caddy also aligns well with PHP, using <em>php-fpm</em>, just as nginx does. As usual with Go applications, the entire program is shipped as a single binary (available for Windows, Mac, Linux and BSD), which includes all of its dependencies. Consequently you don‚Äôt need to install any further libraries to be linked (-&gt; no version conflicts), what really makes the installation a no-brainer. However, this introduces one little drawback in comparison to nginx modules: every middleware you want to use needs to be included into the binary and if it‚Äôs not, you need to re-compile the program (which is done for you by the download script at Caddy website, actually).</p>\n<p>I migrated all of my websites and -apps from nginx to Caddy (which took me hardly more than an hour) and so far I‚Äôm happy with the setup. But what about performance?</p>\n<p>To measure a very basic performance benchmark, I took <a href=\"https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh\" target=\"_blank\" rel=\"noopener\">this script</a>, which I used in <a href=\"https://muetsch.io/http-performance-java-jersey-vs-go-vs-nodejs.html\">an earlier benchmark scenario</a>. This script uses the <a href=\"https://github.com/nghttp2/nghttp2#benchmarking-tool\" target=\"_blank\" rel=\"noopener\">h2load</a> load test tool and I adjusted the parameters in a way that it performs a total of <strong>100,000 requests</strong> against a specific route at my webserver with a number of <strong>32 concurrent clients</strong> (each performing 3,125 requests) on <strong>4 CPU threads</strong>. I ran both servers with almost their default configuration, except that I turned on HTTP/2.0 with a self-signed certificate. The file served was a static HTML file containing 6.2 kBytes of data. Both h2load and the respective web server were executed locally on the same machine with the following specs.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">===CPU:</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\"> </span><br><span class=\"line\">===RAM: </span><br><span class=\"line\">              total        used        free      shared  buff/cache   available</span><br><span class=\"line\">Mem:           7,7G        1,8G        4,2G        316M        1,6G        5,3G</span><br><span class=\"line\">Swap:           29G          0B         29G</span><br><span class=\"line\"> </span><br><span class=\"line\">===OS: </span><br><span class=\"line\">Linux ferdinand-ubuntu 4.8.0-32-generic #34-Ubuntu SMP Tue Dec 13 14:30:43 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure>\n<p>The results look like this.<br>\n<img src=\"images/webserver_performance.png\" alt></p>\n<p><em>Caddy vs. nginx performance comparison</em></p>\n<p>As you can clearly see, nginx still performs way better, at least in this very simple scenario. However, Caddy is much more easy to use, in my opinion. Seems like we are having a classical trade-off here. Anyway, you should really give Caddy a try (and I‚Äôm not getting paid for this üòâ). Concerning memory usage: I didn‚Äôt observe that in detail, but suprisingly I found that neither the Caddy process nor the sum of nginx worker processes exceeded 10 MB of RAM usage (may I have done something wrong?).</p>\n<p>Please note that I only measured one specific figure (concurrent req/s) in one specific scenario. One can think of other benchmark setups where results might be the complete opposite, potentially.</p>\n<p>By the way, Apache2 was not included to this benchmark, because I wanted to use HTTP/2.0. Actually in the meantime there is a <code>mod_http2</code> for Apache2, but it‚Äôs not included in the majority of the builds, yet, and to be honest, I didn‚Äôt want to make an own one. If you‚Äôre interested in that, you can get a rough idea of Apache2 vs. nginx performance in <a href=\"https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison\" target=\"_blank\" rel=\"noopener\">this article</a> (spoiler: it‚Äôs pretty poor).</p>\n<p>So to conclude the discussion: Should you use Caddy in preference to nginx or Apache2? For private projects definitely yes, if you asked me. For more <em>serious</em> projects you should probably wait until it‚Äôs even a little more mature (e.g. when a 1.x.x version is out) and maybe also incorporating dynamic module loading. Until then I‚Äôd stick with nginx. Besides that I can‚Äôt figure out a reason for preferring Apache2 over nginx, except for being too lazy to do the migration.</p>\n<p>Please let me know if you liked my article and also if you don‚Äôt agree with some of my arguments and insights.</p>\n"},{"title":"CartPole with a Deep Q-Network","date":"2017-09-11T16:47:39.000Z","_content":"In my [last post](https://muetsch.io/cartpole-with-qlearning-first-experiences-with-openai-gym.html) I developed a solution to [OpenAI Gym's CartPole environment](https://gym.openai.com/envs/CartPole-v0), based on a classical Q-Learning algorithm. **The best score I achieved with it was 120**, although the score I uploaded to the [leaderboard](https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q) was 188. While this is certainly not a bad result, I wondered if I could do better using more advanced techniques. Besides that I also wanted to practice the concepts I had recently learned in the [Machine Learning 2](http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en) course at university. By the way, to all the students among you: I found that one of the best way to learn about new algorithms etc. is to actually **try to implement them in code**!\n\n## Motivation\nOne major limitation of my classical Q-Learning approach was that the number of possible states had to be reduced from basically infinity (due to the observations' continuous nature) to, in my case, `1 * 1 * 6 * 12 = 72` discrete states. Considering this extreme simpification, the results were astonishingly good! However, what if we could utilize more of the information, the observations give us? One solution is to combine Q-Learning with a (deep) neural network, which results in a technique called Deep Q-Learning (DQN). Neural networks are inherently efficient when handling very high dimensional problems. That's why they are doing so well with image-, video- and audio data. Additionally, they can easily handle continuous inputs, whereas with our classical approach we needed the Q-table to be a finite (in this case (4+1)-dimensional) matrix (or tensor). Accordingly, with DQN we don't need discrete buckets anymore, but are able to directly use the raw observations.\n\n## Deep Q-Learning\nBut how does this even work? While I don't want to explain DQN in detail here, the basic idea is to replace the Q-table by a neural network, which is trained to predict Q-values for a state. The input is a state-vector (or a batch of such) - consisting of four features in this case (which corresponds to four input neurons). The output is a vector of Q-values, one for each possible action - two in our case (corresponding to two output neurons). The training is done using **experience replay**. Basically, the agent begins to try some random actions and stores its \"experiences\" into a memory. An experience is a tuple like `(old_state, performed_action, received_reward, new_state)`. At fixed intervals (e.g. after each training episode, but NOT after each step), batches are sampled from memory and used as training data for the network. Consequently, the network (hopefully) improves every episode and predicts more precise Q-values for state-action pairs. \n\n## My implementation\nMy implementation is essentially based on [this great blog post](https://keon.io/deep-q-learning/) by [Keon](https://github.com/keon). It uses [Keras](http://keras.io) as a high-level abstraction on top of [TensorFlow](http://tensorflow.com). However, while I adopted the general structure, I made several tweaks and fixes to massively improve performance. \n\n### Tweak #1: More hidden neurons\nI slightly modified the network layout by **doubling the number of hidden neurons** in the second hidden layer. While randomly experimenting with some layouts I found that this one seemed to work better on average. Generally, determining a neural network's layout is basically trial and error for the most parts. Mainly you want to master the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), but there is no rule on how to choose network structure in order to do so. Mine looks like this now:\n\n![](images/dqn4.png)\n\n### Tweak #2: Larger replay memory\nIn [Keon](https://github.com/keon)'s original implementation, the replay memory had a maxmimum size of 2,000. Assuming an average \"survival\" time of 100 steps, it would only hold experiences from 20 episodes, which is not much. I didn't see any reason why they shouldn't be a greater variety in training examples, so I increased the **memory size to 100,000**.\n\n### Tweak #3: Mini-batch training\nWhile originally the network trained from batches of 32 examples in each episode, the way the training was conducted was not efficient in my opinion. Instead of giving TensorFlow a **32 x 4 matrix**, it was given a **1 x 4 matrix 32 times**, so the actual training procedure effectively used a mini-batch size of 1. Without having technical knowledge on how TensorFlow works, I'm still pretty sure that training the network with one large batch instead of 32 small ones is faster - especially when using a GPU. \n\n### Tweak #4: Setting Œ≥ = 1\nAs already mentioned in my last post, I'm of the opinion that it wouldn't make sense to set the gamma parameter to less than one. Its purpose is to \"penalize\" the agent if it takes long to reach its goal. However, in CartPole **its even our goal** to do as many steps as possible. \n\n### Tweak #5: Logarithmic Œµ-decay\nSince the adaptive exploration rate from [@tuzzer's solution](https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947) was very effective in my last implementation, I simply adopted it for this one, too. I didn't cross-validate whether it's better or worse than [Keon](https://github.com/keon)'s epsilon decay, but at least it doesn't seem to do bad.\n\n![](images/dqn3.png)\n\n### Tweak #6: tanh activation function\nI'm really not sure about this point, so please correct me if I'm wrong. The original implementation used the ReLU activation function, which is a linear function that maps the input to itself, but thesholded at zero.\n\n![](images/dqn2.png)\n\nHowever, since the input features can be negative, ReLU might cause dead neurons, doesn't it? To overcome that problem, I decided to _tanh_ as an activation function.\n\n### Tweak #7: Cross-validate hyperparameters\nEventually, I conducted a grid search (using my [script](https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033) from the last time) to find good values for `alpha` (learning rate), `alpha_decay` and `epsilon_min` (minimum exploration rate). It turned out that `alpha=0.01`, `alpha_decay=0.01` and `epsilon_min=0.01` seem to work best among all tested values on average.\n\n## Results\nAfter all these optimizations, I ran the algorithm several times and the best score I achieved was actually **24**. However, I didn't record that run, so my best score in the [leaderboard](https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g) in **85**, which is better then with my classical Q-Learning approach.\nHowever, I found that although the DQN approach _can_ converge faster, it also seems to be way more unstable at the same time. Performing two consecutive runs of the exact same algorithm and configuration often resulted in one score to be extremely good, while the second one didn't even solve the environment at all (> 999 episodes). In other words, DQN has a way larger variance, than the table-based approach, as depicted in the chart below, where I performed 25 runs of each algorithm. Additionally, as expected, the neural network is slower. Running DQN 25 times took **744 seconds**, while running table-based Q-Learning 25 times only took **24 seconds** on my machine's CPU using four threads on four cores. \n\n[>> Code on GitHub (dqn_cartpole.py)](https://gist.github.com/muety/2a6722407117e4d668921fce53845432)\n\n![](images/dqn1.png)\n\n**Q**: `Min 120, Max 999, Mean 197.16, Std: 183.223`\n**DQN**: `Min 56, Max 999, Mean 600.04, Std: 356.046`","source":"_posts/cartpole-with-a-deep-q-network.md","raw":"---\ntitle: CartPole with a Deep Q-Network\ndate: 2017-09-11 18:47:39\ntags:\n---\nIn my [last post](https://muetsch.io/cartpole-with-qlearning-first-experiences-with-openai-gym.html) I developed a solution to [OpenAI Gym's CartPole environment](https://gym.openai.com/envs/CartPole-v0), based on a classical Q-Learning algorithm. **The best score I achieved with it was 120**, although the score I uploaded to the [leaderboard](https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q) was 188. While this is certainly not a bad result, I wondered if I could do better using more advanced techniques. Besides that I also wanted to practice the concepts I had recently learned in the [Machine Learning 2](http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en) course at university. By the way, to all the students among you: I found that one of the best way to learn about new algorithms etc. is to actually **try to implement them in code**!\n\n## Motivation\nOne major limitation of my classical Q-Learning approach was that the number of possible states had to be reduced from basically infinity (due to the observations' continuous nature) to, in my case, `1 * 1 * 6 * 12 = 72` discrete states. Considering this extreme simpification, the results were astonishingly good! However, what if we could utilize more of the information, the observations give us? One solution is to combine Q-Learning with a (deep) neural network, which results in a technique called Deep Q-Learning (DQN). Neural networks are inherently efficient when handling very high dimensional problems. That's why they are doing so well with image-, video- and audio data. Additionally, they can easily handle continuous inputs, whereas with our classical approach we needed the Q-table to be a finite (in this case (4+1)-dimensional) matrix (or tensor). Accordingly, with DQN we don't need discrete buckets anymore, but are able to directly use the raw observations.\n\n## Deep Q-Learning\nBut how does this even work? While I don't want to explain DQN in detail here, the basic idea is to replace the Q-table by a neural network, which is trained to predict Q-values for a state. The input is a state-vector (or a batch of such) - consisting of four features in this case (which corresponds to four input neurons). The output is a vector of Q-values, one for each possible action - two in our case (corresponding to two output neurons). The training is done using **experience replay**. Basically, the agent begins to try some random actions and stores its \"experiences\" into a memory. An experience is a tuple like `(old_state, performed_action, received_reward, new_state)`. At fixed intervals (e.g. after each training episode, but NOT after each step), batches are sampled from memory and used as training data for the network. Consequently, the network (hopefully) improves every episode and predicts more precise Q-values for state-action pairs. \n\n## My implementation\nMy implementation is essentially based on [this great blog post](https://keon.io/deep-q-learning/) by [Keon](https://github.com/keon). It uses [Keras](http://keras.io) as a high-level abstraction on top of [TensorFlow](http://tensorflow.com). However, while I adopted the general structure, I made several tweaks and fixes to massively improve performance. \n\n### Tweak #1: More hidden neurons\nI slightly modified the network layout by **doubling the number of hidden neurons** in the second hidden layer. While randomly experimenting with some layouts I found that this one seemed to work better on average. Generally, determining a neural network's layout is basically trial and error for the most parts. Mainly you want to master the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), but there is no rule on how to choose network structure in order to do so. Mine looks like this now:\n\n![](images/dqn4.png)\n\n### Tweak #2: Larger replay memory\nIn [Keon](https://github.com/keon)'s original implementation, the replay memory had a maxmimum size of 2,000. Assuming an average \"survival\" time of 100 steps, it would only hold experiences from 20 episodes, which is not much. I didn't see any reason why they shouldn't be a greater variety in training examples, so I increased the **memory size to 100,000**.\n\n### Tweak #3: Mini-batch training\nWhile originally the network trained from batches of 32 examples in each episode, the way the training was conducted was not efficient in my opinion. Instead of giving TensorFlow a **32 x 4 matrix**, it was given a **1 x 4 matrix 32 times**, so the actual training procedure effectively used a mini-batch size of 1. Without having technical knowledge on how TensorFlow works, I'm still pretty sure that training the network with one large batch instead of 32 small ones is faster - especially when using a GPU. \n\n### Tweak #4: Setting Œ≥ = 1\nAs already mentioned in my last post, I'm of the opinion that it wouldn't make sense to set the gamma parameter to less than one. Its purpose is to \"penalize\" the agent if it takes long to reach its goal. However, in CartPole **its even our goal** to do as many steps as possible. \n\n### Tweak #5: Logarithmic Œµ-decay\nSince the adaptive exploration rate from [@tuzzer's solution](https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947) was very effective in my last implementation, I simply adopted it for this one, too. I didn't cross-validate whether it's better or worse than [Keon](https://github.com/keon)'s epsilon decay, but at least it doesn't seem to do bad.\n\n![](images/dqn3.png)\n\n### Tweak #6: tanh activation function\nI'm really not sure about this point, so please correct me if I'm wrong. The original implementation used the ReLU activation function, which is a linear function that maps the input to itself, but thesholded at zero.\n\n![](images/dqn2.png)\n\nHowever, since the input features can be negative, ReLU might cause dead neurons, doesn't it? To overcome that problem, I decided to _tanh_ as an activation function.\n\n### Tweak #7: Cross-validate hyperparameters\nEventually, I conducted a grid search (using my [script](https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033) from the last time) to find good values for `alpha` (learning rate), `alpha_decay` and `epsilon_min` (minimum exploration rate). It turned out that `alpha=0.01`, `alpha_decay=0.01` and `epsilon_min=0.01` seem to work best among all tested values on average.\n\n## Results\nAfter all these optimizations, I ran the algorithm several times and the best score I achieved was actually **24**. However, I didn't record that run, so my best score in the [leaderboard](https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g) in **85**, which is better then with my classical Q-Learning approach.\nHowever, I found that although the DQN approach _can_ converge faster, it also seems to be way more unstable at the same time. Performing two consecutive runs of the exact same algorithm and configuration often resulted in one score to be extremely good, while the second one didn't even solve the environment at all (> 999 episodes). In other words, DQN has a way larger variance, than the table-based approach, as depicted in the chart below, where I performed 25 runs of each algorithm. Additionally, as expected, the neural network is slower. Running DQN 25 times took **744 seconds**, while running table-based Q-Learning 25 times only took **24 seconds** on my machine's CPU using four threads on four cores. \n\n[>> Code on GitHub (dqn_cartpole.py)](https://gist.github.com/muety/2a6722407117e4d668921fce53845432)\n\n![](images/dqn1.png)\n\n**Q**: `Min 120, Max 999, Mean 197.16, Std: 183.223`\n**DQN**: `Min 56, Max 999, Mean 600.04, Std: 356.046`","slug":"cartpole-with-a-deep-q-network","published":1,"updated":"2020-06-06T15:14:33.457Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhhl000740mqsyyrq980","content":"<p>In my <a href=\"https://muetsch.io/cartpole-with-qlearning-first-experiences-with-openai-gym.html\">last post</a> I developed a solution to <a href=\"https://gym.openai.com/envs/CartPole-v0\" target=\"_blank\" rel=\"noopener\">OpenAI Gym‚Äôs CartPole environment</a>, based on a classical Q-Learning algorithm. <strong>The best score I achieved with it was 120</strong>, although the score I uploaded to the <a href=\"https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q\" target=\"_blank\" rel=\"noopener\">leaderboard</a> was 188. While this is certainly not a bad result, I wondered if I could do better using more advanced techniques. Besides that I also wanted to practice the concepts I had recently learned in the <a href=\"http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en\" target=\"_blank\" rel=\"noopener\">Machine Learning 2</a> course at university. By the way, to all the students among you: I found that one of the best way to learn about new algorithms etc. is to actually <strong>try to implement them in code</strong>!</p>\n<h2 id=\"Motivation\">Motivation</h2>\n<p>One major limitation of my classical Q-Learning approach was that the number of possible states had to be reduced from basically infinity (due to the observations‚Äô continuous nature) to, in my case, <code>1 * 1 * 6 * 12 = 72</code> discrete states. Considering this extreme simpification, the results were astonishingly good! However, what if we could utilize more of the information, the observations give us? One solution is to combine Q-Learning with a (deep) neural network, which results in a technique called Deep Q-Learning (DQN). Neural networks are inherently efficient when handling very high dimensional problems. That‚Äôs why they are doing so well with image-, video- and audio data. Additionally, they can easily handle continuous inputs, whereas with our classical approach we needed the Q-table to be a finite (in this case (4+1)-dimensional) matrix (or tensor). Accordingly, with DQN we don‚Äôt need discrete buckets anymore, but are able to directly use the raw observations.</p>\n<h2 id=\"Deep-Q-Learning\">Deep Q-Learning</h2>\n<p>But how does this even work? While I don‚Äôt want to explain DQN in detail here, the basic idea is to replace the Q-table by a neural network, which is trained to predict Q-values for a state. The input is a state-vector (or a batch of such) - consisting of four features in this case (which corresponds to four input neurons). The output is a vector of Q-values, one for each possible action - two in our case (corresponding to two output neurons). The training is done using <strong>experience replay</strong>. Basically, the agent begins to try some random actions and stores its ‚Äúexperiences‚Äù into a memory. An experience is a tuple like <code>(old_state, performed_action, received_reward, new_state)</code>. At fixed intervals (e.g. after each training episode, but NOT after each step), batches are sampled from memory and used as training data for the network. Consequently, the network (hopefully) improves every episode and predicts more precise Q-values for state-action pairs.</p>\n<h2 id=\"My-implementation\">My implementation</h2>\n<p>My implementation is essentially based on <a href=\"https://keon.io/deep-q-learning/\" target=\"_blank\" rel=\"noopener\">this great blog post</a> by <a href=\"https://github.com/keon\" target=\"_blank\" rel=\"noopener\">Keon</a>. It uses <a href=\"http://keras.io\" target=\"_blank\" rel=\"noopener\">Keras</a> as a high-level abstraction on top of <a href=\"http://tensorflow.com\" target=\"_blank\" rel=\"noopener\">TensorFlow</a>. However, while I adopted the general structure, I made several tweaks and fixes to massively improve performance.</p>\n<h3 id=\"Tweak-1-More-hidden-neurons\">Tweak #1: More hidden neurons</h3>\n<p>I slightly modified the network layout by <strong>doubling the number of hidden neurons</strong> in the second hidden layer. While randomly experimenting with some layouts I found that this one seemed to work better on average. Generally, determining a neural network‚Äôs layout is basically trial and error for the most parts. Mainly you want to master the <a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" target=\"_blank\" rel=\"noopener\">bias-variance tradeoff</a>, but there is no rule on how to choose network structure in order to do so. Mine looks like this now:</p>\n<p><img src=\"images/dqn4.png\" alt></p>\n<h3 id=\"Tweak-2-Larger-replay-memory\">Tweak #2: Larger replay memory</h3>\n<p>In <a href=\"https://github.com/keon\" target=\"_blank\" rel=\"noopener\">Keon</a>‚Äôs original implementation, the replay memory had a maxmimum size of 2,000. Assuming an average ‚Äúsurvival‚Äù time of 100 steps, it would only hold experiences from 20 episodes, which is not much. I didn‚Äôt see any reason why they shouldn‚Äôt be a greater variety in training examples, so I increased the <strong>memory size to 100,000</strong>.</p>\n<h3 id=\"Tweak-3-Mini-batch-training\">Tweak #3: Mini-batch training</h3>\n<p>While originally the network trained from batches of 32 examples in each episode, the way the training was conducted was not efficient in my opinion. Instead of giving TensorFlow a <strong>32 x 4 matrix</strong>, it was given a <strong>1 x 4 matrix 32 times</strong>, so the actual training procedure effectively used a mini-batch size of 1. Without having technical knowledge on how TensorFlow works, I‚Äôm still pretty sure that training the network with one large batch instead of 32 small ones is faster - especially when using a GPU.</p>\n<h3 id=\"Tweak-4-Setting-Œ≥-1\">Tweak #4: Setting Œ≥ = 1</h3>\n<p>As already mentioned in my last post, I‚Äôm of the opinion that it wouldn‚Äôt make sense to set the gamma parameter to less than one. Its purpose is to ‚Äúpenalize‚Äù the agent if it takes long to reach its goal. However, in CartPole <strong>its even our goal</strong> to do as many steps as possible.</p>\n<h3 id=\"Tweak-5-Logarithmic-Œµ-decay\">Tweak #5: Logarithmic Œµ-decay</h3>\n<p>Since the adaptive exploration rate from <a href=\"https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947\" target=\"_blank\" rel=\"noopener\">@tuzzer‚Äôs solution</a> was very effective in my last implementation, I simply adopted it for this one, too. I didn‚Äôt cross-validate whether it‚Äôs better or worse than <a href=\"https://github.com/keon\" target=\"_blank\" rel=\"noopener\">Keon</a>‚Äôs epsilon decay, but at least it doesn‚Äôt seem to do bad.</p>\n<p><img src=\"images/dqn3.png\" alt></p>\n<h3 id=\"Tweak-6-tanh-activation-function\">Tweak #6: tanh activation function</h3>\n<p>I‚Äôm really not sure about this point, so please correct me if I‚Äôm wrong. The original implementation used the ReLU activation function, which is a linear function that maps the input to itself, but thesholded at zero.</p>\n<p><img src=\"images/dqn2.png\" alt></p>\n<p>However, since the input features can be negative, ReLU might cause dead neurons, doesn‚Äôt it? To overcome that problem, I decided to <em>tanh</em> as an activation function.</p>\n<h3 id=\"Tweak-7-Cross-validate-hyperparameters\">Tweak #7: Cross-validate hyperparameters</h3>\n<p>Eventually, I conducted a grid search (using my <a href=\"https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033\" target=\"_blank\" rel=\"noopener\">script</a> from the last time) to find good values for <code>alpha</code> (learning rate), <code>alpha_decay</code> and <code>epsilon_min</code> (minimum exploration rate). It turned out that <code>alpha=0.01</code>, <code>alpha_decay=0.01</code> and <code>epsilon_min=0.01</code> seem to work best among all tested values on average.</p>\n<h2 id=\"Results\">Results</h2>\n<p>After all these optimizations, I ran the algorithm several times and the best score I achieved was actually <strong>24</strong>. However, I didn‚Äôt record that run, so my best score in the <a href=\"https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g\" target=\"_blank\" rel=\"noopener\">leaderboard</a> in <strong>85</strong>, which is better then with my classical Q-Learning approach.<br>\nHowever, I found that although the DQN approach <em>can</em> converge faster, it also seems to be way more unstable at the same time. Performing two consecutive runs of the exact same algorithm and configuration often resulted in one score to be extremely good, while the second one didn‚Äôt even solve the environment at all (&gt; 999 episodes). In other words, DQN has a way larger variance, than the table-based approach, as depicted in the chart below, where I performed 25 runs of each algorithm. Additionally, as expected, the neural network is slower. Running DQN 25 times took <strong>744 seconds</strong>, while running table-based Q-Learning 25 times only took <strong>24 seconds</strong> on my machine‚Äôs CPU using four threads on four cores.</p>\n<p><a href=\"https://gist.github.com/muety/2a6722407117e4d668921fce53845432\" target=\"_blank\" rel=\"noopener\">&gt;&gt; Code on GitHub (dqn_cartpole.py)</a></p>\n<p><img src=\"images/dqn1.png\" alt></p>\n<p><strong>Q</strong>: <code>Min 120, Max 999, Mean 197.16, Std: 183.223</code><br>\n<strong>DQN</strong>: <code>Min 56, Max 999, Mean 600.04, Std: 356.046</code></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>In my <a href=\"https://muetsch.io/cartpole-with-qlearning-first-experiences-with-openai-gym.html\">last post</a> I developed a solution to <a href=\"https://gym.openai.com/envs/CartPole-v0\" target=\"_blank\" rel=\"noopener\">OpenAI Gym‚Äôs CartPole environment</a>, based on a classical Q-Learning algorithm. <strong>The best score I achieved with it was 120</strong>, although the score I uploaded to the <a href=\"https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q\" target=\"_blank\" rel=\"noopener\">leaderboard</a> was 188. While this is certainly not a bad result, I wondered if I could do better using more advanced techniques. Besides that I also wanted to practice the concepts I had recently learned in the <a href=\"http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en\" target=\"_blank\" rel=\"noopener\">Machine Learning 2</a> course at university. By the way, to all the students among you: I found that one of the best way to learn about new algorithms etc. is to actually <strong>try to implement them in code</strong>!</p>\n<h2 id=\"Motivation\">Motivation</h2>\n<p>One major limitation of my classical Q-Learning approach was that the number of possible states had to be reduced from basically infinity (due to the observations‚Äô continuous nature) to, in my case, <code>1 * 1 * 6 * 12 = 72</code> discrete states. Considering this extreme simpification, the results were astonishingly good! However, what if we could utilize more of the information, the observations give us? One solution is to combine Q-Learning with a (deep) neural network, which results in a technique called Deep Q-Learning (DQN). Neural networks are inherently efficient when handling very high dimensional problems. That‚Äôs why they are doing so well with image-, video- and audio data. Additionally, they can easily handle continuous inputs, whereas with our classical approach we needed the Q-table to be a finite (in this case (4+1)-dimensional) matrix (or tensor). Accordingly, with DQN we don‚Äôt need discrete buckets anymore, but are able to directly use the raw observations.</p>\n<h2 id=\"Deep-Q-Learning\">Deep Q-Learning</h2>\n<p>But how does this even work? While I don‚Äôt want to explain DQN in detail here, the basic idea is to replace the Q-table by a neural network, which is trained to predict Q-values for a state. The input is a state-vector (or a batch of such) - consisting of four features in this case (which corresponds to four input neurons). The output is a vector of Q-values, one for each possible action - two in our case (corresponding to two output neurons). The training is done using <strong>experience replay</strong>. Basically, the agent begins to try some random actions and stores its ‚Äúexperiences‚Äù into a memory. An experience is a tuple like <code>(old_state, performed_action, received_reward, new_state)</code>. At fixed intervals (e.g. after each training episode, but NOT after each step), batches are sampled from memory and used as training data for the network. Consequently, the network (hopefully) improves every episode and predicts more precise Q-values for state-action pairs.</p>\n<h2 id=\"My-implementation\">My implementation</h2>\n<p>My implementation is essentially based on <a href=\"https://keon.io/deep-q-learning/\" target=\"_blank\" rel=\"noopener\">this great blog post</a> by <a href=\"https://github.com/keon\" target=\"_blank\" rel=\"noopener\">Keon</a>. It uses <a href=\"http://keras.io\" target=\"_blank\" rel=\"noopener\">Keras</a> as a high-level abstraction on top of <a href=\"http://tensorflow.com\" target=\"_blank\" rel=\"noopener\">TensorFlow</a>. However, while I adopted the general structure, I made several tweaks and fixes to massively improve performance.</p>\n<h3 id=\"Tweak-1-More-hidden-neurons\">Tweak #1: More hidden neurons</h3>\n<p>I slightly modified the network layout by <strong>doubling the number of hidden neurons</strong> in the second hidden layer. While randomly experimenting with some layouts I found that this one seemed to work better on average. Generally, determining a neural network‚Äôs layout is basically trial and error for the most parts. Mainly you want to master the <a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" target=\"_blank\" rel=\"noopener\">bias-variance tradeoff</a>, but there is no rule on how to choose network structure in order to do so. Mine looks like this now:</p>\n<p><img src=\"images/dqn4.png\" alt></p>\n<h3 id=\"Tweak-2-Larger-replay-memory\">Tweak #2: Larger replay memory</h3>\n<p>In <a href=\"https://github.com/keon\" target=\"_blank\" rel=\"noopener\">Keon</a>‚Äôs original implementation, the replay memory had a maxmimum size of 2,000. Assuming an average ‚Äúsurvival‚Äù time of 100 steps, it would only hold experiences from 20 episodes, which is not much. I didn‚Äôt see any reason why they shouldn‚Äôt be a greater variety in training examples, so I increased the <strong>memory size to 100,000</strong>.</p>\n<h3 id=\"Tweak-3-Mini-batch-training\">Tweak #3: Mini-batch training</h3>\n<p>While originally the network trained from batches of 32 examples in each episode, the way the training was conducted was not efficient in my opinion. Instead of giving TensorFlow a <strong>32 x 4 matrix</strong>, it was given a <strong>1 x 4 matrix 32 times</strong>, so the actual training procedure effectively used a mini-batch size of 1. Without having technical knowledge on how TensorFlow works, I‚Äôm still pretty sure that training the network with one large batch instead of 32 small ones is faster - especially when using a GPU.</p>\n<h3 id=\"Tweak-4-Setting-Œ≥-1\">Tweak #4: Setting Œ≥ = 1</h3>\n<p>As already mentioned in my last post, I‚Äôm of the opinion that it wouldn‚Äôt make sense to set the gamma parameter to less than one. Its purpose is to ‚Äúpenalize‚Äù the agent if it takes long to reach its goal. However, in CartPole <strong>its even our goal</strong> to do as many steps as possible.</p>\n<h3 id=\"Tweak-5-Logarithmic-Œµ-decay\">Tweak #5: Logarithmic Œµ-decay</h3>\n<p>Since the adaptive exploration rate from <a href=\"https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947\" target=\"_blank\" rel=\"noopener\">@tuzzer‚Äôs solution</a> was very effective in my last implementation, I simply adopted it for this one, too. I didn‚Äôt cross-validate whether it‚Äôs better or worse than <a href=\"https://github.com/keon\" target=\"_blank\" rel=\"noopener\">Keon</a>‚Äôs epsilon decay, but at least it doesn‚Äôt seem to do bad.</p>\n<p><img src=\"images/dqn3.png\" alt></p>\n<h3 id=\"Tweak-6-tanh-activation-function\">Tweak #6: tanh activation function</h3>\n<p>I‚Äôm really not sure about this point, so please correct me if I‚Äôm wrong. The original implementation used the ReLU activation function, which is a linear function that maps the input to itself, but thesholded at zero.</p>\n<p><img src=\"images/dqn2.png\" alt></p>\n<p>However, since the input features can be negative, ReLU might cause dead neurons, doesn‚Äôt it? To overcome that problem, I decided to <em>tanh</em> as an activation function.</p>\n<h3 id=\"Tweak-7-Cross-validate-hyperparameters\">Tweak #7: Cross-validate hyperparameters</h3>\n<p>Eventually, I conducted a grid search (using my <a href=\"https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033\" target=\"_blank\" rel=\"noopener\">script</a> from the last time) to find good values for <code>alpha</code> (learning rate), <code>alpha_decay</code> and <code>epsilon_min</code> (minimum exploration rate). It turned out that <code>alpha=0.01</code>, <code>alpha_decay=0.01</code> and <code>epsilon_min=0.01</code> seem to work best among all tested values on average.</p>\n<h2 id=\"Results\">Results</h2>\n<p>After all these optimizations, I ran the algorithm several times and the best score I achieved was actually <strong>24</strong>. However, I didn‚Äôt record that run, so my best score in the <a href=\"https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g\" target=\"_blank\" rel=\"noopener\">leaderboard</a> in <strong>85</strong>, which is better then with my classical Q-Learning approach.<br>\nHowever, I found that although the DQN approach <em>can</em> converge faster, it also seems to be way more unstable at the same time. Performing two consecutive runs of the exact same algorithm and configuration often resulted in one score to be extremely good, while the second one didn‚Äôt even solve the environment at all (&gt; 999 episodes). In other words, DQN has a way larger variance, than the table-based approach, as depicted in the chart below, where I performed 25 runs of each algorithm. Additionally, as expected, the neural network is slower. Running DQN 25 times took <strong>744 seconds</strong>, while running table-based Q-Learning 25 times only took <strong>24 seconds</strong> on my machine‚Äôs CPU using four threads on four cores.</p>\n<p><a href=\"https://gist.github.com/muety/2a6722407117e4d668921fce53845432\" target=\"_blank\" rel=\"noopener\">&gt;&gt; Code on GitHub (dqn_cartpole.py)</a></p>\n<p><img src=\"images/dqn1.png\" alt></p>\n<p><strong>Q</strong>: <code>Min 120, Max 999, Mean 197.16, Std: 183.223</code><br>\n<strong>DQN</strong>: <code>Min 56, Max 999, Mean 600.04, Std: 356.046</code></p>\n"},{"title":"CartPole with Q-Learning - First experiences with OpenAI Gym","date":"2017-08-24T14:50:57.000Z","_content":"## OpenAI Gym\nToday I made my first experiences with the [OpenAI gym](https://gym.openai.com), more specifically with the [CartPole](https://gym.openai.com/envs/CartPole-v0) environment. Gym is basically a Python library that includes several machine learning challenges, in which an autonomous agent should be learned to fulfill different tasks, e.g. to master a simple game itself. One of the simplest and most popular challenges is CartPole. It's basically a 2D game in which the agent has to control, i.e. move left or right, a cart to balance a pole standing perpendicularly on the cart. This is a classical reinforcement learning problem, e.g. a scenario, in which the agents starts by trying random actions as a consequence to which it gets rewarded (or not). Based on the rewards, it continuously \"learns\", which action is good in which specific situation. Doing so, it learns how to master the game without ever being told how the game even works. The main advantage of this type of learning is, that it's completely generic and not bound to a specific problem. E.g. to learn a chess agent, you don't need to \"tell\" it the rules of chess, but just let it do trial & error, whereas \"error\" means giving it a negative (or small positive) reward.\n\n## CartPole-v0\nIn machine learning terms, CartPole is basically a binary classification problem. There are four features as inputs, which include the cart position, its velocity, the pole's angle to the cart and its derivative (i.e. how fast the pole is \"falling\"). The output is binary, i.e. either 0 or 1, corresponding to \"left\" or \"right\". One challenge is the fact that all four features are continuous values (floating point numbers), which, naively, implies an infinitely large feature space.\n\n![](images/cartpole1.jpg)\n\n## Approach: Basic Q-Learning\nIn the [Machine Learning 1](https://his.anthropomatik.kit.edu/english/28_315.php) course at my university, I got to know one of the most basic, yet widely-used reinforcement learning approaches, which is [Q-Learning](http://mnemstudio.org/path-finding-q-learning-tutorial.htm). The core of Q-Learning is to estimate a value for every possible pare of a state (s) and an action (a) by getting rewarded. Imagine the following graph, which consists of three states, while your agent is currently in _s0_. It can choose between two actions, one of which results in a good state _s1_ (e.g. having won the game), the other one results in a bad state _s2_ (e.g. having lost the game). Accordingly, the transition leading to the good (bad) state gives a reward of 100 (-100). If the agent performs action _a0_, the q-value of _s0_ will probably become negative (Q(s0, a0) < 0)), while Q(s0, a1) > 0.\n\n![](images/cartpole2.png)\n\nThe update of the q-value is done according to the following equation.\n\n![](images/cartpole3.png)\n\nBasically, a (S, A)-tuple's new q-value depends on its old q-value, the immediate reward received for the action and the maximum q-value achievable in the following state. So a (S, A)-pair's q-value indirectly depends on all its successors' q-values, which is expressed in the recursive function definition. By repeatedly walking through all nodes and transistions, the agent can update any (S, A)-pairs q-value, while the results of good and bad actions are slowly \"backpropagated\" from terminal nodes to early nodes. The agent ends up with a (usually multidimensional) table mapping states and actions to q-values, so that given any state, the best action can be picked by choosing the highest respective q-value.\n\n## My implementation\nSince Q-Learning is pretty straightforward to understand and implement, I decided on picking that algorithm as a starting point for my CartPole challenge. I looked for other solutions, that also use CartPole and found [this blog post](https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947) by [@tuzzer](https://medium.com/@tuzzer), which had partially inspired me during my implementation. \n\n[>> Code on GitHub (qcartpole.py)](https://gist.github.com/muety/af0b8476ae4106ec098fea1dfe57f578)\n\n### Transforming the feature space\nActually the main challenge was to convert the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states. The less states we have, the smaller the Q-table will be, the less steps the agent will need to properly learn its values. However, too few states might not hold enough information to properly represent the environment. The original domains of the input features are these.\n\n* __x__ (cart position) ‚àà [-4.8, 4.8]\n* __x'__ (cart velocity) ‚àà [-3.4 * 10^38, 3.4 * 10^38]\n* __theta__ (angle) ‚àà [-0.42, 0.42]\n* __theta'__ (angle velocity) ‚àà [-3.4 * 10^38, 3.4 * 10^38]\n\n### Finding the parameters\nAs can be seen, especially the velocities' domains are extermely large. However, from [@tuzzer](https://medium.com/@tuzzer)'s post I found that astonishingly small target intervals are sufficient. Therefore, I initially started by scaling __theta__ down to a discrete interval `theta ‚àà [0, 6] ‚äÇ ‚Ñï ` (which is, to be precise, just a set of integers {0..6}) and __theta'__ to `theta' ‚àà [0, 12] ‚äÇ ‚Ñï `. Inspired by [@tuzzer](https://medium.com/@tuzzer/)'s post, I dropped the __x__ and __x'__ features completely, which corresponds to mapping any of their values to a single scalar. The motivation behind this is that the probability of the cart leaving the environment at the left or right border in only 200 time steps (after 200 steps, the environment automatically resets itself) is pretty slow and the resulting reduction in dimensionality more worthy. \n\nThe implementation of the actual Q-Learning algorithm is straightfoward and consits of a function to fetch the best action for a state from the q-table and another function to update the q-table based on the last action. Nothing special here.\n\nMore interesing are the algorithm's hyperparameters, which include __alpha (= learning rate)__, __epsilon (= exploration rate)__ and __gamma (= discount factor)__.\n\nAlpha is used to \"smooth\" the updates and make them less radical, which, in the first place, prevents from errors caused by noise. Epsilon regulates between __exploitation and exploration__. Accordingly, instead of picking the _best_ action in a state, with a chance of Œµ a _random_ action is picked. This should prevent the algorithm from getting stuck in local minima. E.g. if a bad choice was made in the beginning, without Œµ the agent would continue on evaluating that suboptimal path and would never discover any other, potentially better, path. Gamma is used to penalize the agent if it takes long to reach its goal. However, in this case, gamma is set to constant 1 (no discount), since it's even our goal to \"survive\" as long as possible. \n\nFirst I tried to choose the epsilon and alpha parameters as constants and experimented with various combinations. However, I always achieved only a very poor score (~ between 20 and 50 ticks). Then, again inspired by [@tuzzer](https://medium.com/@tuzzer/) I decided to introduce an adaptive learning- and exploration rate, which starts with a high value and decreases by time (with each training episode). Astonishingly, this made a huge difference. Suddenly, my algorithm converged in about ~ 200 steps. Since I never thought that these hyperparameters made such an extreme difference (from not solving the challenge at all to doing pretty well), this was probably the most interesting finding from my CartPole project. I simply adpoted [@tuzzer](https://medium.com/@tuzzer/)'s adaptive function, which is visualized in the figure below (the minimum of _0.1_ is a hyperparameter to be optimized).\n\n![](images/cartpole4.png)\n\n### Grid search\nAs my Q-Learning implementation with adaptive learning- and exploration rates was finished, I implemented an additional grid search to do some hyperparameter tuning. Goal was to find the optimal combination of feature interval lengths and lower bounds for alpha and epsilon. The following parameters turned out to perform best: `'buckets': (1, 1, 6, 12), 'min_alpha': 0.1, 'min_epsilon': 0.1`. Additionally, I also could have evaluated different functions for calculating the adaptive rate, but I haven't, yet. \n\n[>> Code on GitHub (qcartpole_gridsearch.py)](https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033)\n\n## Result & Future Work\nMy final score was __188__, [as can be seen in the leaderboard](https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q). As I progress with my knowledge on machine learning, while practicing for the upcoming [Machine Learning 2](http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en) exam, I want to continue improving my CartPole algorithm. Probably the next step will be to incorporate deep Q-Learning, which basically is Q-Learning with the only difference that the q-values are estimated by a deep neural net. The main (and probably only) advantage is the ability to handle way larger feature spaces. I'll keep you guys up-to-date. I hope that I could encourage you to get started with Gym, or machine learning in general, too!","source":"_posts/cartpole-with-qlearning-first-experiences-with-openai-gym.md","raw":"---\ntitle: CartPole with Q-Learning - First experiences with OpenAI Gym\ndate: 2017-08-24 16:50:57\ntags:\n---\n## OpenAI Gym\nToday I made my first experiences with the [OpenAI gym](https://gym.openai.com), more specifically with the [CartPole](https://gym.openai.com/envs/CartPole-v0) environment. Gym is basically a Python library that includes several machine learning challenges, in which an autonomous agent should be learned to fulfill different tasks, e.g. to master a simple game itself. One of the simplest and most popular challenges is CartPole. It's basically a 2D game in which the agent has to control, i.e. move left or right, a cart to balance a pole standing perpendicularly on the cart. This is a classical reinforcement learning problem, e.g. a scenario, in which the agents starts by trying random actions as a consequence to which it gets rewarded (or not). Based on the rewards, it continuously \"learns\", which action is good in which specific situation. Doing so, it learns how to master the game without ever being told how the game even works. The main advantage of this type of learning is, that it's completely generic and not bound to a specific problem. E.g. to learn a chess agent, you don't need to \"tell\" it the rules of chess, but just let it do trial & error, whereas \"error\" means giving it a negative (or small positive) reward.\n\n## CartPole-v0\nIn machine learning terms, CartPole is basically a binary classification problem. There are four features as inputs, which include the cart position, its velocity, the pole's angle to the cart and its derivative (i.e. how fast the pole is \"falling\"). The output is binary, i.e. either 0 or 1, corresponding to \"left\" or \"right\". One challenge is the fact that all four features are continuous values (floating point numbers), which, naively, implies an infinitely large feature space.\n\n![](images/cartpole1.jpg)\n\n## Approach: Basic Q-Learning\nIn the [Machine Learning 1](https://his.anthropomatik.kit.edu/english/28_315.php) course at my university, I got to know one of the most basic, yet widely-used reinforcement learning approaches, which is [Q-Learning](http://mnemstudio.org/path-finding-q-learning-tutorial.htm). The core of Q-Learning is to estimate a value for every possible pare of a state (s) and an action (a) by getting rewarded. Imagine the following graph, which consists of three states, while your agent is currently in _s0_. It can choose between two actions, one of which results in a good state _s1_ (e.g. having won the game), the other one results in a bad state _s2_ (e.g. having lost the game). Accordingly, the transition leading to the good (bad) state gives a reward of 100 (-100). If the agent performs action _a0_, the q-value of _s0_ will probably become negative (Q(s0, a0) < 0)), while Q(s0, a1) > 0.\n\n![](images/cartpole2.png)\n\nThe update of the q-value is done according to the following equation.\n\n![](images/cartpole3.png)\n\nBasically, a (S, A)-tuple's new q-value depends on its old q-value, the immediate reward received for the action and the maximum q-value achievable in the following state. So a (S, A)-pair's q-value indirectly depends on all its successors' q-values, which is expressed in the recursive function definition. By repeatedly walking through all nodes and transistions, the agent can update any (S, A)-pairs q-value, while the results of good and bad actions are slowly \"backpropagated\" from terminal nodes to early nodes. The agent ends up with a (usually multidimensional) table mapping states and actions to q-values, so that given any state, the best action can be picked by choosing the highest respective q-value.\n\n## My implementation\nSince Q-Learning is pretty straightforward to understand and implement, I decided on picking that algorithm as a starting point for my CartPole challenge. I looked for other solutions, that also use CartPole and found [this blog post](https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947) by [@tuzzer](https://medium.com/@tuzzer), which had partially inspired me during my implementation. \n\n[>> Code on GitHub (qcartpole.py)](https://gist.github.com/muety/af0b8476ae4106ec098fea1dfe57f578)\n\n### Transforming the feature space\nActually the main challenge was to convert the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states. The less states we have, the smaller the Q-table will be, the less steps the agent will need to properly learn its values. However, too few states might not hold enough information to properly represent the environment. The original domains of the input features are these.\n\n* __x__ (cart position) ‚àà [-4.8, 4.8]\n* __x'__ (cart velocity) ‚àà [-3.4 * 10^38, 3.4 * 10^38]\n* __theta__ (angle) ‚àà [-0.42, 0.42]\n* __theta'__ (angle velocity) ‚àà [-3.4 * 10^38, 3.4 * 10^38]\n\n### Finding the parameters\nAs can be seen, especially the velocities' domains are extermely large. However, from [@tuzzer](https://medium.com/@tuzzer)'s post I found that astonishingly small target intervals are sufficient. Therefore, I initially started by scaling __theta__ down to a discrete interval `theta ‚àà [0, 6] ‚äÇ ‚Ñï ` (which is, to be precise, just a set of integers {0..6}) and __theta'__ to `theta' ‚àà [0, 12] ‚äÇ ‚Ñï `. Inspired by [@tuzzer](https://medium.com/@tuzzer/)'s post, I dropped the __x__ and __x'__ features completely, which corresponds to mapping any of their values to a single scalar. The motivation behind this is that the probability of the cart leaving the environment at the left or right border in only 200 time steps (after 200 steps, the environment automatically resets itself) is pretty slow and the resulting reduction in dimensionality more worthy. \n\nThe implementation of the actual Q-Learning algorithm is straightfoward and consits of a function to fetch the best action for a state from the q-table and another function to update the q-table based on the last action. Nothing special here.\n\nMore interesing are the algorithm's hyperparameters, which include __alpha (= learning rate)__, __epsilon (= exploration rate)__ and __gamma (= discount factor)__.\n\nAlpha is used to \"smooth\" the updates and make them less radical, which, in the first place, prevents from errors caused by noise. Epsilon regulates between __exploitation and exploration__. Accordingly, instead of picking the _best_ action in a state, with a chance of Œµ a _random_ action is picked. This should prevent the algorithm from getting stuck in local minima. E.g. if a bad choice was made in the beginning, without Œµ the agent would continue on evaluating that suboptimal path and would never discover any other, potentially better, path. Gamma is used to penalize the agent if it takes long to reach its goal. However, in this case, gamma is set to constant 1 (no discount), since it's even our goal to \"survive\" as long as possible. \n\nFirst I tried to choose the epsilon and alpha parameters as constants and experimented with various combinations. However, I always achieved only a very poor score (~ between 20 and 50 ticks). Then, again inspired by [@tuzzer](https://medium.com/@tuzzer/) I decided to introduce an adaptive learning- and exploration rate, which starts with a high value and decreases by time (with each training episode). Astonishingly, this made a huge difference. Suddenly, my algorithm converged in about ~ 200 steps. Since I never thought that these hyperparameters made such an extreme difference (from not solving the challenge at all to doing pretty well), this was probably the most interesting finding from my CartPole project. I simply adpoted [@tuzzer](https://medium.com/@tuzzer/)'s adaptive function, which is visualized in the figure below (the minimum of _0.1_ is a hyperparameter to be optimized).\n\n![](images/cartpole4.png)\n\n### Grid search\nAs my Q-Learning implementation with adaptive learning- and exploration rates was finished, I implemented an additional grid search to do some hyperparameter tuning. Goal was to find the optimal combination of feature interval lengths and lower bounds for alpha and epsilon. The following parameters turned out to perform best: `'buckets': (1, 1, 6, 12), 'min_alpha': 0.1, 'min_epsilon': 0.1`. Additionally, I also could have evaluated different functions for calculating the adaptive rate, but I haven't, yet. \n\n[>> Code on GitHub (qcartpole_gridsearch.py)](https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033)\n\n## Result & Future Work\nMy final score was __188__, [as can be seen in the leaderboard](https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q). As I progress with my knowledge on machine learning, while practicing for the upcoming [Machine Learning 2](http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en) exam, I want to continue improving my CartPole algorithm. Probably the next step will be to incorporate deep Q-Learning, which basically is Q-Learning with the only difference that the q-values are estimated by a deep neural net. The main (and probably only) advantage is the ability to handle way larger feature spaces. I'll keep you guys up-to-date. I hope that I could encourage you to get started with Gym, or machine learning in general, too!","slug":"cartpole-with-qlearning-first-experiences-with-openai-gym","published":1,"updated":"2020-06-06T15:14:33.457Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhho000840mq3kfod4iw","content":"<h2 id=\"OpenAI-Gym\">OpenAI Gym</h2>\n<p>Today I made my first experiences with the <a href=\"https://gym.openai.com\" target=\"_blank\" rel=\"noopener\">OpenAI gym</a>, more specifically with the <a href=\"https://gym.openai.com/envs/CartPole-v0\" target=\"_blank\" rel=\"noopener\">CartPole</a> environment. Gym is basically a Python library that includes several machine learning challenges, in which an autonomous agent should be learned to fulfill different tasks, e.g. to master a simple game itself. One of the simplest and most popular challenges is CartPole. It‚Äôs basically a 2D game in which the agent has to control, i.e. move left or right, a cart to balance a pole standing perpendicularly on the cart. This is a classical reinforcement learning problem, e.g. a scenario, in which the agents starts by trying random actions as a consequence to which it gets rewarded (or not). Based on the rewards, it continuously ‚Äúlearns‚Äù, which action is good in which specific situation. Doing so, it learns how to master the game without ever being told how the game even works. The main advantage of this type of learning is, that it‚Äôs completely generic and not bound to a specific problem. E.g. to learn a chess agent, you don‚Äôt need to ‚Äútell‚Äù it the rules of chess, but just let it do trial &amp; error, whereas ‚Äúerror‚Äù means giving it a negative (or small positive) reward.</p>\n<h2 id=\"CartPole-v0\">CartPole-v0</h2>\n<p>In machine learning terms, CartPole is basically a binary classification problem. There are four features as inputs, which include the cart position, its velocity, the pole‚Äôs angle to the cart and its derivative (i.e. how fast the pole is ‚Äúfalling‚Äù). The output is binary, i.e. either 0 or 1, corresponding to ‚Äúleft‚Äù or ‚Äúright‚Äù. One challenge is the fact that all four features are continuous values (floating point numbers), which, naively, implies an infinitely large feature space.</p>\n<p><img src=\"images/cartpole1.jpg\" alt></p>\n<h2 id=\"Approach-Basic-Q-Learning\">Approach: Basic Q-Learning</h2>\n<p>In the <a href=\"https://his.anthropomatik.kit.edu/english/28_315.php\" target=\"_blank\" rel=\"noopener\">Machine Learning 1</a> course at my university, I got to know one of the most basic, yet widely-used reinforcement learning approaches, which is <a href=\"http://mnemstudio.org/path-finding-q-learning-tutorial.htm\" target=\"_blank\" rel=\"noopener\">Q-Learning</a>. The core of Q-Learning is to estimate a value for every possible pare of a state (s) and an action (a) by getting rewarded. Imagine the following graph, which consists of three states, while your agent is currently in <em>s0</em>. It can choose between two actions, one of which results in a good state <em>s1</em> (e.g. having won the game), the other one results in a bad state <em>s2</em> (e.g. having lost the game). Accordingly, the transition leading to the good (bad) state gives a reward of 100 (-100). If the agent performs action <em>a0</em>, the q-value of <em>s0</em> will probably become negative (Q(s0, a0) &lt; 0)), while Q(s0, a1) &gt; 0.</p>\n<p><img src=\"images/cartpole2.png\" alt></p>\n<p>The update of the q-value is done according to the following equation.</p>\n<p><img src=\"images/cartpole3.png\" alt></p>\n<p>Basically, a (S, A)-tuple‚Äôs new q-value depends on its old q-value, the immediate reward received for the action and the maximum q-value achievable in the following state. So a (S, A)-pair‚Äôs q-value indirectly depends on all its successors‚Äô q-values, which is expressed in the recursive function definition. By repeatedly walking through all nodes and transistions, the agent can update any (S, A)-pairs q-value, while the results of good and bad actions are slowly ‚Äúbackpropagated‚Äù from terminal nodes to early nodes. The agent ends up with a (usually multidimensional) table mapping states and actions to q-values, so that given any state, the best action can be picked by choosing the highest respective q-value.</p>\n<h2 id=\"My-implementation\">My implementation</h2>\n<p>Since Q-Learning is pretty straightforward to understand and implement, I decided on picking that algorithm as a starting point for my CartPole challenge. I looked for other solutions, that also use CartPole and found <a href=\"https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947\" target=\"_blank\" rel=\"noopener\">this blog post</a> by <a href=\"https://medium.com/@tuzzer\" target=\"_blank\" rel=\"noopener\">@tuzzer</a>, which had partially inspired me during my implementation.</p>\n<p><a href=\"https://gist.github.com/muety/af0b8476ae4106ec098fea1dfe57f578\" target=\"_blank\" rel=\"noopener\">&gt;&gt; Code on GitHub (qcartpole.py)</a></p>\n<h3 id=\"Transforming-the-feature-space\">Transforming the feature space</h3>\n<p>Actually the main challenge was to convert the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states. The less states we have, the smaller the Q-table will be, the less steps the agent will need to properly learn its values. However, too few states might not hold enough information to properly represent the environment. The original domains of the input features are these.</p>\n<ul>\n<li><strong>x</strong> (cart position) ‚àà [-4.8, 4.8]</li>\n<li><strong>x‚Äô</strong> (cart velocity) ‚àà [-3.4 * 10^38, 3.4 * 10^38]</li>\n<li><strong>theta</strong> (angle) ‚àà [-0.42, 0.42]</li>\n<li><strong>theta‚Äô</strong> (angle velocity) ‚àà [-3.4 * 10^38, 3.4 * 10^38]</li>\n</ul>\n<h3 id=\"Finding-the-parameters\">Finding the parameters</h3>\n<p>As can be seen, especially the velocities‚Äô domains are extermely large. However, from <a href=\"https://medium.com/@tuzzer\" target=\"_blank\" rel=\"noopener\">@tuzzer</a>‚Äôs post I found that astonishingly small target intervals are sufficient. Therefore, I initially started by scaling <strong>theta</strong> down to a discrete interval <code>theta ‚àà [0, 6] ‚äÇ ‚Ñï </code> (which is, to be precise, just a set of integers {0‚Ä¶6}) and <strong>theta‚Äô</strong> to <code>theta' ‚àà [0, 12] ‚äÇ ‚Ñï </code>. Inspired by <a href=\"https://medium.com/@tuzzer/\" target=\"_blank\" rel=\"noopener\">@tuzzer</a>‚Äôs post, I dropped the <strong>x</strong> and <strong>x‚Äô</strong> features completely, which corresponds to mapping any of their values to a single scalar. The motivation behind this is that the probability of the cart leaving the environment at the left or right border in only 200 time steps (after 200 steps, the environment automatically resets itself) is pretty slow and the resulting reduction in dimensionality more worthy.</p>\n<p>The implementation of the actual Q-Learning algorithm is straightfoward and consits of a function to fetch the best action for a state from the q-table and another function to update the q-table based on the last action. Nothing special here.</p>\n<p>More interesing are the algorithm‚Äôs hyperparameters, which include <strong>alpha (= learning rate)</strong>, <strong>epsilon (= exploration rate)</strong> and <strong>gamma (= discount factor)</strong>.</p>\n<p>Alpha is used to ‚Äúsmooth‚Äù the updates and make them less radical, which, in the first place, prevents from errors caused by noise. Epsilon regulates between <strong>exploitation and exploration</strong>. Accordingly, instead of picking the <em>best</em> action in a state, with a chance of Œµ a <em>random</em> action is picked. This should prevent the algorithm from getting stuck in local minima. E.g. if a bad choice was made in the beginning, without Œµ the agent would continue on evaluating that suboptimal path and would never discover any other, potentially better, path. Gamma is used to penalize the agent if it takes long to reach its goal. However, in this case, gamma is set to constant 1 (no discount), since it‚Äôs even our goal to ‚Äúsurvive‚Äù as long as possible.</p>\n<p>First I tried to choose the epsilon and alpha parameters as constants and experimented with various combinations. However, I always achieved only a very poor score (~ between 20 and 50 ticks). Then, again inspired by <a href=\"https://medium.com/@tuzzer/\" target=\"_blank\" rel=\"noopener\">@tuzzer</a> I decided to introduce an adaptive learning- and exploration rate, which starts with a high value and decreases by time (with each training episode). Astonishingly, this made a huge difference. Suddenly, my algorithm converged in about ~ 200 steps. Since I never thought that these hyperparameters made such an extreme difference (from not solving the challenge at all to doing pretty well), this was probably the most interesting finding from my CartPole project. I simply adpoted <a href=\"https://medium.com/@tuzzer/\" target=\"_blank\" rel=\"noopener\">@tuzzer</a>‚Äôs adaptive function, which is visualized in the figure below (the minimum of <em>0.1</em> is a hyperparameter to be optimized).</p>\n<p><img src=\"images/cartpole4.png\" alt></p>\n<h3 id=\"Grid-search\">Grid search</h3>\n<p>As my Q-Learning implementation with adaptive learning- and exploration rates was finished, I implemented an additional grid search to do some hyperparameter tuning. Goal was to find the optimal combination of feature interval lengths and lower bounds for alpha and epsilon. The following parameters turned out to perform best: <code>'buckets': (1, 1, 6, 12), 'min_alpha': 0.1, 'min_epsilon': 0.1</code>. Additionally, I also could have evaluated different functions for calculating the adaptive rate, but I haven‚Äôt, yet.</p>\n<p><a href=\"https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033\" target=\"_blank\" rel=\"noopener\">&gt;&gt; Code on GitHub (qcartpole_gridsearch.py)</a></p>\n<h2 id=\"Result-Future-Work\">Result &amp; Future Work</h2>\n<p>My final score was <strong>188</strong>, <a href=\"https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q\" target=\"_blank\" rel=\"noopener\">as can be seen in the leaderboard</a>. As I progress with my knowledge on machine learning, while practicing for the upcoming <a href=\"http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en\" target=\"_blank\" rel=\"noopener\">Machine Learning 2</a> exam, I want to continue improving my CartPole algorithm. Probably the next step will be to incorporate deep Q-Learning, which basically is Q-Learning with the only difference that the q-values are estimated by a deep neural net. The main (and probably only) advantage is the ability to handle way larger feature spaces. I‚Äôll keep you guys up-to-date. I hope that I could encourage you to get started with Gym, or machine learning in general, too!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h2 id=\"OpenAI-Gym\">OpenAI Gym</h2>\n<p>Today I made my first experiences with the <a href=\"https://gym.openai.com\" target=\"_blank\" rel=\"noopener\">OpenAI gym</a>, more specifically with the <a href=\"https://gym.openai.com/envs/CartPole-v0\" target=\"_blank\" rel=\"noopener\">CartPole</a> environment. Gym is basically a Python library that includes several machine learning challenges, in which an autonomous agent should be learned to fulfill different tasks, e.g. to master a simple game itself. One of the simplest and most popular challenges is CartPole. It‚Äôs basically a 2D game in which the agent has to control, i.e. move left or right, a cart to balance a pole standing perpendicularly on the cart. This is a classical reinforcement learning problem, e.g. a scenario, in which the agents starts by trying random actions as a consequence to which it gets rewarded (or not). Based on the rewards, it continuously ‚Äúlearns‚Äù, which action is good in which specific situation. Doing so, it learns how to master the game without ever being told how the game even works. The main advantage of this type of learning is, that it‚Äôs completely generic and not bound to a specific problem. E.g. to learn a chess agent, you don‚Äôt need to ‚Äútell‚Äù it the rules of chess, but just let it do trial &amp; error, whereas ‚Äúerror‚Äù means giving it a negative (or small positive) reward.</p>\n<h2 id=\"CartPole-v0\">CartPole-v0</h2>\n<p>In machine learning terms, CartPole is basically a binary classification problem. There are four features as inputs, which include the cart position, its velocity, the pole‚Äôs angle to the cart and its derivative (i.e. how fast the pole is ‚Äúfalling‚Äù). The output is binary, i.e. either 0 or 1, corresponding to ‚Äúleft‚Äù or ‚Äúright‚Äù. One challenge is the fact that all four features are continuous values (floating point numbers), which, naively, implies an infinitely large feature space.</p>\n<p><img src=\"images/cartpole1.jpg\" alt></p>\n<h2 id=\"Approach-Basic-Q-Learning\">Approach: Basic Q-Learning</h2>\n<p>In the <a href=\"https://his.anthropomatik.kit.edu/english/28_315.php\" target=\"_blank\" rel=\"noopener\">Machine Learning 1</a> course at my university, I got to know one of the most basic, yet widely-used reinforcement learning approaches, which is <a href=\"http://mnemstudio.org/path-finding-q-learning-tutorial.htm\" target=\"_blank\" rel=\"noopener\">Q-Learning</a>. The core of Q-Learning is to estimate a value for every possible pare of a state (s) and an action (a) by getting rewarded. Imagine the following graph, which consists of three states, while your agent is currently in <em>s0</em>. It can choose between two actions, one of which results in a good state <em>s1</em> (e.g. having won the game), the other one results in a bad state <em>s2</em> (e.g. having lost the game). Accordingly, the transition leading to the good (bad) state gives a reward of 100 (-100). If the agent performs action <em>a0</em>, the q-value of <em>s0</em> will probably become negative (Q(s0, a0) &lt; 0)), while Q(s0, a1) &gt; 0.</p>\n<p><img src=\"images/cartpole2.png\" alt></p>\n<p>The update of the q-value is done according to the following equation.</p>\n<p><img src=\"images/cartpole3.png\" alt></p>\n<p>Basically, a (S, A)-tuple‚Äôs new q-value depends on its old q-value, the immediate reward received for the action and the maximum q-value achievable in the following state. So a (S, A)-pair‚Äôs q-value indirectly depends on all its successors‚Äô q-values, which is expressed in the recursive function definition. By repeatedly walking through all nodes and transistions, the agent can update any (S, A)-pairs q-value, while the results of good and bad actions are slowly ‚Äúbackpropagated‚Äù from terminal nodes to early nodes. The agent ends up with a (usually multidimensional) table mapping states and actions to q-values, so that given any state, the best action can be picked by choosing the highest respective q-value.</p>\n<h2 id=\"My-implementation\">My implementation</h2>\n<p>Since Q-Learning is pretty straightforward to understand and implement, I decided on picking that algorithm as a starting point for my CartPole challenge. I looked for other solutions, that also use CartPole and found <a href=\"https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947\" target=\"_blank\" rel=\"noopener\">this blog post</a> by <a href=\"https://medium.com/@tuzzer\" target=\"_blank\" rel=\"noopener\">@tuzzer</a>, which had partially inspired me during my implementation.</p>\n<p><a href=\"https://gist.github.com/muety/af0b8476ae4106ec098fea1dfe57f578\" target=\"_blank\" rel=\"noopener\">&gt;&gt; Code on GitHub (qcartpole.py)</a></p>\n<h3 id=\"Transforming-the-feature-space\">Transforming the feature space</h3>\n<p>Actually the main challenge was to convert the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states. The less states we have, the smaller the Q-table will be, the less steps the agent will need to properly learn its values. However, too few states might not hold enough information to properly represent the environment. The original domains of the input features are these.</p>\n<ul>\n<li><strong>x</strong> (cart position) ‚àà [-4.8, 4.8]</li>\n<li><strong>x‚Äô</strong> (cart velocity) ‚àà [-3.4 * 10^38, 3.4 * 10^38]</li>\n<li><strong>theta</strong> (angle) ‚àà [-0.42, 0.42]</li>\n<li><strong>theta‚Äô</strong> (angle velocity) ‚àà [-3.4 * 10^38, 3.4 * 10^38]</li>\n</ul>\n<h3 id=\"Finding-the-parameters\">Finding the parameters</h3>\n<p>As can be seen, especially the velocities‚Äô domains are extermely large. However, from <a href=\"https://medium.com/@tuzzer\" target=\"_blank\" rel=\"noopener\">@tuzzer</a>‚Äôs post I found that astonishingly small target intervals are sufficient. Therefore, I initially started by scaling <strong>theta</strong> down to a discrete interval <code>theta ‚àà [0, 6] ‚äÇ ‚Ñï </code> (which is, to be precise, just a set of integers {0‚Ä¶6}) and <strong>theta‚Äô</strong> to <code>theta' ‚àà [0, 12] ‚äÇ ‚Ñï </code>. Inspired by <a href=\"https://medium.com/@tuzzer/\" target=\"_blank\" rel=\"noopener\">@tuzzer</a>‚Äôs post, I dropped the <strong>x</strong> and <strong>x‚Äô</strong> features completely, which corresponds to mapping any of their values to a single scalar. The motivation behind this is that the probability of the cart leaving the environment at the left or right border in only 200 time steps (after 200 steps, the environment automatically resets itself) is pretty slow and the resulting reduction in dimensionality more worthy.</p>\n<p>The implementation of the actual Q-Learning algorithm is straightfoward and consits of a function to fetch the best action for a state from the q-table and another function to update the q-table based on the last action. Nothing special here.</p>\n<p>More interesing are the algorithm‚Äôs hyperparameters, which include <strong>alpha (= learning rate)</strong>, <strong>epsilon (= exploration rate)</strong> and <strong>gamma (= discount factor)</strong>.</p>\n<p>Alpha is used to ‚Äúsmooth‚Äù the updates and make them less radical, which, in the first place, prevents from errors caused by noise. Epsilon regulates between <strong>exploitation and exploration</strong>. Accordingly, instead of picking the <em>best</em> action in a state, with a chance of Œµ a <em>random</em> action is picked. This should prevent the algorithm from getting stuck in local minima. E.g. if a bad choice was made in the beginning, without Œµ the agent would continue on evaluating that suboptimal path and would never discover any other, potentially better, path. Gamma is used to penalize the agent if it takes long to reach its goal. However, in this case, gamma is set to constant 1 (no discount), since it‚Äôs even our goal to ‚Äúsurvive‚Äù as long as possible.</p>\n<p>First I tried to choose the epsilon and alpha parameters as constants and experimented with various combinations. However, I always achieved only a very poor score (~ between 20 and 50 ticks). Then, again inspired by <a href=\"https://medium.com/@tuzzer/\" target=\"_blank\" rel=\"noopener\">@tuzzer</a> I decided to introduce an adaptive learning- and exploration rate, which starts with a high value and decreases by time (with each training episode). Astonishingly, this made a huge difference. Suddenly, my algorithm converged in about ~ 200 steps. Since I never thought that these hyperparameters made such an extreme difference (from not solving the challenge at all to doing pretty well), this was probably the most interesting finding from my CartPole project. I simply adpoted <a href=\"https://medium.com/@tuzzer/\" target=\"_blank\" rel=\"noopener\">@tuzzer</a>‚Äôs adaptive function, which is visualized in the figure below (the minimum of <em>0.1</em> is a hyperparameter to be optimized).</p>\n<p><img src=\"images/cartpole4.png\" alt></p>\n<h3 id=\"Grid-search\">Grid search</h3>\n<p>As my Q-Learning implementation with adaptive learning- and exploration rates was finished, I implemented an additional grid search to do some hyperparameter tuning. Goal was to find the optimal combination of feature interval lengths and lower bounds for alpha and epsilon. The following parameters turned out to perform best: <code>'buckets': (1, 1, 6, 12), 'min_alpha': 0.1, 'min_epsilon': 0.1</code>. Additionally, I also could have evaluated different functions for calculating the adaptive rate, but I haven‚Äôt, yet.</p>\n<p><a href=\"https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033\" target=\"_blank\" rel=\"noopener\">&gt;&gt; Code on GitHub (qcartpole_gridsearch.py)</a></p>\n<h2 id=\"Result-Future-Work\">Result &amp; Future Work</h2>\n<p>My final score was <strong>188</strong>, <a href=\"https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q\" target=\"_blank\" rel=\"noopener\">as can be seen in the leaderboard</a>. As I progress with my knowledge on machine learning, while practicing for the upcoming <a href=\"http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en\" target=\"_blank\" rel=\"noopener\">Machine Learning 2</a> exam, I want to continue improving my CartPole algorithm. Probably the next step will be to incorporate deep Q-Learning, which basically is Q-Learning with the only difference that the q-values are estimated by a deep neural net. The main (and probably only) advantage is the ability to handle way larger feature spaces. I‚Äôll keep you guys up-to-date. I hope that I could encourage you to get started with Gym, or machine learning in general, too!</p>\n"},{"title":"Design of a Linked Data-enabled Microservice Platform for the Industrial Internet of Things","date":"2016-10-19T21:03:07.000Z","_content":"\nAs the topic of my bachelor's thesis at the [TECO](http://teco.edu) and part of the [ScaleIT](https://scale-it.org) research project I've designed and developed an IoT software platform with focus on uniformity, openess and ease of adoption to tackle to problem of bringing flexible IT infrastructures to the industrial shopfloor.\nHere's my thesis' abstract to get an idea of the topic.\n\n![](/images/thesis_mockup.png)\n\n### Abstract\n\nWhile recent trends towards highly digitized, smart factories entail substantial chances for manufacturing companies to boost their performance, flexibility and productivity, the industry commonly struggles to adopt suitable technology. One major problem is a lack of uniform, standardized solutions, which could be integrated to the shopfloor without the necessity of highly specialized technical expert knowledge and a large amount of planning and restructuring.\\par \nAddressing that problem, this work proposes an architecture design as well as a concrete implementation of a Internet Of Things software platform, which mainly focuses on technological uniformity and ease of adoption and usage. As a guideline, a real-world use case elaborated in cooperation with industry partners is presented. Further on, it is shown, how general purpose web technology can be combined advantageously with recent architectural-, operational- and cultural trends in software design, powerful machine-to-machine interaction techniques and common user experience concepts. In-depth thoughts on software structure, real-time communication, machine-to-machine interaction, uniform data integration and user experience are conducted to finally obtain a working proof-of-concept software alongside recommendations and best practices for adopting smart technology on the shopfloor. An eventual evaluation investigates the designed platform regarding both performance and suitability for particular real-world scenarios and recommends further endeavor to be conducted towards achieving an actual product. \n\n### Tech stack\n\nWe've used general-purpose web technology, Linked Data techniques, semantic hypermedia APIs with Hydra and state-of-the-art software development practices and cloud technology to fulfil our goal. We also picked up on the commonly understood UX concept of apps to abstract underlying technical details away from the end-user and created kind of an app store for on-shoploor purposes. \n\n![](/images/thesis_stack.png)\n\nMy work is published at the [KITopen repository](https://publikationen.bibliothek.kit.edu/1000061764).\nIf you got curious and are interested in more details or in case you have questions or criticism, please feel free to contact me!\n","source":"_posts/design-of-a-linked-dataenabled-microservice-platform-for-the-industrial-internet-of-things.md","raw":"---\ntitle: >-\n  Design of a Linked Data-enabled Microservice Platform for the Industrial\n  Internet of Things\ndate: 2016-10-19 23:03:07\ntags:\n---\n\nAs the topic of my bachelor's thesis at the [TECO](http://teco.edu) and part of the [ScaleIT](https://scale-it.org) research project I've designed and developed an IoT software platform with focus on uniformity, openess and ease of adoption to tackle to problem of bringing flexible IT infrastructures to the industrial shopfloor.\nHere's my thesis' abstract to get an idea of the topic.\n\n![](/images/thesis_mockup.png)\n\n### Abstract\n\nWhile recent trends towards highly digitized, smart factories entail substantial chances for manufacturing companies to boost their performance, flexibility and productivity, the industry commonly struggles to adopt suitable technology. One major problem is a lack of uniform, standardized solutions, which could be integrated to the shopfloor without the necessity of highly specialized technical expert knowledge and a large amount of planning and restructuring.\\par \nAddressing that problem, this work proposes an architecture design as well as a concrete implementation of a Internet Of Things software platform, which mainly focuses on technological uniformity and ease of adoption and usage. As a guideline, a real-world use case elaborated in cooperation with industry partners is presented. Further on, it is shown, how general purpose web technology can be combined advantageously with recent architectural-, operational- and cultural trends in software design, powerful machine-to-machine interaction techniques and common user experience concepts. In-depth thoughts on software structure, real-time communication, machine-to-machine interaction, uniform data integration and user experience are conducted to finally obtain a working proof-of-concept software alongside recommendations and best practices for adopting smart technology on the shopfloor. An eventual evaluation investigates the designed platform regarding both performance and suitability for particular real-world scenarios and recommends further endeavor to be conducted towards achieving an actual product. \n\n### Tech stack\n\nWe've used general-purpose web technology, Linked Data techniques, semantic hypermedia APIs with Hydra and state-of-the-art software development practices and cloud technology to fulfil our goal. We also picked up on the commonly understood UX concept of apps to abstract underlying technical details away from the end-user and created kind of an app store for on-shoploor purposes. \n\n![](/images/thesis_stack.png)\n\nMy work is published at the [KITopen repository](https://publikationen.bibliothek.kit.edu/1000061764).\nIf you got curious and are interested in more details or in case you have questions or criticism, please feel free to contact me!\n","slug":"design-of-a-linked-dataenabled-microservice-platform-for-the-industrial-internet-of-things","published":1,"updated":"2020-06-06T15:14:33.457Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhhs000940mqwc0aee65","content":"<p>As the topic of my bachelor‚Äôs thesis at the <a href=\"http://teco.edu\" target=\"_blank\" rel=\"noopener\">TECO</a> and part of the <a href=\"https://scale-it.org\" target=\"_blank\" rel=\"noopener\">ScaleIT</a> research project I‚Äôve designed and developed an IoT software platform with focus on uniformity, openess and ease of adoption to tackle to problem of bringing flexible IT infrastructures to the industrial shopfloor.<br>\nHere‚Äôs my thesis‚Äô abstract to get an idea of the topic.</p>\n<p><img src=\"/images/thesis_mockup.png\" alt></p>\n<h3 id=\"Abstract\">Abstract</h3>\n<p>While recent trends towards highly digitized, smart factories entail substantial chances for manufacturing companies to boost their performance, flexibility and productivity, the industry commonly struggles to adopt suitable technology. One major problem is a lack of uniform, standardized solutions, which could be integrated to the shopfloor without the necessity of highly specialized technical expert knowledge and a large amount of planning and restructuring.\\par<br>\nAddressing that problem, this work proposes an architecture design as well as a concrete implementation of a Internet Of Things software platform, which mainly focuses on technological uniformity and ease of adoption and usage. As a guideline, a real-world use case elaborated in cooperation with industry partners is presented. Further on, it is shown, how general purpose web technology can be combined advantageously with recent architectural-, operational- and cultural trends in software design, powerful machine-to-machine interaction techniques and common user experience concepts. In-depth thoughts on software structure, real-time communication, machine-to-machine interaction, uniform data integration and user experience are conducted to finally obtain a working proof-of-concept software alongside recommendations and best practices for adopting smart technology on the shopfloor. An eventual evaluation investigates the designed platform regarding both performance and suitability for particular real-world scenarios and recommends further endeavor to be conducted towards achieving an actual product.</p>\n<h3 id=\"Tech-stack\">Tech stack</h3>\n<p>We‚Äôve used general-purpose web technology, Linked Data techniques, semantic hypermedia APIs with Hydra and state-of-the-art software development practices and cloud technology to fulfil our goal. We also picked up on the commonly understood UX concept of apps to abstract underlying technical details away from the end-user and created kind of an app store for on-shoploor purposes.</p>\n<p><img src=\"/images/thesis_stack.png\" alt></p>\n<p>My work is published at the <a href=\"https://publikationen.bibliothek.kit.edu/1000061764\" target=\"_blank\" rel=\"noopener\">KITopen repository</a>.<br>\nIf you got curious and are interested in more details or in case you have questions or criticism, please feel free to contact me!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>As the topic of my bachelor‚Äôs thesis at the <a href=\"http://teco.edu\" target=\"_blank\" rel=\"noopener\">TECO</a> and part of the <a href=\"https://scale-it.org\" target=\"_blank\" rel=\"noopener\">ScaleIT</a> research project I‚Äôve designed and developed an IoT software platform with focus on uniformity, openess and ease of adoption to tackle to problem of bringing flexible IT infrastructures to the industrial shopfloor.<br>\nHere‚Äôs my thesis‚Äô abstract to get an idea of the topic.</p>\n<p><img src=\"/images/thesis_mockup.png\" alt></p>\n<h3 id=\"Abstract\">Abstract</h3>\n<p>While recent trends towards highly digitized, smart factories entail substantial chances for manufacturing companies to boost their performance, flexibility and productivity, the industry commonly struggles to adopt suitable technology. One major problem is a lack of uniform, standardized solutions, which could be integrated to the shopfloor without the necessity of highly specialized technical expert knowledge and a large amount of planning and restructuring.\\par<br>\nAddressing that problem, this work proposes an architecture design as well as a concrete implementation of a Internet Of Things software platform, which mainly focuses on technological uniformity and ease of adoption and usage. As a guideline, a real-world use case elaborated in cooperation with industry partners is presented. Further on, it is shown, how general purpose web technology can be combined advantageously with recent architectural-, operational- and cultural trends in software design, powerful machine-to-machine interaction techniques and common user experience concepts. In-depth thoughts on software structure, real-time communication, machine-to-machine interaction, uniform data integration and user experience are conducted to finally obtain a working proof-of-concept software alongside recommendations and best practices for adopting smart technology on the shopfloor. An eventual evaluation investigates the designed platform regarding both performance and suitability for particular real-world scenarios and recommends further endeavor to be conducted towards achieving an actual product.</p>\n<h3 id=\"Tech-stack\">Tech stack</h3>\n<p>We‚Äôve used general-purpose web technology, Linked Data techniques, semantic hypermedia APIs with Hydra and state-of-the-art software development practices and cloud technology to fulfil our goal. We also picked up on the commonly understood UX concept of apps to abstract underlying technical details away from the end-user and created kind of an app store for on-shoploor purposes.</p>\n<p><img src=\"/images/thesis_stack.png\" alt></p>\n<p>My work is published at the <a href=\"https://publikationen.bibliothek.kit.edu/1000061764\" target=\"_blank\" rel=\"noopener\">KITopen repository</a>.<br>\nIf you got curious and are interested in more details or in case you have questions or criticism, please feel free to contact me!</p>\n"},{"title":"Detecting academics' major from facial images","date":"2019-01-02T10:02:21.000Z","_content":"\n# The Idea\nA few months ago I read a paper with the title [\"Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images\"](https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual), which caused a lot of [controversy](https://news.ycombinator.com/item?id=15198997). While I don't want to comment on the methodology and quality of the paper (that was already done, e.g. in [an article by Jeremy Howard](https://www.fast.ai/2017/09/13/kosinski/)), I found it very interesting and inspiring. In a nutshell, the researchers collected face pictures from dating websites and built a machine learning model to classify people's sexual orientation and reached quite an impressive accuracy with their approach.\n\n[This guest post](https://scatter.wordpress.com/2017/09/10/guest-post-artificial-intelligence-discovers-gayface-sigh/) summarizes the results as: \n\n> AI Can‚Äôt Tell if You‚Äôre Gay‚Ä¶ But it Can Tell if You‚Äôre a Walking Stereotype.\n\n And indeed, we often see people who look very stereotypical. I tried to think of more such scenarios and came to the conclusion that another environment, where this phenomenon can be found quite often, is a university campus. So often you walk around the campus and see students, who just look like a law student, a computer science nerd, a sportsman, etc. Sometimes I'm so curious that I almost want to ask them whether my assumption is correct.\n \n After having read the above paper, I wondered if some machine learning model might be able to quantify these latent assumptions and find out a stereotypical-looking student's profession or major. \n\nAlthough I only have a little more than basic knowledge in machine learning, especially in image classification using deep neural nets, I took it as a personal challenge to **build a classifier, that detects academics' major based on an image of their face**. \n\n# Disclaimer\nPlease don't take this article too serious. I'm not a machine learning expert or a professional scientist. There might be some mistakes in my methodology or implementation. However, I'd love to hear your thoughts and feedback.\n\n# Approach\nMy first (and final) approach was to **(1.) collect face pictures of students** or other academics, **(2.) label them** with a small, limited set of classes, corresponding to their major, and eventually **(3.) fit a convolutional neural net (CNN)** as a classifier. I thought of fields of study, whose students might potentially look a bit stereotypical and came up with four classes:\n\n1. computer science (~ cs)\n2. economics (~ econ)\n3. (German) linguistics (~ german)\n4. mechanical engineering (~ mechanical)\n\nPlease note that this is not meant to be offending by any means! (I'm a computer science nerd myself üòâ).\n\n# Getting the data\nThe very first prerequisite is training data - as usual, when doing machine learning. And since I aimed at training a convolutional neural net (CNN), there should be a lot of data, preferably.\n\nWhile it would have been a funny approach to walk around my campus and ask students for their major and a picture of their face, I would probably not have ended up with a lot of data. Instead, I decided to **crawl pictures from university websites**. Almost every department at every university has a page called \"[Staff](http://dbis.ipd.kit.edu/english/722.php)\", \"People\", \"Researchers\" or the like on their websites. While these are not particularly lists of students, but of professors, research assistants and PhD candidates, I presumed that those pictures should still be sufficient as training data. \n\nI wrote a bunch of **crawler scripts** using Python and [Selenium WebDriver](https://www.seleniumhq.org/) to crawl **57** different websites, including the websites of various departments of the following universities:\n\n* Karlsruhe Institute of Technology\n* TU Munich\n* University of Munich\n* University of W√ºrzburg\n* University of Siegen\n* University of Bayreuth\n* University of Feiburg\n* University of Heidelberg\n* University of Erlangen\n* University of Bamberg\n* University of Mannheim\n\nAfter a bit of manual data cleaning (removing pictures without faces, rotating pictures, ...), I ended up with a total of **1369** labeled images from four different classes. While this is not very much data for training a CNN, I decided to give it a try anyway.\n\n## Examples\n### Images\n\n**An excerpt from the folder containing all raw images after crawling:**\n![Excerpt from all crawled raw images](images/academic_faces1.png)\n(If you are in one of these pictures and want to get removed, please contact me.)\n\n### Labels\n**An excerpt from `index.csv` containing labels and meta-data for every image:**\n```csv\nid,category,image_url,name\nc35464fd,mechanical,http://www.fast.kit.edu/lff/1011_1105.php,Prof. Dr. rer. nat. Frank Gauterin\na73d11a7,cs,http://h2t.anthropomatik.kit.edu/21_1784.php,Kevin Liang\n97e230ff,econ,http://marketing.iism.kit.edu/21_371.php,Dr. Hanna Schumacher\ncde71a5c,german,https://www.germanistik.uni-muenchen.de/personal/ndl/mitarbeiter/bach/index.html,Dr. Oliver Bach\n```\n\n# Preprocessing the data\nBefore the images could be used as training data for a learning algorithm, a bit of preprocessing needed to be applied. Mainly, I did two major steps of preprocessing.\n\n1. **Cropping** images to faces - As you can see, pictures are taken from different angles, some of them contain a lot of background, some are not centered, etc. To get better training data, the pictures have to be cropped to only the face and nothing else. \n2. **Scaling** - All pictures come in different resolutions, but eventually need to be of exactly the same size in order to be used as input to a neural network. \n\nTo achieve both of these preprocessing steps I used a great, little, open-source, OpenCV-based Python tool called [autocrop](https://github.com/leblancfg/autocrop) with the following command:\n\n`autocrop -i raw -o preprocessed -w 128 -H 128 > autocrop.log`.\n\nThis detects the face in every picture in `raw` folder, crops the picture to that face, re-scales the resulting image to 128 x 128 pixels and saves it to `preprocessed` folder. Of course, there are some pictures in which the algorithm can not detect a face. Those are logged to stdout and persisted to `autocrop.log`.\n\nIn addition, I wrote a script that parses `autocrop.log` to get the failed images and subsequently split the images into _train_ (70 %), _test_ (20 %) and _validation_ (10 %) and copy them to a folder structure that is compatible to the format required by [Keras ImageDataGenerator](https://keras.io/preprocessing/image/) to read training data.\n\n```\n- raw\n    - index.csv\n    - c35464fd.jpg\n    - a73d11a7.jpg\n    - ...\n- preprocessed \n    - train\n        - cs\n            - a73d11a7.jpg\n            - ...\n        - econ\n            - 97e230ff.jpg\n            - ...\n        - german\n            - cde71a5c.jpg\n            - ...\n        - mechanical\n            - c35464fd.jpg\n            - ...\n    - test\n        - cs\n            - ...\n        - ...\n    - validation\n        - cs\n            - ...\n        - ...\n```\n\n# Building a model\n## Approach 1: Simple, custom CNN\n\n**Code**\n* [custom_model.ipynb](https://gist.github.com/muety/78bf6d7929e4facd199ad0ffea0b3ad9)\n\n\nI decided to start simple and see if anything can be learned from the data at all. I defined the following simple CNN architecture in Keras: \n\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 62, 62, 32)        320       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 12, 12, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1152)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                73792     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 260       \n=================================================================\nTotal params: 92,868\nTrainable params: 92,868\nNon-trainable params: 0\n```\n\nI used Keras' [ImageDataGenerator](https://keras.io/preprocessing/image/) (great tool!) to read images into NumPy arrays, re-scale them to a shape of `(64, 63, 3)` (64 x 64 pixels, RGB) and perform some data augmentation using transformations like rotations, zooming, horizontal flipping, etc. to blow up my training data and hopefully build more robust, less overfitted models.\n\nI let the model train for **100 epochs**, using the **Adam optimizer** with default parameters and **categorical crossentropy loss**, a mini-batch size of **32** and **3x augmentation** (use transformations to blow up training data by a factor of three). \n\n### Results (57.1 % accuracy)\nThe maximum **validation accuracy of 0.66** was reached after 74 epochs. **Test accuracy** turned out to be **0.571**. Considering that a quite simple model was trained completely from scratch with less than 1000 training examples, I am quite impressed by that result. It means that on average the model predicts more than every second student's major correctly. The **a-priori probability** of a correct classification **is 0.25**, so the model has definitely learned at least something.\n\n## Approach 2: Fine-tuning VGGFace\n\n**Code**\n* [vggfaces_bottleneck_model.ipynb](https://gist.github.com/muety/a079dcb27d921d58323c9574152b2c2d)\n* [vggfaces_finetuned_model.ipynb](https://gist.github.com/muety/c3b9e9401f178807c91ad890a6c67e18)\n\n\nAs an alternative to a simple, custom-defined CNN model, that is trained from scratch, I wanted to follow the common approach of fine-tuning the weights of an existing, pre-trained model. The basic idea of such an approach is to not \"re-invent the wheel\", but take advantage of what was already learned before and only slightly adapt that \"knowledge\" (in form of weights) to a certain problem. Latent features in images, which a learning algorithm had already extracted from a giant set of training data before, can just be leveraged. [\"Image Classification using pre-trained models in Keras\"](https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/) gives an excellent overview of how **fine-tuning** works and how it is different from **transfer learning** and custom models. Expectations are that my given classification problem can be solved more accurately with less data. \n\n I decided to take a **VGG16** model architecture trained on [**VGGFace**](http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/) as a base (using the [keras-vggface](https://github.com/rcmalli/keras-vggface) implementation) and followed [this guide](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) to fine-tune it. VGGFace is a dataset published by the University of Oxford that contains more than 3.3 million face images. Accordingly, I expected it to have extracted very robust facial features and to be quite well-suited for face classification. \n\n### Step 1: Transfer-learning to initialize weights\nMy implementation consists of two steps, since [it is recommended](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) that\n\n> in order to perform fine-tuning, all layers should start with properly trained weights.\n\nIn this first step, transfer-learning is used to find proper weights for a set of a few newly added, custom, fully-connected classification layers. These are used as the initial weights in step 2 later on. To perform this initialization, a pre-trained VGGFace model, with the final classification layers cut off, is used to extract 128 _bottleneck features_ for every image. Subsequently, another tiny model, consisting of fully-connected layers, is trained on these features to perform the eventual classification. The weights are persisted to a file and loaded again in step 2.\n\nThe model architecture looks like this:\n\n```\n________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 128)               65664     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 516       \n=================================================================\nTotal params: 66,180\nTrainable params: 66,180\nNon-trainable params: 0\n```\n\n### Step 2: Fine-tuning \nIn this second step, a pre-trained VGGFace model (with the first n - 3 layers freezed) is used in combination with the pre-trained top layers from step 1 to fine-tune weights for our specific classification task. It takes mini-batches of (128, 128, 3)-shaped tensors (128 x 128 pixels, RGB) as input and predicts probabilities for each of our four target classes.\n\nThe architecture of the combined model looks like this:\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvggface_vgg16 (Model)        (None, 512)               14714688  \n_________________________________________________________________\ntop (Sequential)             (None, 4)                 66180     \n=================================================================\nTotal params: 14,780,868\nTrainable params: 2,425,988\nNon-trainable params: 12,354,880\n```\n\n`top` is the model described in step 1, `vggface_vgg16` is a VGG16 model and looks like this:\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         (None, 128, 128, 3)       0         \n_________________________________________________________________\nconv1_1 (Conv2D)             (None, 128, 128, 64)      1792      \n_________________________________________________________________\nconv1_2 (Conv2D)             (None, 128, 128, 64)      36928     \n_________________________________________________________________\npool1 (MaxPooling2D)         (None, 64, 64, 64)        0         \n_________________________________________________________________\nconv2_1 (Conv2D)             (None, 64, 64, 128)       73856     \n_________________________________________________________________\nconv2_2 (Conv2D)             (None, 64, 64, 128)       147584    \n_________________________________________________________________\npool2 (MaxPooling2D)         (None, 32, 32, 128)       0         \n_________________________________________________________________\nconv3_1 (Conv2D)             (None, 32, 32, 256)       295168    \n_________________________________________________________________\nconv3_2 (Conv2D)             (None, 32, 32, 256)       590080    \n_________________________________________________________________\nconv3_3 (Conv2D)             (None, 32, 32, 256)       590080    \n_________________________________________________________________\npool3 (MaxPooling2D)         (None, 16, 16, 256)       0         \n_________________________________________________________________\nconv4_1 (Conv2D)             (None, 16, 16, 512)       1180160   \n_________________________________________________________________\nconv4_2 (Conv2D)             (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nconv4_3 (Conv2D)             (None, 16, 16, 512)       2359808   \n_________________________________________________________________\npool4 (MaxPooling2D)         (None, 8, 8, 512)         0         \n_________________________________________________________________\nconv5_1 (Conv2D)             (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nconv5_2 (Conv2D)             (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nconv5_3 (Conv2D)             (None, 8, 8, 512)         2359808   \n_________________________________________________________________\npool5 (MaxPooling2D)         (None, 4, 4, 512)         0         \n_________________________________________________________________\nglobal_max_pooling2d_3 (Glob (None, 512)               0         \n=================================================================\nTotal params: 14,714,688\nTrainable params: 2,359,808\nNon-trainable params: 12,354,880\n```\n\n\nI was using Keras _ImageDataGenerator_ again for loading the data, augmenting (3x) and resizing it. As [recommended](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), _stochastic gradient descent_ is used with a small learning rate (10^-4) to carefully adapt weights. The model was trained for **100 epochs** on **batches of 32 images** and, again, used **categorical cross entropy** as a loss function. \n\n### Results (54.6 % accuracy)\nThe maximum **validation accuracy of 0.64** was reached after 38 epochs already. **Test accuracy** turned out to be **0.546**, which is a quite disappointing result, considering that even our simple, custom CNN-model achieved a higher accuracy. Maybe the model's complexity is too high for the small amount of training data?\n\n# Inspecting the model\nTo get better insights on how the model performs, I briefly inspected it with regards to several criteria. This is a short summary of my finding. \n\n## Code\n* [inspection.ipynb](https://gist.github.com/muety/404befcfb2eef4b59398f3c8590ce692) \n\n## Class distribution\nThe first thing I looked at was the class distribution. How are the four study major subjects represented in our data and what does the model predict?\n\n | cs | econ | german | mechanical\n- | - | - | - | -\n*real* | 0.2510 | 0.2808 | 0.2127 | 0.2553\n*pred* | 0.2595 | 0.2936 | 0.1361 | 0.3106\n\nApparently, the model neglects the class of _german linguists_ a bit. That is also the class for which we have the least training data. Probably I should collect more.\n\n## Examples of false classifications\nI wanted to get an idea of what the model does wrong and what it does right. Consequently, I took a look at the top (with respect to confidence) five **(1) false negatives**, **(2) false positives** and **(3) true positives**. \n\nHere is an excerpt for class _econ_:\n\n![](images/academic_faces2.png)\n\nThe top row shows examples of economists, who the model didn't recognize as such.\nThe center row depicts examples of what the model \"thinks\" economists look like, but who are actually students / researchers with a different major.\nFinally, the bottom row shows examples of good matches, i.e. people for whom the model had a very high confidence for their actual class.\n\nAgain, if you are in one of these pictures and want to get removed, please contact me.\n\n## Confusion matrix\nTo see which profession the model is unsure about, I calculated the confusion matrix.\n\n```\narray([[12.76595745,  5.95744681,  0.        ,  6.38297872],\n       [ 3.40425532, 12.76595745,  3.82978723,  8.08510638],\n       [ 3.82978723,  5.53191489,  8.5106383 ,  3.40425532],\n       [ 5.95744681,  5.10638298,  1.27659574, 13.19148936]])\n```\n\n![](images/academic_faces3.png)\n**Legend:**\n* 0 = cs, 1 = econ, 2 = german, 3 = mechanical\n* Brighter colors ~ higher value\n\nWhat we can read from the confusion matrix is that, for instance, the model tends to classify economists as mechanical engineers quite often. \n\n# Conclusion\nFirst of all, this is not a scientific study, but rather a small hobby project of mine. Also, it does not have a lot of real-world importance, since one might rarely want to classify students into four categories.\n\nAlthough the results are not spectacular, I am still quite happy about them and at least my model was able to do a lot better than random guessing. Given an **accuracy of 57 %** with four classes, you could definitely say that it is, to some extent, possible to learn a stereotypical-looking person's study major from only in image of their face. Of course, this only holds true within a bounded context and under a set of restrictions, but it is still an interesting insight to me. \n\nMoreover, I am quite sure that there is still a lot of room for improvements to the model, which could yield a better performance. Those might include:\n* More training data from a wider range of sources\n* More thorough preprocessing (e.g. filter out images of secretaries)\n* Different model architecture\n* Hyper-parameter tuning\n* Manual feature engineering\n* ...\n\nPlease let me know what you think of this project. I would love to get some feedback!","source":"_posts/detecting-academics-major-from-facial-images.md","raw":"---\ntitle: Detecting academics' major from facial images\ndate: 2019-01-02 11:02:21\ntags:\n---\n\n# The Idea\nA few months ago I read a paper with the title [\"Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images\"](https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual), which caused a lot of [controversy](https://news.ycombinator.com/item?id=15198997). While I don't want to comment on the methodology and quality of the paper (that was already done, e.g. in [an article by Jeremy Howard](https://www.fast.ai/2017/09/13/kosinski/)), I found it very interesting and inspiring. In a nutshell, the researchers collected face pictures from dating websites and built a machine learning model to classify people's sexual orientation and reached quite an impressive accuracy with their approach.\n\n[This guest post](https://scatter.wordpress.com/2017/09/10/guest-post-artificial-intelligence-discovers-gayface-sigh/) summarizes the results as: \n\n> AI Can‚Äôt Tell if You‚Äôre Gay‚Ä¶ But it Can Tell if You‚Äôre a Walking Stereotype.\n\n And indeed, we often see people who look very stereotypical. I tried to think of more such scenarios and came to the conclusion that another environment, where this phenomenon can be found quite often, is a university campus. So often you walk around the campus and see students, who just look like a law student, a computer science nerd, a sportsman, etc. Sometimes I'm so curious that I almost want to ask them whether my assumption is correct.\n \n After having read the above paper, I wondered if some machine learning model might be able to quantify these latent assumptions and find out a stereotypical-looking student's profession or major. \n\nAlthough I only have a little more than basic knowledge in machine learning, especially in image classification using deep neural nets, I took it as a personal challenge to **build a classifier, that detects academics' major based on an image of their face**. \n\n# Disclaimer\nPlease don't take this article too serious. I'm not a machine learning expert or a professional scientist. There might be some mistakes in my methodology or implementation. However, I'd love to hear your thoughts and feedback.\n\n# Approach\nMy first (and final) approach was to **(1.) collect face pictures of students** or other academics, **(2.) label them** with a small, limited set of classes, corresponding to their major, and eventually **(3.) fit a convolutional neural net (CNN)** as a classifier. I thought of fields of study, whose students might potentially look a bit stereotypical and came up with four classes:\n\n1. computer science (~ cs)\n2. economics (~ econ)\n3. (German) linguistics (~ german)\n4. mechanical engineering (~ mechanical)\n\nPlease note that this is not meant to be offending by any means! (I'm a computer science nerd myself üòâ).\n\n# Getting the data\nThe very first prerequisite is training data - as usual, when doing machine learning. And since I aimed at training a convolutional neural net (CNN), there should be a lot of data, preferably.\n\nWhile it would have been a funny approach to walk around my campus and ask students for their major and a picture of their face, I would probably not have ended up with a lot of data. Instead, I decided to **crawl pictures from university websites**. Almost every department at every university has a page called \"[Staff](http://dbis.ipd.kit.edu/english/722.php)\", \"People\", \"Researchers\" or the like on their websites. While these are not particularly lists of students, but of professors, research assistants and PhD candidates, I presumed that those pictures should still be sufficient as training data. \n\nI wrote a bunch of **crawler scripts** using Python and [Selenium WebDriver](https://www.seleniumhq.org/) to crawl **57** different websites, including the websites of various departments of the following universities:\n\n* Karlsruhe Institute of Technology\n* TU Munich\n* University of Munich\n* University of W√ºrzburg\n* University of Siegen\n* University of Bayreuth\n* University of Feiburg\n* University of Heidelberg\n* University of Erlangen\n* University of Bamberg\n* University of Mannheim\n\nAfter a bit of manual data cleaning (removing pictures without faces, rotating pictures, ...), I ended up with a total of **1369** labeled images from four different classes. While this is not very much data for training a CNN, I decided to give it a try anyway.\n\n## Examples\n### Images\n\n**An excerpt from the folder containing all raw images after crawling:**\n![Excerpt from all crawled raw images](images/academic_faces1.png)\n(If you are in one of these pictures and want to get removed, please contact me.)\n\n### Labels\n**An excerpt from `index.csv` containing labels and meta-data for every image:**\n```csv\nid,category,image_url,name\nc35464fd,mechanical,http://www.fast.kit.edu/lff/1011_1105.php,Prof. Dr. rer. nat. Frank Gauterin\na73d11a7,cs,http://h2t.anthropomatik.kit.edu/21_1784.php,Kevin Liang\n97e230ff,econ,http://marketing.iism.kit.edu/21_371.php,Dr. Hanna Schumacher\ncde71a5c,german,https://www.germanistik.uni-muenchen.de/personal/ndl/mitarbeiter/bach/index.html,Dr. Oliver Bach\n```\n\n# Preprocessing the data\nBefore the images could be used as training data for a learning algorithm, a bit of preprocessing needed to be applied. Mainly, I did two major steps of preprocessing.\n\n1. **Cropping** images to faces - As you can see, pictures are taken from different angles, some of them contain a lot of background, some are not centered, etc. To get better training data, the pictures have to be cropped to only the face and nothing else. \n2. **Scaling** - All pictures come in different resolutions, but eventually need to be of exactly the same size in order to be used as input to a neural network. \n\nTo achieve both of these preprocessing steps I used a great, little, open-source, OpenCV-based Python tool called [autocrop](https://github.com/leblancfg/autocrop) with the following command:\n\n`autocrop -i raw -o preprocessed -w 128 -H 128 > autocrop.log`.\n\nThis detects the face in every picture in `raw` folder, crops the picture to that face, re-scales the resulting image to 128 x 128 pixels and saves it to `preprocessed` folder. Of course, there are some pictures in which the algorithm can not detect a face. Those are logged to stdout and persisted to `autocrop.log`.\n\nIn addition, I wrote a script that parses `autocrop.log` to get the failed images and subsequently split the images into _train_ (70 %), _test_ (20 %) and _validation_ (10 %) and copy them to a folder structure that is compatible to the format required by [Keras ImageDataGenerator](https://keras.io/preprocessing/image/) to read training data.\n\n```\n- raw\n    - index.csv\n    - c35464fd.jpg\n    - a73d11a7.jpg\n    - ...\n- preprocessed \n    - train\n        - cs\n            - a73d11a7.jpg\n            - ...\n        - econ\n            - 97e230ff.jpg\n            - ...\n        - german\n            - cde71a5c.jpg\n            - ...\n        - mechanical\n            - c35464fd.jpg\n            - ...\n    - test\n        - cs\n            - ...\n        - ...\n    - validation\n        - cs\n            - ...\n        - ...\n```\n\n# Building a model\n## Approach 1: Simple, custom CNN\n\n**Code**\n* [custom_model.ipynb](https://gist.github.com/muety/78bf6d7929e4facd199ad0ffea0b3ad9)\n\n\nI decided to start simple and see if anything can be learned from the data at all. I defined the following simple CNN architecture in Keras: \n\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 62, 62, 32)        320       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 12, 12, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1152)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                73792     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 260       \n=================================================================\nTotal params: 92,868\nTrainable params: 92,868\nNon-trainable params: 0\n```\n\nI used Keras' [ImageDataGenerator](https://keras.io/preprocessing/image/) (great tool!) to read images into NumPy arrays, re-scale them to a shape of `(64, 63, 3)` (64 x 64 pixels, RGB) and perform some data augmentation using transformations like rotations, zooming, horizontal flipping, etc. to blow up my training data and hopefully build more robust, less overfitted models.\n\nI let the model train for **100 epochs**, using the **Adam optimizer** with default parameters and **categorical crossentropy loss**, a mini-batch size of **32** and **3x augmentation** (use transformations to blow up training data by a factor of three). \n\n### Results (57.1 % accuracy)\nThe maximum **validation accuracy of 0.66** was reached after 74 epochs. **Test accuracy** turned out to be **0.571**. Considering that a quite simple model was trained completely from scratch with less than 1000 training examples, I am quite impressed by that result. It means that on average the model predicts more than every second student's major correctly. The **a-priori probability** of a correct classification **is 0.25**, so the model has definitely learned at least something.\n\n## Approach 2: Fine-tuning VGGFace\n\n**Code**\n* [vggfaces_bottleneck_model.ipynb](https://gist.github.com/muety/a079dcb27d921d58323c9574152b2c2d)\n* [vggfaces_finetuned_model.ipynb](https://gist.github.com/muety/c3b9e9401f178807c91ad890a6c67e18)\n\n\nAs an alternative to a simple, custom-defined CNN model, that is trained from scratch, I wanted to follow the common approach of fine-tuning the weights of an existing, pre-trained model. The basic idea of such an approach is to not \"re-invent the wheel\", but take advantage of what was already learned before and only slightly adapt that \"knowledge\" (in form of weights) to a certain problem. Latent features in images, which a learning algorithm had already extracted from a giant set of training data before, can just be leveraged. [\"Image Classification using pre-trained models in Keras\"](https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/) gives an excellent overview of how **fine-tuning** works and how it is different from **transfer learning** and custom models. Expectations are that my given classification problem can be solved more accurately with less data. \n\n I decided to take a **VGG16** model architecture trained on [**VGGFace**](http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/) as a base (using the [keras-vggface](https://github.com/rcmalli/keras-vggface) implementation) and followed [this guide](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) to fine-tune it. VGGFace is a dataset published by the University of Oxford that contains more than 3.3 million face images. Accordingly, I expected it to have extracted very robust facial features and to be quite well-suited for face classification. \n\n### Step 1: Transfer-learning to initialize weights\nMy implementation consists of two steps, since [it is recommended](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) that\n\n> in order to perform fine-tuning, all layers should start with properly trained weights.\n\nIn this first step, transfer-learning is used to find proper weights for a set of a few newly added, custom, fully-connected classification layers. These are used as the initial weights in step 2 later on. To perform this initialization, a pre-trained VGGFace model, with the final classification layers cut off, is used to extract 128 _bottleneck features_ for every image. Subsequently, another tiny model, consisting of fully-connected layers, is trained on these features to perform the eventual classification. The weights are persisted to a file and loaded again in step 2.\n\nThe model architecture looks like this:\n\n```\n________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 128)               65664     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 516       \n=================================================================\nTotal params: 66,180\nTrainable params: 66,180\nNon-trainable params: 0\n```\n\n### Step 2: Fine-tuning \nIn this second step, a pre-trained VGGFace model (with the first n - 3 layers freezed) is used in combination with the pre-trained top layers from step 1 to fine-tune weights for our specific classification task. It takes mini-batches of (128, 128, 3)-shaped tensors (128 x 128 pixels, RGB) as input and predicts probabilities for each of our four target classes.\n\nThe architecture of the combined model looks like this:\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvggface_vgg16 (Model)        (None, 512)               14714688  \n_________________________________________________________________\ntop (Sequential)             (None, 4)                 66180     \n=================================================================\nTotal params: 14,780,868\nTrainable params: 2,425,988\nNon-trainable params: 12,354,880\n```\n\n`top` is the model described in step 1, `vggface_vgg16` is a VGG16 model and looks like this:\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         (None, 128, 128, 3)       0         \n_________________________________________________________________\nconv1_1 (Conv2D)             (None, 128, 128, 64)      1792      \n_________________________________________________________________\nconv1_2 (Conv2D)             (None, 128, 128, 64)      36928     \n_________________________________________________________________\npool1 (MaxPooling2D)         (None, 64, 64, 64)        0         \n_________________________________________________________________\nconv2_1 (Conv2D)             (None, 64, 64, 128)       73856     \n_________________________________________________________________\nconv2_2 (Conv2D)             (None, 64, 64, 128)       147584    \n_________________________________________________________________\npool2 (MaxPooling2D)         (None, 32, 32, 128)       0         \n_________________________________________________________________\nconv3_1 (Conv2D)             (None, 32, 32, 256)       295168    \n_________________________________________________________________\nconv3_2 (Conv2D)             (None, 32, 32, 256)       590080    \n_________________________________________________________________\nconv3_3 (Conv2D)             (None, 32, 32, 256)       590080    \n_________________________________________________________________\npool3 (MaxPooling2D)         (None, 16, 16, 256)       0         \n_________________________________________________________________\nconv4_1 (Conv2D)             (None, 16, 16, 512)       1180160   \n_________________________________________________________________\nconv4_2 (Conv2D)             (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nconv4_3 (Conv2D)             (None, 16, 16, 512)       2359808   \n_________________________________________________________________\npool4 (MaxPooling2D)         (None, 8, 8, 512)         0         \n_________________________________________________________________\nconv5_1 (Conv2D)             (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nconv5_2 (Conv2D)             (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nconv5_3 (Conv2D)             (None, 8, 8, 512)         2359808   \n_________________________________________________________________\npool5 (MaxPooling2D)         (None, 4, 4, 512)         0         \n_________________________________________________________________\nglobal_max_pooling2d_3 (Glob (None, 512)               0         \n=================================================================\nTotal params: 14,714,688\nTrainable params: 2,359,808\nNon-trainable params: 12,354,880\n```\n\n\nI was using Keras _ImageDataGenerator_ again for loading the data, augmenting (3x) and resizing it. As [recommended](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), _stochastic gradient descent_ is used with a small learning rate (10^-4) to carefully adapt weights. The model was trained for **100 epochs** on **batches of 32 images** and, again, used **categorical cross entropy** as a loss function. \n\n### Results (54.6 % accuracy)\nThe maximum **validation accuracy of 0.64** was reached after 38 epochs already. **Test accuracy** turned out to be **0.546**, which is a quite disappointing result, considering that even our simple, custom CNN-model achieved a higher accuracy. Maybe the model's complexity is too high for the small amount of training data?\n\n# Inspecting the model\nTo get better insights on how the model performs, I briefly inspected it with regards to several criteria. This is a short summary of my finding. \n\n## Code\n* [inspection.ipynb](https://gist.github.com/muety/404befcfb2eef4b59398f3c8590ce692) \n\n## Class distribution\nThe first thing I looked at was the class distribution. How are the four study major subjects represented in our data and what does the model predict?\n\n | cs | econ | german | mechanical\n- | - | - | - | -\n*real* | 0.2510 | 0.2808 | 0.2127 | 0.2553\n*pred* | 0.2595 | 0.2936 | 0.1361 | 0.3106\n\nApparently, the model neglects the class of _german linguists_ a bit. That is also the class for which we have the least training data. Probably I should collect more.\n\n## Examples of false classifications\nI wanted to get an idea of what the model does wrong and what it does right. Consequently, I took a look at the top (with respect to confidence) five **(1) false negatives**, **(2) false positives** and **(3) true positives**. \n\nHere is an excerpt for class _econ_:\n\n![](images/academic_faces2.png)\n\nThe top row shows examples of economists, who the model didn't recognize as such.\nThe center row depicts examples of what the model \"thinks\" economists look like, but who are actually students / researchers with a different major.\nFinally, the bottom row shows examples of good matches, i.e. people for whom the model had a very high confidence for their actual class.\n\nAgain, if you are in one of these pictures and want to get removed, please contact me.\n\n## Confusion matrix\nTo see which profession the model is unsure about, I calculated the confusion matrix.\n\n```\narray([[12.76595745,  5.95744681,  0.        ,  6.38297872],\n       [ 3.40425532, 12.76595745,  3.82978723,  8.08510638],\n       [ 3.82978723,  5.53191489,  8.5106383 ,  3.40425532],\n       [ 5.95744681,  5.10638298,  1.27659574, 13.19148936]])\n```\n\n![](images/academic_faces3.png)\n**Legend:**\n* 0 = cs, 1 = econ, 2 = german, 3 = mechanical\n* Brighter colors ~ higher value\n\nWhat we can read from the confusion matrix is that, for instance, the model tends to classify economists as mechanical engineers quite often. \n\n# Conclusion\nFirst of all, this is not a scientific study, but rather a small hobby project of mine. Also, it does not have a lot of real-world importance, since one might rarely want to classify students into four categories.\n\nAlthough the results are not spectacular, I am still quite happy about them and at least my model was able to do a lot better than random guessing. Given an **accuracy of 57 %** with four classes, you could definitely say that it is, to some extent, possible to learn a stereotypical-looking person's study major from only in image of their face. Of course, this only holds true within a bounded context and under a set of restrictions, but it is still an interesting insight to me. \n\nMoreover, I am quite sure that there is still a lot of room for improvements to the model, which could yield a better performance. Those might include:\n* More training data from a wider range of sources\n* More thorough preprocessing (e.g. filter out images of secretaries)\n* Different model architecture\n* Hyper-parameter tuning\n* Manual feature engineering\n* ...\n\nPlease let me know what you think of this project. I would love to get some feedback!","slug":"detecting-academics-major-from-facial-images","published":1,"updated":"2020-06-06T15:14:33.457Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhhv000a40mqvw3c6vnv","content":"<h1>The Idea</h1>\n<p>A few months ago I read a paper with the title <a href=\"https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual\" target=\"_blank\" rel=\"noopener\">‚ÄúDeep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images‚Äù</a>, which caused a lot of <a href=\"https://news.ycombinator.com/item?id=15198997\" target=\"_blank\" rel=\"noopener\">controversy</a>. While I don‚Äôt want to comment on the methodology and quality of the paper (that was already done, e.g. in <a href=\"https://www.fast.ai/2017/09/13/kosinski/\" target=\"_blank\" rel=\"noopener\">an article by Jeremy Howard</a>), I found it very interesting and inspiring. In a nutshell, the researchers collected face pictures from dating websites and built a machine learning model to classify people‚Äôs sexual orientation and reached quite an impressive accuracy with their approach.</p>\n<p><a href=\"https://scatter.wordpress.com/2017/09/10/guest-post-artificial-intelligence-discovers-gayface-sigh/\" target=\"_blank\" rel=\"noopener\">This guest post</a> summarizes the results as:</p>\n<blockquote>\n<p>AI Can‚Äôt Tell if You‚Äôre Gay‚Ä¶ But it Can Tell if You‚Äôre a Walking Stereotype.</p>\n</blockquote>\n<p>And indeed, we often see people who look very stereotypical. I tried to think of more such scenarios and came to the conclusion that another environment, where this phenomenon can be found quite often, is a university campus. So often you walk around the campus and see students, who just look like a law student, a computer science nerd, a sportsman, etc. Sometimes I‚Äôm so curious that I almost want to ask them whether my assumption is correct.</p>\n<p>After having read the above paper, I wondered if some machine learning model might be able to quantify these latent assumptions and find out a stereotypical-looking student‚Äôs profession or major.</p>\n<p>Although I only have a little more than basic knowledge in machine learning, especially in image classification using deep neural nets, I took it as a personal challenge to <strong>build a classifier, that detects academics‚Äô major based on an image of their face</strong>.</p>\n<h1>Disclaimer</h1>\n<p>Please don‚Äôt take this article too serious. I‚Äôm not a machine learning expert or a professional scientist. There might be some mistakes in my methodology or implementation. However, I‚Äôd love to hear your thoughts and feedback.</p>\n<h1>Approach</h1>\n<p>My first (and final) approach was to <strong>(1.) collect face pictures of students</strong> or other academics, <strong>(2.) label them</strong> with a small, limited set of classes, corresponding to their major, and eventually <strong>(3.) fit a convolutional neural net (CNN)</strong> as a classifier. I thought of fields of study, whose students might potentially look a bit stereotypical and came up with four classes:</p>\n<ol>\n<li>computer science (~ cs)</li>\n<li>economics (~ econ)</li>\n<li>(German) linguistics (~ german)</li>\n<li>mechanical engineering (~ mechanical)</li>\n</ol>\n<p>Please note that this is not meant to be offending by any means! (I‚Äôm a computer science nerd myself üòâ).</p>\n<h1>Getting the data</h1>\n<p>The very first prerequisite is training data - as usual, when doing machine learning. And since I aimed at training a convolutional neural net (CNN), there should be a lot of data, preferably.</p>\n<p>While it would have been a funny approach to walk around my campus and ask students for their major and a picture of their face, I would probably not have ended up with a lot of data. Instead, I decided to <strong>crawl pictures from university websites</strong>. Almost every department at every university has a page called ‚Äú<a href=\"http://dbis.ipd.kit.edu/english/722.php\" target=\"_blank\" rel=\"noopener\">Staff</a>‚Äù, ‚ÄúPeople‚Äù, ‚ÄúResearchers‚Äù or the like on their websites. While these are not particularly lists of students, but of professors, research assistants and PhD candidates, I presumed that those pictures should still be sufficient as training data.</p>\n<p>I wrote a bunch of <strong>crawler scripts</strong> using Python and <a href=\"https://www.seleniumhq.org/\" target=\"_blank\" rel=\"noopener\">Selenium WebDriver</a> to crawl <strong>57</strong> different websites, including the websites of various departments of the following universities:</p>\n<ul>\n<li>Karlsruhe Institute of Technology</li>\n<li>TU Munich</li>\n<li>University of Munich</li>\n<li>University of W√ºrzburg</li>\n<li>University of Siegen</li>\n<li>University of Bayreuth</li>\n<li>University of Feiburg</li>\n<li>University of Heidelberg</li>\n<li>University of Erlangen</li>\n<li>University of Bamberg</li>\n<li>University of Mannheim</li>\n</ul>\n<p>After a bit of manual data cleaning (removing pictures without faces, rotating pictures, ‚Ä¶), I ended up with a total of <strong>1369</strong> labeled images from four different classes. While this is not very much data for training a CNN, I decided to give it a try anyway.</p>\n<h2 id=\"Examples\">Examples</h2>\n<h3 id=\"Images\">Images</h3>\n<p><strong>An excerpt from the folder containing all raw images after crawling:</strong><br>\n<img src=\"images/academic_faces1.png\" alt=\"Excerpt from all crawled raw images\"><br>\n(If you are in one of these pictures and want to get removed, please contact me.)</p>\n<h3 id=\"Labels\">Labels</h3>\n<p><strong>An excerpt from <code>index.csv</code> containing labels and meta-data for every image:</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">id,category,image_url,name</span><br><span class=\"line\">c35464fd,mechanical,http://www.fast.kit.edu/lff/1011_1105.php,Prof. Dr. rer. nat. Frank Gauterin</span><br><span class=\"line\">a73d11a7,cs,http://h2t.anthropomatik.kit.edu/21_1784.php,Kevin Liang</span><br><span class=\"line\">97e230ff,econ,http://marketing.iism.kit.edu/21_371.php,Dr. Hanna Schumacher</span><br><span class=\"line\">cde71a5c,german,https://www.germanistik.uni-muenchen.de/personal/ndl/mitarbeiter/bach/index.html,Dr. Oliver Bach</span><br></pre></td></tr></table></figure>\n<h1>Preprocessing the data</h1>\n<p>Before the images could be used as training data for a learning algorithm, a bit of preprocessing needed to be applied. Mainly, I did two major steps of preprocessing.</p>\n<ol>\n<li><strong>Cropping</strong> images to faces - As you can see, pictures are taken from different angles, some of them contain a lot of background, some are not centered, etc. To get better training data, the pictures have to be cropped to only the face and nothing else.</li>\n<li><strong>Scaling</strong> - All pictures come in different resolutions, but eventually need to be of exactly the same size in order to be used as input to a neural network.</li>\n</ol>\n<p>To achieve both of these preprocessing steps I used a great, little, open-source, OpenCV-based Python tool called <a href=\"https://github.com/leblancfg/autocrop\" target=\"_blank\" rel=\"noopener\">autocrop</a> with the following command:</p>\n<p><code>autocrop -i raw -o preprocessed -w 128 -H 128 &gt; autocrop.log</code>.</p>\n<p>This detects the face in every picture in <code>raw</code> folder, crops the picture to that face, re-scales the resulting image to 128 x 128 pixels and saves it to <code>preprocessed</code> folder. Of course, there are some pictures in which the algorithm can not detect a face. Those are logged to stdout and persisted to <code>autocrop.log</code>.</p>\n<p>In addition, I wrote a script that parses <code>autocrop.log</code> to get the failed images and subsequently split the images into <em>train</em> (70 %), <em>test</em> (20 %) and <em>validation</em> (10 %) and copy them to a folder structure that is compatible to the format required by <a href=\"https://keras.io/preprocessing/image/\" target=\"_blank\" rel=\"noopener\">Keras ImageDataGenerator</a> to read training data.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- raw</span><br><span class=\"line\">    - index.csv</span><br><span class=\"line\">    - c35464fd.jpg</span><br><span class=\"line\">    - a73d11a7.jpg</span><br><span class=\"line\">    - ...</span><br><span class=\"line\">- preprocessed </span><br><span class=\"line\">    - train</span><br><span class=\"line\">        - cs</span><br><span class=\"line\">            - a73d11a7.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - econ</span><br><span class=\"line\">            - 97e230ff.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - german</span><br><span class=\"line\">            - cde71a5c.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - mechanical</span><br><span class=\"line\">            - c35464fd.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">    - test</span><br><span class=\"line\">        - cs</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - ...</span><br><span class=\"line\">    - validation</span><br><span class=\"line\">        - cs</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - ...</span><br></pre></td></tr></table></figure>\n<h1>Building a model</h1>\n<h2 id=\"Approach-1-Simple-custom-CNN\">Approach 1: Simple, custom CNN</h2>\n<p><strong>Code</strong></p>\n<ul>\n<li><a href=\"https://gist.github.com/muety/78bf6d7929e4facd199ad0ffea0b3ad9\" target=\"_blank\" rel=\"noopener\">custom_model.ipynb</a></li>\n</ul>\n<p>I decided to start simple and see if anything can be learned from the data at all. I defined the following simple CNN architecture in Keras:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">conv2d_1 (Conv2D)            (None, 62, 62, 32)        320       </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_3 (Conv2D)            (None, 12, 12, 32)        9248      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">flatten_1 (Flatten)          (None, 1152)              0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_1 (Dense)              (None, 64)                73792     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dropout_1 (Dropout)          (None, 64)                0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_2 (Dense)              (None, 4)                 260       </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 92,868</span><br><span class=\"line\">Trainable params: 92,868</span><br><span class=\"line\">Non-trainable params: 0</span><br></pre></td></tr></table></figure>\n<p>I used Keras‚Äô <a href=\"https://keras.io/preprocessing/image/\" target=\"_blank\" rel=\"noopener\">ImageDataGenerator</a> (great tool!) to read images into NumPy arrays, re-scale them to a shape of <code>(64, 63, 3)</code> (64 x 64 pixels, RGB) and perform some data augmentation using transformations like rotations, zooming, horizontal flipping, etc. to blow up my training data and hopefully build more robust, less overfitted models.</p>\n<p>I let the model train for <strong>100 epochs</strong>, using the <strong>Adam optimizer</strong> with default parameters and <strong>categorical crossentropy loss</strong>, a mini-batch size of <strong>32</strong> and <strong>3x augmentation</strong> (use transformations to blow up training data by a factor of three).</p>\n<h3 id=\"Results-57-1-accuracy\">Results (57.1 % accuracy)</h3>\n<p>The maximum <strong>validation accuracy of 0.66</strong> was reached after 74 epochs. <strong>Test accuracy</strong> turned out to be <strong>0.571</strong>. Considering that a quite simple model was trained completely from scratch with less than 1000 training examples, I am quite impressed by that result. It means that on average the model predicts more than every second student‚Äôs major correctly. The <strong>a-priori probability</strong> of a correct classification <strong>is 0.25</strong>, so the model has definitely learned at least something.</p>\n<h2 id=\"Approach-2-Fine-tuning-VGGFace\">Approach 2: Fine-tuning VGGFace</h2>\n<p><strong>Code</strong></p>\n<ul>\n<li><a href=\"https://gist.github.com/muety/a079dcb27d921d58323c9574152b2c2d\" target=\"_blank\" rel=\"noopener\">vggfaces_bottleneck_model.ipynb</a></li>\n<li><a href=\"https://gist.github.com/muety/c3b9e9401f178807c91ad890a6c67e18\" target=\"_blank\" rel=\"noopener\">vggfaces_finetuned_model.ipynb</a></li>\n</ul>\n<p>As an alternative to a simple, custom-defined CNN model, that is trained from scratch, I wanted to follow the common approach of fine-tuning the weights of an existing, pre-trained model. The basic idea of such an approach is to not ‚Äúre-invent the wheel‚Äù, but take advantage of what was already learned before and only slightly adapt that ‚Äúknowledge‚Äù (in form of weights) to a certain problem. Latent features in images, which a learning algorithm had already extracted from a giant set of training data before, can just be leveraged. <a href=\"https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/\" target=\"_blank\" rel=\"noopener\">‚ÄúImage Classification using pre-trained models in Keras‚Äù</a> gives an excellent overview of how <strong>fine-tuning</strong> works and how it is different from <strong>transfer learning</strong> and custom models. Expectations are that my given classification problem can be solved more accurately with less data.</p>\n<p>I decided to take a <strong>VGG16</strong> model architecture trained on <a href=\"http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/\" target=\"_blank\" rel=\"noopener\"><strong>VGGFace</strong></a> as a base (using the <a href=\"https://github.com/rcmalli/keras-vggface\" target=\"_blank\" rel=\"noopener\">keras-vggface</a> implementation) and followed <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\" target=\"_blank\" rel=\"noopener\">this guide</a> to fine-tune it. VGGFace is a dataset published by the University of Oxford that contains more than 3.3 million face images. Accordingly, I expected it to have extracted very robust facial features and to be quite well-suited for face classification.</p>\n<h3 id=\"Step-1-Transfer-learning-to-initialize-weights\">Step 1: Transfer-learning to initialize weights</h3>\n<p>My implementation consists of two steps, since <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\" target=\"_blank\" rel=\"noopener\">it is recommended</a> that</p>\n<blockquote>\n<p>in order to perform fine-tuning, all layers should start with properly trained weights.</p>\n</blockquote>\n<p>In this first step, transfer-learning is used to find proper weights for a set of a few newly added, custom, fully-connected classification layers. These are used as the initial weights in step 2 later on. To perform this initialization, a pre-trained VGGFace model, with the final classification layers cut off, is used to extract 128 <em>bottleneck features</em> for every image. Subsequently, another tiny model, consisting of fully-connected layers, is trained on these features to perform the eventual classification. The weights are persisted to a file and loaded again in step 2.</p>\n<p>The model architecture looks like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">dense_1 (Dense)              (None, 128)               65664     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dropout_1 (Dropout)          (None, 128)               0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_2 (Dense)              (None, 4)                 516       </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 66,180</span><br><span class=\"line\">Trainable params: 66,180</span><br><span class=\"line\">Non-trainable params: 0</span><br></pre></td></tr></table></figure>\n<h3 id=\"Step-2-Fine-tuning\">Step 2: Fine-tuning</h3>\n<p>In this second step, a pre-trained VGGFace model (with the first n - 3 layers freezed) is used in combination with the pre-trained top layers from step 1 to fine-tune weights for our specific classification task. It takes mini-batches of (128, 128, 3)-shaped tensors (128 x 128 pixels, RGB) as input and predicts probabilities for each of our four target classes.</p>\n<p>The architecture of the combined model looks like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">vggface_vgg16 (Model)        (None, 512)               14714688  </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">top (Sequential)             (None, 4)                 66180     </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 14,780,868</span><br><span class=\"line\">Trainable params: 2,425,988</span><br><span class=\"line\">Non-trainable params: 12,354,880</span><br></pre></td></tr></table></figure>\n<p><code>top</code> is the model described in step 1, <code>vggface_vgg16</code> is a VGG16 model and looks like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">input_3 (InputLayer)         (None, 128, 128, 3)       0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv1_1 (Conv2D)             (None, 128, 128, 64)      1792      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv1_2 (Conv2D)             (None, 128, 128, 64)      36928     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool1 (MaxPooling2D)         (None, 64, 64, 64)        0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2_1 (Conv2D)             (None, 64, 64, 128)       73856     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2_2 (Conv2D)             (None, 64, 64, 128)       147584    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool2 (MaxPooling2D)         (None, 32, 32, 128)       0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv3_1 (Conv2D)             (None, 32, 32, 256)       295168    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv3_2 (Conv2D)             (None, 32, 32, 256)       590080    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv3_3 (Conv2D)             (None, 32, 32, 256)       590080    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool3 (MaxPooling2D)         (None, 16, 16, 256)       0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv4_1 (Conv2D)             (None, 16, 16, 512)       1180160   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv4_2 (Conv2D)             (None, 16, 16, 512)       2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv4_3 (Conv2D)             (None, 16, 16, 512)       2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool4 (MaxPooling2D)         (None, 8, 8, 512)         0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv5_1 (Conv2D)             (None, 8, 8, 512)         2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv5_2 (Conv2D)             (None, 8, 8, 512)         2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv5_3 (Conv2D)             (None, 8, 8, 512)         2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool5 (MaxPooling2D)         (None, 4, 4, 512)         0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">global_max_pooling2d_3 (Glob (None, 512)               0         </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 14,714,688</span><br><span class=\"line\">Trainable params: 2,359,808</span><br><span class=\"line\">Non-trainable params: 12,354,880</span><br></pre></td></tr></table></figure>\n<p>I was using Keras <em>ImageDataGenerator</em> again for loading the data, augmenting (3x) and resizing it. As <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\" target=\"_blank\" rel=\"noopener\">recommended</a>, <em>stochastic gradient descent</em> is used with a small learning rate (10^-4) to carefully adapt weights. The model was trained for <strong>100 epochs</strong> on <strong>batches of 32 images</strong> and, again, used <strong>categorical cross entropy</strong> as a loss function.</p>\n<h3 id=\"Results-54-6-accuracy\">Results (54.6 % accuracy)</h3>\n<p>The maximum <strong>validation accuracy of 0.64</strong> was reached after 38 epochs already. <strong>Test accuracy</strong> turned out to be <strong>0.546</strong>, which is a quite disappointing result, considering that even our simple, custom CNN-model achieved a higher accuracy. Maybe the model‚Äôs complexity is too high for the small amount of training data?</p>\n<h1>Inspecting the model</h1>\n<p>To get better insights on how the model performs, I briefly inspected it with regards to several criteria. This is a short summary of my finding.</p>\n<h2 id=\"Code\">Code</h2>\n<ul>\n<li><a href=\"https://gist.github.com/muety/404befcfb2eef4b59398f3c8590ce692\" target=\"_blank\" rel=\"noopener\">inspection.ipynb</a></li>\n</ul>\n<h2 id=\"Class-distribution\">Class distribution</h2>\n<p>The first thing I looked at was the class distribution. How are the four study major subjects represented in our data and what does the model predict?</p>\n<table>\n<thead>\n<tr>\n<th>cs</th>\n<th>econ</th>\n<th>german</th>\n<th>mechanical</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><em>real</em></td>\n<td>0.2510</td>\n<td>0.2808</td>\n<td>0.2127</td>\n</tr>\n<tr>\n<td><em>pred</em></td>\n<td>0.2595</td>\n<td>0.2936</td>\n<td>0.1361</td>\n</tr>\n</tbody>\n</table>\n<p>Apparently, the model neglects the class of <em>german linguists</em> a bit. That is also the class for which we have the least training data. Probably I should collect more.</p>\n<h2 id=\"Examples-of-false-classifications\">Examples of false classifications</h2>\n<p>I wanted to get an idea of what the model does wrong and what it does right. Consequently, I took a look at the top (with respect to confidence) five <strong>(1) false negatives</strong>, <strong>(2) false positives</strong> and <strong>(3) true positives</strong>.</p>\n<p>Here is an excerpt for class <em>econ</em>:</p>\n<p><img src=\"images/academic_faces2.png\" alt></p>\n<p>The top row shows examples of economists, who the model didn‚Äôt recognize as such.<br>\nThe center row depicts examples of what the model ‚Äúthinks‚Äù economists look like, but who are actually students / researchers with a different major.<br>\nFinally, the bottom row shows examples of good matches, i.e. people for whom the model had a very high confidence for their actual class.</p>\n<p>Again, if you are in one of these pictures and want to get removed, please contact me.</p>\n<h2 id=\"Confusion-matrix\">Confusion matrix</h2>\n<p>To see which profession the model is unsure about, I calculated the confusion matrix.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array([[12.76595745,  5.95744681,  0.        ,  6.38297872],</span><br><span class=\"line\">       [ 3.40425532, 12.76595745,  3.82978723,  8.08510638],</span><br><span class=\"line\">       [ 3.82978723,  5.53191489,  8.5106383 ,  3.40425532],</span><br><span class=\"line\">       [ 5.95744681,  5.10638298,  1.27659574, 13.19148936]])</span><br></pre></td></tr></table></figure>\n<p><img src=\"images/academic_faces3.png\" alt><br>\n<strong>Legend:</strong></p>\n<ul>\n<li>0 = cs, 1 = econ, 2 = german, 3 = mechanical</li>\n<li>Brighter colors ~ higher value</li>\n</ul>\n<p>What we can read from the confusion matrix is that, for instance, the model tends to classify economists as mechanical engineers quite often.</p>\n<h1>Conclusion</h1>\n<p>First of all, this is not a scientific study, but rather a small hobby project of mine. Also, it does not have a lot of real-world importance, since one might rarely want to classify students into four categories.</p>\n<p>Although the results are not spectacular, I am still quite happy about them and at least my model was able to do a lot better than random guessing. Given an <strong>accuracy of 57 %</strong> with four classes, you could definitely say that it is, to some extent, possible to learn a stereotypical-looking person‚Äôs study major from only in image of their face. Of course, this only holds true within a bounded context and under a set of restrictions, but it is still an interesting insight to me.</p>\n<p>Moreover, I am quite sure that there is still a lot of room for improvements to the model, which could yield a better performance. Those might include:</p>\n<ul>\n<li>More training data from a wider range of sources</li>\n<li>More thorough preprocessing (e.g. filter out images of secretaries)</li>\n<li>Different model architecture</li>\n<li>Hyper-parameter tuning</li>\n<li>Manual feature engineering</li>\n<li>‚Ä¶</li>\n</ul>\n<p>Please let me know what you think of this project. I would love to get some feedback!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1>The Idea</h1>\n<p>A few months ago I read a paper with the title <a href=\"https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual\" target=\"_blank\" rel=\"noopener\">‚ÄúDeep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images‚Äù</a>, which caused a lot of <a href=\"https://news.ycombinator.com/item?id=15198997\" target=\"_blank\" rel=\"noopener\">controversy</a>. While I don‚Äôt want to comment on the methodology and quality of the paper (that was already done, e.g. in <a href=\"https://www.fast.ai/2017/09/13/kosinski/\" target=\"_blank\" rel=\"noopener\">an article by Jeremy Howard</a>), I found it very interesting and inspiring. In a nutshell, the researchers collected face pictures from dating websites and built a machine learning model to classify people‚Äôs sexual orientation and reached quite an impressive accuracy with their approach.</p>\n<p><a href=\"https://scatter.wordpress.com/2017/09/10/guest-post-artificial-intelligence-discovers-gayface-sigh/\" target=\"_blank\" rel=\"noopener\">This guest post</a> summarizes the results as:</p>\n<blockquote>\n<p>AI Can‚Äôt Tell if You‚Äôre Gay‚Ä¶ But it Can Tell if You‚Äôre a Walking Stereotype.</p>\n</blockquote>\n<p>And indeed, we often see people who look very stereotypical. I tried to think of more such scenarios and came to the conclusion that another environment, where this phenomenon can be found quite often, is a university campus. So often you walk around the campus and see students, who just look like a law student, a computer science nerd, a sportsman, etc. Sometimes I‚Äôm so curious that I almost want to ask them whether my assumption is correct.</p>\n<p>After having read the above paper, I wondered if some machine learning model might be able to quantify these latent assumptions and find out a stereotypical-looking student‚Äôs profession or major.</p>\n<p>Although I only have a little more than basic knowledge in machine learning, especially in image classification using deep neural nets, I took it as a personal challenge to <strong>build a classifier, that detects academics‚Äô major based on an image of their face</strong>.</p>\n<h1>Disclaimer</h1>\n<p>Please don‚Äôt take this article too serious. I‚Äôm not a machine learning expert or a professional scientist. There might be some mistakes in my methodology or implementation. However, I‚Äôd love to hear your thoughts and feedback.</p>\n<h1>Approach</h1>\n<p>My first (and final) approach was to <strong>(1.) collect face pictures of students</strong> or other academics, <strong>(2.) label them</strong> with a small, limited set of classes, corresponding to their major, and eventually <strong>(3.) fit a convolutional neural net (CNN)</strong> as a classifier. I thought of fields of study, whose students might potentially look a bit stereotypical and came up with four classes:</p>\n<ol>\n<li>computer science (~ cs)</li>\n<li>economics (~ econ)</li>\n<li>(German) linguistics (~ german)</li>\n<li>mechanical engineering (~ mechanical)</li>\n</ol>\n<p>Please note that this is not meant to be offending by any means! (I‚Äôm a computer science nerd myself üòâ).</p>\n<h1>Getting the data</h1>\n<p>The very first prerequisite is training data - as usual, when doing machine learning. And since I aimed at training a convolutional neural net (CNN), there should be a lot of data, preferably.</p>\n<p>While it would have been a funny approach to walk around my campus and ask students for their major and a picture of their face, I would probably not have ended up with a lot of data. Instead, I decided to <strong>crawl pictures from university websites</strong>. Almost every department at every university has a page called ‚Äú<a href=\"http://dbis.ipd.kit.edu/english/722.php\" target=\"_blank\" rel=\"noopener\">Staff</a>‚Äù, ‚ÄúPeople‚Äù, ‚ÄúResearchers‚Äù or the like on their websites. While these are not particularly lists of students, but of professors, research assistants and PhD candidates, I presumed that those pictures should still be sufficient as training data.</p>\n<p>I wrote a bunch of <strong>crawler scripts</strong> using Python and <a href=\"https://www.seleniumhq.org/\" target=\"_blank\" rel=\"noopener\">Selenium WebDriver</a> to crawl <strong>57</strong> different websites, including the websites of various departments of the following universities:</p>\n<ul>\n<li>Karlsruhe Institute of Technology</li>\n<li>TU Munich</li>\n<li>University of Munich</li>\n<li>University of W√ºrzburg</li>\n<li>University of Siegen</li>\n<li>University of Bayreuth</li>\n<li>University of Feiburg</li>\n<li>University of Heidelberg</li>\n<li>University of Erlangen</li>\n<li>University of Bamberg</li>\n<li>University of Mannheim</li>\n</ul>\n<p>After a bit of manual data cleaning (removing pictures without faces, rotating pictures, ‚Ä¶), I ended up with a total of <strong>1369</strong> labeled images from four different classes. While this is not very much data for training a CNN, I decided to give it a try anyway.</p>\n<h2 id=\"Examples\">Examples</h2>\n<h3 id=\"Images\">Images</h3>\n<p><strong>An excerpt from the folder containing all raw images after crawling:</strong><br>\n<img src=\"images/academic_faces1.png\" alt=\"Excerpt from all crawled raw images\"><br>\n(If you are in one of these pictures and want to get removed, please contact me.)</p>\n<h3 id=\"Labels\">Labels</h3>\n<p><strong>An excerpt from <code>index.csv</code> containing labels and meta-data for every image:</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">id,category,image_url,name</span><br><span class=\"line\">c35464fd,mechanical,http://www.fast.kit.edu/lff/1011_1105.php,Prof. Dr. rer. nat. Frank Gauterin</span><br><span class=\"line\">a73d11a7,cs,http://h2t.anthropomatik.kit.edu/21_1784.php,Kevin Liang</span><br><span class=\"line\">97e230ff,econ,http://marketing.iism.kit.edu/21_371.php,Dr. Hanna Schumacher</span><br><span class=\"line\">cde71a5c,german,https://www.germanistik.uni-muenchen.de/personal/ndl/mitarbeiter/bach/index.html,Dr. Oliver Bach</span><br></pre></td></tr></table></figure>\n<h1>Preprocessing the data</h1>\n<p>Before the images could be used as training data for a learning algorithm, a bit of preprocessing needed to be applied. Mainly, I did two major steps of preprocessing.</p>\n<ol>\n<li><strong>Cropping</strong> images to faces - As you can see, pictures are taken from different angles, some of them contain a lot of background, some are not centered, etc. To get better training data, the pictures have to be cropped to only the face and nothing else.</li>\n<li><strong>Scaling</strong> - All pictures come in different resolutions, but eventually need to be of exactly the same size in order to be used as input to a neural network.</li>\n</ol>\n<p>To achieve both of these preprocessing steps I used a great, little, open-source, OpenCV-based Python tool called <a href=\"https://github.com/leblancfg/autocrop\" target=\"_blank\" rel=\"noopener\">autocrop</a> with the following command:</p>\n<p><code>autocrop -i raw -o preprocessed -w 128 -H 128 &gt; autocrop.log</code>.</p>\n<p>This detects the face in every picture in <code>raw</code> folder, crops the picture to that face, re-scales the resulting image to 128 x 128 pixels and saves it to <code>preprocessed</code> folder. Of course, there are some pictures in which the algorithm can not detect a face. Those are logged to stdout and persisted to <code>autocrop.log</code>.</p>\n<p>In addition, I wrote a script that parses <code>autocrop.log</code> to get the failed images and subsequently split the images into <em>train</em> (70 %), <em>test</em> (20 %) and <em>validation</em> (10 %) and copy them to a folder structure that is compatible to the format required by <a href=\"https://keras.io/preprocessing/image/\" target=\"_blank\" rel=\"noopener\">Keras ImageDataGenerator</a> to read training data.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- raw</span><br><span class=\"line\">    - index.csv</span><br><span class=\"line\">    - c35464fd.jpg</span><br><span class=\"line\">    - a73d11a7.jpg</span><br><span class=\"line\">    - ...</span><br><span class=\"line\">- preprocessed </span><br><span class=\"line\">    - train</span><br><span class=\"line\">        - cs</span><br><span class=\"line\">            - a73d11a7.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - econ</span><br><span class=\"line\">            - 97e230ff.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - german</span><br><span class=\"line\">            - cde71a5c.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - mechanical</span><br><span class=\"line\">            - c35464fd.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">    - test</span><br><span class=\"line\">        - cs</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - ...</span><br><span class=\"line\">    - validation</span><br><span class=\"line\">        - cs</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - ...</span><br></pre></td></tr></table></figure>\n<h1>Building a model</h1>\n<h2 id=\"Approach-1-Simple-custom-CNN\">Approach 1: Simple, custom CNN</h2>\n<p><strong>Code</strong></p>\n<ul>\n<li><a href=\"https://gist.github.com/muety/78bf6d7929e4facd199ad0ffea0b3ad9\" target=\"_blank\" rel=\"noopener\">custom_model.ipynb</a></li>\n</ul>\n<p>I decided to start simple and see if anything can be learned from the data at all. I defined the following simple CNN architecture in Keras:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">conv2d_1 (Conv2D)            (None, 62, 62, 32)        320       </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_3 (Conv2D)            (None, 12, 12, 32)        9248      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">flatten_1 (Flatten)          (None, 1152)              0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_1 (Dense)              (None, 64)                73792     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dropout_1 (Dropout)          (None, 64)                0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_2 (Dense)              (None, 4)                 260       </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 92,868</span><br><span class=\"line\">Trainable params: 92,868</span><br><span class=\"line\">Non-trainable params: 0</span><br></pre></td></tr></table></figure>\n<p>I used Keras‚Äô <a href=\"https://keras.io/preprocessing/image/\" target=\"_blank\" rel=\"noopener\">ImageDataGenerator</a> (great tool!) to read images into NumPy arrays, re-scale them to a shape of <code>(64, 63, 3)</code> (64 x 64 pixels, RGB) and perform some data augmentation using transformations like rotations, zooming, horizontal flipping, etc. to blow up my training data and hopefully build more robust, less overfitted models.</p>\n<p>I let the model train for <strong>100 epochs</strong>, using the <strong>Adam optimizer</strong> with default parameters and <strong>categorical crossentropy loss</strong>, a mini-batch size of <strong>32</strong> and <strong>3x augmentation</strong> (use transformations to blow up training data by a factor of three).</p>\n<h3 id=\"Results-57-1-accuracy\">Results (57.1 % accuracy)</h3>\n<p>The maximum <strong>validation accuracy of 0.66</strong> was reached after 74 epochs. <strong>Test accuracy</strong> turned out to be <strong>0.571</strong>. Considering that a quite simple model was trained completely from scratch with less than 1000 training examples, I am quite impressed by that result. It means that on average the model predicts more than every second student‚Äôs major correctly. The <strong>a-priori probability</strong> of a correct classification <strong>is 0.25</strong>, so the model has definitely learned at least something.</p>\n<h2 id=\"Approach-2-Fine-tuning-VGGFace\">Approach 2: Fine-tuning VGGFace</h2>\n<p><strong>Code</strong></p>\n<ul>\n<li><a href=\"https://gist.github.com/muety/a079dcb27d921d58323c9574152b2c2d\" target=\"_blank\" rel=\"noopener\">vggfaces_bottleneck_model.ipynb</a></li>\n<li><a href=\"https://gist.github.com/muety/c3b9e9401f178807c91ad890a6c67e18\" target=\"_blank\" rel=\"noopener\">vggfaces_finetuned_model.ipynb</a></li>\n</ul>\n<p>As an alternative to a simple, custom-defined CNN model, that is trained from scratch, I wanted to follow the common approach of fine-tuning the weights of an existing, pre-trained model. The basic idea of such an approach is to not ‚Äúre-invent the wheel‚Äù, but take advantage of what was already learned before and only slightly adapt that ‚Äúknowledge‚Äù (in form of weights) to a certain problem. Latent features in images, which a learning algorithm had already extracted from a giant set of training data before, can just be leveraged. <a href=\"https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/\" target=\"_blank\" rel=\"noopener\">‚ÄúImage Classification using pre-trained models in Keras‚Äù</a> gives an excellent overview of how <strong>fine-tuning</strong> works and how it is different from <strong>transfer learning</strong> and custom models. Expectations are that my given classification problem can be solved more accurately with less data.</p>\n<p>I decided to take a <strong>VGG16</strong> model architecture trained on <a href=\"http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/\" target=\"_blank\" rel=\"noopener\"><strong>VGGFace</strong></a> as a base (using the <a href=\"https://github.com/rcmalli/keras-vggface\" target=\"_blank\" rel=\"noopener\">keras-vggface</a> implementation) and followed <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\" target=\"_blank\" rel=\"noopener\">this guide</a> to fine-tune it. VGGFace is a dataset published by the University of Oxford that contains more than 3.3 million face images. Accordingly, I expected it to have extracted very robust facial features and to be quite well-suited for face classification.</p>\n<h3 id=\"Step-1-Transfer-learning-to-initialize-weights\">Step 1: Transfer-learning to initialize weights</h3>\n<p>My implementation consists of two steps, since <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\" target=\"_blank\" rel=\"noopener\">it is recommended</a> that</p>\n<blockquote>\n<p>in order to perform fine-tuning, all layers should start with properly trained weights.</p>\n</blockquote>\n<p>In this first step, transfer-learning is used to find proper weights for a set of a few newly added, custom, fully-connected classification layers. These are used as the initial weights in step 2 later on. To perform this initialization, a pre-trained VGGFace model, with the final classification layers cut off, is used to extract 128 <em>bottleneck features</em> for every image. Subsequently, another tiny model, consisting of fully-connected layers, is trained on these features to perform the eventual classification. The weights are persisted to a file and loaded again in step 2.</p>\n<p>The model architecture looks like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">dense_1 (Dense)              (None, 128)               65664     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dropout_1 (Dropout)          (None, 128)               0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_2 (Dense)              (None, 4)                 516       </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 66,180</span><br><span class=\"line\">Trainable params: 66,180</span><br><span class=\"line\">Non-trainable params: 0</span><br></pre></td></tr></table></figure>\n<h3 id=\"Step-2-Fine-tuning\">Step 2: Fine-tuning</h3>\n<p>In this second step, a pre-trained VGGFace model (with the first n - 3 layers freezed) is used in combination with the pre-trained top layers from step 1 to fine-tune weights for our specific classification task. It takes mini-batches of (128, 128, 3)-shaped tensors (128 x 128 pixels, RGB) as input and predicts probabilities for each of our four target classes.</p>\n<p>The architecture of the combined model looks like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">vggface_vgg16 (Model)        (None, 512)               14714688  </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">top (Sequential)             (None, 4)                 66180     </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 14,780,868</span><br><span class=\"line\">Trainable params: 2,425,988</span><br><span class=\"line\">Non-trainable params: 12,354,880</span><br></pre></td></tr></table></figure>\n<p><code>top</code> is the model described in step 1, <code>vggface_vgg16</code> is a VGG16 model and looks like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">input_3 (InputLayer)         (None, 128, 128, 3)       0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv1_1 (Conv2D)             (None, 128, 128, 64)      1792      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv1_2 (Conv2D)             (None, 128, 128, 64)      36928     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool1 (MaxPooling2D)         (None, 64, 64, 64)        0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2_1 (Conv2D)             (None, 64, 64, 128)       73856     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2_2 (Conv2D)             (None, 64, 64, 128)       147584    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool2 (MaxPooling2D)         (None, 32, 32, 128)       0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv3_1 (Conv2D)             (None, 32, 32, 256)       295168    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv3_2 (Conv2D)             (None, 32, 32, 256)       590080    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv3_3 (Conv2D)             (None, 32, 32, 256)       590080    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool3 (MaxPooling2D)         (None, 16, 16, 256)       0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv4_1 (Conv2D)             (None, 16, 16, 512)       1180160   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv4_2 (Conv2D)             (None, 16, 16, 512)       2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv4_3 (Conv2D)             (None, 16, 16, 512)       2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool4 (MaxPooling2D)         (None, 8, 8, 512)         0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv5_1 (Conv2D)             (None, 8, 8, 512)         2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv5_2 (Conv2D)             (None, 8, 8, 512)         2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv5_3 (Conv2D)             (None, 8, 8, 512)         2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool5 (MaxPooling2D)         (None, 4, 4, 512)         0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">global_max_pooling2d_3 (Glob (None, 512)               0         </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 14,714,688</span><br><span class=\"line\">Trainable params: 2,359,808</span><br><span class=\"line\">Non-trainable params: 12,354,880</span><br></pre></td></tr></table></figure>\n<p>I was using Keras <em>ImageDataGenerator</em> again for loading the data, augmenting (3x) and resizing it. As <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\" target=\"_blank\" rel=\"noopener\">recommended</a>, <em>stochastic gradient descent</em> is used with a small learning rate (10^-4) to carefully adapt weights. The model was trained for <strong>100 epochs</strong> on <strong>batches of 32 images</strong> and, again, used <strong>categorical cross entropy</strong> as a loss function.</p>\n<h3 id=\"Results-54-6-accuracy\">Results (54.6 % accuracy)</h3>\n<p>The maximum <strong>validation accuracy of 0.64</strong> was reached after 38 epochs already. <strong>Test accuracy</strong> turned out to be <strong>0.546</strong>, which is a quite disappointing result, considering that even our simple, custom CNN-model achieved a higher accuracy. Maybe the model‚Äôs complexity is too high for the small amount of training data?</p>\n<h1>Inspecting the model</h1>\n<p>To get better insights on how the model performs, I briefly inspected it with regards to several criteria. This is a short summary of my finding.</p>\n<h2 id=\"Code\">Code</h2>\n<ul>\n<li><a href=\"https://gist.github.com/muety/404befcfb2eef4b59398f3c8590ce692\" target=\"_blank\" rel=\"noopener\">inspection.ipynb</a></li>\n</ul>\n<h2 id=\"Class-distribution\">Class distribution</h2>\n<p>The first thing I looked at was the class distribution. How are the four study major subjects represented in our data and what does the model predict?</p>\n<table>\n<thead>\n<tr>\n<th>cs</th>\n<th>econ</th>\n<th>german</th>\n<th>mechanical</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><em>real</em></td>\n<td>0.2510</td>\n<td>0.2808</td>\n<td>0.2127</td>\n</tr>\n<tr>\n<td><em>pred</em></td>\n<td>0.2595</td>\n<td>0.2936</td>\n<td>0.1361</td>\n</tr>\n</tbody>\n</table>\n<p>Apparently, the model neglects the class of <em>german linguists</em> a bit. That is also the class for which we have the least training data. Probably I should collect more.</p>\n<h2 id=\"Examples-of-false-classifications\">Examples of false classifications</h2>\n<p>I wanted to get an idea of what the model does wrong and what it does right. Consequently, I took a look at the top (with respect to confidence) five <strong>(1) false negatives</strong>, <strong>(2) false positives</strong> and <strong>(3) true positives</strong>.</p>\n<p>Here is an excerpt for class <em>econ</em>:</p>\n<p><img src=\"images/academic_faces2.png\" alt></p>\n<p>The top row shows examples of economists, who the model didn‚Äôt recognize as such.<br>\nThe center row depicts examples of what the model ‚Äúthinks‚Äù economists look like, but who are actually students / researchers with a different major.<br>\nFinally, the bottom row shows examples of good matches, i.e. people for whom the model had a very high confidence for their actual class.</p>\n<p>Again, if you are in one of these pictures and want to get removed, please contact me.</p>\n<h2 id=\"Confusion-matrix\">Confusion matrix</h2>\n<p>To see which profession the model is unsure about, I calculated the confusion matrix.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array([[12.76595745,  5.95744681,  0.        ,  6.38297872],</span><br><span class=\"line\">       [ 3.40425532, 12.76595745,  3.82978723,  8.08510638],</span><br><span class=\"line\">       [ 3.82978723,  5.53191489,  8.5106383 ,  3.40425532],</span><br><span class=\"line\">       [ 5.95744681,  5.10638298,  1.27659574, 13.19148936]])</span><br></pre></td></tr></table></figure>\n<p><img src=\"images/academic_faces3.png\" alt><br>\n<strong>Legend:</strong></p>\n<ul>\n<li>0 = cs, 1 = econ, 2 = german, 3 = mechanical</li>\n<li>Brighter colors ~ higher value</li>\n</ul>\n<p>What we can read from the confusion matrix is that, for instance, the model tends to classify economists as mechanical engineers quite often.</p>\n<h1>Conclusion</h1>\n<p>First of all, this is not a scientific study, but rather a small hobby project of mine. Also, it does not have a lot of real-world importance, since one might rarely want to classify students into four categories.</p>\n<p>Although the results are not spectacular, I am still quite happy about them and at least my model was able to do a lot better than random guessing. Given an <strong>accuracy of 57 %</strong> with four classes, you could definitely say that it is, to some extent, possible to learn a stereotypical-looking person‚Äôs study major from only in image of their face. Of course, this only holds true within a bounded context and under a set of restrictions, but it is still an interesting insight to me.</p>\n<p>Moreover, I am quite sure that there is still a lot of room for improvements to the model, which could yield a better performance. Those might include:</p>\n<ul>\n<li>More training data from a wider range of sources</li>\n<li>More thorough preprocessing (e.g. filter out images of secretaries)</li>\n<li>Different model architecture</li>\n<li>Hyper-parameter tuning</li>\n<li>Manual feature engineering</li>\n<li>‚Ä¶</li>\n</ul>\n<p>Please let me know what you think of this project. I would love to get some feedback!</p>\n"},{"title":"Digitalocean ‚Äì My preferred Cloud Hosting Provider","date":"2016-04-06T20:55:18.000Z","_content":"\n![](/images/do.png)\n[DigitalOcean.com](https://digitalocean.com) is a service that offers you on-demand virtual server instances that you can use to host any server application, be it a simple webpage, a Node.js written backend, any Docker container or anything else.\n\nIt is especially useful if you have developed a web application and want to bring it to the internet without owning a root server. In this case you can go to DigitalOcean, choose any boilerplate (or Droplet, as they call it) for your new virtual, cloud-hosted machine, additionally choose a the datacenter region which is closest to you or your customers, add your SSH keys for quick access and hit the create button. Within less than a minute your machine is up and running with a dedicated IPv4 assigned where you can ssh in.\n\nAs a template / boilerplate you can either choose from the common, plain Linux distributions (even CoreOS) in almost any version or take one of the various pre-configured environments like a machine already running Docker, Node.js, ownCloud, Joomla or plenty other runtimes and applications.\n\nFor scalability you can choose between different sizes, which basically means different memory capacity, cpu cores, ssd capacity and amount of traffic.\n\nA feature which I only know from DigitalOcean by now is the ability to create a cluster of multiple machines (Droplets) with private networking, meaning they can communicate with every other node in the cluster but are kind of isolated from the internet. I haven‚Äôt tried this feature too much but it is similar to what you might know from linking multiple Docker containers together.\n\nWhat I also like about the service is the ultra simple-to-use, minimalistic and intuitive web-interface that abstracts away this entire technical complexity running in the background when users do a single click on a button until a pre-installed machine comes up.\n\nDigitalOcean is my personal favorite service of this type, but I also want to mention some alternatives which are [Microsoft Azure](https://azure.microsoft.com/en-us/), [Google Compute Engine](https://cloud.google.com/compute/), [Amazon EC2](https://aws.amazon.com/de/ec2), [Linode](https://www.linode.com/) or in a wider sense also [JiffyBox.de](http://jiffybox.de).\n\nIf you want to give DigitalOcean a try (and support me), follow [this referral link](https://m.do.co/c/4abee7f659ad) where you get $10 in credits, which is enough for running the smallest container for two months. I will get $25 in credits in case you in turn spend $25 for credits. Of course I would be very pleased if you did so.\n\n![](assets/img/simple-smile.png)","source":"_posts/digitalocean-my-preferred-cloud-hosting-provider.md","raw":"---\ntitle: Digitalocean ‚Äì My preferred Cloud Hosting Provider\ndate: 2016-04-06 22:55:18\ntags:\n---\n\n![](/images/do.png)\n[DigitalOcean.com](https://digitalocean.com) is a service that offers you on-demand virtual server instances that you can use to host any server application, be it a simple webpage, a Node.js written backend, any Docker container or anything else.\n\nIt is especially useful if you have developed a web application and want to bring it to the internet without owning a root server. In this case you can go to DigitalOcean, choose any boilerplate (or Droplet, as they call it) for your new virtual, cloud-hosted machine, additionally choose a the datacenter region which is closest to you or your customers, add your SSH keys for quick access and hit the create button. Within less than a minute your machine is up and running with a dedicated IPv4 assigned where you can ssh in.\n\nAs a template / boilerplate you can either choose from the common, plain Linux distributions (even CoreOS) in almost any version or take one of the various pre-configured environments like a machine already running Docker, Node.js, ownCloud, Joomla or plenty other runtimes and applications.\n\nFor scalability you can choose between different sizes, which basically means different memory capacity, cpu cores, ssd capacity and amount of traffic.\n\nA feature which I only know from DigitalOcean by now is the ability to create a cluster of multiple machines (Droplets) with private networking, meaning they can communicate with every other node in the cluster but are kind of isolated from the internet. I haven‚Äôt tried this feature too much but it is similar to what you might know from linking multiple Docker containers together.\n\nWhat I also like about the service is the ultra simple-to-use, minimalistic and intuitive web-interface that abstracts away this entire technical complexity running in the background when users do a single click on a button until a pre-installed machine comes up.\n\nDigitalOcean is my personal favorite service of this type, but I also want to mention some alternatives which are [Microsoft Azure](https://azure.microsoft.com/en-us/), [Google Compute Engine](https://cloud.google.com/compute/), [Amazon EC2](https://aws.amazon.com/de/ec2), [Linode](https://www.linode.com/) or in a wider sense also [JiffyBox.de](http://jiffybox.de).\n\nIf you want to give DigitalOcean a try (and support me), follow [this referral link](https://m.do.co/c/4abee7f659ad) where you get $10 in credits, which is enough for running the smallest container for two months. I will get $25 in credits in case you in turn spend $25 for credits. Of course I would be very pleased if you did so.\n\n![](assets/img/simple-smile.png)","slug":"digitalocean-my-preferred-cloud-hosting-provider","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhhx000b40mqn6hs64po","content":"<p><img src=\"/images/do.png\" alt><br>\n<a href=\"https://digitalocean.com\" target=\"_blank\" rel=\"noopener\">DigitalOcean.com</a> is a service that offers you on-demand virtual server instances that you can use to host any server application, be it a simple webpage, a Node.js written backend, any Docker container or anything else.</p>\n<p>It is especially useful if you have developed a web application and want to bring it to the internet without owning a root server. In this case you can go to DigitalOcean, choose any boilerplate (or Droplet, as they call it) for your new virtual, cloud-hosted machine, additionally choose a the datacenter region which is closest to you or your customers, add your SSH keys for quick access and hit the create button. Within less than a minute your machine is up and running with a dedicated IPv4 assigned where you can ssh in.</p>\n<p>As a template / boilerplate you can either choose from the common, plain Linux distributions (even CoreOS) in almost any version or take one of the various pre-configured environments like a machine already running Docker, Node.js, ownCloud, Joomla or plenty other runtimes and applications.</p>\n<p>For scalability you can choose between different sizes, which basically means different memory capacity, cpu cores, ssd capacity and amount of traffic.</p>\n<p>A feature which I only know from DigitalOcean by now is the ability to create a cluster of multiple machines (Droplets) with private networking, meaning they can communicate with every other node in the cluster but are kind of isolated from the internet. I haven‚Äôt tried this feature too much but it is similar to what you might know from linking multiple Docker containers together.</p>\n<p>What I also like about the service is the ultra simple-to-use, minimalistic and intuitive web-interface that abstracts away this entire technical complexity running in the background when users do a single click on a button until a pre-installed machine comes up.</p>\n<p>DigitalOcean is my personal favorite service of this type, but I also want to mention some alternatives which are <a href=\"https://azure.microsoft.com/en-us/\" target=\"_blank\" rel=\"noopener\">Microsoft Azure</a>, <a href=\"https://cloud.google.com/compute/\" target=\"_blank\" rel=\"noopener\">Google Compute Engine</a>, <a href=\"https://aws.amazon.com/de/ec2\" target=\"_blank\" rel=\"noopener\">Amazon EC2</a>, <a href=\"https://www.linode.com/\" target=\"_blank\" rel=\"noopener\">Linode</a> or in a wider sense also <a href=\"http://jiffybox.de\" target=\"_blank\" rel=\"noopener\">JiffyBox.de</a>.</p>\n<p>If you want to give DigitalOcean a try (and support me), follow <a href=\"https://m.do.co/c/4abee7f659ad\" target=\"_blank\" rel=\"noopener\">this referral link</a> where you get $10 in credits, which is enough for running the smallest container for two months. I will get $25 in credits in case you in turn spend $25 for credits. Of course I would be very pleased if you did so.</p>\n<p><img src=\"assets/img/simple-smile.png\" alt></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"/images/do.png\" alt><br>\n<a href=\"https://digitalocean.com\" target=\"_blank\" rel=\"noopener\">DigitalOcean.com</a> is a service that offers you on-demand virtual server instances that you can use to host any server application, be it a simple webpage, a Node.js written backend, any Docker container or anything else.</p>\n<p>It is especially useful if you have developed a web application and want to bring it to the internet without owning a root server. In this case you can go to DigitalOcean, choose any boilerplate (or Droplet, as they call it) for your new virtual, cloud-hosted machine, additionally choose a the datacenter region which is closest to you or your customers, add your SSH keys for quick access and hit the create button. Within less than a minute your machine is up and running with a dedicated IPv4 assigned where you can ssh in.</p>\n<p>As a template / boilerplate you can either choose from the common, plain Linux distributions (even CoreOS) in almost any version or take one of the various pre-configured environments like a machine already running Docker, Node.js, ownCloud, Joomla or plenty other runtimes and applications.</p>\n<p>For scalability you can choose between different sizes, which basically means different memory capacity, cpu cores, ssd capacity and amount of traffic.</p>\n<p>A feature which I only know from DigitalOcean by now is the ability to create a cluster of multiple machines (Droplets) with private networking, meaning they can communicate with every other node in the cluster but are kind of isolated from the internet. I haven‚Äôt tried this feature too much but it is similar to what you might know from linking multiple Docker containers together.</p>\n<p>What I also like about the service is the ultra simple-to-use, minimalistic and intuitive web-interface that abstracts away this entire technical complexity running in the background when users do a single click on a button until a pre-installed machine comes up.</p>\n<p>DigitalOcean is my personal favorite service of this type, but I also want to mention some alternatives which are <a href=\"https://azure.microsoft.com/en-us/\" target=\"_blank\" rel=\"noopener\">Microsoft Azure</a>, <a href=\"https://cloud.google.com/compute/\" target=\"_blank\" rel=\"noopener\">Google Compute Engine</a>, <a href=\"https://aws.amazon.com/de/ec2\" target=\"_blank\" rel=\"noopener\">Amazon EC2</a>, <a href=\"https://www.linode.com/\" target=\"_blank\" rel=\"noopener\">Linode</a> or in a wider sense also <a href=\"http://jiffybox.de\" target=\"_blank\" rel=\"noopener\">JiffyBox.de</a>.</p>\n<p>If you want to give DigitalOcean a try (and support me), follow <a href=\"https://m.do.co/c/4abee7f659ad\" target=\"_blank\" rel=\"noopener\">this referral link</a> where you get $10 in credits, which is enough for running the smallest container for two months. I will get $25 in credits in case you in turn spend $25 for credits. Of course I would be very pleased if you did so.</p>\n<p><img src=\"assets/img/simple-smile.png\" alt></p>\n"},{"title":"Exploratory Analysis on GitHub Data","date":"2019-04-11T19:00:59.000Z","image":"images/gh_location_langs.png","_content":"\n# Background\nA few days ago, I wrote a crawler (with NodeJS and [Sequelize](http://docs.sequelizejs.com/)) that fetches publicly available data from GitHub's [GraphQL API](https://developer.github.com/v4/). More precisely, I downloaded information about users, repositories, programming languages and topics.\n\nAfter running the crawler for a few days, I ended up with **154,248 user profiles**, **993,919 repositories** and **351 languages**, many of which I had never heard of (e.g. did you know about _PogoScript_?). However, although my MySQL database is already 953 MB in size with only these data, I barely crawled 0.4 % of all user profiles (~ 31 million).\n\nThe first (less extensive) version of my database ‚Äì which I performed the following analyses on ‚Äì looked like this.\n\n![](images/gh_eer.png)\n\nWhile one could argue that the data I collected is not of a representative sample size, I still wanted to do some data analysis on it ‚Äì just for fun.\n\n# Analyses\nTo perform the analyses, I used Python 3 with Pandas and Matplotlib. \n```python\nimport apriori\nimport pymysql\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sqlalchemy import create_engine\n\n%matplotlib inline\n\npymysql.install_as_MySQLdb()\n\nsql_engine = create_engine('mysql://user:heheyouwish@localhost:3306/github_data', echo=False)\nconnection = sql_engine.raw_connection()\n```\n\n## Most popular programming languages\nOne of the first and most obvious thing to check (for the sake of brevity I'll skip basic data set statistics like count, mean, variance, ...) is which languages are most widely used.\n\n```python\ndf_top_langs = pd.read_sql_query('''\n    select LanguageName, count(LanguageName) as count from RepositoryLanguages\n    group by LanguageName\n    order by count(LanguageName) desc\n    limit 10;\n''', con=connection)\ndf_top_langs.set_index('LanguageName').plot.bar(figsize=(12,8))\n```\n\n![](images/gh_popular_lang.png)\n\nNot too surprisingly, the typical web stack consisting of JavaScript, HTML and CSS, is among the most popular programming languages, according to how often they appear in repositories.\n\n## Least popular programming languages\nA little more interesting is to see, which programming languages occur least.\n```python\ndf_last_langs = pd.read_sql_query('''\n    select LanguageName, count(LanguageName) as count from RepositoryLanguages\n    group by LanguageName\n    order by count(LanguageName) asc\n    limit 10;\n''', con=connection)\nprint(df_last_langs)\n```\n\nHere are the results. Have you heard of any one of them? I didn't.\n```\n  LanguageName  count\n0          Nit      1\n1       Myghty      1\n2   Public Key      1\n3  DCPU-16 ASM      1\n4   TI Program      1\n5        Genie      1\n6           Ox      1\n7   PogoScript      1\n8        Cirru      1\n9        JFlex      2\n```\n\n# User Skills\nLet's analyze the users' skills in terms of languages. I decided to consider a user being \"skilled\" in a certain language if at least 10 % of her repositories' code is in that language. \n\n```python\nN = int(1e7)\nMIN_SUPP = .0005\nMIN_CONF = .45\nMIN_LANG_RATIO = .1\n\ndf_skills = pd.read_sql_query(f'''\n    select RepositoryLanguages.LanguageName, RepositoryLanguages.size, Users.login, Users.location from RepositoryLanguages\n    left join Repositories on Repositories.id = RepositoryLanguages.RepositoryId\n    right join Users on Users.login = Repositories.userLogin\n    limit {N}\n''', con=connection)\n\ndf_skills = df_skills.merge(pd.DataFrame(df_skills.groupby('login')['size'].sum()), how='left', on='login').rename(columns={'size_x': 'size', 'size_y': 'totalSize'})\ndf_skills = df_skills[df_skills['totalSize'] > 0]\ndf_skills['sizeRatio'] = df_skills['size'] / df_skills['totalSize']\n\nprint(f\"{df_skills['login'].unique().size} users\")\nprint(f\"{df_skills['LanguageName'].unique().size} languages\")\n\n# Output:\n# 130402 users\n# 351 languages\n```\n\n### Association Rules\nWhat I wanted to look at is combinations of different skills, i.e. languages that usually occur together as developer skills. One approach to get insights like these is to mine the data for _association rules_, e.g. using an algorithm like [Apriori](https://en.wikipedia.org/wiki/Apriori_algorithm) (as I did). The implementation I used was [asaini/Apriori](https://github.com/asaini/Apriori).\n\n```python\nuser_langs = df_skills[df_skills['sizeRatio'] >= MIN_LANG_RATIO].groupby('login')['LanguageName'].apply(set).values\nitems1, rules1 = apriori.runApriori(user_langs, MIN_SUPP, MIN_CONF)\nrules1 = sorted(rules1, key=lambda e: e[1], reverse=True)\nprint(rules1)\n```\n\n**Output:**\n```\n[((('ShaderLab',), ('C#',)), 0.904),\n ((('Vue',), ('JavaScript',)), 0.671277997364954),\n ((('Vue', 'CSS'), ('JavaScript',)), 0.656140350877193),\n ((('GLSL',), ('C#',)), 0.625),\n ((('CMake',), ('C++',)), 0.6229508196721312),\n ((('CSS',), ('JavaScript',)), 0.5807683959192532),\n ((('Tcl',), ('Python',)), 0.5658914728682171),\n ((('Kotlin',), ('Java',)), 0.5655375552282769),\n ((('ASP',), ('C#',)), 0.5488215488215488),\n ((('Vue', 'HTML'), ('JavaScript',)), 0.5404411764705882),\n ((('CoffeeScript',), ('JavaScript',)), 0.5339578454332553),\n ((('CSS', 'PHP'), ('JavaScript',)), 0.5116117850953206),\n ((('Elm',), ('JavaScript',)), 0.4951923076923077),\n ((('CSS', 'HTML'), ('JavaScript',)), 0.4906486271388778),\n ((('Smarty',), ('PHP',)), 0.4788732394366197),\n ((('TypeScript',), ('JavaScript',)), 0.4739540607054964),\n ((('CSS', 'C#'), ('JavaScript',)), 0.464926590538336),\n ((('Groovy',), ('Java',)), 0.4604651162790698)]\n```\n\nThe left part of each row is a tuple of tuples of programming languages that represent an association rule. The right part is the [confidence](https://en.wikipedia.org/wiki/Association_rule_learning#Confidence) of that rule.\n\n**For example:**\nRead `((('ShaderLab',), ('C#',)), 0.904)` as \"90 % of all people who know _ShaderLab_ also know C#\".\n\nThe results reflect common sense. For instance, the rule that developers, who know _VueJS_, also know _JavaScript_ seems to make sense, given that VueJS is a JavaScript framework. Analogously, _CMake_ is a common build tool for _C++_, etc. Nothing too fancy here, except for that I didn't know about _ShaderLab_ and _GLSL_.\n\n## Locations\nLet's take a look at where most GitHub users are from. Obviously, this only respects profiles where users have set their locations.\n\n```python\ndf_locations = df1.reindex(['location'], axis=1).groupby('location').size()\ndf_locations = df_locations.sort_values(ascending=False)[:20]\ndf_locations.plot.bar(figsize=(12,8))\n```\n\n![](images/gh_locations.png)\n\nClearly, San Francisco seems to be the most popular city for developers (or at least for those who are active on GitHub). \n\n## Skills by location\nTo take this a step further, let's take a look at which skills users tend to have in what cities.\n\n```python\ndef language_replace(df):\n    df = df.copy()\n    # Little bit of manual cleaning\n    replace = {'San Francisco': 'San Francisco, CA',\n               'Berlin': 'Berlin, Germany',\n               'New York': 'New York, NY',\n               'London': 'London, UK',\n               'Beijing': 'Beijing, China',\n               'Paris': 'Paris, France'}\n    for (k, v) in replace.items():\n        if isinstance(df, pd.DataFrame):\n            if k in df.columns and v in df.columns:\n                df[k] = df[k] + df[v]\n                df = df.drop([v], axis=1, errors='ignore')\n        else:\n            if k in df.index and v in df.index:\n                df[k] = df[k] + df[v]\n                #df = df.drop([v], axis=1)\n                del df[v]\n    return df\n\nlangs_by_loc = {}\nfor l in df_locations.index:\n    langs_by_loc[l] = df1[df1['location'] == l][['LanguageName']].groupby('LanguageName').size()\ndf_loc_langs = pd.DataFrame.from_dict(langs_by_loc).fillna(0)\n\ndf_loc_langs = language_replace(df_loc_langs)\ndf_loc_langs = df_loc_langs.T\ndf_loc_langs = df_loc_langs.drop([c for c in df_loc_langs.columns if c not in df_top_langs['LanguageName'].values], axis=1)\n\ndf_loc_langs = (df_loc_langs.T / df_loc_langs.T.sum()).T # normalize heights\ndf_loc_langs.plot.bar(stacked=True, figsize=(16,10))\n```\n\n![](images/gh_location_langs.png)\n\nLook like there are no real outliers in the distribution of developer skills between different cities of the world. Maybe you could say that, e.g., Indians like web frontends a little more than command-line hacking.\n\n## Skills: Karlsruhe vs. the World\nWhile an overview is cool, I found it even more interesting to specifically compare between to cities. So in the following chart I compare language-specific programming skills in Karlsruhe (the city where I live and study) to the rest of the world's average.\n\n```python\ndf_ka = df1[df1['location'] == 'Karlsruhe'][['LanguageName']].groupby('LanguageName').size()\ndf_ka = pd.DataFrame(df_ka, index=df_ka.index, columns=['Karlsruhe']) / df_ka.sum()\ndf_world = pd.DataFrame(df_loc_langs.mean(), index=df_loc_langs.mean().index, columns=['World'])\ndf_compare = df_world.merge(df_ka, how='left', left_index=True, right_index=True)\nax = df_compare.plot.barh(title='Languages: World vs. Karlsruhe', legend=True, figsize=(10,5))\nax.set_xlabel('Percentage (Top 10)')\nax.set_ylabel('Programming Language Skills')\n```\n\n![](images/gh_ka_world.png) \n\nThese results are a bit surprising to me. Clearly, Karlsruhe-based developers seem to dislike JavaScript compared to the world. However, this is different from what I experienced in several student jobs and internships here.\n\n## Project Tech Stacks\nLast but not least, let's apply Apriori once more, but this time in a slightly different way. Instead of looking at user skills, let's look at languages that occur together on a per-repository basis. And instead of trying to find rules, let's only look at _frequent item sets_ (which are the basis for rules). My expectation was to get back sets of commonly used tech stacks. \n\n```python\nN = int(1e7)\nMIN_SUPP = .0005\nMIN_CONF = .45\nMIN_LANG_RATIO = .1\n\ndf_stacks = pd.read_sql_query(f'''\n    select LanguageName, size, RepositoryId from RepositoryLanguages\n    order by RepositoryId\n    limit {N}\n''', con=connection)\n\ndf_stacks = df_stacks.merge(pd.DataFrame(df_stacks.groupby('RepositoryId')['size'].sum()), how='left', on='RepositoryId').rename(columns={'size_x': 'size', 'size_y': 'totalSize'})\ndf_stacks = df_stacks[df_stacks['totalSize'] > 0]\ndf_stacks['sizeRatio'] = df_stacks['size'] / df_stacks['totalSize']\n\nprint(f\"{df_stacks['RepositoryId'].unique().size} repositories\")\nprint(f\"{df_stacks['LanguageName'].unique().size} languages\")\n\n# Output: \n# 853114 repositories\n# 351 languages\n```\n\n```python\nrepo_langs = df_stacks[df_stacks['sizeRatio'] >= MIN_LANG_RATIO].groupby('RepositoryId')['LanguageName'].apply(set).values\nitems2, rules2 = apriori.runApriori(repo_langs, MIN_SUPP, MIN_CONF)\nitemsets2 = sorted(list(filter(lambda i: len(i[0]) > 2, items2)), key=lambda i: i[1], reverse=True)\nprint(itemsets2)\n```\n\n**Output:**\n```\n[(('CSS', 'JavaScript', 'HTML'), 0.04360026913167525),\n (('CSS', 'JavaScript', 'PHP'), 0.0045574213997191465),\n (('Ruby', 'CSS', 'HTML'), 0.004456614239128651),\n (('TypeScript', 'JavaScript', 'HTML'), 0.0042034241613664765),\n (('TypeScript', 'HTML', 'CSS'), 0.0035024627423767517),\n (('Python', 'JavaScript', 'HTML'), 0.002962089474560258),\n (('Python', 'HTML', 'CSS'), 0.002769852563666755),\n (('Ruby', 'JavaScript', 'HTML'), 0.0022400288824236856),\n (('JavaScript', 'HTML', 'PHP'), 0.0022154131804190294),\n (('Ruby', 'CSS', 'JavaScript'), 0.0021532878372644217),\n (('CSS', 'HTML', 'PHP'), 0.0019915275098052547),\n (('JavaScript', 'Objective-C', 'Java'), 0.0018614159420663593),\n (('CSS', 'JavaScript', 'Python'), 0.0017992905989117516),\n (('Python', 'JavaScript', 'Objective-C'), 0.0017735027206211597),\n (('Python', 'JavaScript', 'Java'), 0.001508590879999625),\n (('CSS', 'JavaScript', 'TypeScript'), 0.0014745977677074812),\n (('Python', 'Objective-C', 'Java'), 0.0014066115431231934),\n (('Python', 'JavaScript', 'Objective-C', 'Java'), 0.0013222148505358019),\n (('Vue', 'CSS', 'JavaScript'), 0.0012554008022374501)]\n```\n\nHere, the left side is sets of frequently occurring combinations of languages. The right side is the set's [support](https://en.wikipedia.org/wiki/Association_rule_learning#Support), which is the relative occurrences of that set among the whole data set.\nObviously, many of these are actually common \"tech stacks\" and almost all of them are web technologies. I guess GitHub is most popular among web developers. \n\n# Conclusion\nThere is a lot of more complex analyses that could be might on rich data like this and probably tools like [BigQuery](https://cloud.google.com/bigquery/public-data/) are better suitable than Pandas, operating on a tiny sample. However, I used this little project to improve my EDA skills and hopefully give you guys an interesting article to read. Let me know if you like it!","source":"_posts/exploratory-analysis-on-github-data.md","raw":"---\ntitle: Exploratory Analysis on GitHub Data\ndate: 2019-04-11 21:00:59\ntags:\nimage: images/gh_location_langs.png\n---\n\n# Background\nA few days ago, I wrote a crawler (with NodeJS and [Sequelize](http://docs.sequelizejs.com/)) that fetches publicly available data from GitHub's [GraphQL API](https://developer.github.com/v4/). More precisely, I downloaded information about users, repositories, programming languages and topics.\n\nAfter running the crawler for a few days, I ended up with **154,248 user profiles**, **993,919 repositories** and **351 languages**, many of which I had never heard of (e.g. did you know about _PogoScript_?). However, although my MySQL database is already 953 MB in size with only these data, I barely crawled 0.4 % of all user profiles (~ 31 million).\n\nThe first (less extensive) version of my database ‚Äì which I performed the following analyses on ‚Äì looked like this.\n\n![](images/gh_eer.png)\n\nWhile one could argue that the data I collected is not of a representative sample size, I still wanted to do some data analysis on it ‚Äì just for fun.\n\n# Analyses\nTo perform the analyses, I used Python 3 with Pandas and Matplotlib. \n```python\nimport apriori\nimport pymysql\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sqlalchemy import create_engine\n\n%matplotlib inline\n\npymysql.install_as_MySQLdb()\n\nsql_engine = create_engine('mysql://user:heheyouwish@localhost:3306/github_data', echo=False)\nconnection = sql_engine.raw_connection()\n```\n\n## Most popular programming languages\nOne of the first and most obvious thing to check (for the sake of brevity I'll skip basic data set statistics like count, mean, variance, ...) is which languages are most widely used.\n\n```python\ndf_top_langs = pd.read_sql_query('''\n    select LanguageName, count(LanguageName) as count from RepositoryLanguages\n    group by LanguageName\n    order by count(LanguageName) desc\n    limit 10;\n''', con=connection)\ndf_top_langs.set_index('LanguageName').plot.bar(figsize=(12,8))\n```\n\n![](images/gh_popular_lang.png)\n\nNot too surprisingly, the typical web stack consisting of JavaScript, HTML and CSS, is among the most popular programming languages, according to how often they appear in repositories.\n\n## Least popular programming languages\nA little more interesting is to see, which programming languages occur least.\n```python\ndf_last_langs = pd.read_sql_query('''\n    select LanguageName, count(LanguageName) as count from RepositoryLanguages\n    group by LanguageName\n    order by count(LanguageName) asc\n    limit 10;\n''', con=connection)\nprint(df_last_langs)\n```\n\nHere are the results. Have you heard of any one of them? I didn't.\n```\n  LanguageName  count\n0          Nit      1\n1       Myghty      1\n2   Public Key      1\n3  DCPU-16 ASM      1\n4   TI Program      1\n5        Genie      1\n6           Ox      1\n7   PogoScript      1\n8        Cirru      1\n9        JFlex      2\n```\n\n# User Skills\nLet's analyze the users' skills in terms of languages. I decided to consider a user being \"skilled\" in a certain language if at least 10 % of her repositories' code is in that language. \n\n```python\nN = int(1e7)\nMIN_SUPP = .0005\nMIN_CONF = .45\nMIN_LANG_RATIO = .1\n\ndf_skills = pd.read_sql_query(f'''\n    select RepositoryLanguages.LanguageName, RepositoryLanguages.size, Users.login, Users.location from RepositoryLanguages\n    left join Repositories on Repositories.id = RepositoryLanguages.RepositoryId\n    right join Users on Users.login = Repositories.userLogin\n    limit {N}\n''', con=connection)\n\ndf_skills = df_skills.merge(pd.DataFrame(df_skills.groupby('login')['size'].sum()), how='left', on='login').rename(columns={'size_x': 'size', 'size_y': 'totalSize'})\ndf_skills = df_skills[df_skills['totalSize'] > 0]\ndf_skills['sizeRatio'] = df_skills['size'] / df_skills['totalSize']\n\nprint(f\"{df_skills['login'].unique().size} users\")\nprint(f\"{df_skills['LanguageName'].unique().size} languages\")\n\n# Output:\n# 130402 users\n# 351 languages\n```\n\n### Association Rules\nWhat I wanted to look at is combinations of different skills, i.e. languages that usually occur together as developer skills. One approach to get insights like these is to mine the data for _association rules_, e.g. using an algorithm like [Apriori](https://en.wikipedia.org/wiki/Apriori_algorithm) (as I did). The implementation I used was [asaini/Apriori](https://github.com/asaini/Apriori).\n\n```python\nuser_langs = df_skills[df_skills['sizeRatio'] >= MIN_LANG_RATIO].groupby('login')['LanguageName'].apply(set).values\nitems1, rules1 = apriori.runApriori(user_langs, MIN_SUPP, MIN_CONF)\nrules1 = sorted(rules1, key=lambda e: e[1], reverse=True)\nprint(rules1)\n```\n\n**Output:**\n```\n[((('ShaderLab',), ('C#',)), 0.904),\n ((('Vue',), ('JavaScript',)), 0.671277997364954),\n ((('Vue', 'CSS'), ('JavaScript',)), 0.656140350877193),\n ((('GLSL',), ('C#',)), 0.625),\n ((('CMake',), ('C++',)), 0.6229508196721312),\n ((('CSS',), ('JavaScript',)), 0.5807683959192532),\n ((('Tcl',), ('Python',)), 0.5658914728682171),\n ((('Kotlin',), ('Java',)), 0.5655375552282769),\n ((('ASP',), ('C#',)), 0.5488215488215488),\n ((('Vue', 'HTML'), ('JavaScript',)), 0.5404411764705882),\n ((('CoffeeScript',), ('JavaScript',)), 0.5339578454332553),\n ((('CSS', 'PHP'), ('JavaScript',)), 0.5116117850953206),\n ((('Elm',), ('JavaScript',)), 0.4951923076923077),\n ((('CSS', 'HTML'), ('JavaScript',)), 0.4906486271388778),\n ((('Smarty',), ('PHP',)), 0.4788732394366197),\n ((('TypeScript',), ('JavaScript',)), 0.4739540607054964),\n ((('CSS', 'C#'), ('JavaScript',)), 0.464926590538336),\n ((('Groovy',), ('Java',)), 0.4604651162790698)]\n```\n\nThe left part of each row is a tuple of tuples of programming languages that represent an association rule. The right part is the [confidence](https://en.wikipedia.org/wiki/Association_rule_learning#Confidence) of that rule.\n\n**For example:**\nRead `((('ShaderLab',), ('C#',)), 0.904)` as \"90 % of all people who know _ShaderLab_ also know C#\".\n\nThe results reflect common sense. For instance, the rule that developers, who know _VueJS_, also know _JavaScript_ seems to make sense, given that VueJS is a JavaScript framework. Analogously, _CMake_ is a common build tool for _C++_, etc. Nothing too fancy here, except for that I didn't know about _ShaderLab_ and _GLSL_.\n\n## Locations\nLet's take a look at where most GitHub users are from. Obviously, this only respects profiles where users have set their locations.\n\n```python\ndf_locations = df1.reindex(['location'], axis=1).groupby('location').size()\ndf_locations = df_locations.sort_values(ascending=False)[:20]\ndf_locations.plot.bar(figsize=(12,8))\n```\n\n![](images/gh_locations.png)\n\nClearly, San Francisco seems to be the most popular city for developers (or at least for those who are active on GitHub). \n\n## Skills by location\nTo take this a step further, let's take a look at which skills users tend to have in what cities.\n\n```python\ndef language_replace(df):\n    df = df.copy()\n    # Little bit of manual cleaning\n    replace = {'San Francisco': 'San Francisco, CA',\n               'Berlin': 'Berlin, Germany',\n               'New York': 'New York, NY',\n               'London': 'London, UK',\n               'Beijing': 'Beijing, China',\n               'Paris': 'Paris, France'}\n    for (k, v) in replace.items():\n        if isinstance(df, pd.DataFrame):\n            if k in df.columns and v in df.columns:\n                df[k] = df[k] + df[v]\n                df = df.drop([v], axis=1, errors='ignore')\n        else:\n            if k in df.index and v in df.index:\n                df[k] = df[k] + df[v]\n                #df = df.drop([v], axis=1)\n                del df[v]\n    return df\n\nlangs_by_loc = {}\nfor l in df_locations.index:\n    langs_by_loc[l] = df1[df1['location'] == l][['LanguageName']].groupby('LanguageName').size()\ndf_loc_langs = pd.DataFrame.from_dict(langs_by_loc).fillna(0)\n\ndf_loc_langs = language_replace(df_loc_langs)\ndf_loc_langs = df_loc_langs.T\ndf_loc_langs = df_loc_langs.drop([c for c in df_loc_langs.columns if c not in df_top_langs['LanguageName'].values], axis=1)\n\ndf_loc_langs = (df_loc_langs.T / df_loc_langs.T.sum()).T # normalize heights\ndf_loc_langs.plot.bar(stacked=True, figsize=(16,10))\n```\n\n![](images/gh_location_langs.png)\n\nLook like there are no real outliers in the distribution of developer skills between different cities of the world. Maybe you could say that, e.g., Indians like web frontends a little more than command-line hacking.\n\n## Skills: Karlsruhe vs. the World\nWhile an overview is cool, I found it even more interesting to specifically compare between to cities. So in the following chart I compare language-specific programming skills in Karlsruhe (the city where I live and study) to the rest of the world's average.\n\n```python\ndf_ka = df1[df1['location'] == 'Karlsruhe'][['LanguageName']].groupby('LanguageName').size()\ndf_ka = pd.DataFrame(df_ka, index=df_ka.index, columns=['Karlsruhe']) / df_ka.sum()\ndf_world = pd.DataFrame(df_loc_langs.mean(), index=df_loc_langs.mean().index, columns=['World'])\ndf_compare = df_world.merge(df_ka, how='left', left_index=True, right_index=True)\nax = df_compare.plot.barh(title='Languages: World vs. Karlsruhe', legend=True, figsize=(10,5))\nax.set_xlabel('Percentage (Top 10)')\nax.set_ylabel('Programming Language Skills')\n```\n\n![](images/gh_ka_world.png) \n\nThese results are a bit surprising to me. Clearly, Karlsruhe-based developers seem to dislike JavaScript compared to the world. However, this is different from what I experienced in several student jobs and internships here.\n\n## Project Tech Stacks\nLast but not least, let's apply Apriori once more, but this time in a slightly different way. Instead of looking at user skills, let's look at languages that occur together on a per-repository basis. And instead of trying to find rules, let's only look at _frequent item sets_ (which are the basis for rules). My expectation was to get back sets of commonly used tech stacks. \n\n```python\nN = int(1e7)\nMIN_SUPP = .0005\nMIN_CONF = .45\nMIN_LANG_RATIO = .1\n\ndf_stacks = pd.read_sql_query(f'''\n    select LanguageName, size, RepositoryId from RepositoryLanguages\n    order by RepositoryId\n    limit {N}\n''', con=connection)\n\ndf_stacks = df_stacks.merge(pd.DataFrame(df_stacks.groupby('RepositoryId')['size'].sum()), how='left', on='RepositoryId').rename(columns={'size_x': 'size', 'size_y': 'totalSize'})\ndf_stacks = df_stacks[df_stacks['totalSize'] > 0]\ndf_stacks['sizeRatio'] = df_stacks['size'] / df_stacks['totalSize']\n\nprint(f\"{df_stacks['RepositoryId'].unique().size} repositories\")\nprint(f\"{df_stacks['LanguageName'].unique().size} languages\")\n\n# Output: \n# 853114 repositories\n# 351 languages\n```\n\n```python\nrepo_langs = df_stacks[df_stacks['sizeRatio'] >= MIN_LANG_RATIO].groupby('RepositoryId')['LanguageName'].apply(set).values\nitems2, rules2 = apriori.runApriori(repo_langs, MIN_SUPP, MIN_CONF)\nitemsets2 = sorted(list(filter(lambda i: len(i[0]) > 2, items2)), key=lambda i: i[1], reverse=True)\nprint(itemsets2)\n```\n\n**Output:**\n```\n[(('CSS', 'JavaScript', 'HTML'), 0.04360026913167525),\n (('CSS', 'JavaScript', 'PHP'), 0.0045574213997191465),\n (('Ruby', 'CSS', 'HTML'), 0.004456614239128651),\n (('TypeScript', 'JavaScript', 'HTML'), 0.0042034241613664765),\n (('TypeScript', 'HTML', 'CSS'), 0.0035024627423767517),\n (('Python', 'JavaScript', 'HTML'), 0.002962089474560258),\n (('Python', 'HTML', 'CSS'), 0.002769852563666755),\n (('Ruby', 'JavaScript', 'HTML'), 0.0022400288824236856),\n (('JavaScript', 'HTML', 'PHP'), 0.0022154131804190294),\n (('Ruby', 'CSS', 'JavaScript'), 0.0021532878372644217),\n (('CSS', 'HTML', 'PHP'), 0.0019915275098052547),\n (('JavaScript', 'Objective-C', 'Java'), 0.0018614159420663593),\n (('CSS', 'JavaScript', 'Python'), 0.0017992905989117516),\n (('Python', 'JavaScript', 'Objective-C'), 0.0017735027206211597),\n (('Python', 'JavaScript', 'Java'), 0.001508590879999625),\n (('CSS', 'JavaScript', 'TypeScript'), 0.0014745977677074812),\n (('Python', 'Objective-C', 'Java'), 0.0014066115431231934),\n (('Python', 'JavaScript', 'Objective-C', 'Java'), 0.0013222148505358019),\n (('Vue', 'CSS', 'JavaScript'), 0.0012554008022374501)]\n```\n\nHere, the left side is sets of frequently occurring combinations of languages. The right side is the set's [support](https://en.wikipedia.org/wiki/Association_rule_learning#Support), which is the relative occurrences of that set among the whole data set.\nObviously, many of these are actually common \"tech stacks\" and almost all of them are web technologies. I guess GitHub is most popular among web developers. \n\n# Conclusion\nThere is a lot of more complex analyses that could be might on rich data like this and probably tools like [BigQuery](https://cloud.google.com/bigquery/public-data/) are better suitable than Pandas, operating on a tiny sample. However, I used this little project to improve my EDA skills and hopefully give you guys an interesting article to read. Let me know if you like it!","slug":"exploratory-analysis-on-github-data","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhhz000c40mqcovcwo0n","content":"<h1>Background</h1>\n<p>A few days ago, I wrote a crawler (with NodeJS and <a href=\"http://docs.sequelizejs.com/\" target=\"_blank\" rel=\"noopener\">Sequelize</a>) that fetches publicly available data from GitHub‚Äôs <a href=\"https://developer.github.com/v4/\" target=\"_blank\" rel=\"noopener\">GraphQL API</a>. More precisely, I downloaded information about users, repositories, programming languages and topics.</p>\n<p>After running the crawler for a few days, I ended up with <strong>154,248 user profiles</strong>, <strong>993,919 repositories</strong> and <strong>351 languages</strong>, many of which I had never heard of (e.g. did you know about <em>PogoScript</em>?). However, although my MySQL database is already 953 MB in size with only these data, I barely crawled 0.4 % of all user profiles (~ 31 million).</p>\n<p>The first (less extensive) version of my database ‚Äì which I performed the following analyses on ‚Äì looked like this.</p>\n<p><img src=\"images/gh_eer.png\" alt></p>\n<p>While one could argue that the data I collected is not of a representative sample size, I still wanted to do some data analysis on it ‚Äì just for fun.</p>\n<h1>Analyses</h1>\n<p>To perform the analyses, I used Python 3 with Pandas and Matplotlib.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> apriori</span><br><span class=\"line\"><span class=\"keyword\">import</span> pymysql</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sqlalchemy <span class=\"keyword\">import</span> create_engine</span><br><span class=\"line\"></span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"></span><br><span class=\"line\">pymysql.install_as_MySQLdb()</span><br><span class=\"line\"></span><br><span class=\"line\">sql_engine = create_engine(<span class=\"string\">'mysql://user:heheyouwish@localhost:3306/github_data'</span>, echo=<span class=\"literal\">False</span>)</span><br><span class=\"line\">connection = sql_engine.raw_connection()</span><br></pre></td></tr></table></figure>\n<h2 id=\"Most-popular-programming-languages\">Most popular programming languages</h2>\n<p>One of the first and most obvious thing to check (for the sake of brevity I‚Äôll skip basic data set statistics like count, mean, variance, ‚Ä¶) is which languages are most widely used.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_top_langs = pd.read_sql_query(<span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    select LanguageName, count(LanguageName) as count from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    group by LanguageName</span></span><br><span class=\"line\"><span class=\"string\">    order by count(LanguageName) desc</span></span><br><span class=\"line\"><span class=\"string\">    limit 10;</span></span><br><span class=\"line\"><span class=\"string\">'''</span>, con=connection)</span><br><span class=\"line\">df_top_langs.set_index(<span class=\"string\">'LanguageName'</span>).plot.bar(figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"images/gh_popular_lang.png\" alt></p>\n<p>Not too surprisingly, the typical web stack consisting of JavaScript, HTML and CSS, is among the most popular programming languages, according to how often they appear in repositories.</p>\n<h2 id=\"Least-popular-programming-languages\">Least popular programming languages</h2>\n<p>A little more interesting is to see, which programming languages occur least.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_last_langs = pd.read_sql_query(<span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    select LanguageName, count(LanguageName) as count from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    group by LanguageName</span></span><br><span class=\"line\"><span class=\"string\">    order by count(LanguageName) asc</span></span><br><span class=\"line\"><span class=\"string\">    limit 10;</span></span><br><span class=\"line\"><span class=\"string\">'''</span>, con=connection)</span><br><span class=\"line\">print(df_last_langs)</span><br></pre></td></tr></table></figure>\n<p>Here are the results. Have you heard of any one of them? I didn‚Äôt.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  LanguageName  count</span><br><span class=\"line\">0          Nit      1</span><br><span class=\"line\">1       Myghty      1</span><br><span class=\"line\">2   Public Key      1</span><br><span class=\"line\">3  DCPU-16 ASM      1</span><br><span class=\"line\">4   TI Program      1</span><br><span class=\"line\">5        Genie      1</span><br><span class=\"line\">6           Ox      1</span><br><span class=\"line\">7   PogoScript      1</span><br><span class=\"line\">8        Cirru      1</span><br><span class=\"line\">9        JFlex      2</span><br></pre></td></tr></table></figure>\n<h1>User Skills</h1>\n<p>Let‚Äôs analyze the users‚Äô skills in terms of languages. I decided to consider a user being ‚Äúskilled‚Äù in a certain language if at least 10 % of her repositories‚Äô code is in that language.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">N = int(<span class=\"number\">1e7</span>)</span><br><span class=\"line\">MIN_SUPP = <span class=\"number\">.0005</span></span><br><span class=\"line\">MIN_CONF = <span class=\"number\">.45</span></span><br><span class=\"line\">MIN_LANG_RATIO = <span class=\"number\">.1</span></span><br><span class=\"line\"></span><br><span class=\"line\">df_skills = pd.read_sql_query(<span class=\"string\">f'''</span></span><br><span class=\"line\"><span class=\"string\">    select RepositoryLanguages.LanguageName, RepositoryLanguages.size, Users.login, Users.location from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    left join Repositories on Repositories.id = RepositoryLanguages.RepositoryId</span></span><br><span class=\"line\"><span class=\"string\">    right join Users on Users.login = Repositories.userLogin</span></span><br><span class=\"line\"><span class=\"string\">    limit <span class=\"subst\">&#123;N&#125;</span></span></span><br><span class=\"line\"><span class=\"string\">'''</span>, con=connection)</span><br><span class=\"line\"></span><br><span class=\"line\">df_skills = df_skills.merge(pd.DataFrame(df_skills.groupby(<span class=\"string\">'login'</span>)[<span class=\"string\">'size'</span>].sum()), how=<span class=\"string\">'left'</span>, on=<span class=\"string\">'login'</span>).rename(columns=&#123;<span class=\"string\">'size_x'</span>: <span class=\"string\">'size'</span>, <span class=\"string\">'size_y'</span>: <span class=\"string\">'totalSize'</span>&#125;)</span><br><span class=\"line\">df_skills = df_skills[df_skills[<span class=\"string\">'totalSize'</span>] &gt; <span class=\"number\">0</span>]</span><br><span class=\"line\">df_skills[<span class=\"string\">'sizeRatio'</span>] = df_skills[<span class=\"string\">'size'</span>] / df_skills[<span class=\"string\">'totalSize'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">f\"<span class=\"subst\">&#123;df_skills[<span class=\"string\">'login'</span>].unique().size&#125;</span> users\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">f\"<span class=\"subst\">&#123;df_skills[<span class=\"string\">'LanguageName'</span>].unique().size&#125;</span> languages\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Output:</span></span><br><span class=\"line\"><span class=\"comment\"># 130402 users</span></span><br><span class=\"line\"><span class=\"comment\"># 351 languages</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"Association-Rules\">Association Rules</h3>\n<p>What I wanted to look at is combinations of different skills, i.e. languages that usually occur together as developer skills. One approach to get insights like these is to mine the data for <em>association rules</em>, e.g. using an algorithm like <a href=\"https://en.wikipedia.org/wiki/Apriori_algorithm\" target=\"_blank\" rel=\"noopener\">Apriori</a> (as I did). The implementation I used was <a href=\"https://github.com/asaini/Apriori\" target=\"_blank\" rel=\"noopener\">asaini/Apriori</a>.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">user_langs = df_skills[df_skills[<span class=\"string\">'sizeRatio'</span>] &gt;= MIN_LANG_RATIO].groupby(<span class=\"string\">'login'</span>)[<span class=\"string\">'LanguageName'</span>].apply(set).values</span><br><span class=\"line\">items1, rules1 = apriori.runApriori(user_langs, MIN_SUPP, MIN_CONF)</span><br><span class=\"line\">rules1 = sorted(rules1, key=<span class=\"keyword\">lambda</span> e: e[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">print(rules1)</span><br></pre></td></tr></table></figure>\n<p><strong>Output:</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[(((&apos;ShaderLab&apos;,), (&apos;C#&apos;,)), 0.904),</span><br><span class=\"line\"> (((&apos;Vue&apos;,), (&apos;JavaScript&apos;,)), 0.671277997364954),</span><br><span class=\"line\"> (((&apos;Vue&apos;, &apos;CSS&apos;), (&apos;JavaScript&apos;,)), 0.656140350877193),</span><br><span class=\"line\"> (((&apos;GLSL&apos;,), (&apos;C#&apos;,)), 0.625),</span><br><span class=\"line\"> (((&apos;CMake&apos;,), (&apos;C++&apos;,)), 0.6229508196721312),</span><br><span class=\"line\"> (((&apos;CSS&apos;,), (&apos;JavaScript&apos;,)), 0.5807683959192532),</span><br><span class=\"line\"> (((&apos;Tcl&apos;,), (&apos;Python&apos;,)), 0.5658914728682171),</span><br><span class=\"line\"> (((&apos;Kotlin&apos;,), (&apos;Java&apos;,)), 0.5655375552282769),</span><br><span class=\"line\"> (((&apos;ASP&apos;,), (&apos;C#&apos;,)), 0.5488215488215488),</span><br><span class=\"line\"> (((&apos;Vue&apos;, &apos;HTML&apos;), (&apos;JavaScript&apos;,)), 0.5404411764705882),</span><br><span class=\"line\"> (((&apos;CoffeeScript&apos;,), (&apos;JavaScript&apos;,)), 0.5339578454332553),</span><br><span class=\"line\"> (((&apos;CSS&apos;, &apos;PHP&apos;), (&apos;JavaScript&apos;,)), 0.5116117850953206),</span><br><span class=\"line\"> (((&apos;Elm&apos;,), (&apos;JavaScript&apos;,)), 0.4951923076923077),</span><br><span class=\"line\"> (((&apos;CSS&apos;, &apos;HTML&apos;), (&apos;JavaScript&apos;,)), 0.4906486271388778),</span><br><span class=\"line\"> (((&apos;Smarty&apos;,), (&apos;PHP&apos;,)), 0.4788732394366197),</span><br><span class=\"line\"> (((&apos;TypeScript&apos;,), (&apos;JavaScript&apos;,)), 0.4739540607054964),</span><br><span class=\"line\"> (((&apos;CSS&apos;, &apos;C#&apos;), (&apos;JavaScript&apos;,)), 0.464926590538336),</span><br><span class=\"line\"> (((&apos;Groovy&apos;,), (&apos;Java&apos;,)), 0.4604651162790698)]</span><br></pre></td></tr></table></figure>\n<p>The left part of each row is a tuple of tuples of programming languages that represent an association rule. The right part is the <a href=\"https://en.wikipedia.org/wiki/Association_rule_learning#Confidence\" target=\"_blank\" rel=\"noopener\">confidence</a> of that rule.</p>\n<p><strong>For example:</strong><br>\nRead <code>((('ShaderLab',), ('C#',)), 0.904)</code> as ‚Äú90 % of all people who know <em>ShaderLab</em> also know C#‚Äù.</p>\n<p>The results reflect common sense. For instance, the rule that developers, who know <em>VueJS</em>, also know <em>JavaScript</em> seems to make sense, given that VueJS is a JavaScript framework. Analogously, <em>CMake</em> is a common build tool for <em>C++</em>, etc. Nothing too fancy here, except for that I didn‚Äôt know about <em>ShaderLab</em> and <em>GLSL</em>.</p>\n<h2 id=\"Locations\">Locations</h2>\n<p>Let‚Äôs take a look at where most GitHub users are from. Obviously, this only respects profiles where users have set their locations.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_locations = df1.reindex([<span class=\"string\">'location'</span>], axis=<span class=\"number\">1</span>).groupby(<span class=\"string\">'location'</span>).size()</span><br><span class=\"line\">df_locations = df_locations.sort_values(ascending=<span class=\"literal\">False</span>)[:<span class=\"number\">20</span>]</span><br><span class=\"line\">df_locations.plot.bar(figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"images/gh_locations.png\" alt></p>\n<p>Clearly, San Francisco seems to be the most popular city for developers (or at least for those who are active on GitHub).</p>\n<h2 id=\"Skills-by-location\">Skills by location</h2>\n<p>To take this a step further, let‚Äôs take a look at which skills users tend to have in what cities.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">language_replace</span><span class=\"params\">(df)</span>:</span></span><br><span class=\"line\">    df = df.copy()</span><br><span class=\"line\">    <span class=\"comment\"># Little bit of manual cleaning</span></span><br><span class=\"line\">    replace = &#123;<span class=\"string\">'San Francisco'</span>: <span class=\"string\">'San Francisco, CA'</span>,</span><br><span class=\"line\">               <span class=\"string\">'Berlin'</span>: <span class=\"string\">'Berlin, Germany'</span>,</span><br><span class=\"line\">               <span class=\"string\">'New York'</span>: <span class=\"string\">'New York, NY'</span>,</span><br><span class=\"line\">               <span class=\"string\">'London'</span>: <span class=\"string\">'London, UK'</span>,</span><br><span class=\"line\">               <span class=\"string\">'Beijing'</span>: <span class=\"string\">'Beijing, China'</span>,</span><br><span class=\"line\">               <span class=\"string\">'Paris'</span>: <span class=\"string\">'Paris, France'</span>&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (k, v) <span class=\"keyword\">in</span> replace.items():</span><br><span class=\"line\">        <span class=\"keyword\">if</span> isinstance(df, pd.DataFrame):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> k <span class=\"keyword\">in</span> df.columns <span class=\"keyword\">and</span> v <span class=\"keyword\">in</span> df.columns:</span><br><span class=\"line\">                df[k] = df[k] + df[v]</span><br><span class=\"line\">                df = df.drop([v], axis=<span class=\"number\">1</span>, errors=<span class=\"string\">'ignore'</span>)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> k <span class=\"keyword\">in</span> df.index <span class=\"keyword\">and</span> v <span class=\"keyword\">in</span> df.index:</span><br><span class=\"line\">                df[k] = df[k] + df[v]</span><br><span class=\"line\">                <span class=\"comment\">#df = df.drop([v], axis=1)</span></span><br><span class=\"line\">                <span class=\"keyword\">del</span> df[v]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> df</span><br><span class=\"line\"></span><br><span class=\"line\">langs_by_loc = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> df_locations.index:</span><br><span class=\"line\">    langs_by_loc[l] = df1[df1[<span class=\"string\">'location'</span>] == l][[<span class=\"string\">'LanguageName'</span>]].groupby(<span class=\"string\">'LanguageName'</span>).size()</span><br><span class=\"line\">df_loc_langs = pd.DataFrame.from_dict(langs_by_loc).fillna(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">df_loc_langs = language_replace(df_loc_langs)</span><br><span class=\"line\">df_loc_langs = df_loc_langs.T</span><br><span class=\"line\">df_loc_langs = df_loc_langs.drop([c <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> df_loc_langs.columns <span class=\"keyword\">if</span> c <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> df_top_langs[<span class=\"string\">'LanguageName'</span>].values], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">df_loc_langs = (df_loc_langs.T / df_loc_langs.T.sum()).T <span class=\"comment\"># normalize heights</span></span><br><span class=\"line\">df_loc_langs.plot.bar(stacked=<span class=\"literal\">True</span>, figsize=(<span class=\"number\">16</span>,<span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"images/gh_location_langs.png\" alt></p>\n<p>Look like there are no real outliers in the distribution of developer skills between different cities of the world. Maybe you could say that, e.g., Indians like web frontends a little more than command-line hacking.</p>\n<h2 id=\"Skills-Karlsruhe-vs-the-World\">Skills: Karlsruhe vs. the World</h2>\n<p>While an overview is cool, I found it even more interesting to specifically compare between to cities. So in the following chart I compare language-specific programming skills in Karlsruhe (the city where I live and study) to the rest of the world‚Äôs average.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_ka = df1[df1[<span class=\"string\">'location'</span>] == <span class=\"string\">'Karlsruhe'</span>][[<span class=\"string\">'LanguageName'</span>]].groupby(<span class=\"string\">'LanguageName'</span>).size()</span><br><span class=\"line\">df_ka = pd.DataFrame(df_ka, index=df_ka.index, columns=[<span class=\"string\">'Karlsruhe'</span>]) / df_ka.sum()</span><br><span class=\"line\">df_world = pd.DataFrame(df_loc_langs.mean(), index=df_loc_langs.mean().index, columns=[<span class=\"string\">'World'</span>])</span><br><span class=\"line\">df_compare = df_world.merge(df_ka, how=<span class=\"string\">'left'</span>, left_index=<span class=\"literal\">True</span>, right_index=<span class=\"literal\">True</span>)</span><br><span class=\"line\">ax = df_compare.plot.barh(title=<span class=\"string\">'Languages: World vs. Karlsruhe'</span>, legend=<span class=\"literal\">True</span>, figsize=(<span class=\"number\">10</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">ax.set_xlabel(<span class=\"string\">'Percentage (Top 10)'</span>)</span><br><span class=\"line\">ax.set_ylabel(<span class=\"string\">'Programming Language Skills'</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"images/gh_ka_world.png\" alt></p>\n<p>These results are a bit surprising to me. Clearly, Karlsruhe-based developers seem to dislike JavaScript compared to the world. However, this is different from what I experienced in several student jobs and internships here.</p>\n<h2 id=\"Project-Tech-Stacks\">Project Tech Stacks</h2>\n<p>Last but not least, let‚Äôs apply Apriori once more, but this time in a slightly different way. Instead of looking at user skills, let‚Äôs look at languages that occur together on a per-repository basis. And instead of trying to find rules, let‚Äôs only look at <em>frequent item sets</em> (which are the basis for rules). My expectation was to get back sets of commonly used tech stacks.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">N = int(<span class=\"number\">1e7</span>)</span><br><span class=\"line\">MIN_SUPP = <span class=\"number\">.0005</span></span><br><span class=\"line\">MIN_CONF = <span class=\"number\">.45</span></span><br><span class=\"line\">MIN_LANG_RATIO = <span class=\"number\">.1</span></span><br><span class=\"line\"></span><br><span class=\"line\">df_stacks = pd.read_sql_query(<span class=\"string\">f'''</span></span><br><span class=\"line\"><span class=\"string\">    select LanguageName, size, RepositoryId from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    order by RepositoryId</span></span><br><span class=\"line\"><span class=\"string\">    limit <span class=\"subst\">&#123;N&#125;</span></span></span><br><span class=\"line\"><span class=\"string\">'''</span>, con=connection)</span><br><span class=\"line\"></span><br><span class=\"line\">df_stacks = df_stacks.merge(pd.DataFrame(df_stacks.groupby(<span class=\"string\">'RepositoryId'</span>)[<span class=\"string\">'size'</span>].sum()), how=<span class=\"string\">'left'</span>, on=<span class=\"string\">'RepositoryId'</span>).rename(columns=&#123;<span class=\"string\">'size_x'</span>: <span class=\"string\">'size'</span>, <span class=\"string\">'size_y'</span>: <span class=\"string\">'totalSize'</span>&#125;)</span><br><span class=\"line\">df_stacks = df_stacks[df_stacks[<span class=\"string\">'totalSize'</span>] &gt; <span class=\"number\">0</span>]</span><br><span class=\"line\">df_stacks[<span class=\"string\">'sizeRatio'</span>] = df_stacks[<span class=\"string\">'size'</span>] / df_stacks[<span class=\"string\">'totalSize'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">f\"<span class=\"subst\">&#123;df_stacks[<span class=\"string\">'RepositoryId'</span>].unique().size&#125;</span> repositories\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">f\"<span class=\"subst\">&#123;df_stacks[<span class=\"string\">'LanguageName'</span>].unique().size&#125;</span> languages\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Output: </span></span><br><span class=\"line\"><span class=\"comment\"># 853114 repositories</span></span><br><span class=\"line\"><span class=\"comment\"># 351 languages</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">repo_langs = df_stacks[df_stacks[<span class=\"string\">'sizeRatio'</span>] &gt;= MIN_LANG_RATIO].groupby(<span class=\"string\">'RepositoryId'</span>)[<span class=\"string\">'LanguageName'</span>].apply(set).values</span><br><span class=\"line\">items2, rules2 = apriori.runApriori(repo_langs, MIN_SUPP, MIN_CONF)</span><br><span class=\"line\">itemsets2 = sorted(list(filter(<span class=\"keyword\">lambda</span> i: len(i[<span class=\"number\">0</span>]) &gt; <span class=\"number\">2</span>, items2)), key=<span class=\"keyword\">lambda</span> i: i[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">print(itemsets2)</span><br></pre></td></tr></table></figure>\n<p><strong>Output:</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[((&apos;CSS&apos;, &apos;JavaScript&apos;, &apos;HTML&apos;), 0.04360026913167525),</span><br><span class=\"line\"> ((&apos;CSS&apos;, &apos;JavaScript&apos;, &apos;PHP&apos;), 0.0045574213997191465),</span><br><span class=\"line\"> ((&apos;Ruby&apos;, &apos;CSS&apos;, &apos;HTML&apos;), 0.004456614239128651),</span><br><span class=\"line\"> ((&apos;TypeScript&apos;, &apos;JavaScript&apos;, &apos;HTML&apos;), 0.0042034241613664765),</span><br><span class=\"line\"> ((&apos;TypeScript&apos;, &apos;HTML&apos;, &apos;CSS&apos;), 0.0035024627423767517),</span><br><span class=\"line\"> ((&apos;Python&apos;, &apos;JavaScript&apos;, &apos;HTML&apos;), 0.002962089474560258),</span><br><span class=\"line\"> ((&apos;Python&apos;, &apos;HTML&apos;, &apos;CSS&apos;), 0.002769852563666755),</span><br><span class=\"line\"> ((&apos;Ruby&apos;, &apos;JavaScript&apos;, &apos;HTML&apos;), 0.0022400288824236856),</span><br><span class=\"line\"> ((&apos;JavaScript&apos;, &apos;HTML&apos;, &apos;PHP&apos;), 0.0022154131804190294),</span><br><span class=\"line\"> ((&apos;Ruby&apos;, &apos;CSS&apos;, &apos;JavaScript&apos;), 0.0021532878372644217),</span><br><span class=\"line\"> ((&apos;CSS&apos;, &apos;HTML&apos;, &apos;PHP&apos;), 0.0019915275098052547),</span><br><span class=\"line\"> ((&apos;JavaScript&apos;, &apos;Objective-C&apos;, &apos;Java&apos;), 0.0018614159420663593),</span><br><span class=\"line\"> ((&apos;CSS&apos;, &apos;JavaScript&apos;, &apos;Python&apos;), 0.0017992905989117516),</span><br><span class=\"line\"> ((&apos;Python&apos;, &apos;JavaScript&apos;, &apos;Objective-C&apos;), 0.0017735027206211597),</span><br><span class=\"line\"> ((&apos;Python&apos;, &apos;JavaScript&apos;, &apos;Java&apos;), 0.001508590879999625),</span><br><span class=\"line\"> ((&apos;CSS&apos;, &apos;JavaScript&apos;, &apos;TypeScript&apos;), 0.0014745977677074812),</span><br><span class=\"line\"> ((&apos;Python&apos;, &apos;Objective-C&apos;, &apos;Java&apos;), 0.0014066115431231934),</span><br><span class=\"line\"> ((&apos;Python&apos;, &apos;JavaScript&apos;, &apos;Objective-C&apos;, &apos;Java&apos;), 0.0013222148505358019),</span><br><span class=\"line\"> ((&apos;Vue&apos;, &apos;CSS&apos;, &apos;JavaScript&apos;), 0.0012554008022374501)]</span><br></pre></td></tr></table></figure>\n<p>Here, the left side is sets of frequently occurring combinations of languages. The right side is the set‚Äôs <a href=\"https://en.wikipedia.org/wiki/Association_rule_learning#Support\" target=\"_blank\" rel=\"noopener\">support</a>, which is the relative occurrences of that set among the whole data set.<br>\nObviously, many of these are actually common ‚Äútech stacks‚Äù and almost all of them are web technologies. I guess GitHub is most popular among web developers.</p>\n<h1>Conclusion</h1>\n<p>There is a lot of more complex analyses that could be might on rich data like this and probably tools like <a href=\"https://cloud.google.com/bigquery/public-data/\" target=\"_blank\" rel=\"noopener\">BigQuery</a> are better suitable than Pandas, operating on a tiny sample. However, I used this little project to improve my EDA skills and hopefully give you guys an interesting article to read. Let me know if you like it!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1>Background</h1>\n<p>A few days ago, I wrote a crawler (with NodeJS and <a href=\"http://docs.sequelizejs.com/\" target=\"_blank\" rel=\"noopener\">Sequelize</a>) that fetches publicly available data from GitHub‚Äôs <a href=\"https://developer.github.com/v4/\" target=\"_blank\" rel=\"noopener\">GraphQL API</a>. More precisely, I downloaded information about users, repositories, programming languages and topics.</p>\n<p>After running the crawler for a few days, I ended up with <strong>154,248 user profiles</strong>, <strong>993,919 repositories</strong> and <strong>351 languages</strong>, many of which I had never heard of (e.g. did you know about <em>PogoScript</em>?). However, although my MySQL database is already 953 MB in size with only these data, I barely crawled 0.4 % of all user profiles (~ 31 million).</p>\n<p>The first (less extensive) version of my database ‚Äì which I performed the following analyses on ‚Äì looked like this.</p>\n<p><img src=\"images/gh_eer.png\" alt></p>\n<p>While one could argue that the data I collected is not of a representative sample size, I still wanted to do some data analysis on it ‚Äì just for fun.</p>\n<h1>Analyses</h1>\n<p>To perform the analyses, I used Python 3 with Pandas and Matplotlib.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> apriori</span><br><span class=\"line\"><span class=\"keyword\">import</span> pymysql</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sqlalchemy <span class=\"keyword\">import</span> create_engine</span><br><span class=\"line\"></span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"></span><br><span class=\"line\">pymysql.install_as_MySQLdb()</span><br><span class=\"line\"></span><br><span class=\"line\">sql_engine = create_engine(<span class=\"string\">'mysql://user:heheyouwish@localhost:3306/github_data'</span>, echo=<span class=\"literal\">False</span>)</span><br><span class=\"line\">connection = sql_engine.raw_connection()</span><br></pre></td></tr></table></figure>\n<h2 id=\"Most-popular-programming-languages\">Most popular programming languages</h2>\n<p>One of the first and most obvious thing to check (for the sake of brevity I‚Äôll skip basic data set statistics like count, mean, variance, ‚Ä¶) is which languages are most widely used.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_top_langs = pd.read_sql_query(<span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    select LanguageName, count(LanguageName) as count from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    group by LanguageName</span></span><br><span class=\"line\"><span class=\"string\">    order by count(LanguageName) desc</span></span><br><span class=\"line\"><span class=\"string\">    limit 10;</span></span><br><span class=\"line\"><span class=\"string\">'''</span>, con=connection)</span><br><span class=\"line\">df_top_langs.set_index(<span class=\"string\">'LanguageName'</span>).plot.bar(figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"images/gh_popular_lang.png\" alt></p>\n<p>Not too surprisingly, the typical web stack consisting of JavaScript, HTML and CSS, is among the most popular programming languages, according to how often they appear in repositories.</p>\n<h2 id=\"Least-popular-programming-languages\">Least popular programming languages</h2>\n<p>A little more interesting is to see, which programming languages occur least.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_last_langs = pd.read_sql_query(<span class=\"string\">'''</span></span><br><span class=\"line\"><span class=\"string\">    select LanguageName, count(LanguageName) as count from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    group by LanguageName</span></span><br><span class=\"line\"><span class=\"string\">    order by count(LanguageName) asc</span></span><br><span class=\"line\"><span class=\"string\">    limit 10;</span></span><br><span class=\"line\"><span class=\"string\">'''</span>, con=connection)</span><br><span class=\"line\">print(df_last_langs)</span><br></pre></td></tr></table></figure>\n<p>Here are the results. Have you heard of any one of them? I didn‚Äôt.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  LanguageName  count</span><br><span class=\"line\">0          Nit      1</span><br><span class=\"line\">1       Myghty      1</span><br><span class=\"line\">2   Public Key      1</span><br><span class=\"line\">3  DCPU-16 ASM      1</span><br><span class=\"line\">4   TI Program      1</span><br><span class=\"line\">5        Genie      1</span><br><span class=\"line\">6           Ox      1</span><br><span class=\"line\">7   PogoScript      1</span><br><span class=\"line\">8        Cirru      1</span><br><span class=\"line\">9        JFlex      2</span><br></pre></td></tr></table></figure>\n<h1>User Skills</h1>\n<p>Let‚Äôs analyze the users‚Äô skills in terms of languages. I decided to consider a user being ‚Äúskilled‚Äù in a certain language if at least 10 % of her repositories‚Äô code is in that language.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">N = int(<span class=\"number\">1e7</span>)</span><br><span class=\"line\">MIN_SUPP = <span class=\"number\">.0005</span></span><br><span class=\"line\">MIN_CONF = <span class=\"number\">.45</span></span><br><span class=\"line\">MIN_LANG_RATIO = <span class=\"number\">.1</span></span><br><span class=\"line\"></span><br><span class=\"line\">df_skills = pd.read_sql_query(<span class=\"string\">f'''</span></span><br><span class=\"line\"><span class=\"string\">    select RepositoryLanguages.LanguageName, RepositoryLanguages.size, Users.login, Users.location from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    left join Repositories on Repositories.id = RepositoryLanguages.RepositoryId</span></span><br><span class=\"line\"><span class=\"string\">    right join Users on Users.login = Repositories.userLogin</span></span><br><span class=\"line\"><span class=\"string\">    limit <span class=\"subst\">&#123;N&#125;</span></span></span><br><span class=\"line\"><span class=\"string\">'''</span>, con=connection)</span><br><span class=\"line\"></span><br><span class=\"line\">df_skills = df_skills.merge(pd.DataFrame(df_skills.groupby(<span class=\"string\">'login'</span>)[<span class=\"string\">'size'</span>].sum()), how=<span class=\"string\">'left'</span>, on=<span class=\"string\">'login'</span>).rename(columns=&#123;<span class=\"string\">'size_x'</span>: <span class=\"string\">'size'</span>, <span class=\"string\">'size_y'</span>: <span class=\"string\">'totalSize'</span>&#125;)</span><br><span class=\"line\">df_skills = df_skills[df_skills[<span class=\"string\">'totalSize'</span>] &gt; <span class=\"number\">0</span>]</span><br><span class=\"line\">df_skills[<span class=\"string\">'sizeRatio'</span>] = df_skills[<span class=\"string\">'size'</span>] / df_skills[<span class=\"string\">'totalSize'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">f\"<span class=\"subst\">&#123;df_skills[<span class=\"string\">'login'</span>].unique().size&#125;</span> users\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">f\"<span class=\"subst\">&#123;df_skills[<span class=\"string\">'LanguageName'</span>].unique().size&#125;</span> languages\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Output:</span></span><br><span class=\"line\"><span class=\"comment\"># 130402 users</span></span><br><span class=\"line\"><span class=\"comment\"># 351 languages</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"Association-Rules\">Association Rules</h3>\n<p>What I wanted to look at is combinations of different skills, i.e. languages that usually occur together as developer skills. One approach to get insights like these is to mine the data for <em>association rules</em>, e.g. using an algorithm like <a href=\"https://en.wikipedia.org/wiki/Apriori_algorithm\" target=\"_blank\" rel=\"noopener\">Apriori</a> (as I did). The implementation I used was <a href=\"https://github.com/asaini/Apriori\" target=\"_blank\" rel=\"noopener\">asaini/Apriori</a>.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">user_langs = df_skills[df_skills[<span class=\"string\">'sizeRatio'</span>] &gt;= MIN_LANG_RATIO].groupby(<span class=\"string\">'login'</span>)[<span class=\"string\">'LanguageName'</span>].apply(set).values</span><br><span class=\"line\">items1, rules1 = apriori.runApriori(user_langs, MIN_SUPP, MIN_CONF)</span><br><span class=\"line\">rules1 = sorted(rules1, key=<span class=\"keyword\">lambda</span> e: e[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">print(rules1)</span><br></pre></td></tr></table></figure>\n<p><strong>Output:</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[(((&apos;ShaderLab&apos;,), (&apos;C#&apos;,)), 0.904),</span><br><span class=\"line\"> (((&apos;Vue&apos;,), (&apos;JavaScript&apos;,)), 0.671277997364954),</span><br><span class=\"line\"> (((&apos;Vue&apos;, &apos;CSS&apos;), (&apos;JavaScript&apos;,)), 0.656140350877193),</span><br><span class=\"line\"> (((&apos;GLSL&apos;,), (&apos;C#&apos;,)), 0.625),</span><br><span class=\"line\"> (((&apos;CMake&apos;,), (&apos;C++&apos;,)), 0.6229508196721312),</span><br><span class=\"line\"> (((&apos;CSS&apos;,), (&apos;JavaScript&apos;,)), 0.5807683959192532),</span><br><span class=\"line\"> (((&apos;Tcl&apos;,), (&apos;Python&apos;,)), 0.5658914728682171),</span><br><span class=\"line\"> (((&apos;Kotlin&apos;,), (&apos;Java&apos;,)), 0.5655375552282769),</span><br><span class=\"line\"> (((&apos;ASP&apos;,), (&apos;C#&apos;,)), 0.5488215488215488),</span><br><span class=\"line\"> (((&apos;Vue&apos;, &apos;HTML&apos;), (&apos;JavaScript&apos;,)), 0.5404411764705882),</span><br><span class=\"line\"> (((&apos;CoffeeScript&apos;,), (&apos;JavaScript&apos;,)), 0.5339578454332553),</span><br><span class=\"line\"> (((&apos;CSS&apos;, &apos;PHP&apos;), (&apos;JavaScript&apos;,)), 0.5116117850953206),</span><br><span class=\"line\"> (((&apos;Elm&apos;,), (&apos;JavaScript&apos;,)), 0.4951923076923077),</span><br><span class=\"line\"> (((&apos;CSS&apos;, &apos;HTML&apos;), (&apos;JavaScript&apos;,)), 0.4906486271388778),</span><br><span class=\"line\"> (((&apos;Smarty&apos;,), (&apos;PHP&apos;,)), 0.4788732394366197),</span><br><span class=\"line\"> (((&apos;TypeScript&apos;,), (&apos;JavaScript&apos;,)), 0.4739540607054964),</span><br><span class=\"line\"> (((&apos;CSS&apos;, &apos;C#&apos;), (&apos;JavaScript&apos;,)), 0.464926590538336),</span><br><span class=\"line\"> (((&apos;Groovy&apos;,), (&apos;Java&apos;,)), 0.4604651162790698)]</span><br></pre></td></tr></table></figure>\n<p>The left part of each row is a tuple of tuples of programming languages that represent an association rule. The right part is the <a href=\"https://en.wikipedia.org/wiki/Association_rule_learning#Confidence\" target=\"_blank\" rel=\"noopener\">confidence</a> of that rule.</p>\n<p><strong>For example:</strong><br>\nRead <code>((('ShaderLab',), ('C#',)), 0.904)</code> as ‚Äú90 % of all people who know <em>ShaderLab</em> also know C#‚Äù.</p>\n<p>The results reflect common sense. For instance, the rule that developers, who know <em>VueJS</em>, also know <em>JavaScript</em> seems to make sense, given that VueJS is a JavaScript framework. Analogously, <em>CMake</em> is a common build tool for <em>C++</em>, etc. Nothing too fancy here, except for that I didn‚Äôt know about <em>ShaderLab</em> and <em>GLSL</em>.</p>\n<h2 id=\"Locations\">Locations</h2>\n<p>Let‚Äôs take a look at where most GitHub users are from. Obviously, this only respects profiles where users have set their locations.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_locations = df1.reindex([<span class=\"string\">'location'</span>], axis=<span class=\"number\">1</span>).groupby(<span class=\"string\">'location'</span>).size()</span><br><span class=\"line\">df_locations = df_locations.sort_values(ascending=<span class=\"literal\">False</span>)[:<span class=\"number\">20</span>]</span><br><span class=\"line\">df_locations.plot.bar(figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"images/gh_locations.png\" alt></p>\n<p>Clearly, San Francisco seems to be the most popular city for developers (or at least for those who are active on GitHub).</p>\n<h2 id=\"Skills-by-location\">Skills by location</h2>\n<p>To take this a step further, let‚Äôs take a look at which skills users tend to have in what cities.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">language_replace</span><span class=\"params\">(df)</span>:</span></span><br><span class=\"line\">    df = df.copy()</span><br><span class=\"line\">    <span class=\"comment\"># Little bit of manual cleaning</span></span><br><span class=\"line\">    replace = &#123;<span class=\"string\">'San Francisco'</span>: <span class=\"string\">'San Francisco, CA'</span>,</span><br><span class=\"line\">               <span class=\"string\">'Berlin'</span>: <span class=\"string\">'Berlin, Germany'</span>,</span><br><span class=\"line\">               <span class=\"string\">'New York'</span>: <span class=\"string\">'New York, NY'</span>,</span><br><span class=\"line\">               <span class=\"string\">'London'</span>: <span class=\"string\">'London, UK'</span>,</span><br><span class=\"line\">               <span class=\"string\">'Beijing'</span>: <span class=\"string\">'Beijing, China'</span>,</span><br><span class=\"line\">               <span class=\"string\">'Paris'</span>: <span class=\"string\">'Paris, France'</span>&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (k, v) <span class=\"keyword\">in</span> replace.items():</span><br><span class=\"line\">        <span class=\"keyword\">if</span> isinstance(df, pd.DataFrame):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> k <span class=\"keyword\">in</span> df.columns <span class=\"keyword\">and</span> v <span class=\"keyword\">in</span> df.columns:</span><br><span class=\"line\">                df[k] = df[k] + df[v]</span><br><span class=\"line\">                df = df.drop([v], axis=<span class=\"number\">1</span>, errors=<span class=\"string\">'ignore'</span>)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> k <span class=\"keyword\">in</span> df.index <span class=\"keyword\">and</span> v <span class=\"keyword\">in</span> df.index:</span><br><span class=\"line\">                df[k] = df[k] + df[v]</span><br><span class=\"line\">                <span class=\"comment\">#df = df.drop([v], axis=1)</span></span><br><span class=\"line\">                <span class=\"keyword\">del</span> df[v]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> df</span><br><span class=\"line\"></span><br><span class=\"line\">langs_by_loc = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> df_locations.index:</span><br><span class=\"line\">    langs_by_loc[l] = df1[df1[<span class=\"string\">'location'</span>] == l][[<span class=\"string\">'LanguageName'</span>]].groupby(<span class=\"string\">'LanguageName'</span>).size()</span><br><span class=\"line\">df_loc_langs = pd.DataFrame.from_dict(langs_by_loc).fillna(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">df_loc_langs = language_replace(df_loc_langs)</span><br><span class=\"line\">df_loc_langs = df_loc_langs.T</span><br><span class=\"line\">df_loc_langs = df_loc_langs.drop([c <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> df_loc_langs.columns <span class=\"keyword\">if</span> c <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> df_top_langs[<span class=\"string\">'LanguageName'</span>].values], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">df_loc_langs = (df_loc_langs.T / df_loc_langs.T.sum()).T <span class=\"comment\"># normalize heights</span></span><br><span class=\"line\">df_loc_langs.plot.bar(stacked=<span class=\"literal\">True</span>, figsize=(<span class=\"number\">16</span>,<span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n<p><img src=\"images/gh_location_langs.png\" alt></p>\n<p>Look like there are no real outliers in the distribution of developer skills between different cities of the world. Maybe you could say that, e.g., Indians like web frontends a little more than command-line hacking.</p>\n<h2 id=\"Skills-Karlsruhe-vs-the-World\">Skills: Karlsruhe vs. the World</h2>\n<p>While an overview is cool, I found it even more interesting to specifically compare between to cities. So in the following chart I compare language-specific programming skills in Karlsruhe (the city where I live and study) to the rest of the world‚Äôs average.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_ka = df1[df1[<span class=\"string\">'location'</span>] == <span class=\"string\">'Karlsruhe'</span>][[<span class=\"string\">'LanguageName'</span>]].groupby(<span class=\"string\">'LanguageName'</span>).size()</span><br><span class=\"line\">df_ka = pd.DataFrame(df_ka, index=df_ka.index, columns=[<span class=\"string\">'Karlsruhe'</span>]) / df_ka.sum()</span><br><span class=\"line\">df_world = pd.DataFrame(df_loc_langs.mean(), index=df_loc_langs.mean().index, columns=[<span class=\"string\">'World'</span>])</span><br><span class=\"line\">df_compare = df_world.merge(df_ka, how=<span class=\"string\">'left'</span>, left_index=<span class=\"literal\">True</span>, right_index=<span class=\"literal\">True</span>)</span><br><span class=\"line\">ax = df_compare.plot.barh(title=<span class=\"string\">'Languages: World vs. Karlsruhe'</span>, legend=<span class=\"literal\">True</span>, figsize=(<span class=\"number\">10</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">ax.set_xlabel(<span class=\"string\">'Percentage (Top 10)'</span>)</span><br><span class=\"line\">ax.set_ylabel(<span class=\"string\">'Programming Language Skills'</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"images/gh_ka_world.png\" alt></p>\n<p>These results are a bit surprising to me. Clearly, Karlsruhe-based developers seem to dislike JavaScript compared to the world. However, this is different from what I experienced in several student jobs and internships here.</p>\n<h2 id=\"Project-Tech-Stacks\">Project Tech Stacks</h2>\n<p>Last but not least, let‚Äôs apply Apriori once more, but this time in a slightly different way. Instead of looking at user skills, let‚Äôs look at languages that occur together on a per-repository basis. And instead of trying to find rules, let‚Äôs only look at <em>frequent item sets</em> (which are the basis for rules). My expectation was to get back sets of commonly used tech stacks.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">N = int(<span class=\"number\">1e7</span>)</span><br><span class=\"line\">MIN_SUPP = <span class=\"number\">.0005</span></span><br><span class=\"line\">MIN_CONF = <span class=\"number\">.45</span></span><br><span class=\"line\">MIN_LANG_RATIO = <span class=\"number\">.1</span></span><br><span class=\"line\"></span><br><span class=\"line\">df_stacks = pd.read_sql_query(<span class=\"string\">f'''</span></span><br><span class=\"line\"><span class=\"string\">    select LanguageName, size, RepositoryId from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    order by RepositoryId</span></span><br><span class=\"line\"><span class=\"string\">    limit <span class=\"subst\">&#123;N&#125;</span></span></span><br><span class=\"line\"><span class=\"string\">'''</span>, con=connection)</span><br><span class=\"line\"></span><br><span class=\"line\">df_stacks = df_stacks.merge(pd.DataFrame(df_stacks.groupby(<span class=\"string\">'RepositoryId'</span>)[<span class=\"string\">'size'</span>].sum()), how=<span class=\"string\">'left'</span>, on=<span class=\"string\">'RepositoryId'</span>).rename(columns=&#123;<span class=\"string\">'size_x'</span>: <span class=\"string\">'size'</span>, <span class=\"string\">'size_y'</span>: <span class=\"string\">'totalSize'</span>&#125;)</span><br><span class=\"line\">df_stacks = df_stacks[df_stacks[<span class=\"string\">'totalSize'</span>] &gt; <span class=\"number\">0</span>]</span><br><span class=\"line\">df_stacks[<span class=\"string\">'sizeRatio'</span>] = df_stacks[<span class=\"string\">'size'</span>] / df_stacks[<span class=\"string\">'totalSize'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">f\"<span class=\"subst\">&#123;df_stacks[<span class=\"string\">'RepositoryId'</span>].unique().size&#125;</span> repositories\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">f\"<span class=\"subst\">&#123;df_stacks[<span class=\"string\">'LanguageName'</span>].unique().size&#125;</span> languages\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Output: </span></span><br><span class=\"line\"><span class=\"comment\"># 853114 repositories</span></span><br><span class=\"line\"><span class=\"comment\"># 351 languages</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">repo_langs = df_stacks[df_stacks[<span class=\"string\">'sizeRatio'</span>] &gt;= MIN_LANG_RATIO].groupby(<span class=\"string\">'RepositoryId'</span>)[<span class=\"string\">'LanguageName'</span>].apply(set).values</span><br><span class=\"line\">items2, rules2 = apriori.runApriori(repo_langs, MIN_SUPP, MIN_CONF)</span><br><span class=\"line\">itemsets2 = sorted(list(filter(<span class=\"keyword\">lambda</span> i: len(i[<span class=\"number\">0</span>]) &gt; <span class=\"number\">2</span>, items2)), key=<span class=\"keyword\">lambda</span> i: i[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">print(itemsets2)</span><br></pre></td></tr></table></figure>\n<p><strong>Output:</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[((&apos;CSS&apos;, &apos;JavaScript&apos;, &apos;HTML&apos;), 0.04360026913167525),</span><br><span class=\"line\"> ((&apos;CSS&apos;, &apos;JavaScript&apos;, &apos;PHP&apos;), 0.0045574213997191465),</span><br><span class=\"line\"> ((&apos;Ruby&apos;, &apos;CSS&apos;, &apos;HTML&apos;), 0.004456614239128651),</span><br><span class=\"line\"> ((&apos;TypeScript&apos;, &apos;JavaScript&apos;, &apos;HTML&apos;), 0.0042034241613664765),</span><br><span class=\"line\"> ((&apos;TypeScript&apos;, &apos;HTML&apos;, &apos;CSS&apos;), 0.0035024627423767517),</span><br><span class=\"line\"> ((&apos;Python&apos;, &apos;JavaScript&apos;, &apos;HTML&apos;), 0.002962089474560258),</span><br><span class=\"line\"> ((&apos;Python&apos;, &apos;HTML&apos;, &apos;CSS&apos;), 0.002769852563666755),</span><br><span class=\"line\"> ((&apos;Ruby&apos;, &apos;JavaScript&apos;, &apos;HTML&apos;), 0.0022400288824236856),</span><br><span class=\"line\"> ((&apos;JavaScript&apos;, &apos;HTML&apos;, &apos;PHP&apos;), 0.0022154131804190294),</span><br><span class=\"line\"> ((&apos;Ruby&apos;, &apos;CSS&apos;, &apos;JavaScript&apos;), 0.0021532878372644217),</span><br><span class=\"line\"> ((&apos;CSS&apos;, &apos;HTML&apos;, &apos;PHP&apos;), 0.0019915275098052547),</span><br><span class=\"line\"> ((&apos;JavaScript&apos;, &apos;Objective-C&apos;, &apos;Java&apos;), 0.0018614159420663593),</span><br><span class=\"line\"> ((&apos;CSS&apos;, &apos;JavaScript&apos;, &apos;Python&apos;), 0.0017992905989117516),</span><br><span class=\"line\"> ((&apos;Python&apos;, &apos;JavaScript&apos;, &apos;Objective-C&apos;), 0.0017735027206211597),</span><br><span class=\"line\"> ((&apos;Python&apos;, &apos;JavaScript&apos;, &apos;Java&apos;), 0.001508590879999625),</span><br><span class=\"line\"> ((&apos;CSS&apos;, &apos;JavaScript&apos;, &apos;TypeScript&apos;), 0.0014745977677074812),</span><br><span class=\"line\"> ((&apos;Python&apos;, &apos;Objective-C&apos;, &apos;Java&apos;), 0.0014066115431231934),</span><br><span class=\"line\"> ((&apos;Python&apos;, &apos;JavaScript&apos;, &apos;Objective-C&apos;, &apos;Java&apos;), 0.0013222148505358019),</span><br><span class=\"line\"> ((&apos;Vue&apos;, &apos;CSS&apos;, &apos;JavaScript&apos;), 0.0012554008022374501)]</span><br></pre></td></tr></table></figure>\n<p>Here, the left side is sets of frequently occurring combinations of languages. The right side is the set‚Äôs <a href=\"https://en.wikipedia.org/wiki/Association_rule_learning#Support\" target=\"_blank\" rel=\"noopener\">support</a>, which is the relative occurrences of that set among the whole data set.<br>\nObviously, many of these are actually common ‚Äútech stacks‚Äù and almost all of them are web technologies. I guess GitHub is most popular among web developers.</p>\n<h1>Conclusion</h1>\n<p>There is a lot of more complex analyses that could be might on rich data like this and probably tools like <a href=\"https://cloud.google.com/bigquery/public-data/\" target=\"_blank\" rel=\"noopener\">BigQuery</a> are better suitable than Pandas, operating on a tiny sample. However, I used this little project to improve my EDA skills and hopefully give you guys an interesting article to read. Let me know if you like it!</p>\n"},{"title":"Flying a DJI Tello Drone with Go","date":"2019-10-01T12:21:53.000Z","description":"In this post, we program a DJI Tello drone using Go and control it from the command line.","_content":"\n# The Idea\nA few months ago, I bought a [DJI Tello](https://amzn.to/2neAwVr) (affiliate link) drone on Amazon for ~ 80 ‚Ç¨, which is quite an impressive price, considering that you can also pay several hundred or even thousand Euro for a DJI drone. Of course, this one is only meant for fun and tinkering, not for professional photography or so. \nEven though I was amazed by how easy it is to control the drone from your smartphone ‚Äì even with windy weather conditions - the Tello started to bore me after a few weeks. I wanted to do something more interesting with the drone ‚Äì I wanted to **program it**!\n\n![](images/tello1.jpg)\n\n# Using Gobot SDK\nDuring my research on how to program drones - specifically the Tello ‚Äì I found an article called [\"Hello, Tello - Hacking Drones With Go\"](https://gobot.io/blog/2018/04/20/hello-tello-hacking-drones-with-go/), which referenced the documentation of a robot programming toolkit called [Gobot](https://gobot.io). As it turned out, Gobot is a Go SDK to control several different micro-robots like [GoPiGo3](https://gobot.io/documentation/platforms/gopigo3/), the [Parrot Ardrone](https://gobot.io/documentation/platforms/ardrone/), any drone using the [MAVLink protocol](https://mavlink.io/en/), [Pebble smartwatches](https://gobot.io/documentation/platforms/pebble/) and many more. It also has support for different microcontrollers, including Arduino, RaspberryPi and Intel Edison as well as communication protocols like MQTT or [NATS](https://nats.io/). Gobot's API to interface with any of the supported platforms appeared to be quite straightforward and easy to understand, so I decided to give it a try.\n\nMy first goal was to just have a little program that allows me to **control the drone from my PC** instead of the [official Tello app](https://play.google.com/store/apps/details?id=com.ryzerobotics.tello). Luckily, Gobot also provides a [keyboard driver](https://godoc.org/gobot.io/x/gobot/platforms/keyboard) (there's also an Xbox360 gamepad driver) that can be used to subscribe to certain key events, etc.\n\n# First Prototype\n\n![](images/tello2.png)\n\nAfter tinkering for two hours (and having my poor drone hit the wall several times), I got a basic program that:\n - **connects** to the drone\n - prints incoming **status information** (like battery state) to the console\n - handles **keyboard events** (‚Üë‚Üì‚Üê‚Üí for lateral and longitudinal navigation, WASD for rotation and altitude control, spacebar to start and land)\n - runs something like a **\"game loop\"** with a tick rate of 10 FPS to sync the latest user commands with the drone\n - renders the drone's **video stream** to an [MPlayer](https://wiki.debian.org/MPlayer) window with 10 FPS\n\nThe program's main method is as simple as this:\n\n```go\n// tello.go\n// [...]\nfunc main() {\n\t// Init Gobot drivers\n\tkeys := keyboard.NewDriver()\n\tdrone := tello.NewDriver(\"8890\")\n\n\twork := func() {\n\t\t// Handle keyboard inputs\n\t\tkeys.On(keyboard.Key, handleKeyboardInput(drone))\n\n\t\t// Handle drone events\n\t\tdrone.On(tello.FlightDataEvent, handleFlightData(drone))\n\t\tdrone.On(tello.ConnectedEvent, handleConnected(drone))\n\t\tdrone.On(tello.LandingEvent, handleLanding(drone))\n\t\tdrone.On(tello.VideoFrameEvent, handleVideo(drone))\n\t}\n\n\trobot := gobot.NewRobot(\n\t\t\"tello\",\n\t\t[]gobot.Connection{},\n\t\t[]gobot.Device{keys, drone},\n\t\twork,\n\t)\n\n\trobot.Start()\n}\n// [...]\n```\n\nIn addition, here's a little excerpt from the `tick()` method, that checks for the latest key input and calls the corresponding control methods.\n\n```go\n// tello.go\n// [...]\nif currentControl == keyboard.A {\n    fmt.Println(\"Going left.\")\n    if !dry {\n        drone.Left(intensity)\n    }\n// [...]\n} else if currentControl == keyboard.ArrowRight {\n    fmt.Println(\"Rotating clockwise.\")\n    if !dry {\n        drone.Clockwise(intensity)\n    }\n} else {\n    fmt.Println(\"Resetting steering.\")\n    resetSteering(drone)\n}\n// [...]\n```\n\nThe entire code is **available on GitHub** at [muety/tello](https://github.com/muety/tello).\n\n# Challenges\nOne thing that made me stuck for a while was the way API commands like `drone.Left(100)` (where the integer parameter represents the movement's \"intensity\" or speed) work. Once called, they are being applied continuously until manually stopped. In this case, the drone would go left with max speed until you explicitly send `drone.Left(0)`. This behavior combined with the fact that Gobot's keyboard driver only supports to communicate when a key **is pressed**, but **not when it's released** again, made it a little difficult to smoothly control the drone. To cope with that, I introduced a **debouncing logic**. When a key is pressed (e.g. ‚Üê), a flag for that key is toggled on for 250 ms before it's automatically reset again, if the key was released in the meantime. Within the `tick()` method, the only thing done is to translate the binary key states to API commands and sync them to the device. \n\n# Future Plans\nIn case I have some time to further work in this little project, I would love to add basic **\"self-flying\" capabilities**. How cool would it be to have the ability to set a marker on a map and have the Tello fly there autonomously, while automatically avoiding obstacles in its way? A good starting point for this might be to take a look into [GoCV](https://gocv.io/), which is a Go interface to OpenCV. Alternatively, Microsoft's [AirSim](https://github.com/Microsoft/AirSim) simulator provides explicit support for training machine learning models for self-flying drones. Very cool!","source":"_posts/flying-a-dji-tello-drone-with-go.md","raw":"---\ntitle: Flying a DJI Tello Drone with Go\ndate: 2019-10-01 14:21:53\ntags:\ndescription: In this post, we program a DJI Tello drone using Go and control it from the command line.\n---\n\n# The Idea\nA few months ago, I bought a [DJI Tello](https://amzn.to/2neAwVr) (affiliate link) drone on Amazon for ~ 80 ‚Ç¨, which is quite an impressive price, considering that you can also pay several hundred or even thousand Euro for a DJI drone. Of course, this one is only meant for fun and tinkering, not for professional photography or so. \nEven though I was amazed by how easy it is to control the drone from your smartphone ‚Äì even with windy weather conditions - the Tello started to bore me after a few weeks. I wanted to do something more interesting with the drone ‚Äì I wanted to **program it**!\n\n![](images/tello1.jpg)\n\n# Using Gobot SDK\nDuring my research on how to program drones - specifically the Tello ‚Äì I found an article called [\"Hello, Tello - Hacking Drones With Go\"](https://gobot.io/blog/2018/04/20/hello-tello-hacking-drones-with-go/), which referenced the documentation of a robot programming toolkit called [Gobot](https://gobot.io). As it turned out, Gobot is a Go SDK to control several different micro-robots like [GoPiGo3](https://gobot.io/documentation/platforms/gopigo3/), the [Parrot Ardrone](https://gobot.io/documentation/platforms/ardrone/), any drone using the [MAVLink protocol](https://mavlink.io/en/), [Pebble smartwatches](https://gobot.io/documentation/platforms/pebble/) and many more. It also has support for different microcontrollers, including Arduino, RaspberryPi and Intel Edison as well as communication protocols like MQTT or [NATS](https://nats.io/). Gobot's API to interface with any of the supported platforms appeared to be quite straightforward and easy to understand, so I decided to give it a try.\n\nMy first goal was to just have a little program that allows me to **control the drone from my PC** instead of the [official Tello app](https://play.google.com/store/apps/details?id=com.ryzerobotics.tello). Luckily, Gobot also provides a [keyboard driver](https://godoc.org/gobot.io/x/gobot/platforms/keyboard) (there's also an Xbox360 gamepad driver) that can be used to subscribe to certain key events, etc.\n\n# First Prototype\n\n![](images/tello2.png)\n\nAfter tinkering for two hours (and having my poor drone hit the wall several times), I got a basic program that:\n - **connects** to the drone\n - prints incoming **status information** (like battery state) to the console\n - handles **keyboard events** (‚Üë‚Üì‚Üê‚Üí for lateral and longitudinal navigation, WASD for rotation and altitude control, spacebar to start and land)\n - runs something like a **\"game loop\"** with a tick rate of 10 FPS to sync the latest user commands with the drone\n - renders the drone's **video stream** to an [MPlayer](https://wiki.debian.org/MPlayer) window with 10 FPS\n\nThe program's main method is as simple as this:\n\n```go\n// tello.go\n// [...]\nfunc main() {\n\t// Init Gobot drivers\n\tkeys := keyboard.NewDriver()\n\tdrone := tello.NewDriver(\"8890\")\n\n\twork := func() {\n\t\t// Handle keyboard inputs\n\t\tkeys.On(keyboard.Key, handleKeyboardInput(drone))\n\n\t\t// Handle drone events\n\t\tdrone.On(tello.FlightDataEvent, handleFlightData(drone))\n\t\tdrone.On(tello.ConnectedEvent, handleConnected(drone))\n\t\tdrone.On(tello.LandingEvent, handleLanding(drone))\n\t\tdrone.On(tello.VideoFrameEvent, handleVideo(drone))\n\t}\n\n\trobot := gobot.NewRobot(\n\t\t\"tello\",\n\t\t[]gobot.Connection{},\n\t\t[]gobot.Device{keys, drone},\n\t\twork,\n\t)\n\n\trobot.Start()\n}\n// [...]\n```\n\nIn addition, here's a little excerpt from the `tick()` method, that checks for the latest key input and calls the corresponding control methods.\n\n```go\n// tello.go\n// [...]\nif currentControl == keyboard.A {\n    fmt.Println(\"Going left.\")\n    if !dry {\n        drone.Left(intensity)\n    }\n// [...]\n} else if currentControl == keyboard.ArrowRight {\n    fmt.Println(\"Rotating clockwise.\")\n    if !dry {\n        drone.Clockwise(intensity)\n    }\n} else {\n    fmt.Println(\"Resetting steering.\")\n    resetSteering(drone)\n}\n// [...]\n```\n\nThe entire code is **available on GitHub** at [muety/tello](https://github.com/muety/tello).\n\n# Challenges\nOne thing that made me stuck for a while was the way API commands like `drone.Left(100)` (where the integer parameter represents the movement's \"intensity\" or speed) work. Once called, they are being applied continuously until manually stopped. In this case, the drone would go left with max speed until you explicitly send `drone.Left(0)`. This behavior combined with the fact that Gobot's keyboard driver only supports to communicate when a key **is pressed**, but **not when it's released** again, made it a little difficult to smoothly control the drone. To cope with that, I introduced a **debouncing logic**. When a key is pressed (e.g. ‚Üê), a flag for that key is toggled on for 250 ms before it's automatically reset again, if the key was released in the meantime. Within the `tick()` method, the only thing done is to translate the binary key states to API commands and sync them to the device. \n\n# Future Plans\nIn case I have some time to further work in this little project, I would love to add basic **\"self-flying\" capabilities**. How cool would it be to have the ability to set a marker on a map and have the Tello fly there autonomously, while automatically avoiding obstacles in its way? A good starting point for this might be to take a look into [GoCV](https://gocv.io/), which is a Go interface to OpenCV. Alternatively, Microsoft's [AirSim](https://github.com/Microsoft/AirSim) simulator provides explicit support for training machine learning models for self-flying drones. Very cool!","slug":"flying-a-dji-tello-drone-with-go","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhi1000d40mq1gia6idv","content":"<h1>The Idea</h1>\n<p>A few months ago, I bought a <a href=\"https://amzn.to/2neAwVr\" target=\"_blank\" rel=\"noopener\">DJI Tello</a> (affiliate link) drone on Amazon for ~ 80 ‚Ç¨, which is quite an impressive price, considering that you can also pay several hundred or even thousand Euro for a DJI drone. Of course, this one is only meant for fun and tinkering, not for professional photography or so.<br>\nEven though I was amazed by how easy it is to control the drone from your smartphone ‚Äì even with windy weather conditions - the Tello started to bore me after a few weeks. I wanted to do something more interesting with the drone ‚Äì I wanted to <strong>program it</strong>!</p>\n<p><img src=\"images/tello1.jpg\" alt></p>\n<h1>Using Gobot SDK</h1>\n<p>During my research on how to program drones - specifically the Tello ‚Äì I found an article called <a href=\"https://gobot.io/blog/2018/04/20/hello-tello-hacking-drones-with-go/\" target=\"_blank\" rel=\"noopener\">‚ÄúHello, Tello - Hacking Drones With Go‚Äù</a>, which referenced the documentation of a robot programming toolkit called <a href=\"https://gobot.io\" target=\"_blank\" rel=\"noopener\">Gobot</a>. As it turned out, Gobot is a Go SDK to control several different micro-robots like <a href=\"https://gobot.io/documentation/platforms/gopigo3/\" target=\"_blank\" rel=\"noopener\">GoPiGo3</a>, the <a href=\"https://gobot.io/documentation/platforms/ardrone/\" target=\"_blank\" rel=\"noopener\">Parrot Ardrone</a>, any drone using the <a href=\"https://mavlink.io/en/\" target=\"_blank\" rel=\"noopener\">MAVLink protocol</a>, <a href=\"https://gobot.io/documentation/platforms/pebble/\" target=\"_blank\" rel=\"noopener\">Pebble smartwatches</a> and many more. It also has support for different microcontrollers, including Arduino, RaspberryPi and Intel Edison as well as communication protocols like MQTT or <a href=\"https://nats.io/\" target=\"_blank\" rel=\"noopener\">NATS</a>. Gobot‚Äôs API to interface with any of the supported platforms appeared to be quite straightforward and easy to understand, so I decided to give it a try.</p>\n<p>My first goal was to just have a little program that allows me to <strong>control the drone from my PC</strong> instead of the <a href=\"https://play.google.com/store/apps/details?id=com.ryzerobotics.tello\" target=\"_blank\" rel=\"noopener\">official Tello app</a>. Luckily, Gobot also provides a <a href=\"https://godoc.org/gobot.io/x/gobot/platforms/keyboard\" target=\"_blank\" rel=\"noopener\">keyboard driver</a> (there‚Äôs also an Xbox360 gamepad driver) that can be used to subscribe to certain key events, etc.</p>\n<h1>First Prototype</h1>\n<p><img src=\"images/tello2.png\" alt></p>\n<p>After tinkering for two hours (and having my poor drone hit the wall several times), I got a basic program that:</p>\n<ul>\n<li><strong>connects</strong> to the drone</li>\n<li>prints incoming <strong>status information</strong> (like battery state) to the console</li>\n<li>handles <strong>keyboard events</strong> (‚Üë‚Üì‚Üê‚Üí for lateral and longitudinal navigation, WASD for rotation and altitude control, spacebar to start and land)</li>\n<li>runs something like a <strong>‚Äúgame loop‚Äù</strong> with a tick rate of 10 FPS to sync the latest user commands with the drone</li>\n<li>renders the drone‚Äôs <strong>video stream</strong> to an <a href=\"https://wiki.debian.org/MPlayer\" target=\"_blank\" rel=\"noopener\">MPlayer</a> window with 10 FPS</li>\n</ul>\n<p>The program‚Äôs main method is as simple as this:</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// tello.go</span></span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">main</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"comment\">// Init Gobot drivers</span></span><br><span class=\"line\">\tkeys := keyboard.NewDriver()</span><br><span class=\"line\">\tdrone := tello.NewDriver(<span class=\"string\">\"8890\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\twork := <span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// Handle keyboard inputs</span></span><br><span class=\"line\">\t\tkeys.On(keyboard.Key, handleKeyboardInput(drone))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// Handle drone events</span></span><br><span class=\"line\">\t\tdrone.On(tello.FlightDataEvent, handleFlightData(drone))</span><br><span class=\"line\">\t\tdrone.On(tello.ConnectedEvent, handleConnected(drone))</span><br><span class=\"line\">\t\tdrone.On(tello.LandingEvent, handleLanding(drone))</span><br><span class=\"line\">\t\tdrone.On(tello.VideoFrameEvent, handleVideo(drone))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\trobot := gobot.NewRobot(</span><br><span class=\"line\">\t\t<span class=\"string\">\"tello\"</span>,</span><br><span class=\"line\">\t\t[]gobot.Connection&#123;&#125;,</span><br><span class=\"line\">\t\t[]gobot.Device&#123;keys, drone&#125;,</span><br><span class=\"line\">\t\twork,</span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\trobot.Start()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br></pre></td></tr></table></figure>\n<p>In addition, here‚Äôs a little excerpt from the <code>tick()</code> method, that checks for the latest key input and calls the corresponding control methods.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// tello.go</span></span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> currentControl == keyboard.A &#123;</span><br><span class=\"line\">    fmt.Println(<span class=\"string\">\"Going left.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> !dry &#123;</span><br><span class=\"line\">        drone.Left(intensity)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> currentControl == keyboard.ArrowRight &#123;</span><br><span class=\"line\">    fmt.Println(<span class=\"string\">\"Rotating clockwise.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> !dry &#123;</span><br><span class=\"line\">        drone.Clockwise(intensity)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    fmt.Println(<span class=\"string\">\"Resetting steering.\"</span>)</span><br><span class=\"line\">    resetSteering(drone)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br></pre></td></tr></table></figure>\n<p>The entire code is <strong>available on GitHub</strong> at <a href=\"https://github.com/muety/tello\" target=\"_blank\" rel=\"noopener\">muety/tello</a>.</p>\n<h1>Challenges</h1>\n<p>One thing that made me stuck for a while was the way API commands like <code>drone.Left(100)</code> (where the integer parameter represents the movement‚Äôs ‚Äúintensity‚Äù or speed) work. Once called, they are being applied continuously until manually stopped. In this case, the drone would go left with max speed until you explicitly send <code>drone.Left(0)</code>. This behavior combined with the fact that Gobot‚Äôs keyboard driver only supports to communicate when a key <strong>is pressed</strong>, but <strong>not when it‚Äôs released</strong> again, made it a little difficult to smoothly control the drone. To cope with that, I introduced a <strong>debouncing logic</strong>. When a key is pressed (e.g. ‚Üê), a flag for that key is toggled on for 250 ms before it‚Äôs automatically reset again, if the key was released in the meantime. Within the <code>tick()</code> method, the only thing done is to translate the binary key states to API commands and sync them to the device.</p>\n<h1>Future Plans</h1>\n<p>In case I have some time to further work in this little project, I would love to add basic <strong>‚Äúself-flying‚Äù capabilities</strong>. How cool would it be to have the ability to set a marker on a map and have the Tello fly there autonomously, while automatically avoiding obstacles in its way? A good starting point for this might be to take a look into <a href=\"https://gocv.io/\" target=\"_blank\" rel=\"noopener\">GoCV</a>, which is a Go interface to OpenCV. Alternatively, Microsoft‚Äôs <a href=\"https://github.com/Microsoft/AirSim\" target=\"_blank\" rel=\"noopener\">AirSim</a> simulator provides explicit support for training machine learning models for self-flying drones. Very cool!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1>The Idea</h1>\n<p>A few months ago, I bought a <a href=\"https://amzn.to/2neAwVr\" target=\"_blank\" rel=\"noopener\">DJI Tello</a> (affiliate link) drone on Amazon for ~ 80 ‚Ç¨, which is quite an impressive price, considering that you can also pay several hundred or even thousand Euro for a DJI drone. Of course, this one is only meant for fun and tinkering, not for professional photography or so.<br>\nEven though I was amazed by how easy it is to control the drone from your smartphone ‚Äì even with windy weather conditions - the Tello started to bore me after a few weeks. I wanted to do something more interesting with the drone ‚Äì I wanted to <strong>program it</strong>!</p>\n<p><img src=\"images/tello1.jpg\" alt></p>\n<h1>Using Gobot SDK</h1>\n<p>During my research on how to program drones - specifically the Tello ‚Äì I found an article called <a href=\"https://gobot.io/blog/2018/04/20/hello-tello-hacking-drones-with-go/\" target=\"_blank\" rel=\"noopener\">‚ÄúHello, Tello - Hacking Drones With Go‚Äù</a>, which referenced the documentation of a robot programming toolkit called <a href=\"https://gobot.io\" target=\"_blank\" rel=\"noopener\">Gobot</a>. As it turned out, Gobot is a Go SDK to control several different micro-robots like <a href=\"https://gobot.io/documentation/platforms/gopigo3/\" target=\"_blank\" rel=\"noopener\">GoPiGo3</a>, the <a href=\"https://gobot.io/documentation/platforms/ardrone/\" target=\"_blank\" rel=\"noopener\">Parrot Ardrone</a>, any drone using the <a href=\"https://mavlink.io/en/\" target=\"_blank\" rel=\"noopener\">MAVLink protocol</a>, <a href=\"https://gobot.io/documentation/platforms/pebble/\" target=\"_blank\" rel=\"noopener\">Pebble smartwatches</a> and many more. It also has support for different microcontrollers, including Arduino, RaspberryPi and Intel Edison as well as communication protocols like MQTT or <a href=\"https://nats.io/\" target=\"_blank\" rel=\"noopener\">NATS</a>. Gobot‚Äôs API to interface with any of the supported platforms appeared to be quite straightforward and easy to understand, so I decided to give it a try.</p>\n<p>My first goal was to just have a little program that allows me to <strong>control the drone from my PC</strong> instead of the <a href=\"https://play.google.com/store/apps/details?id=com.ryzerobotics.tello\" target=\"_blank\" rel=\"noopener\">official Tello app</a>. Luckily, Gobot also provides a <a href=\"https://godoc.org/gobot.io/x/gobot/platforms/keyboard\" target=\"_blank\" rel=\"noopener\">keyboard driver</a> (there‚Äôs also an Xbox360 gamepad driver) that can be used to subscribe to certain key events, etc.</p>\n<h1>First Prototype</h1>\n<p><img src=\"images/tello2.png\" alt></p>\n<p>After tinkering for two hours (and having my poor drone hit the wall several times), I got a basic program that:</p>\n<ul>\n<li><strong>connects</strong> to the drone</li>\n<li>prints incoming <strong>status information</strong> (like battery state) to the console</li>\n<li>handles <strong>keyboard events</strong> (‚Üë‚Üì‚Üê‚Üí for lateral and longitudinal navigation, WASD for rotation and altitude control, spacebar to start and land)</li>\n<li>runs something like a <strong>‚Äúgame loop‚Äù</strong> with a tick rate of 10 FPS to sync the latest user commands with the drone</li>\n<li>renders the drone‚Äôs <strong>video stream</strong> to an <a href=\"https://wiki.debian.org/MPlayer\" target=\"_blank\" rel=\"noopener\">MPlayer</a> window with 10 FPS</li>\n</ul>\n<p>The program‚Äôs main method is as simple as this:</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// tello.go</span></span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">main</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"comment\">// Init Gobot drivers</span></span><br><span class=\"line\">\tkeys := keyboard.NewDriver()</span><br><span class=\"line\">\tdrone := tello.NewDriver(<span class=\"string\">\"8890\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\twork := <span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// Handle keyboard inputs</span></span><br><span class=\"line\">\t\tkeys.On(keyboard.Key, handleKeyboardInput(drone))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// Handle drone events</span></span><br><span class=\"line\">\t\tdrone.On(tello.FlightDataEvent, handleFlightData(drone))</span><br><span class=\"line\">\t\tdrone.On(tello.ConnectedEvent, handleConnected(drone))</span><br><span class=\"line\">\t\tdrone.On(tello.LandingEvent, handleLanding(drone))</span><br><span class=\"line\">\t\tdrone.On(tello.VideoFrameEvent, handleVideo(drone))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\trobot := gobot.NewRobot(</span><br><span class=\"line\">\t\t<span class=\"string\">\"tello\"</span>,</span><br><span class=\"line\">\t\t[]gobot.Connection&#123;&#125;,</span><br><span class=\"line\">\t\t[]gobot.Device&#123;keys, drone&#125;,</span><br><span class=\"line\">\t\twork,</span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\trobot.Start()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br></pre></td></tr></table></figure>\n<p>In addition, here‚Äôs a little excerpt from the <code>tick()</code> method, that checks for the latest key input and calls the corresponding control methods.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// tello.go</span></span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> currentControl == keyboard.A &#123;</span><br><span class=\"line\">    fmt.Println(<span class=\"string\">\"Going left.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> !dry &#123;</span><br><span class=\"line\">        drone.Left(intensity)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> currentControl == keyboard.ArrowRight &#123;</span><br><span class=\"line\">    fmt.Println(<span class=\"string\">\"Rotating clockwise.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> !dry &#123;</span><br><span class=\"line\">        drone.Clockwise(intensity)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    fmt.Println(<span class=\"string\">\"Resetting steering.\"</span>)</span><br><span class=\"line\">    resetSteering(drone)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br></pre></td></tr></table></figure>\n<p>The entire code is <strong>available on GitHub</strong> at <a href=\"https://github.com/muety/tello\" target=\"_blank\" rel=\"noopener\">muety/tello</a>.</p>\n<h1>Challenges</h1>\n<p>One thing that made me stuck for a while was the way API commands like <code>drone.Left(100)</code> (where the integer parameter represents the movement‚Äôs ‚Äúintensity‚Äù or speed) work. Once called, they are being applied continuously until manually stopped. In this case, the drone would go left with max speed until you explicitly send <code>drone.Left(0)</code>. This behavior combined with the fact that Gobot‚Äôs keyboard driver only supports to communicate when a key <strong>is pressed</strong>, but <strong>not when it‚Äôs released</strong> again, made it a little difficult to smoothly control the drone. To cope with that, I introduced a <strong>debouncing logic</strong>. When a key is pressed (e.g. ‚Üê), a flag for that key is toggled on for 250 ms before it‚Äôs automatically reset again, if the key was released in the meantime. Within the <code>tick()</code> method, the only thing done is to translate the binary key states to API commands and sync them to the device.</p>\n<h1>Future Plans</h1>\n<p>In case I have some time to further work in this little project, I would love to add basic <strong>‚Äúself-flying‚Äù capabilities</strong>. How cool would it be to have the ability to set a marker on a map and have the Tello fly there autonomously, while automatically avoiding obstacles in its way? A good starting point for this might be to take a look into <a href=\"https://gocv.io/\" target=\"_blank\" rel=\"noopener\">GoCV</a>, which is a Go interface to OpenCV. Alternatively, Microsoft‚Äôs <a href=\"https://github.com/Microsoft/AirSim\" target=\"_blank\" rel=\"noopener\">AirSim</a> simulator provides explicit support for training machine learning models for self-flying drones. Very cool!</p>\n"},{"title":"Halite - A rule-based AI bot","date":"2018-01-03T07:04:08.000Z","_content":"\nAfter having spent a considerable amount of time with it last weekend, I wanted to make a short comment on [Halite.io](https://halite.io). Halite is a programming- and AI competition where people can write programs or train algorithms to control a bot that plays in a virtual 2D game environment. There is a leaderboard to track how your bot competes with other players' bots and you can watch a replay of every game your bot has played, which helps _debugging_ your bot as well as figuring out other people's strategies. Originally, I got aware of this challenge through a video by [one of my favorite](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ) YouTube channels and became slightly addicted from that moment on. \n\n## The Game\n![](images/halite_game.png)\n\nWhile the complete rule set of Halite can be viewed in [their documentation](https://halite.io/learn-programming-challenge/), I only want to explain very basics here. In Halite you play in a space scenario comprising _ships_ and _planets_, while you (= your bot) controls your ships. A ship can do three actions: _move_, _dock_ to or _undock_ from a planet. The more ships you have docked at a planet, the faster you are _mining_ the planet, which means to produce new ships. When two ships get close enough, they can fight and only the winner's ship survives. The game is turn-based, so each of the up to four players' programs are queried (by the game environment) for a list of moves for each of their ships in every turn. Input is the current game state (player- & planet positions, ships' health, ships' current status, ...) and output is a move for each ship. The final goal is to either completely dominate (= destroy every other players' ships) or own the strongest ship fleet and the most planets after 300 turns. \n\nAt the time of writing this article, the leaderboard comprises __~ 4700 players__ from __98 countries__. Most of them are either university students or professionals, who have, in total, played __10.9 million__ games. More interesting statistics can be found at the [stats](http://stats.halite.io:3000/public/dashboard/545ebc3c-4cdb-4940-acf1-e4d1332defac) page. \n\n## My bot\nFirst of all, what I found especially cool is the fact that players are completely free in their choice of how to realize their bot (hard-coding, machine learning, ...) and what programming language to use. Halite offers community-created starter templates for C++, C#, Dart, Elixir, Go, Haskell, JavaScript, Julia, Kotlin, PHP, Ruby, Rust, Scala, Swift and more. You can choose whatever language you like - which was __Java__ for me - and eventually submit a ZIP file with your code or binary, as well as a script for executing it, to their website. The interface between your program and the game environment is _stdin_ / _stdout_. \n\n![](images/halite_langs.png)\n\nI decided to build my bot based on rather simple rules first, which I figured out by watching some other players' replays. Probably applying machine learning to solve Halite would be even more challenging, but at the time I got aware of this competition, it was about to last only three more weeks (until January, 22nd), so I picked up on a rather simple approach.\n\n## Strategies\nI developed three different strategies for playing Halite, while my bot could dynamically switch between them during the game. The first one is called the __BalancedStrategy__. It's a fair mixture of conquering new planets, raising ship production and destroying enemy ships. In this strategy, a ship's most preferable goal is to take empty planets. However, if the next empty planet is unavailable ot too far away, it may also dock at a planet my bot already owns in order to increase production. However, no more than three of my own ships will ever dock at the same planet. If both the next empty planet as well as the next own planet are unavailable or too far away, the ship starts chasing the nearest enemy ship. \n\nIn addition to this compromise-like strategy, I found that more extreme ones can be successful, too. For instance, in a 1 vs. 1 match, my opponents usually won because they almost instantly destroyed all of my ships in the very beginning. So I decided to further introduce the __AggressiveStrategy__, which is only applied in matched of two players. Following this strategy, all of my three initial ships immediately start chasing the enemy's three ships. If they're successful, the game is usually over after only a few turns. \n\nFinally I built the __MiningStrategy__. Goal is to maxmimize mining rate right in the beginning. My three ships are immediately docking at the nearest and largest (in terms of how many ships can dock at it) free planet and dock at it. New ships will dock at the same planet until it's full. Doing so, I can quickly produce new ships which then, after the first planet is full, switch over to applying the __BalancedStrategy__. \n\n## Results\nUsing the bot described above, the best rank I achieved in the leaderboard was __250 of ~ 4500__ (top 6 %) and I'm still ambitious to get even better üòÉ. My bot is playing as [n1try](https://halite.io/user/?user_id=7481).\n\n\\>> [Source code on GitHub](https://github.com/muety/halite-bot-java). ","source":"_posts/halite-a-rule-based-ai-bot.md","raw":"---\ntitle: Halite - A rule-based AI bot\ndate: 2018-01-03 08:04:08\ntags:\n---\n\nAfter having spent a considerable amount of time with it last weekend, I wanted to make a short comment on [Halite.io](https://halite.io). Halite is a programming- and AI competition where people can write programs or train algorithms to control a bot that plays in a virtual 2D game environment. There is a leaderboard to track how your bot competes with other players' bots and you can watch a replay of every game your bot has played, which helps _debugging_ your bot as well as figuring out other people's strategies. Originally, I got aware of this challenge through a video by [one of my favorite](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ) YouTube channels and became slightly addicted from that moment on. \n\n## The Game\n![](images/halite_game.png)\n\nWhile the complete rule set of Halite can be viewed in [their documentation](https://halite.io/learn-programming-challenge/), I only want to explain very basics here. In Halite you play in a space scenario comprising _ships_ and _planets_, while you (= your bot) controls your ships. A ship can do three actions: _move_, _dock_ to or _undock_ from a planet. The more ships you have docked at a planet, the faster you are _mining_ the planet, which means to produce new ships. When two ships get close enough, they can fight and only the winner's ship survives. The game is turn-based, so each of the up to four players' programs are queried (by the game environment) for a list of moves for each of their ships in every turn. Input is the current game state (player- & planet positions, ships' health, ships' current status, ...) and output is a move for each ship. The final goal is to either completely dominate (= destroy every other players' ships) or own the strongest ship fleet and the most planets after 300 turns. \n\nAt the time of writing this article, the leaderboard comprises __~ 4700 players__ from __98 countries__. Most of them are either university students or professionals, who have, in total, played __10.9 million__ games. More interesting statistics can be found at the [stats](http://stats.halite.io:3000/public/dashboard/545ebc3c-4cdb-4940-acf1-e4d1332defac) page. \n\n## My bot\nFirst of all, what I found especially cool is the fact that players are completely free in their choice of how to realize their bot (hard-coding, machine learning, ...) and what programming language to use. Halite offers community-created starter templates for C++, C#, Dart, Elixir, Go, Haskell, JavaScript, Julia, Kotlin, PHP, Ruby, Rust, Scala, Swift and more. You can choose whatever language you like - which was __Java__ for me - and eventually submit a ZIP file with your code or binary, as well as a script for executing it, to their website. The interface between your program and the game environment is _stdin_ / _stdout_. \n\n![](images/halite_langs.png)\n\nI decided to build my bot based on rather simple rules first, which I figured out by watching some other players' replays. Probably applying machine learning to solve Halite would be even more challenging, but at the time I got aware of this competition, it was about to last only three more weeks (until January, 22nd), so I picked up on a rather simple approach.\n\n## Strategies\nI developed three different strategies for playing Halite, while my bot could dynamically switch between them during the game. The first one is called the __BalancedStrategy__. It's a fair mixture of conquering new planets, raising ship production and destroying enemy ships. In this strategy, a ship's most preferable goal is to take empty planets. However, if the next empty planet is unavailable ot too far away, it may also dock at a planet my bot already owns in order to increase production. However, no more than three of my own ships will ever dock at the same planet. If both the next empty planet as well as the next own planet are unavailable or too far away, the ship starts chasing the nearest enemy ship. \n\nIn addition to this compromise-like strategy, I found that more extreme ones can be successful, too. For instance, in a 1 vs. 1 match, my opponents usually won because they almost instantly destroyed all of my ships in the very beginning. So I decided to further introduce the __AggressiveStrategy__, which is only applied in matched of two players. Following this strategy, all of my three initial ships immediately start chasing the enemy's three ships. If they're successful, the game is usually over after only a few turns. \n\nFinally I built the __MiningStrategy__. Goal is to maxmimize mining rate right in the beginning. My three ships are immediately docking at the nearest and largest (in terms of how many ships can dock at it) free planet and dock at it. New ships will dock at the same planet until it's full. Doing so, I can quickly produce new ships which then, after the first planet is full, switch over to applying the __BalancedStrategy__. \n\n## Results\nUsing the bot described above, the best rank I achieved in the leaderboard was __250 of ~ 4500__ (top 6 %) and I'm still ambitious to get even better üòÉ. My bot is playing as [n1try](https://halite.io/user/?user_id=7481).\n\n\\>> [Source code on GitHub](https://github.com/muety/halite-bot-java). ","slug":"halite-a-rule-based-ai-bot","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhi2000e40mqb7uvjpx8","content":"<p>After having spent a considerable amount of time with it last weekend, I wanted to make a short comment on <a href=\"https://halite.io\" target=\"_blank\" rel=\"noopener\">Halite.io</a>. Halite is a programming- and AI competition where people can write programs or train algorithms to control a bot that plays in a virtual 2D game environment. There is a leaderboard to track how your bot competes with other players‚Äô bots and you can watch a replay of every game your bot has played, which helps <em>debugging</em> your bot as well as figuring out other people‚Äôs strategies. Originally, I got aware of this challenge through a video by <a href=\"https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ\" target=\"_blank\" rel=\"noopener\">one of my favorite</a> YouTube channels and became slightly addicted from that moment on.</p>\n<h2 id=\"The-Game\">The Game</h2>\n<p><img src=\"images/halite_game.png\" alt></p>\n<p>While the complete rule set of Halite can be viewed in <a href=\"https://halite.io/learn-programming-challenge/\" target=\"_blank\" rel=\"noopener\">their documentation</a>, I only want to explain very basics here. In Halite you play in a space scenario comprising <em>ships</em> and <em>planets</em>, while you (= your bot) controls your ships. A ship can do three actions: <em>move</em>, <em>dock</em> to or <em>undock</em> from a planet. The more ships you have docked at a planet, the faster you are <em>mining</em> the planet, which means to produce new ships. When two ships get close enough, they can fight and only the winner‚Äôs ship survives. The game is turn-based, so each of the up to four players‚Äô programs are queried (by the game environment) for a list of moves for each of their ships in every turn. Input is the current game state (player- &amp; planet positions, ships‚Äô health, ships‚Äô current status, ‚Ä¶) and output is a move for each ship. The final goal is to either completely dominate (= destroy every other players‚Äô ships) or own the strongest ship fleet and the most planets after 300 turns.</p>\n<p>At the time of writing this article, the leaderboard comprises <strong>~ 4700 players</strong> from <strong>98 countries</strong>. Most of them are either university students or professionals, who have, in total, played <strong>10.9 million</strong> games. More interesting statistics can be found at the <a href=\"http://stats.halite.io:3000/public/dashboard/545ebc3c-4cdb-4940-acf1-e4d1332defac\" target=\"_blank\" rel=\"noopener\">stats</a> page.</p>\n<h2 id=\"My-bot\">My bot</h2>\n<p>First of all, what I found especially cool is the fact that players are completely free in their choice of how to realize their bot (hard-coding, machine learning, ‚Ä¶) and what programming language to use. Halite offers community-created starter templates for C++, C#, Dart, Elixir, Go, Haskell, JavaScript, Julia, Kotlin, PHP, Ruby, Rust, Scala, Swift and more. You can choose whatever language you like - which was <strong>Java</strong> for me - and eventually submit a ZIP file with your code or binary, as well as a script for executing it, to their website. The interface between your program and the game environment is <em>stdin</em> / <em>stdout</em>.</p>\n<p><img src=\"images/halite_langs.png\" alt></p>\n<p>I decided to build my bot based on rather simple rules first, which I figured out by watching some other players‚Äô replays. Probably applying machine learning to solve Halite would be even more challenging, but at the time I got aware of this competition, it was about to last only three more weeks (until January, 22nd), so I picked up on a rather simple approach.</p>\n<h2 id=\"Strategies\">Strategies</h2>\n<p>I developed three different strategies for playing Halite, while my bot could dynamically switch between them during the game. The first one is called the <strong>BalancedStrategy</strong>. It‚Äôs a fair mixture of conquering new planets, raising ship production and destroying enemy ships. In this strategy, a ship‚Äôs most preferable goal is to take empty planets. However, if the next empty planet is unavailable ot too far away, it may also dock at a planet my bot already owns in order to increase production. However, no more than three of my own ships will ever dock at the same planet. If both the next empty planet as well as the next own planet are unavailable or too far away, the ship starts chasing the nearest enemy ship.</p>\n<p>In addition to this compromise-like strategy, I found that more extreme ones can be successful, too. For instance, in a 1 vs. 1 match, my opponents usually won because they almost instantly destroyed all of my ships in the very beginning. So I decided to further introduce the <strong>AggressiveStrategy</strong>, which is only applied in matched of two players. Following this strategy, all of my three initial ships immediately start chasing the enemy‚Äôs three ships. If they‚Äôre successful, the game is usually over after only a few turns.</p>\n<p>Finally I built the <strong>MiningStrategy</strong>. Goal is to maxmimize mining rate right in the beginning. My three ships are immediately docking at the nearest and largest (in terms of how many ships can dock at it) free planet and dock at it. New ships will dock at the same planet until it‚Äôs full. Doing so, I can quickly produce new ships which then, after the first planet is full, switch over to applying the <strong>BalancedStrategy</strong>.</p>\n<h2 id=\"Results\">Results</h2>\n<p>Using the bot described above, the best rank I achieved in the leaderboard was <strong>250 of ~ 4500</strong> (top 6 %) and I‚Äôm still ambitious to get even better üòÉ. My bot is playing as <a href=\"https://halite.io/user/?user_id=7481\" target=\"_blank\" rel=\"noopener\">n1try</a>.</p>\n<p>&gt;&gt; <a href=\"https://github.com/muety/halite-bot-java\" target=\"_blank\" rel=\"noopener\">Source code on GitHub</a>.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>After having spent a considerable amount of time with it last weekend, I wanted to make a short comment on <a href=\"https://halite.io\" target=\"_blank\" rel=\"noopener\">Halite.io</a>. Halite is a programming- and AI competition where people can write programs or train algorithms to control a bot that plays in a virtual 2D game environment. There is a leaderboard to track how your bot competes with other players‚Äô bots and you can watch a replay of every game your bot has played, which helps <em>debugging</em> your bot as well as figuring out other people‚Äôs strategies. Originally, I got aware of this challenge through a video by <a href=\"https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ\" target=\"_blank\" rel=\"noopener\">one of my favorite</a> YouTube channels and became slightly addicted from that moment on.</p>\n<h2 id=\"The-Game\">The Game</h2>\n<p><img src=\"images/halite_game.png\" alt></p>\n<p>While the complete rule set of Halite can be viewed in <a href=\"https://halite.io/learn-programming-challenge/\" target=\"_blank\" rel=\"noopener\">their documentation</a>, I only want to explain very basics here. In Halite you play in a space scenario comprising <em>ships</em> and <em>planets</em>, while you (= your bot) controls your ships. A ship can do three actions: <em>move</em>, <em>dock</em> to or <em>undock</em> from a planet. The more ships you have docked at a planet, the faster you are <em>mining</em> the planet, which means to produce new ships. When two ships get close enough, they can fight and only the winner‚Äôs ship survives. The game is turn-based, so each of the up to four players‚Äô programs are queried (by the game environment) for a list of moves for each of their ships in every turn. Input is the current game state (player- &amp; planet positions, ships‚Äô health, ships‚Äô current status, ‚Ä¶) and output is a move for each ship. The final goal is to either completely dominate (= destroy every other players‚Äô ships) or own the strongest ship fleet and the most planets after 300 turns.</p>\n<p>At the time of writing this article, the leaderboard comprises <strong>~ 4700 players</strong> from <strong>98 countries</strong>. Most of them are either university students or professionals, who have, in total, played <strong>10.9 million</strong> games. More interesting statistics can be found at the <a href=\"http://stats.halite.io:3000/public/dashboard/545ebc3c-4cdb-4940-acf1-e4d1332defac\" target=\"_blank\" rel=\"noopener\">stats</a> page.</p>\n<h2 id=\"My-bot\">My bot</h2>\n<p>First of all, what I found especially cool is the fact that players are completely free in their choice of how to realize their bot (hard-coding, machine learning, ‚Ä¶) and what programming language to use. Halite offers community-created starter templates for C++, C#, Dart, Elixir, Go, Haskell, JavaScript, Julia, Kotlin, PHP, Ruby, Rust, Scala, Swift and more. You can choose whatever language you like - which was <strong>Java</strong> for me - and eventually submit a ZIP file with your code or binary, as well as a script for executing it, to their website. The interface between your program and the game environment is <em>stdin</em> / <em>stdout</em>.</p>\n<p><img src=\"images/halite_langs.png\" alt></p>\n<p>I decided to build my bot based on rather simple rules first, which I figured out by watching some other players‚Äô replays. Probably applying machine learning to solve Halite would be even more challenging, but at the time I got aware of this competition, it was about to last only three more weeks (until January, 22nd), so I picked up on a rather simple approach.</p>\n<h2 id=\"Strategies\">Strategies</h2>\n<p>I developed three different strategies for playing Halite, while my bot could dynamically switch between them during the game. The first one is called the <strong>BalancedStrategy</strong>. It‚Äôs a fair mixture of conquering new planets, raising ship production and destroying enemy ships. In this strategy, a ship‚Äôs most preferable goal is to take empty planets. However, if the next empty planet is unavailable ot too far away, it may also dock at a planet my bot already owns in order to increase production. However, no more than three of my own ships will ever dock at the same planet. If both the next empty planet as well as the next own planet are unavailable or too far away, the ship starts chasing the nearest enemy ship.</p>\n<p>In addition to this compromise-like strategy, I found that more extreme ones can be successful, too. For instance, in a 1 vs. 1 match, my opponents usually won because they almost instantly destroyed all of my ships in the very beginning. So I decided to further introduce the <strong>AggressiveStrategy</strong>, which is only applied in matched of two players. Following this strategy, all of my three initial ships immediately start chasing the enemy‚Äôs three ships. If they‚Äôre successful, the game is usually over after only a few turns.</p>\n<p>Finally I built the <strong>MiningStrategy</strong>. Goal is to maxmimize mining rate right in the beginning. My three ships are immediately docking at the nearest and largest (in terms of how many ships can dock at it) free planet and dock at it. New ships will dock at the same planet until it‚Äôs full. Doing so, I can quickly produce new ships which then, after the first planet is full, switch over to applying the <strong>BalancedStrategy</strong>.</p>\n<h2 id=\"Results\">Results</h2>\n<p>Using the bot described above, the best rank I achieved in the leaderboard was <strong>250 of ~ 4500</strong> (top 6 %) and I‚Äôm still ambitious to get even better üòÉ. My bot is playing as <a href=\"https://halite.io/user/?user_id=7481\" target=\"_blank\" rel=\"noopener\">n1try</a>.</p>\n<p>&gt;&gt; <a href=\"https://github.com/muety/halite-bot-java\" target=\"_blank\" rel=\"noopener\">Source code on GitHub</a>.</p>\n"},{"title":"How to enable DNS-over-TLS on Ubuntu using CoreDNS","date":"2020-04-11T18:41:57.000Z","description":"This article describes, how to browse the web more privately using DNS-over-TLS. Therefore, it is shown how to set up CoreDNS on a Ubuntu machine.","_content":"\n# Privacy on the Web\nLuckily, most traffic on the web is encrypted today, which means nobody between your computer and the web server knows what you are sending or receiving. This includes your internet service provider (ISP), any kind of government agency or a potential attacker on your network. Since the entire HTTP packet, including its headers, is encrypted, they will not even see what website you are visiting. At least not for sure. What they can see is the target web server's IP address from the IP packet's header. However, there might be several different web servers for different web sites listening on that IP and there is no chance to find out which one you intended to visit.\n\n# The Problem with DNS\nHowever, although HTTP is usually encrypted, DNS is usually not. So before your browser performs the actual HTTP request, your operating system will perform a DNS query to resolve, for instance, _\"google.com\"_ to `216.58.207.46`. Your question ‚Äì _\"What's the IP for google.com?\"_ ‚Äì is contained in the DNS query as plain text, so everyone between your computer and the DNS server will know that you are trying to access Google ‚Äì or whatever website. And, of course, the provider of your DNS server will know as well, since it has to answer the query.\n\nUsually, your default DNS server is the one provided by your ISP. And since the ISP can directly associate your internet connection with your name and address it will know about any website that you ‚Äì as a person ‚Äì visit. Even if you change the default to something else (e.g. [Google's public DNS resolver](https://developers.google.com/speed/public-dns/) 8.8.8.8 or [CloudFlare's](https://1.1.1.1/) `1.1.1.1`), your ISP can still read your DNS queries as they mandatorily pass through its network. Consequently, in order to browse more privately on the web ‚Äì in addition to using HTTPS - there are two steps you need to consider:\n\n1. Change your DNS provider to one that is more anonymous and does not have personal information about you\n2. Encrypt your DNS queries to prevent anyone in the middle (especially your ISP) from reading them\n\n![Example of a non-encrypted DNS request](images/dns1.png)\nExample of a non-encrypted DNS request for `kit.edu` to Google's `8.8.8.8` DNS resolver \n\nLuckily, the [DNS-over-TLS](https://en.wikipedia.org/wiki/DNS_over_TLS) specification already provides a solution and it is already supported by the three largest public DNS providers [CloudFlare](https://1.1.1.1/), [Google](https://developers.google.com/speed/public-dns/) and [Quad9](https://www.quad9.net/). You only have to configure your computer to use it.\n\n# CoreDNS Setup\nIn this article, I show you how to use DNS-over-TLS with [CoreDNS](https://coredns.io/) as a local DNS recursor on your machine. It is an open-source software and primarily known for being used as a nameserver in Kubernetes networks. Please note that I decided to use CoreDNS, because it is particularly easy to configure and offers a variety of cool [plugins](https://coredns.io/plugins/), like [metrics collection with Prometheus](https://coredns.io/plugins/metrics/) and more. However, Ubuntu's default DNS recursor [systemd-resolved apparently supports DNS-over-TLS](https://www.internetsociety.org/blog/2018/12/dns-privacy-in-linux-systemd/) as well and is probably easier to get started with initially. So if you prefer to go the easy way, just head over to the previously mentioned blog post and follow its instructions.\n\n**Disclaimer:** Please use this guide at your own risk. I do not take any responsibility in case you accidentally crash your DNS setup.\n\nIn order to set up CoreDNS, there are a few steps to follow.\n\n1. Download CoreDNS from the [website](https://coredns.io), unpack the binary to `/usr/local/bin` and make it executable (`sudo chmod +x /usr/local/bin/coredns`)\n2. Install `resolvconf` as a tool to manually manage `/etc/resolv.conf`: `sudo apt install resolvconf`  \n3. Set `dns=default` in `/etc/NetworkManager/NetworkManager.conf`\n4. Add `nameserver 127.0.0.1` to `/etc/resolvconf/resolv.conf.d/head`\n5. Create `/etc/coredns/Corefile` and paste the configuration shown below. In this example, we are using CloudFlare as a DNS provider. You can use Google or Quad9 as well, just change the IPs.\n6. Create a new user for CoreDNS: `sudo useradd -d /var/lib/coredns -m coredns`\n7. Set some permissions: `sudo chown coredns:coredns /opt/coredns`\n8. Download the SystemD service unit file from [coredns/coredns](https://github.com/coredns/deployment/tree/master/systemd) to `/etc/systemd/system/coredns.service`\n9. Disable SystemD's default DNS server: `sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved`\n  1. **Please Note:** From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again\n10. Enable and start CoreDNS: `sudo systemctl enable coredns && sudo systemctl start coredns`\n11. You should be able to resolve domain names, again. E.g. try `dig +short kit.edu`. If an IP address is printed, everything works fine.\n\n```\n# /etc/coredns/Corefile\n\n.:53 {\n    forward . tls://2606:4700:4700::1111 tls://1.1.1.1\n    log\n    errors\n    cache\n}\n```\n\n![Example of an encrypted DNS request](images/dns2.png)\nExample of an encrypted DNS request for `kit.edu` to CloudFlare's `1.1.1.1` DNS resolver\n\nAlternatively, use the following `forward` statement to use the independent [BlahDNS](https://blahdns.com) instead of CloudFlare as provider.\n\n```\nforward . tls://2a01:4f8:1c1c:6b4b::1 tls://159.69.198.101 {\n    tls_servername dot-de.blahdns.com\n}\n```\n\n# What happens?\nLet us examine what happens (in terms of DNS) when you type _\"kit.edu\"_ in your browser's address bar and hit enter.\n\n1. Your browser asks your operating system to resolve `kit.edu`\n2. Your OS finds out that its primary configured nameserver is `127.0.0.1:53`, i.e. your local CoreDNS, and consults that one\n3. CoreDNS checks its cache and in case of a miss consults its configured nameserver at `1.1.1.1`, i.e. CloudFlare\n4. CloudFlare, again, checks its cache and in case of a miss goes up the hierarchical chain of nameservers until one of them has an answer\n5. Eventually, your browser performs `GET / HTTP/2.0` to `129.13.40.10` with `Host: kit.edu`\n\n# Further Reading\nHere are a few additional posts about DNS, which I found very useful.\n\n* [What Is DNS? | How DNS Works](https://www.cloudflare.com/learning/dns/what-is-dns/)\n* [DNS-over-https vs. DNS-over-TLS vs DNSCrypt](https://www.reddit.com/r/privacy/comments/89pr15/dnsoverhttps_vs_dns_overtls_vs_dnscrypt/dwsosjr?utm_source=share&utm_medium=web2x)\n\nPlease let me know if my guide is missing any required steps. Good luck, have fun and browse safely!","source":"_posts/how-to-enable-dns-over-tls-on-ubuntu-using-coredns.md","raw":"---\ntitle: How to enable DNS-over-TLS on Ubuntu using CoreDNS\ndate: 2020-04-11 20:41:57\ntags:\ndescription: This article describes, how to browse the web more privately using DNS-over-TLS. Therefore, it is shown how to set up CoreDNS on a Ubuntu machine.\n---\n\n# Privacy on the Web\nLuckily, most traffic on the web is encrypted today, which means nobody between your computer and the web server knows what you are sending or receiving. This includes your internet service provider (ISP), any kind of government agency or a potential attacker on your network. Since the entire HTTP packet, including its headers, is encrypted, they will not even see what website you are visiting. At least not for sure. What they can see is the target web server's IP address from the IP packet's header. However, there might be several different web servers for different web sites listening on that IP and there is no chance to find out which one you intended to visit.\n\n# The Problem with DNS\nHowever, although HTTP is usually encrypted, DNS is usually not. So before your browser performs the actual HTTP request, your operating system will perform a DNS query to resolve, for instance, _\"google.com\"_ to `216.58.207.46`. Your question ‚Äì _\"What's the IP for google.com?\"_ ‚Äì is contained in the DNS query as plain text, so everyone between your computer and the DNS server will know that you are trying to access Google ‚Äì or whatever website. And, of course, the provider of your DNS server will know as well, since it has to answer the query.\n\nUsually, your default DNS server is the one provided by your ISP. And since the ISP can directly associate your internet connection with your name and address it will know about any website that you ‚Äì as a person ‚Äì visit. Even if you change the default to something else (e.g. [Google's public DNS resolver](https://developers.google.com/speed/public-dns/) 8.8.8.8 or [CloudFlare's](https://1.1.1.1/) `1.1.1.1`), your ISP can still read your DNS queries as they mandatorily pass through its network. Consequently, in order to browse more privately on the web ‚Äì in addition to using HTTPS - there are two steps you need to consider:\n\n1. Change your DNS provider to one that is more anonymous and does not have personal information about you\n2. Encrypt your DNS queries to prevent anyone in the middle (especially your ISP) from reading them\n\n![Example of a non-encrypted DNS request](images/dns1.png)\nExample of a non-encrypted DNS request for `kit.edu` to Google's `8.8.8.8` DNS resolver \n\nLuckily, the [DNS-over-TLS](https://en.wikipedia.org/wiki/DNS_over_TLS) specification already provides a solution and it is already supported by the three largest public DNS providers [CloudFlare](https://1.1.1.1/), [Google](https://developers.google.com/speed/public-dns/) and [Quad9](https://www.quad9.net/). You only have to configure your computer to use it.\n\n# CoreDNS Setup\nIn this article, I show you how to use DNS-over-TLS with [CoreDNS](https://coredns.io/) as a local DNS recursor on your machine. It is an open-source software and primarily known for being used as a nameserver in Kubernetes networks. Please note that I decided to use CoreDNS, because it is particularly easy to configure and offers a variety of cool [plugins](https://coredns.io/plugins/), like [metrics collection with Prometheus](https://coredns.io/plugins/metrics/) and more. However, Ubuntu's default DNS recursor [systemd-resolved apparently supports DNS-over-TLS](https://www.internetsociety.org/blog/2018/12/dns-privacy-in-linux-systemd/) as well and is probably easier to get started with initially. So if you prefer to go the easy way, just head over to the previously mentioned blog post and follow its instructions.\n\n**Disclaimer:** Please use this guide at your own risk. I do not take any responsibility in case you accidentally crash your DNS setup.\n\nIn order to set up CoreDNS, there are a few steps to follow.\n\n1. Download CoreDNS from the [website](https://coredns.io), unpack the binary to `/usr/local/bin` and make it executable (`sudo chmod +x /usr/local/bin/coredns`)\n2. Install `resolvconf` as a tool to manually manage `/etc/resolv.conf`: `sudo apt install resolvconf`  \n3. Set `dns=default` in `/etc/NetworkManager/NetworkManager.conf`\n4. Add `nameserver 127.0.0.1` to `/etc/resolvconf/resolv.conf.d/head`\n5. Create `/etc/coredns/Corefile` and paste the configuration shown below. In this example, we are using CloudFlare as a DNS provider. You can use Google or Quad9 as well, just change the IPs.\n6. Create a new user for CoreDNS: `sudo useradd -d /var/lib/coredns -m coredns`\n7. Set some permissions: `sudo chown coredns:coredns /opt/coredns`\n8. Download the SystemD service unit file from [coredns/coredns](https://github.com/coredns/deployment/tree/master/systemd) to `/etc/systemd/system/coredns.service`\n9. Disable SystemD's default DNS server: `sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved`\n  1. **Please Note:** From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again\n10. Enable and start CoreDNS: `sudo systemctl enable coredns && sudo systemctl start coredns`\n11. You should be able to resolve domain names, again. E.g. try `dig +short kit.edu`. If an IP address is printed, everything works fine.\n\n```\n# /etc/coredns/Corefile\n\n.:53 {\n    forward . tls://2606:4700:4700::1111 tls://1.1.1.1\n    log\n    errors\n    cache\n}\n```\n\n![Example of an encrypted DNS request](images/dns2.png)\nExample of an encrypted DNS request for `kit.edu` to CloudFlare's `1.1.1.1` DNS resolver\n\nAlternatively, use the following `forward` statement to use the independent [BlahDNS](https://blahdns.com) instead of CloudFlare as provider.\n\n```\nforward . tls://2a01:4f8:1c1c:6b4b::1 tls://159.69.198.101 {\n    tls_servername dot-de.blahdns.com\n}\n```\n\n# What happens?\nLet us examine what happens (in terms of DNS) when you type _\"kit.edu\"_ in your browser's address bar and hit enter.\n\n1. Your browser asks your operating system to resolve `kit.edu`\n2. Your OS finds out that its primary configured nameserver is `127.0.0.1:53`, i.e. your local CoreDNS, and consults that one\n3. CoreDNS checks its cache and in case of a miss consults its configured nameserver at `1.1.1.1`, i.e. CloudFlare\n4. CloudFlare, again, checks its cache and in case of a miss goes up the hierarchical chain of nameservers until one of them has an answer\n5. Eventually, your browser performs `GET / HTTP/2.0` to `129.13.40.10` with `Host: kit.edu`\n\n# Further Reading\nHere are a few additional posts about DNS, which I found very useful.\n\n* [What Is DNS? | How DNS Works](https://www.cloudflare.com/learning/dns/what-is-dns/)\n* [DNS-over-https vs. DNS-over-TLS vs DNSCrypt](https://www.reddit.com/r/privacy/comments/89pr15/dnsoverhttps_vs_dns_overtls_vs_dnscrypt/dwsosjr?utm_source=share&utm_medium=web2x)\n\nPlease let me know if my guide is missing any required steps. Good luck, have fun and browse safely!","slug":"how-to-enable-dns-over-tls-on-ubuntu-using-coredns","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhi4000f40mqmqg154pl","content":"<h1>Privacy on the Web</h1>\n<p>Luckily, most traffic on the web is encrypted today, which means nobody between your computer and the web server knows what you are sending or receiving. This includes your internet service provider (ISP), any kind of government agency or a potential attacker on your network. Since the entire HTTP packet, including its headers, is encrypted, they will not even see what website you are visiting. At least not for sure. What they can see is the target web server‚Äôs IP address from the IP packet‚Äôs header. However, there might be several different web servers for different web sites listening on that IP and there is no chance to find out which one you intended to visit.</p>\n<h1>The Problem with DNS</h1>\n<p>However, although HTTP is usually encrypted, DNS is usually not. So before your browser performs the actual HTTP request, your operating system will perform a DNS query to resolve, for instance, <em>‚Äú<a href=\"http://google.com\" target=\"_blank\" rel=\"noopener\">google.com</a>‚Äù</em> to <code>216.58.207.46</code>. Your question ‚Äì <em>‚ÄúWhat‚Äôs the IP for <a href=\"http://google.com\" target=\"_blank\" rel=\"noopener\">google.com</a>?‚Äù</em> ‚Äì is contained in the DNS query as plain text, so everyone between your computer and the DNS server will know that you are trying to access Google ‚Äì or whatever website. And, of course, the provider of your DNS server will know as well, since it has to answer the query.</p>\n<p>Usually, your default DNS server is the one provided by your ISP. And since the ISP can directly associate your internet connection with your name and address it will know about any website that you ‚Äì as a person ‚Äì visit. Even if you change the default to something else (e.g. <a href=\"https://developers.google.com/speed/public-dns/\" target=\"_blank\" rel=\"noopener\">Google‚Äôs public DNS resolver</a> 8.8.8.8 or <a href=\"https://1.1.1.1/\" target=\"_blank\" rel=\"noopener\">CloudFlare‚Äôs</a> <code>1.1.1.1</code>), your ISP can still read your DNS queries as they mandatorily pass through its network. Consequently, in order to browse more privately on the web ‚Äì in addition to using HTTPS - there are two steps you need to consider:</p>\n<ol>\n<li>Change your DNS provider to one that is more anonymous and does not have personal information about you</li>\n<li>Encrypt your DNS queries to prevent anyone in the middle (especially your ISP) from reading them</li>\n</ol>\n<p><img src=\"images/dns1.png\" alt=\"Example of a non-encrypted DNS request\"><br>\nExample of a non-encrypted DNS request for <code>kit.edu</code> to Google‚Äôs <code>8.8.8.8</code> DNS resolver</p>\n<p>Luckily, the <a href=\"https://en.wikipedia.org/wiki/DNS_over_TLS\" target=\"_blank\" rel=\"noopener\">DNS-over-TLS</a> specification already provides a solution and it is already supported by the three largest public DNS providers <a href=\"https://1.1.1.1/\" target=\"_blank\" rel=\"noopener\">CloudFlare</a>, <a href=\"https://developers.google.com/speed/public-dns/\" target=\"_blank\" rel=\"noopener\">Google</a> and <a href=\"https://www.quad9.net/\" target=\"_blank\" rel=\"noopener\">Quad9</a>. You only have to configure your computer to use it.</p>\n<h1>CoreDNS Setup</h1>\n<p>In this article, I show you how to use DNS-over-TLS with <a href=\"https://coredns.io/\" target=\"_blank\" rel=\"noopener\">CoreDNS</a> as a local DNS recursor on your machine. It is an open-source software and primarily known for being used as a nameserver in Kubernetes networks. Please note that I decided to use CoreDNS, because it is particularly easy to configure and offers a variety of cool <a href=\"https://coredns.io/plugins/\" target=\"_blank\" rel=\"noopener\">plugins</a>, like <a href=\"https://coredns.io/plugins/metrics/\" target=\"_blank\" rel=\"noopener\">metrics collection with Prometheus</a> and more. However, Ubuntu‚Äôs default DNS recursor <a href=\"https://www.internetsociety.org/blog/2018/12/dns-privacy-in-linux-systemd/\" target=\"_blank\" rel=\"noopener\">systemd-resolved apparently supports DNS-over-TLS</a> as well and is probably easier to get started with initially. So if you prefer to go the easy way, just head over to the previously mentioned blog post and follow its instructions.</p>\n<p><strong>Disclaimer:</strong> Please use this guide at your own risk. I do not take any responsibility in case you accidentally crash your DNS setup.</p>\n<p>In order to set up CoreDNS, there are a few steps to follow.</p>\n<ol>\n<li>Download CoreDNS from the <a href=\"https://coredns.io\" target=\"_blank\" rel=\"noopener\">website</a>, unpack the binary to <code>/usr/local/bin</code> and make it executable (<code>sudo chmod +x /usr/local/bin/coredns</code>)</li>\n<li>Install <code>resolvconf</code> as a tool to manually manage <code>/etc/resolv.conf</code>: <code>sudo apt install resolvconf</code></li>\n<li>Set <code>dns=default</code> in <code>/etc/NetworkManager/NetworkManager.conf</code></li>\n<li>Add <code>nameserver 127.0.0.1</code> to <code>/etc/resolvconf/resolv.conf.d/head</code></li>\n<li>Create <code>/etc/coredns/Corefile</code> and paste the configuration shown below. In this example, we are using CloudFlare as a DNS provider. You can use Google or Quad9 as well, just change the IPs.</li>\n<li>Create a new user for CoreDNS: <code>sudo useradd -d /var/lib/coredns -m coredns</code></li>\n<li>Set some permissions: <code>sudo chown coredns:coredns /opt/coredns</code></li>\n<li>Download the SystemD service unit file from <a href=\"https://github.com/coredns/deployment/tree/master/systemd\" target=\"_blank\" rel=\"noopener\">coredns/coredns</a> to <code>/etc/systemd/system/coredns.service</code></li>\n<li>Disable SystemD‚Äôs default DNS server: <code>sudo systemctl stop systemd-resolved &amp;&amp; sudo systemctl disable systemd-resolved</code></li>\n<li><strong>Please Note:</strong> From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again</li>\n<li>Enable and start CoreDNS: <code>sudo systemctl enable coredns &amp;&amp; sudo systemctl start coredns</code></li>\n<li>You should be able to resolve domain names, again. E.g. try <code>dig +short kit.edu</code>. If an IP address is printed, everything works fine.</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># /etc/coredns/Corefile</span><br><span class=\"line\"></span><br><span class=\"line\">.:53 &#123;</span><br><span class=\"line\">    forward . tls://2606:4700:4700::1111 tls://1.1.1.1</span><br><span class=\"line\">    log</span><br><span class=\"line\">    errors</span><br><span class=\"line\">    cache</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"images/dns2.png\" alt=\"Example of an encrypted DNS request\"><br>\nExample of an encrypted DNS request for <code>kit.edu</code> to CloudFlare‚Äôs <code>1.1.1.1</code> DNS resolver</p>\n<p>Alternatively, use the following <code>forward</code> statement to use the independent <a href=\"https://blahdns.com\" target=\"_blank\" rel=\"noopener\">BlahDNS</a> instead of CloudFlare as provider.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">forward . tls://2a01:4f8:1c1c:6b4b::1 tls://159.69.198.101 &#123;</span><br><span class=\"line\">    tls_servername dot-de.blahdns.com</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1>What happens?</h1>\n<p>Let us examine what happens (in terms of DNS) when you type <em>‚Äú<a href=\"http://kit.edu\" target=\"_blank\" rel=\"noopener\">kit.edu</a>‚Äù</em> in your browser‚Äôs address bar and hit enter.</p>\n<ol>\n<li>Your browser asks your operating system to resolve <code>kit.edu</code></li>\n<li>Your OS finds out that its primary configured nameserver is <code>127.0.0.1:53</code>, i.e. your local CoreDNS, and consults that one</li>\n<li>CoreDNS checks its cache and in case of a miss consults its configured nameserver at <code>1.1.1.1</code>, i.e. CloudFlare</li>\n<li>CloudFlare, again, checks its cache and in case of a miss goes up the hierarchical chain of nameservers until one of them has an answer</li>\n<li>Eventually, your browser performs <code>GET / HTTP/2.0</code> to <code>129.13.40.10</code> with <code>Host: kit.edu</code></li>\n</ol>\n<h1>Further Reading</h1>\n<p>Here are a few additional posts about DNS, which I found very useful.</p>\n<ul>\n<li><a href=\"https://www.cloudflare.com/learning/dns/what-is-dns/\" target=\"_blank\" rel=\"noopener\">What Is DNS? | How DNS Works</a></li>\n<li><a href=\"https://www.reddit.com/r/privacy/comments/89pr15/dnsoverhttps_vs_dns_overtls_vs_dnscrypt/dwsosjr?utm_source=share&amp;utm_medium=web2x\" target=\"_blank\" rel=\"noopener\">DNS-over-https vs. DNS-over-TLS vs DNSCrypt</a></li>\n</ul>\n<p>Please let me know if my guide is missing any required steps. Good luck, have fun and browse safely!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1>Privacy on the Web</h1>\n<p>Luckily, most traffic on the web is encrypted today, which means nobody between your computer and the web server knows what you are sending or receiving. This includes your internet service provider (ISP), any kind of government agency or a potential attacker on your network. Since the entire HTTP packet, including its headers, is encrypted, they will not even see what website you are visiting. At least not for sure. What they can see is the target web server‚Äôs IP address from the IP packet‚Äôs header. However, there might be several different web servers for different web sites listening on that IP and there is no chance to find out which one you intended to visit.</p>\n<h1>The Problem with DNS</h1>\n<p>However, although HTTP is usually encrypted, DNS is usually not. So before your browser performs the actual HTTP request, your operating system will perform a DNS query to resolve, for instance, <em>‚Äú<a href=\"http://google.com\" target=\"_blank\" rel=\"noopener\">google.com</a>‚Äù</em> to <code>216.58.207.46</code>. Your question ‚Äì <em>‚ÄúWhat‚Äôs the IP for <a href=\"http://google.com\" target=\"_blank\" rel=\"noopener\">google.com</a>?‚Äù</em> ‚Äì is contained in the DNS query as plain text, so everyone between your computer and the DNS server will know that you are trying to access Google ‚Äì or whatever website. And, of course, the provider of your DNS server will know as well, since it has to answer the query.</p>\n<p>Usually, your default DNS server is the one provided by your ISP. And since the ISP can directly associate your internet connection with your name and address it will know about any website that you ‚Äì as a person ‚Äì visit. Even if you change the default to something else (e.g. <a href=\"https://developers.google.com/speed/public-dns/\" target=\"_blank\" rel=\"noopener\">Google‚Äôs public DNS resolver</a> 8.8.8.8 or <a href=\"https://1.1.1.1/\" target=\"_blank\" rel=\"noopener\">CloudFlare‚Äôs</a> <code>1.1.1.1</code>), your ISP can still read your DNS queries as they mandatorily pass through its network. Consequently, in order to browse more privately on the web ‚Äì in addition to using HTTPS - there are two steps you need to consider:</p>\n<ol>\n<li>Change your DNS provider to one that is more anonymous and does not have personal information about you</li>\n<li>Encrypt your DNS queries to prevent anyone in the middle (especially your ISP) from reading them</li>\n</ol>\n<p><img src=\"images/dns1.png\" alt=\"Example of a non-encrypted DNS request\"><br>\nExample of a non-encrypted DNS request for <code>kit.edu</code> to Google‚Äôs <code>8.8.8.8</code> DNS resolver</p>\n<p>Luckily, the <a href=\"https://en.wikipedia.org/wiki/DNS_over_TLS\" target=\"_blank\" rel=\"noopener\">DNS-over-TLS</a> specification already provides a solution and it is already supported by the three largest public DNS providers <a href=\"https://1.1.1.1/\" target=\"_blank\" rel=\"noopener\">CloudFlare</a>, <a href=\"https://developers.google.com/speed/public-dns/\" target=\"_blank\" rel=\"noopener\">Google</a> and <a href=\"https://www.quad9.net/\" target=\"_blank\" rel=\"noopener\">Quad9</a>. You only have to configure your computer to use it.</p>\n<h1>CoreDNS Setup</h1>\n<p>In this article, I show you how to use DNS-over-TLS with <a href=\"https://coredns.io/\" target=\"_blank\" rel=\"noopener\">CoreDNS</a> as a local DNS recursor on your machine. It is an open-source software and primarily known for being used as a nameserver in Kubernetes networks. Please note that I decided to use CoreDNS, because it is particularly easy to configure and offers a variety of cool <a href=\"https://coredns.io/plugins/\" target=\"_blank\" rel=\"noopener\">plugins</a>, like <a href=\"https://coredns.io/plugins/metrics/\" target=\"_blank\" rel=\"noopener\">metrics collection with Prometheus</a> and more. However, Ubuntu‚Äôs default DNS recursor <a href=\"https://www.internetsociety.org/blog/2018/12/dns-privacy-in-linux-systemd/\" target=\"_blank\" rel=\"noopener\">systemd-resolved apparently supports DNS-over-TLS</a> as well and is probably easier to get started with initially. So if you prefer to go the easy way, just head over to the previously mentioned blog post and follow its instructions.</p>\n<p><strong>Disclaimer:</strong> Please use this guide at your own risk. I do not take any responsibility in case you accidentally crash your DNS setup.</p>\n<p>In order to set up CoreDNS, there are a few steps to follow.</p>\n<ol>\n<li>Download CoreDNS from the <a href=\"https://coredns.io\" target=\"_blank\" rel=\"noopener\">website</a>, unpack the binary to <code>/usr/local/bin</code> and make it executable (<code>sudo chmod +x /usr/local/bin/coredns</code>)</li>\n<li>Install <code>resolvconf</code> as a tool to manually manage <code>/etc/resolv.conf</code>: <code>sudo apt install resolvconf</code></li>\n<li>Set <code>dns=default</code> in <code>/etc/NetworkManager/NetworkManager.conf</code></li>\n<li>Add <code>nameserver 127.0.0.1</code> to <code>/etc/resolvconf/resolv.conf.d/head</code></li>\n<li>Create <code>/etc/coredns/Corefile</code> and paste the configuration shown below. In this example, we are using CloudFlare as a DNS provider. You can use Google or Quad9 as well, just change the IPs.</li>\n<li>Create a new user for CoreDNS: <code>sudo useradd -d /var/lib/coredns -m coredns</code></li>\n<li>Set some permissions: <code>sudo chown coredns:coredns /opt/coredns</code></li>\n<li>Download the SystemD service unit file from <a href=\"https://github.com/coredns/deployment/tree/master/systemd\" target=\"_blank\" rel=\"noopener\">coredns/coredns</a> to <code>/etc/systemd/system/coredns.service</code></li>\n<li>Disable SystemD‚Äôs default DNS server: <code>sudo systemctl stop systemd-resolved &amp;&amp; sudo systemctl disable systemd-resolved</code></li>\n<li><strong>Please Note:</strong> From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again</li>\n<li>Enable and start CoreDNS: <code>sudo systemctl enable coredns &amp;&amp; sudo systemctl start coredns</code></li>\n<li>You should be able to resolve domain names, again. E.g. try <code>dig +short kit.edu</code>. If an IP address is printed, everything works fine.</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># /etc/coredns/Corefile</span><br><span class=\"line\"></span><br><span class=\"line\">.:53 &#123;</span><br><span class=\"line\">    forward . tls://2606:4700:4700::1111 tls://1.1.1.1</span><br><span class=\"line\">    log</span><br><span class=\"line\">    errors</span><br><span class=\"line\">    cache</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><img src=\"images/dns2.png\" alt=\"Example of an encrypted DNS request\"><br>\nExample of an encrypted DNS request for <code>kit.edu</code> to CloudFlare‚Äôs <code>1.1.1.1</code> DNS resolver</p>\n<p>Alternatively, use the following <code>forward</code> statement to use the independent <a href=\"https://blahdns.com\" target=\"_blank\" rel=\"noopener\">BlahDNS</a> instead of CloudFlare as provider.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">forward . tls://2a01:4f8:1c1c:6b4b::1 tls://159.69.198.101 &#123;</span><br><span class=\"line\">    tls_servername dot-de.blahdns.com</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1>What happens?</h1>\n<p>Let us examine what happens (in terms of DNS) when you type <em>‚Äú<a href=\"http://kit.edu\" target=\"_blank\" rel=\"noopener\">kit.edu</a>‚Äù</em> in your browser‚Äôs address bar and hit enter.</p>\n<ol>\n<li>Your browser asks your operating system to resolve <code>kit.edu</code></li>\n<li>Your OS finds out that its primary configured nameserver is <code>127.0.0.1:53</code>, i.e. your local CoreDNS, and consults that one</li>\n<li>CoreDNS checks its cache and in case of a miss consults its configured nameserver at <code>1.1.1.1</code>, i.e. CloudFlare</li>\n<li>CloudFlare, again, checks its cache and in case of a miss goes up the hierarchical chain of nameservers until one of them has an answer</li>\n<li>Eventually, your browser performs <code>GET / HTTP/2.0</code> to <code>129.13.40.10</code> with <code>Host: kit.edu</code></li>\n</ol>\n<h1>Further Reading</h1>\n<p>Here are a few additional posts about DNS, which I found very useful.</p>\n<ul>\n<li><a href=\"https://www.cloudflare.com/learning/dns/what-is-dns/\" target=\"_blank\" rel=\"noopener\">What Is DNS? | How DNS Works</a></li>\n<li><a href=\"https://www.reddit.com/r/privacy/comments/89pr15/dnsoverhttps_vs_dns_overtls_vs_dnscrypt/dwsosjr?utm_source=share&amp;utm_medium=web2x\" target=\"_blank\" rel=\"noopener\">DNS-over-https vs. DNS-over-TLS vs DNSCrypt</a></li>\n</ul>\n<p>Please let me know if my guide is missing any required steps. Good luck, have fun and browse safely!</p>\n"},{"title":"How to load SVG into ImageView by URL in Android","date":"2018-07-12T22:19:19.000Z","_content":"\nI am writing this short article since I had a pretty hard time figuring out on how to do what the title says: fetching an SVG from the web and displaying it in an app.\n\nBy default, Android's `ImageView` does not support SVGs (why?). After googling for a while I found a complicated solution for the above problem using [Glide](https://github.com/bumptech/glide) with a custom module in combination with [AndroidSVG](http://bigbadaboom.github.io/androidsvg/). However, the latter library is quite outdated and caused some - apparently randomly occuring - errors on API level 28. Finally I found [Pixplicity/sharp](https://github.com/Pixplicity/sharp), which seemed to be a light-weight library for almost exactly my purpose with a minimal API. The only thing I needed to add is the ability to fetch the SVG from the web instead of from a local resource. I built a small example as can be seen below.\n\n## Code\n### build.gradle\n```\ndependencies {\n    // ...\n    implementation 'com.squareup.okhttp3:okhttp:3.10.0'\n    implementation 'com.pixplicity.sharp:library:1.1.0'\n    // ...\n}\n```\n\n### MainActivity.java\n```java\n    // ...\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        ImageView userAvatarView = findViewById(R.id.user_avatar);\n        String userAvatarUrl = \"https://avatars.dicebear.com/v2/female/anna.svg\";\n        Utils.fetchSvg(this, userAvatarUrl, userAvatarView);\n    }\n    // ...\n```\n\n### Utils.java\nA simple util class with static methods that receive an Android Context. Having a static `OkHttpClient` that gets conditionally initialized from within the static methods might arguably not be the best solution, but it works for this example. Alternatives would be to have it being autowired, passed as a parameter or to make `Utils` a singleton and the http client a member variable.\n```java\npublic class Utils {\n    private static OkHttpClient httpClient;\n\n    public static void fetchSvg(Context context, String url, final ImageView target) {\n        if (httpClient == null) {\n            // Use cache for performance and basic offline capability\n            httpClient = new OkHttpClient.Builder()\n                    .cache(new Cache(context.getCacheDir(), 5 * 1024 * 1014))\n                    .build();\n        }\n\n        Request request = new Request.Builder().url(url).build();\n        httpClient.newCall(request).enqueue(new Callback() {\n            @Override\n            public void onFailure(Call call, IOException e) {\n                target.setImageDrawable(R.drawable.fallback_image);\n            }\n\n            @Override\n            public void onResponse(Call call, Response response) throws IOException {\n                InputStream stream = response.body().byteStream();\n                Sharp.loadInputStream(stream).into(target);\n                stream.close();\n            }\n        });\n    }\n}\n```\n\nIf you know a simpler solution to load SVGs, please let me know!","source":"_posts/how-to-load-svg-into-imageview-by-url-in-android.md","raw":"---\ntitle: How to load SVG into ImageView by URL in Android\ndate: 2018-07-13 00:19:19\ntags:\n---\n\nI am writing this short article since I had a pretty hard time figuring out on how to do what the title says: fetching an SVG from the web and displaying it in an app.\n\nBy default, Android's `ImageView` does not support SVGs (why?). After googling for a while I found a complicated solution for the above problem using [Glide](https://github.com/bumptech/glide) with a custom module in combination with [AndroidSVG](http://bigbadaboom.github.io/androidsvg/). However, the latter library is quite outdated and caused some - apparently randomly occuring - errors on API level 28. Finally I found [Pixplicity/sharp](https://github.com/Pixplicity/sharp), which seemed to be a light-weight library for almost exactly my purpose with a minimal API. The only thing I needed to add is the ability to fetch the SVG from the web instead of from a local resource. I built a small example as can be seen below.\n\n## Code\n### build.gradle\n```\ndependencies {\n    // ...\n    implementation 'com.squareup.okhttp3:okhttp:3.10.0'\n    implementation 'com.pixplicity.sharp:library:1.1.0'\n    // ...\n}\n```\n\n### MainActivity.java\n```java\n    // ...\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        ImageView userAvatarView = findViewById(R.id.user_avatar);\n        String userAvatarUrl = \"https://avatars.dicebear.com/v2/female/anna.svg\";\n        Utils.fetchSvg(this, userAvatarUrl, userAvatarView);\n    }\n    // ...\n```\n\n### Utils.java\nA simple util class with static methods that receive an Android Context. Having a static `OkHttpClient` that gets conditionally initialized from within the static methods might arguably not be the best solution, but it works for this example. Alternatives would be to have it being autowired, passed as a parameter or to make `Utils` a singleton and the http client a member variable.\n```java\npublic class Utils {\n    private static OkHttpClient httpClient;\n\n    public static void fetchSvg(Context context, String url, final ImageView target) {\n        if (httpClient == null) {\n            // Use cache for performance and basic offline capability\n            httpClient = new OkHttpClient.Builder()\n                    .cache(new Cache(context.getCacheDir(), 5 * 1024 * 1014))\n                    .build();\n        }\n\n        Request request = new Request.Builder().url(url).build();\n        httpClient.newCall(request).enqueue(new Callback() {\n            @Override\n            public void onFailure(Call call, IOException e) {\n                target.setImageDrawable(R.drawable.fallback_image);\n            }\n\n            @Override\n            public void onResponse(Call call, Response response) throws IOException {\n                InputStream stream = response.body().byteStream();\n                Sharp.loadInputStream(stream).into(target);\n                stream.close();\n            }\n        });\n    }\n}\n```\n\nIf you know a simpler solution to load SVGs, please let me know!","slug":"how-to-load-svg-into-imageview-by-url-in-android","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhi6000g40mq8h3zl0o2","content":"<p>I am writing this short article since I had a pretty hard time figuring out on how to do what the title says: fetching an SVG from the web and displaying it in an app.</p>\n<p>By default, Android‚Äôs <code>ImageView</code> does not support SVGs (why?). After googling for a while I found a complicated solution for the above problem using <a href=\"https://github.com/bumptech/glide\" target=\"_blank\" rel=\"noopener\">Glide</a> with a custom module in combination with <a href=\"http://bigbadaboom.github.io/androidsvg/\" target=\"_blank\" rel=\"noopener\">AndroidSVG</a>. However, the latter library is quite outdated and caused some - apparently randomly occuring - errors on API level 28. Finally I found <a href=\"https://github.com/Pixplicity/sharp\" target=\"_blank\" rel=\"noopener\">Pixplicity/sharp</a>, which seemed to be a light-weight library for almost exactly my purpose with a minimal API. The only thing I needed to add is the ability to fetch the SVG from the web instead of from a local resource. I built a small example as can be seen below.</p>\n<h2 id=\"Code\">Code</h2>\n<h3 id=\"build-gradle\">build.gradle</h3>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dependencies &#123;</span><br><span class=\"line\">    // ...</span><br><span class=\"line\">    implementation &apos;com.squareup.okhttp3:okhttp:3.10.0&apos;</span><br><span class=\"line\">    implementation &apos;com.pixplicity.sharp:library:1.1.0&apos;</span><br><span class=\"line\">    // ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"MainActivity-java\">MainActivity.java</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// ...</span></span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">onCreate</span><span class=\"params\">(Bundle savedInstanceState)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">super</span>.onCreate(savedInstanceState);</span><br><span class=\"line\">    setContentView(R.layout.activity_main);</span><br><span class=\"line\"></span><br><span class=\"line\">    ImageView userAvatarView = findViewById(R.id.user_avatar);</span><br><span class=\"line\">    String userAvatarUrl = <span class=\"string\">\"https://avatars.dicebear.com/v2/female/anna.svg\"</span>;</span><br><span class=\"line\">    Utils.fetchSvg(<span class=\"keyword\">this</span>, userAvatarUrl, userAvatarView);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// ...</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"Utils-java\">Utils.java</h3>\n<p>A simple util class with static methods that receive an Android Context. Having a static <code>OkHttpClient</code> that gets conditionally initialized from within the static methods might arguably not be the best solution, but it works for this example. Alternatives would be to have it being autowired, passed as a parameter or to make <code>Utils</code> a singleton and the http client a member variable.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Utils</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> OkHttpClient httpClient;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">fetchSvg</span><span class=\"params\">(Context context, String url, <span class=\"keyword\">final</span> ImageView target)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (httpClient == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Use cache for performance and basic offline capability</span></span><br><span class=\"line\">            httpClient = <span class=\"keyword\">new</span> OkHttpClient.Builder()</span><br><span class=\"line\">                    .cache(<span class=\"keyword\">new</span> Cache(context.getCacheDir(), <span class=\"number\">5</span> * <span class=\"number\">1024</span> * <span class=\"number\">1014</span>))</span><br><span class=\"line\">                    .build();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        Request request = <span class=\"keyword\">new</span> Request.Builder().url(url).build();</span><br><span class=\"line\">        httpClient.newCall(request).enqueue(<span class=\"keyword\">new</span> Callback() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onFailure</span><span class=\"params\">(Call call, IOException e)</span> </span>&#123;</span><br><span class=\"line\">                target.setImageDrawable(R.drawable.fallback_image);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onResponse</span><span class=\"params\">(Call call, Response response)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">                InputStream stream = response.body().byteStream();</span><br><span class=\"line\">                Sharp.loadInputStream(stream).into(target);</span><br><span class=\"line\">                stream.close();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>If you know a simpler solution to load SVGs, please let me know!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>I am writing this short article since I had a pretty hard time figuring out on how to do what the title says: fetching an SVG from the web and displaying it in an app.</p>\n<p>By default, Android‚Äôs <code>ImageView</code> does not support SVGs (why?). After googling for a while I found a complicated solution for the above problem using <a href=\"https://github.com/bumptech/glide\" target=\"_blank\" rel=\"noopener\">Glide</a> with a custom module in combination with <a href=\"http://bigbadaboom.github.io/androidsvg/\" target=\"_blank\" rel=\"noopener\">AndroidSVG</a>. However, the latter library is quite outdated and caused some - apparently randomly occuring - errors on API level 28. Finally I found <a href=\"https://github.com/Pixplicity/sharp\" target=\"_blank\" rel=\"noopener\">Pixplicity/sharp</a>, which seemed to be a light-weight library for almost exactly my purpose with a minimal API. The only thing I needed to add is the ability to fetch the SVG from the web instead of from a local resource. I built a small example as can be seen below.</p>\n<h2 id=\"Code\">Code</h2>\n<h3 id=\"build-gradle\">build.gradle</h3>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dependencies &#123;</span><br><span class=\"line\">    // ...</span><br><span class=\"line\">    implementation &apos;com.squareup.okhttp3:okhttp:3.10.0&apos;</span><br><span class=\"line\">    implementation &apos;com.pixplicity.sharp:library:1.1.0&apos;</span><br><span class=\"line\">    // ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"MainActivity-java\">MainActivity.java</h3>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// ...</span></span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">onCreate</span><span class=\"params\">(Bundle savedInstanceState)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">super</span>.onCreate(savedInstanceState);</span><br><span class=\"line\">    setContentView(R.layout.activity_main);</span><br><span class=\"line\"></span><br><span class=\"line\">    ImageView userAvatarView = findViewById(R.id.user_avatar);</span><br><span class=\"line\">    String userAvatarUrl = <span class=\"string\">\"https://avatars.dicebear.com/v2/female/anna.svg\"</span>;</span><br><span class=\"line\">    Utils.fetchSvg(<span class=\"keyword\">this</span>, userAvatarUrl, userAvatarView);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// ...</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"Utils-java\">Utils.java</h3>\n<p>A simple util class with static methods that receive an Android Context. Having a static <code>OkHttpClient</code> that gets conditionally initialized from within the static methods might arguably not be the best solution, but it works for this example. Alternatives would be to have it being autowired, passed as a parameter or to make <code>Utils</code> a singleton and the http client a member variable.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Utils</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> OkHttpClient httpClient;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">fetchSvg</span><span class=\"params\">(Context context, String url, <span class=\"keyword\">final</span> ImageView target)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (httpClient == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Use cache for performance and basic offline capability</span></span><br><span class=\"line\">            httpClient = <span class=\"keyword\">new</span> OkHttpClient.Builder()</span><br><span class=\"line\">                    .cache(<span class=\"keyword\">new</span> Cache(context.getCacheDir(), <span class=\"number\">5</span> * <span class=\"number\">1024</span> * <span class=\"number\">1014</span>))</span><br><span class=\"line\">                    .build();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        Request request = <span class=\"keyword\">new</span> Request.Builder().url(url).build();</span><br><span class=\"line\">        httpClient.newCall(request).enqueue(<span class=\"keyword\">new</span> Callback() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onFailure</span><span class=\"params\">(Call call, IOException e)</span> </span>&#123;</span><br><span class=\"line\">                target.setImageDrawable(R.drawable.fallback_image);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onResponse</span><span class=\"params\">(Call call, Response response)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">                InputStream stream = response.body().byteStream();</span><br><span class=\"line\">                Sharp.loadInputStream(stream).into(target);</span><br><span class=\"line\">                stream.close();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>If you know a simpler solution to load SVGs, please let me know!</p>\n"},{"title":"How to load Yago into Apache Jena / Fuseki","date":"2016-11-11T22:04:09.000Z","_content":"\nThis article describes how to load the [Yago](http://yago-knowledge.org) Linked Data knowledge collection into an [Apache Jena](https://jena.apache.org/) triple store database on Windows 10 as well as on Linux.\n\n1. At very first, please make sure you have Java 8 Runtime Environment installed on your system.\n\n2. Download all Yago graphs you need from the [downloads section](http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/) as .ttl files. In my case I took all graphs from _TAXONOMY_, _CORE_ and additonally the _yagoDBpediaInstances_ and _yagoDBpediaClasses_ collections to have relations from Yago entities to DBpedia ones. Download the files to a folder on your system, let's say `/home/ferdinand/yago/` on Linux or `C:\\Users\\Ferdinand\\yago` on Windows and extract them using 7zip.\n\n3. Delete all `.7z` files.\n\n4. Download `apache-jena-3.1.1.zip` (or newer version) and `apache-jena-fuseki-2.4.1.zip` from [here](https://jena.apache.org/download/index.cgi) and extract them to, let's say `/home/ferdinand/jena/` and `/home/ferdinand/fuseki/` (or the analogue directories on Windows).\n\n5. Now the .ttl files needs to get some kind of preprocessed, where non-unicode characters are replaced in order for Jena to accept the data. On Linux run `sed -i 's/|/-/g' ./* && sed -i 's/\\\\\\\\/-/g' ./* && sed -i 's/‚Äì/-/g' ./*` from within the directory where your .ttl files are. On Windows, start the Ubuntu Bash, navigate to the respective directory (e.g. `/mnt/c/Users/Ferdinand/yago`) and do the same command. It will take several minutes. I mean, really several...\n\n6. Create a folder to be used for the database later, e.g. `/home/ferdinand/yago/data`.\n\n7. Add the Fuseki root directory (e.g. `/home/ferdinand/fuseki`) and the Jena _bin_ (or _bat_ on Win) (e.g. `/home/ferdinand/jena/bin`) to your `PATH` environment variable. On Linux you would do this by editing your `~/.bash_profile`, on Windows you can search for _\"envionment variables\"_ and then use the Windows system settings dialog.\n\n7. Load the graphs using _tdbloader_: `tdbloader.bat --loc data ./*` from the directory where your .ttl files are located. This may take several hours. Not joking...\n\n8. Start Fuseki typing `java -jar fuseki-server.jar --update --loc /home/ferdinand/yago/data /myGraph` to run fuseki with your entire Yago graph available under the _myGraph_ alias.\n\n9. Open [http://localhost:3030](http://localhost:3030) in your browser and start making queries.\n\nIf you're about to run really expensive queries, consider the following.\n\n1. Set the `JVM_ARGS` environment variable to `-Xms512m -Xmx2048M -XX:-UseGCOverheadLimit -XX:+UseParallelGC`. This will basically prevent you from getting _OutOfMemory_ errors.\n\n2. Use _tdbquery_ since it might be a little more performant than the web SPARQL endpoint. An example _tdbquery_ command might look like this, assuming you have a file `q.txt` that contains your SPARQL query: `tdbquery --loc=/home/ferdinand/yago/data --time --results=CSV --query=q.txt > output.txt`","source":"_posts/how-to-load-yago-into-apache-jena-fuseki.md","raw":"---\ntitle: How to load Yago into Apache Jena / Fuseki\ndate: 2016-11-11 23:04:09\ntags:\n---\n\nThis article describes how to load the [Yago](http://yago-knowledge.org) Linked Data knowledge collection into an [Apache Jena](https://jena.apache.org/) triple store database on Windows 10 as well as on Linux.\n\n1. At very first, please make sure you have Java 8 Runtime Environment installed on your system.\n\n2. Download all Yago graphs you need from the [downloads section](http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/) as .ttl files. In my case I took all graphs from _TAXONOMY_, _CORE_ and additonally the _yagoDBpediaInstances_ and _yagoDBpediaClasses_ collections to have relations from Yago entities to DBpedia ones. Download the files to a folder on your system, let's say `/home/ferdinand/yago/` on Linux or `C:\\Users\\Ferdinand\\yago` on Windows and extract them using 7zip.\n\n3. Delete all `.7z` files.\n\n4. Download `apache-jena-3.1.1.zip` (or newer version) and `apache-jena-fuseki-2.4.1.zip` from [here](https://jena.apache.org/download/index.cgi) and extract them to, let's say `/home/ferdinand/jena/` and `/home/ferdinand/fuseki/` (or the analogue directories on Windows).\n\n5. Now the .ttl files needs to get some kind of preprocessed, where non-unicode characters are replaced in order for Jena to accept the data. On Linux run `sed -i 's/|/-/g' ./* && sed -i 's/\\\\\\\\/-/g' ./* && sed -i 's/‚Äì/-/g' ./*` from within the directory where your .ttl files are. On Windows, start the Ubuntu Bash, navigate to the respective directory (e.g. `/mnt/c/Users/Ferdinand/yago`) and do the same command. It will take several minutes. I mean, really several...\n\n6. Create a folder to be used for the database later, e.g. `/home/ferdinand/yago/data`.\n\n7. Add the Fuseki root directory (e.g. `/home/ferdinand/fuseki`) and the Jena _bin_ (or _bat_ on Win) (e.g. `/home/ferdinand/jena/bin`) to your `PATH` environment variable. On Linux you would do this by editing your `~/.bash_profile`, on Windows you can search for _\"envionment variables\"_ and then use the Windows system settings dialog.\n\n7. Load the graphs using _tdbloader_: `tdbloader.bat --loc data ./*` from the directory where your .ttl files are located. This may take several hours. Not joking...\n\n8. Start Fuseki typing `java -jar fuseki-server.jar --update --loc /home/ferdinand/yago/data /myGraph` to run fuseki with your entire Yago graph available under the _myGraph_ alias.\n\n9. Open [http://localhost:3030](http://localhost:3030) in your browser and start making queries.\n\nIf you're about to run really expensive queries, consider the following.\n\n1. Set the `JVM_ARGS` environment variable to `-Xms512m -Xmx2048M -XX:-UseGCOverheadLimit -XX:+UseParallelGC`. This will basically prevent you from getting _OutOfMemory_ errors.\n\n2. Use _tdbquery_ since it might be a little more performant than the web SPARQL endpoint. An example _tdbquery_ command might look like this, assuming you have a file `q.txt` that contains your SPARQL query: `tdbquery --loc=/home/ferdinand/yago/data --time --results=CSV --query=q.txt > output.txt`","slug":"how-to-load-yago-into-apache-jena-fuseki","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhi8000h40mqvxn5f3j9","content":"<p>This article describes how to load the <a href=\"http://yago-knowledge.org\" target=\"_blank\" rel=\"noopener\">Yago</a> Linked Data knowledge collection into an <a href=\"https://jena.apache.org/\" target=\"_blank\" rel=\"noopener\">Apache Jena</a> triple store database on Windows 10 as well as on Linux.</p>\n<ol>\n<li>\n<p>At very first, please make sure you have Java 8 Runtime Environment installed on your system.</p>\n</li>\n<li>\n<p>Download all Yago graphs you need from the <a href=\"http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/\" target=\"_blank\" rel=\"noopener\">downloads section</a> as .ttl files. In my case I took all graphs from <em>TAXONOMY</em>, <em>CORE</em> and additonally the <em>yagoDBpediaInstances</em> and <em>yagoDBpediaClasses</em> collections to have relations from Yago entities to DBpedia ones. Download the files to a folder on your system, let‚Äôs say <code>/home/ferdinand/yago/</code> on Linux or <code>C:\\Users\\Ferdinand\\yago</code> on Windows and extract them using 7zip.</p>\n</li>\n<li>\n<p>Delete all <code>.7z</code> files.</p>\n</li>\n<li>\n<p>Download <code>apache-jena-3.1.1.zip</code> (or newer version) and <code>apache-jena-fuseki-2.4.1.zip</code> from <a href=\"https://jena.apache.org/download/index.cgi\" target=\"_blank\" rel=\"noopener\">here</a> and extract them to, let‚Äôs say <code>/home/ferdinand/jena/</code> and <code>/home/ferdinand/fuseki/</code> (or the analogue directories on Windows).</p>\n</li>\n<li>\n<p>Now the .ttl files needs to get some kind of preprocessed, where non-unicode characters are replaced in order for Jena to accept the data. On Linux run <code>sed -i 's/|/-/g' ./* &amp;&amp; sed -i 's/\\\\\\\\/-/g' ./* &amp;&amp; sed -i 's/‚Äì/-/g' ./*</code> from within the directory where your .ttl files are. On Windows, start the Ubuntu Bash, navigate to the respective directory (e.g. <code>/mnt/c/Users/Ferdinand/yago</code>) and do the same command. It will take several minutes. I mean, really several‚Ä¶</p>\n</li>\n<li>\n<p>Create a folder to be used for the database later, e.g. <code>/home/ferdinand/yago/data</code>.</p>\n</li>\n<li>\n<p>Add the Fuseki root directory (e.g. <code>/home/ferdinand/fuseki</code>) and the Jena <em>bin</em> (or <em>bat</em> on Win) (e.g. <code>/home/ferdinand/jena/bin</code>) to your <code>PATH</code> environment variable. On Linux you would do this by editing your <code>~/.bash_profile</code>, on Windows you can search for <em>‚Äúenvionment variables‚Äù</em> and then use the Windows system settings dialog.</p>\n</li>\n<li>\n<p>Load the graphs using <em>tdbloader</em>: <code>tdbloader.bat --loc data ./*</code> from the directory where your .ttl files are located. This may take several hours. Not joking‚Ä¶</p>\n</li>\n<li>\n<p>Start Fuseki typing <code>java -jar fuseki-server.jar --update --loc /home/ferdinand/yago/data /myGraph</code> to run fuseki with your entire Yago graph available under the <em>myGraph</em> alias.</p>\n</li>\n<li>\n<p>Open <a href=\"http://localhost:3030\" target=\"_blank\" rel=\"noopener\">http://localhost:3030</a> in your browser and start making queries.</p>\n</li>\n</ol>\n<p>If you‚Äôre about to run really expensive queries, consider the following.</p>\n<ol>\n<li>\n<p>Set the <code>JVM_ARGS</code> environment variable to <code>-Xms512m -Xmx2048M -XX:-UseGCOverheadLimit -XX:+UseParallelGC</code>. This will basically prevent you from getting <em>OutOfMemory</em> errors.</p>\n</li>\n<li>\n<p>Use <em>tdbquery</em> since it might be a little more performant than the web SPARQL endpoint. An example <em>tdbquery</em> command might look like this, assuming you have a file <code>q.txt</code> that contains your SPARQL query: <code>tdbquery --loc=/home/ferdinand/yago/data --time --results=CSV --query=q.txt &gt; output.txt</code></p>\n</li>\n</ol>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>This article describes how to load the <a href=\"http://yago-knowledge.org\" target=\"_blank\" rel=\"noopener\">Yago</a> Linked Data knowledge collection into an <a href=\"https://jena.apache.org/\" target=\"_blank\" rel=\"noopener\">Apache Jena</a> triple store database on Windows 10 as well as on Linux.</p>\n<ol>\n<li>\n<p>At very first, please make sure you have Java 8 Runtime Environment installed on your system.</p>\n</li>\n<li>\n<p>Download all Yago graphs you need from the <a href=\"http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/\" target=\"_blank\" rel=\"noopener\">downloads section</a> as .ttl files. In my case I took all graphs from <em>TAXONOMY</em>, <em>CORE</em> and additonally the <em>yagoDBpediaInstances</em> and <em>yagoDBpediaClasses</em> collections to have relations from Yago entities to DBpedia ones. Download the files to a folder on your system, let‚Äôs say <code>/home/ferdinand/yago/</code> on Linux or <code>C:\\Users\\Ferdinand\\yago</code> on Windows and extract them using 7zip.</p>\n</li>\n<li>\n<p>Delete all <code>.7z</code> files.</p>\n</li>\n<li>\n<p>Download <code>apache-jena-3.1.1.zip</code> (or newer version) and <code>apache-jena-fuseki-2.4.1.zip</code> from <a href=\"https://jena.apache.org/download/index.cgi\" target=\"_blank\" rel=\"noopener\">here</a> and extract them to, let‚Äôs say <code>/home/ferdinand/jena/</code> and <code>/home/ferdinand/fuseki/</code> (or the analogue directories on Windows).</p>\n</li>\n<li>\n<p>Now the .ttl files needs to get some kind of preprocessed, where non-unicode characters are replaced in order for Jena to accept the data. On Linux run <code>sed -i 's/|/-/g' ./* &amp;&amp; sed -i 's/\\\\\\\\/-/g' ./* &amp;&amp; sed -i 's/‚Äì/-/g' ./*</code> from within the directory where your .ttl files are. On Windows, start the Ubuntu Bash, navigate to the respective directory (e.g. <code>/mnt/c/Users/Ferdinand/yago</code>) and do the same command. It will take several minutes. I mean, really several‚Ä¶</p>\n</li>\n<li>\n<p>Create a folder to be used for the database later, e.g. <code>/home/ferdinand/yago/data</code>.</p>\n</li>\n<li>\n<p>Add the Fuseki root directory (e.g. <code>/home/ferdinand/fuseki</code>) and the Jena <em>bin</em> (or <em>bat</em> on Win) (e.g. <code>/home/ferdinand/jena/bin</code>) to your <code>PATH</code> environment variable. On Linux you would do this by editing your <code>~/.bash_profile</code>, on Windows you can search for <em>‚Äúenvionment variables‚Äù</em> and then use the Windows system settings dialog.</p>\n</li>\n<li>\n<p>Load the graphs using <em>tdbloader</em>: <code>tdbloader.bat --loc data ./*</code> from the directory where your .ttl files are located. This may take several hours. Not joking‚Ä¶</p>\n</li>\n<li>\n<p>Start Fuseki typing <code>java -jar fuseki-server.jar --update --loc /home/ferdinand/yago/data /myGraph</code> to run fuseki with your entire Yago graph available under the <em>myGraph</em> alias.</p>\n</li>\n<li>\n<p>Open <a href=\"http://localhost:3030\" target=\"_blank\" rel=\"noopener\">http://localhost:3030</a> in your browser and start making queries.</p>\n</li>\n</ol>\n<p>If you‚Äôre about to run really expensive queries, consider the following.</p>\n<ol>\n<li>\n<p>Set the <code>JVM_ARGS</code> environment variable to <code>-Xms512m -Xmx2048M -XX:-UseGCOverheadLimit -XX:+UseParallelGC</code>. This will basically prevent you from getting <em>OutOfMemory</em> errors.</p>\n</li>\n<li>\n<p>Use <em>tdbquery</em> since it might be a little more performant than the web SPARQL endpoint. An example <em>tdbquery</em> command might look like this, assuming you have a file <code>q.txt</code> that contains your SPARQL query: <code>tdbquery --loc=/home/ferdinand/yago/data --time --results=CSV --query=q.txt &gt; output.txt</code></p>\n</li>\n</ol>\n"},{"title":"How to make Telegram Bots","date":"2015-06-28T20:39:44.000Z","_content":"\nRecently [Telegram](http://telegram.org \"Telegram\") has introduced a new feature, the bots. Basically the bots enable human Telegram users to talk to machines, i.e. customly written little programs. A bot could be a daily helper, be it a bot that you can ask for the current temperature, one that googles something for you, one to manage your todo‚Äôs or even a little text based game ‚Äì everything within the Telegram chat. The nice thing about them is that they‚Äôre really simple to create. You can read more about the bots in general here: [https://telegram.org/blog/bot-revolution](https://telegram.org/blog/bot-revolution)\n\nThis article shouldn‚Äôt cover how to create and publish a bot (which actually is the same step), but how to write its actual functionality ‚Äì its backend. On how to initially set up one, please refer to this little guide: [https://core.telegram.org/bots](https://core.telegram.org/bots). It‚Äôs very easy, trust me. Everything you need for that is your Telegram app.\n\nWhat you need further to program your bot basically is a favorite programming language, an IDE or at least a text editor, [this](https://core.telegram.org/bots/api \"this\") page to be open and again your app. In this tutorial we will use Node.js as programming language (or more precisely as programming platform), but you really could use any other language, as well. Probably Python or PHP would do the job well, but you could even write a Java application.\n\nFirst some basic things to understand. For the most parts your bot is your own application which we will try to create here, running on your own local PC or server. Telegram won‚Äôt host any code. All functionality and data storage is kept in your program on your machine. Basically Telegram doesn‚Äôt provide more than kind of an interface between your users‚Äô Telegram client (app) and your bot application. The flow would be like:\n\n1.  You create a new bot with @BotFather and set its description and commands (the commands you set there are ‚Äì strictly speaking ‚Äì completely independent of which commands your program will actually accept ‚Äì they are just strings which the user gets suggested in a chat with your bot) so that the bot gets publicly accessible as any other (human) Telegram user is via her @ nickname.\n\n2.  You write the backend and run it to be listening.\n\n3.  A user sends a message to your bot.\n\n4.  The (JSON formatted) Telegram message object gets passed to your backend via HTTPS by Telegram.\n\n5.  Your backend parses the message text, extract the commands and arguments, processes some logic, possibly does some database actions and so on.\n\n6.  Your backend passes a response message object to the Telegram API via HTTPS (the other way round now).\n\n7.  Telegram shows the message to your user‚Äôs chat window.\n\nAlright, now let‚Äôs get into the code. I require you to already have set up a bot and got its authentication token (all using @BotFather). As I said, we will make a little Node.js program here, but for those who don‚Äôt understand JavaScript too well, I‚Äôll try to explain everything as clear as you need to re-do this in your programming language.\n\nFirst, you need any library that can do HTTP requests, because basically your backend will be busy doing POSTs and GETs for the most time. For Node.js we‚Äôll use the [Unirest](http://unirest.io/nodejs.html \"Unirest\") library, which probably is a little bit overkill, but very simple for our purpose. The library is available for Python, PHP, Ruby, Java and many others as well.\n\nSo first step is setting up a Node application and requiring Unirest (i assume you know how to set up a Node application and you‚Äôre familiar with npm).\n\n```javascript\n/* app.js */\n\nvar unirest = require('unirest');\n```\n\nNext is to receive messages from your users. Telegram offers two ways to do this. The first way would be to use Webhooks, which basically means that you‚Äôre backend runs on a webserver (or for Node it actually is one itself), Telegram knows your HTTPS endpoint and does a request to your backend every time a message is sent by a user. I would consider this the more elegant way. But the minus about this is that you would need to have a valid SSL certificate (which costs monthly charge) to provide a secure HTTPS connection. Telegram won‚Äôt send any data to an unsecure or unverified endpoint. This is why we will take the other approach, which works kind of the other way round. Your backend will continuously request the Telegram API if there are new messages available. In detail your backend won‚Äôt do request after request after request (because this would be so inefficient!) but use [long polling](http://www.pubnub.com/blog/http-long-polling/ \"long polling\"). To put it simple long polling means that a request won‚Äôt be answered instantly, but kept open until there is some data available.\n\nWe need to specify some constants. As i said, everything between your backend and Telegram happens via their HTTPS interface. We set up constants for the base API url, containing your bot token, the URL for the [getUpdates](https://core.telegram.org/bots/api#getupdates \"getUpdates\") method endpoint and the URL for the [sendMessage](https://core.telegram.org/bots/api#sendmessage \"sendMessage\") method endpoint. The *:offset:* within the URL string will get replaced by a number, specifying, which new messages to fetch from Telegram later.\n\n```javascript\n/* app.js */\n\n/* ... */\n\nvar BASE_URL = \"https://api.telegram.org/botYOUR_TOKEN_HERE/\";\nvar POLLING_URL = BASE_URL + \"getUpdates?offset=:offset:&timeout=60\";\nvar SEND_MESSAGE_URL = BASE_URL + \"sendMessage\";\n```\n\nNow we‚Äôll introduce a function called *poll* (you can choose any other name), which basically is kind of the main loop of our program. Here‚Äôs the code for this method, explanation follows.\n\n```javascript\n/* ... */\nfunction poll(offset) {\n    var url = POLLING_URL.replace(\":offset:\", offset);\n\n    unirest.get(url)\n        .end(function(response) {\n            var body = response.raw_body;\n            if (response.status == 200) {\n                var jsonData = JSON.parse(body);\n                var result = jsonData.result;\n\n                if (result.length > 0) {\n                    for (i in result) {\n                        if (runCommand(result[i].message)) continue;\n                    }\n\n                    max_offset = parseInt(result[result.length - 1].update_id) + 1; // update max offset\n                }\n                poll(max_offset);\n            }\n        });\n};\n```\n\nAlright. The function is recursive, meaning it will call itself ‚Äì namely each time, a request was answered (which is, when a new message was fetched). The http request to Telegram API to get updates will be pending until a message arrives. Then it gets answered, but has to be re-opened again instantly now to continue listening for message updates again. As a parameter it takes an offset number, which replaces the *:offset:* placeholder in the url string. To read more about this parameter, go to [https://core.telegram.org/bots/api#getupdates](https://core.telegram.org/bots/api#getupdates). Unirest opens an http request to the specified url and executes the callback function given to end(), if the request was answered. First, we extract the response body. Afterwards, we check if our request was successful. If this is the case, we parse the body (which is a JSON object, consisting of an *ok* field and a *result* array. The result array contains one or more message objects. These are the ones that are relevant for us. For each message object, we try to parse it as a command (i‚Äôll give you the runCommand() function is a second‚Ä¶) and depending on which command we got, execute the respective method. Afterwards the offset gets updates to the id of the latest message plus one (to not receive it again next time) and a new request gets opened.\n\n```javascript\n/* ... */\n\nvar dosth = function(message) {\n    // to be implemented....\n}\n\nvar COMMANDS = {\n    \"dosth\" : dosth\n};\n```\n\nNow we specify a map, which maps strings (representing the users‚Äô command input ‚Äì in this case */dosth* to actual functions.\n\n```javascript\n    var msgtext = message.text;\n\n    if (msgtext.indexOf(\"/\") != 0) return false; // no slash at beginning?\n    var command = msgtext.substring(1, msgtext.indexOf(\" \"));\n    if (COMMANDS[command] == null) return false; // not a valid command?\n    COMMANDS[command](message);\n    return true;\n}\n```\n\nAnd this is the runCommand method. It takes the entire Telegram message object, which we got as a response from the Telegram API above and tries to parse its text as a command. A command string always starts with a slash. So if there is not slash in the beginning, we can be sure that we didn‚Äôt get a valid command. We simply return here, but we also could send a message to the user telling him ‚ÄúHey, please enter a valid command.‚Äù. But let‚Äôs keep it simple. In the following line we extract everything after the slash and before the first blank space (assuming a command mustn‚Äôt contain a blank space) as the command. Afterwards we look into our map if the command actually is a key for a method and if so, we just run this method, passing it the message object as a parameter as the function will need information out of it.\n\nWhat have we done so far? We wrote a program that requests the Telegram API for new messages, parses potentially contained commands and runs functions depending on these commands.\n\nAll we still need is to implement the method belonging to the */dosth* command.\n\n```javascript\n    var caps = message.text.toUpperCase();\n    var answer = {\n        chat_id : message.chat.id,\n        text : \"You told be to do something, so I took your input and made it all caps. Look: \" + caps\n    };\n\n    unirest.post(SEND_MESSAGE_URL)\n        .send(answer)\n        .end(function (response) {\n            if (response.status == 200) console.log(\"Successfully sent message to \" + message.chat.id);\n        });\n}\n```\n\nMost times you‚Äôll want to send a message as response to your user. You could also send an image, an audio, a location, ‚Ä¶ (see [https://core.telegram.org/bots/api#available-methods](https://core.telegram.org/bots/api#available-methods)). Every message object needs a *chat_id* field, containing a Telegram user id, so that Telegram knows which user to deliver your message to. We simply extract this id out of the message object‚Äôs chat object we received from the user. The second mandatory field in a message object is the *text*. This is up to you. After having set up the new message object (you could add other fields, e.g. to show bot-buttons to the user ‚Äì see [https://core.telegram.org/bots/api#message](https://core.telegram.org/bots/api#message) for this), we just need to HTTP POST it to Telegram using Unirest again. Finished.\n\nThis example was kept veeeery simple. Of course you could implement your bot to do really fancy things. You could process media, do location-specific operations, include a database (MongoDB suits really well!) and many, many other things. You could write any complex application you can imagine ‚Äì with a Telegram chat as the text i/o interface. Please tell be your ideas ‚Äì what would be a great bot?\n\nIf you like to try by bot, simply write a message to **@FavoriteBot** and share it to your friends, if you like it.\n\nIf you have any questions, contact me via mail to *ferdinand(at)muetsch.io.*","source":"_posts/how-to-make-telegram-bots.md","raw":"---\ntitle: How to make Telegram Bots\ndate: 2015-06-28 22:39:44\ntags:\n---\n\nRecently [Telegram](http://telegram.org \"Telegram\") has introduced a new feature, the bots. Basically the bots enable human Telegram users to talk to machines, i.e. customly written little programs. A bot could be a daily helper, be it a bot that you can ask for the current temperature, one that googles something for you, one to manage your todo‚Äôs or even a little text based game ‚Äì everything within the Telegram chat. The nice thing about them is that they‚Äôre really simple to create. You can read more about the bots in general here: [https://telegram.org/blog/bot-revolution](https://telegram.org/blog/bot-revolution)\n\nThis article shouldn‚Äôt cover how to create and publish a bot (which actually is the same step), but how to write its actual functionality ‚Äì its backend. On how to initially set up one, please refer to this little guide: [https://core.telegram.org/bots](https://core.telegram.org/bots). It‚Äôs very easy, trust me. Everything you need for that is your Telegram app.\n\nWhat you need further to program your bot basically is a favorite programming language, an IDE or at least a text editor, [this](https://core.telegram.org/bots/api \"this\") page to be open and again your app. In this tutorial we will use Node.js as programming language (or more precisely as programming platform), but you really could use any other language, as well. Probably Python or PHP would do the job well, but you could even write a Java application.\n\nFirst some basic things to understand. For the most parts your bot is your own application which we will try to create here, running on your own local PC or server. Telegram won‚Äôt host any code. All functionality and data storage is kept in your program on your machine. Basically Telegram doesn‚Äôt provide more than kind of an interface between your users‚Äô Telegram client (app) and your bot application. The flow would be like:\n\n1.  You create a new bot with @BotFather and set its description and commands (the commands you set there are ‚Äì strictly speaking ‚Äì completely independent of which commands your program will actually accept ‚Äì they are just strings which the user gets suggested in a chat with your bot) so that the bot gets publicly accessible as any other (human) Telegram user is via her @ nickname.\n\n2.  You write the backend and run it to be listening.\n\n3.  A user sends a message to your bot.\n\n4.  The (JSON formatted) Telegram message object gets passed to your backend via HTTPS by Telegram.\n\n5.  Your backend parses the message text, extract the commands and arguments, processes some logic, possibly does some database actions and so on.\n\n6.  Your backend passes a response message object to the Telegram API via HTTPS (the other way round now).\n\n7.  Telegram shows the message to your user‚Äôs chat window.\n\nAlright, now let‚Äôs get into the code. I require you to already have set up a bot and got its authentication token (all using @BotFather). As I said, we will make a little Node.js program here, but for those who don‚Äôt understand JavaScript too well, I‚Äôll try to explain everything as clear as you need to re-do this in your programming language.\n\nFirst, you need any library that can do HTTP requests, because basically your backend will be busy doing POSTs and GETs for the most time. For Node.js we‚Äôll use the [Unirest](http://unirest.io/nodejs.html \"Unirest\") library, which probably is a little bit overkill, but very simple for our purpose. The library is available for Python, PHP, Ruby, Java and many others as well.\n\nSo first step is setting up a Node application and requiring Unirest (i assume you know how to set up a Node application and you‚Äôre familiar with npm).\n\n```javascript\n/* app.js */\n\nvar unirest = require('unirest');\n```\n\nNext is to receive messages from your users. Telegram offers two ways to do this. The first way would be to use Webhooks, which basically means that you‚Äôre backend runs on a webserver (or for Node it actually is one itself), Telegram knows your HTTPS endpoint and does a request to your backend every time a message is sent by a user. I would consider this the more elegant way. But the minus about this is that you would need to have a valid SSL certificate (which costs monthly charge) to provide a secure HTTPS connection. Telegram won‚Äôt send any data to an unsecure or unverified endpoint. This is why we will take the other approach, which works kind of the other way round. Your backend will continuously request the Telegram API if there are new messages available. In detail your backend won‚Äôt do request after request after request (because this would be so inefficient!) but use [long polling](http://www.pubnub.com/blog/http-long-polling/ \"long polling\"). To put it simple long polling means that a request won‚Äôt be answered instantly, but kept open until there is some data available.\n\nWe need to specify some constants. As i said, everything between your backend and Telegram happens via their HTTPS interface. We set up constants for the base API url, containing your bot token, the URL for the [getUpdates](https://core.telegram.org/bots/api#getupdates \"getUpdates\") method endpoint and the URL for the [sendMessage](https://core.telegram.org/bots/api#sendmessage \"sendMessage\") method endpoint. The *:offset:* within the URL string will get replaced by a number, specifying, which new messages to fetch from Telegram later.\n\n```javascript\n/* app.js */\n\n/* ... */\n\nvar BASE_URL = \"https://api.telegram.org/botYOUR_TOKEN_HERE/\";\nvar POLLING_URL = BASE_URL + \"getUpdates?offset=:offset:&timeout=60\";\nvar SEND_MESSAGE_URL = BASE_URL + \"sendMessage\";\n```\n\nNow we‚Äôll introduce a function called *poll* (you can choose any other name), which basically is kind of the main loop of our program. Here‚Äôs the code for this method, explanation follows.\n\n```javascript\n/* ... */\nfunction poll(offset) {\n    var url = POLLING_URL.replace(\":offset:\", offset);\n\n    unirest.get(url)\n        .end(function(response) {\n            var body = response.raw_body;\n            if (response.status == 200) {\n                var jsonData = JSON.parse(body);\n                var result = jsonData.result;\n\n                if (result.length > 0) {\n                    for (i in result) {\n                        if (runCommand(result[i].message)) continue;\n                    }\n\n                    max_offset = parseInt(result[result.length - 1].update_id) + 1; // update max offset\n                }\n                poll(max_offset);\n            }\n        });\n};\n```\n\nAlright. The function is recursive, meaning it will call itself ‚Äì namely each time, a request was answered (which is, when a new message was fetched). The http request to Telegram API to get updates will be pending until a message arrives. Then it gets answered, but has to be re-opened again instantly now to continue listening for message updates again. As a parameter it takes an offset number, which replaces the *:offset:* placeholder in the url string. To read more about this parameter, go to [https://core.telegram.org/bots/api#getupdates](https://core.telegram.org/bots/api#getupdates). Unirest opens an http request to the specified url and executes the callback function given to end(), if the request was answered. First, we extract the response body. Afterwards, we check if our request was successful. If this is the case, we parse the body (which is a JSON object, consisting of an *ok* field and a *result* array. The result array contains one or more message objects. These are the ones that are relevant for us. For each message object, we try to parse it as a command (i‚Äôll give you the runCommand() function is a second‚Ä¶) and depending on which command we got, execute the respective method. Afterwards the offset gets updates to the id of the latest message plus one (to not receive it again next time) and a new request gets opened.\n\n```javascript\n/* ... */\n\nvar dosth = function(message) {\n    // to be implemented....\n}\n\nvar COMMANDS = {\n    \"dosth\" : dosth\n};\n```\n\nNow we specify a map, which maps strings (representing the users‚Äô command input ‚Äì in this case */dosth* to actual functions.\n\n```javascript\n    var msgtext = message.text;\n\n    if (msgtext.indexOf(\"/\") != 0) return false; // no slash at beginning?\n    var command = msgtext.substring(1, msgtext.indexOf(\" \"));\n    if (COMMANDS[command] == null) return false; // not a valid command?\n    COMMANDS[command](message);\n    return true;\n}\n```\n\nAnd this is the runCommand method. It takes the entire Telegram message object, which we got as a response from the Telegram API above and tries to parse its text as a command. A command string always starts with a slash. So if there is not slash in the beginning, we can be sure that we didn‚Äôt get a valid command. We simply return here, but we also could send a message to the user telling him ‚ÄúHey, please enter a valid command.‚Äù. But let‚Äôs keep it simple. In the following line we extract everything after the slash and before the first blank space (assuming a command mustn‚Äôt contain a blank space) as the command. Afterwards we look into our map if the command actually is a key for a method and if so, we just run this method, passing it the message object as a parameter as the function will need information out of it.\n\nWhat have we done so far? We wrote a program that requests the Telegram API for new messages, parses potentially contained commands and runs functions depending on these commands.\n\nAll we still need is to implement the method belonging to the */dosth* command.\n\n```javascript\n    var caps = message.text.toUpperCase();\n    var answer = {\n        chat_id : message.chat.id,\n        text : \"You told be to do something, so I took your input and made it all caps. Look: \" + caps\n    };\n\n    unirest.post(SEND_MESSAGE_URL)\n        .send(answer)\n        .end(function (response) {\n            if (response.status == 200) console.log(\"Successfully sent message to \" + message.chat.id);\n        });\n}\n```\n\nMost times you‚Äôll want to send a message as response to your user. You could also send an image, an audio, a location, ‚Ä¶ (see [https://core.telegram.org/bots/api#available-methods](https://core.telegram.org/bots/api#available-methods)). Every message object needs a *chat_id* field, containing a Telegram user id, so that Telegram knows which user to deliver your message to. We simply extract this id out of the message object‚Äôs chat object we received from the user. The second mandatory field in a message object is the *text*. This is up to you. After having set up the new message object (you could add other fields, e.g. to show bot-buttons to the user ‚Äì see [https://core.telegram.org/bots/api#message](https://core.telegram.org/bots/api#message) for this), we just need to HTTP POST it to Telegram using Unirest again. Finished.\n\nThis example was kept veeeery simple. Of course you could implement your bot to do really fancy things. You could process media, do location-specific operations, include a database (MongoDB suits really well!) and many, many other things. You could write any complex application you can imagine ‚Äì with a Telegram chat as the text i/o interface. Please tell be your ideas ‚Äì what would be a great bot?\n\nIf you like to try by bot, simply write a message to **@FavoriteBot** and share it to your friends, if you like it.\n\nIf you have any questions, contact me via mail to *ferdinand(at)muetsch.io.*","slug":"how-to-make-telegram-bots","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhi9000i40mq9fgv7ddw","content":"<p>Recently <a href=\"http://telegram.org\" title=\"Telegram\" target=\"_blank\" rel=\"noopener\">Telegram</a> has introduced a new feature, the bots. Basically the bots enable human Telegram users to talk to machines, i.e. customly written little programs. A bot could be a daily helper, be it a bot that you can ask for the current temperature, one that googles something for you, one to manage your todo‚Äôs or even a little text based game ‚Äì everything within the Telegram chat. The nice thing about them is that they‚Äôre really simple to create. You can read more about the bots in general here: <a href=\"https://telegram.org/blog/bot-revolution\" target=\"_blank\" rel=\"noopener\">https://telegram.org/blog/bot-revolution</a></p>\n<p>This article shouldn‚Äôt cover how to create and publish a bot (which actually is the same step), but how to write its actual functionality ‚Äì its backend. On how to initially set up one, please refer to this little guide: <a href=\"https://core.telegram.org/bots\" target=\"_blank\" rel=\"noopener\">https://core.telegram.org/bots</a>. It‚Äôs very easy, trust me. Everything you need for that is your Telegram app.</p>\n<p>What you need further to program your bot basically is a favorite programming language, an IDE or at least a text editor, <a href=\"https://core.telegram.org/bots/api\" title=\"this\" target=\"_blank\" rel=\"noopener\">this</a> page to be open and again your app. In this tutorial we will use Node.js as programming language (or more precisely as programming platform), but you really could use any other language, as well. Probably Python or PHP would do the job well, but you could even write a Java application.</p>\n<p>First some basic things to understand. For the most parts your bot is your own application which we will try to create here, running on your own local PC or server. Telegram won‚Äôt host any code. All functionality and data storage is kept in your program on your machine. Basically Telegram doesn‚Äôt provide more than kind of an interface between your users‚Äô Telegram client (app) and your bot application. The flow would be like:</p>\n<ol>\n<li>\n<p>You create a new bot with @BotFather and set its description and commands (the commands you set there are ‚Äì strictly speaking ‚Äì completely independent of which commands your program will actually accept ‚Äì they are just strings which the user gets suggested in a chat with your bot) so that the bot gets publicly accessible as any other (human) Telegram user is via her @ nickname.</p>\n</li>\n<li>\n<p>You write the backend and run it to be listening.</p>\n</li>\n<li>\n<p>A user sends a message to your bot.</p>\n</li>\n<li>\n<p>The (JSON formatted) Telegram message object gets passed to your backend via HTTPS by Telegram.</p>\n</li>\n<li>\n<p>Your backend parses the message text, extract the commands and arguments, processes some logic, possibly does some database actions and so on.</p>\n</li>\n<li>\n<p>Your backend passes a response message object to the Telegram API via HTTPS (the other way round now).</p>\n</li>\n<li>\n<p>Telegram shows the message to your user‚Äôs chat window.</p>\n</li>\n</ol>\n<p>Alright, now let‚Äôs get into the code. I require you to already have set up a bot and got its authentication token (all using @BotFather). As I said, we will make a little Node.js program here, but for those who don‚Äôt understand JavaScript too well, I‚Äôll try to explain everything as clear as you need to re-do this in your programming language.</p>\n<p>First, you need any library that can do HTTP requests, because basically your backend will be busy doing POSTs and GETs for the most time. For Node.js we‚Äôll use the <a href=\"http://unirest.io/nodejs.html\" title=\"Unirest\" target=\"_blank\" rel=\"noopener\">Unirest</a> library, which probably is a little bit overkill, but very simple for our purpose. The library is available for Python, PHP, Ruby, Java and many others as well.</p>\n<p>So first step is setting up a Node application and requiring Unirest (i assume you know how to set up a Node application and you‚Äôre familiar with npm).</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* app.js */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> unirest = <span class=\"built_in\">require</span>(<span class=\"string\">'unirest'</span>);</span><br></pre></td></tr></table></figure>\n<p>Next is to receive messages from your users. Telegram offers two ways to do this. The first way would be to use Webhooks, which basically means that you‚Äôre backend runs on a webserver (or for Node it actually is one itself), Telegram knows your HTTPS endpoint and does a request to your backend every time a message is sent by a user. I would consider this the more elegant way. But the minus about this is that you would need to have a valid SSL certificate (which costs monthly charge) to provide a secure HTTPS connection. Telegram won‚Äôt send any data to an unsecure or unverified endpoint. This is why we will take the other approach, which works kind of the other way round. Your backend will continuously request the Telegram API if there are new messages available. In detail your backend won‚Äôt do request after request after request (because this would be so inefficient!) but use <a href=\"http://www.pubnub.com/blog/http-long-polling/\" title=\"long polling\" target=\"_blank\" rel=\"noopener\">long polling</a>. To put it simple long polling means that a request won‚Äôt be answered instantly, but kept open until there is some data available.</p>\n<p>We need to specify some constants. As i said, everything between your backend and Telegram happens via their HTTPS interface. We set up constants for the base API url, containing your bot token, the URL for the <a href=\"https://core.telegram.org/bots/api#getupdates\" title=\"getUpdates\" target=\"_blank\" rel=\"noopener\">getUpdates</a> method endpoint and the URL for the <a href=\"https://core.telegram.org/bots/api#sendmessage\" title=\"sendMessage\" target=\"_blank\" rel=\"noopener\">sendMessage</a> method endpoint. The <em>:offset:</em> within the URL string will get replaced by a number, specifying, which new messages to fetch from Telegram later.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* app.js */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* ... */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> BASE_URL = <span class=\"string\">\"https://api.telegram.org/botYOUR_TOKEN_HERE/\"</span>;</span><br><span class=\"line\"><span class=\"keyword\">var</span> POLLING_URL = BASE_URL + <span class=\"string\">\"getUpdates?offset=:offset:&amp;timeout=60\"</span>;</span><br><span class=\"line\"><span class=\"keyword\">var</span> SEND_MESSAGE_URL = BASE_URL + <span class=\"string\">\"sendMessage\"</span>;</span><br></pre></td></tr></table></figure>\n<p>Now we‚Äôll introduce a function called <em>poll</em> (you can choose any other name), which basically is kind of the main loop of our program. Here‚Äôs the code for this method, explanation follows.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* ... */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">poll</span>(<span class=\"params\">offset</span>) </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> url = POLLING_URL.replace(<span class=\"string\">\":offset:\"</span>, offset);</span><br><span class=\"line\"></span><br><span class=\"line\">    unirest.get(url)</span><br><span class=\"line\">        .end(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">var</span> body = response.raw_body;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.status == <span class=\"number\">200</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">var</span> jsonData = <span class=\"built_in\">JSON</span>.parse(body);</span><br><span class=\"line\">                <span class=\"keyword\">var</span> result = jsonData.result;</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (result.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> (i <span class=\"keyword\">in</span> result) &#123;</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (runCommand(result[i].message)) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">                    max_offset = <span class=\"built_in\">parseInt</span>(result[result.length - <span class=\"number\">1</span>].update_id) + <span class=\"number\">1</span>; <span class=\"comment\">// update max offset</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                poll(max_offset);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>Alright. The function is recursive, meaning it will call itself ‚Äì namely each time, a request was answered (which is, when a new message was fetched). The http request to Telegram API to get updates will be pending until a message arrives. Then it gets answered, but has to be re-opened again instantly now to continue listening for message updates again. As a parameter it takes an offset number, which replaces the <em>:offset:</em> placeholder in the url string. To read more about this parameter, go to <a href=\"https://core.telegram.org/bots/api#getupdates\" target=\"_blank\" rel=\"noopener\">https://core.telegram.org/bots/api#getupdates</a>. Unirest opens an http request to the specified url and executes the callback function given to end(), if the request was answered. First, we extract the response body. Afterwards, we check if our request was successful. If this is the case, we parse the body (which is a JSON object, consisting of an <em>ok</em> field and a <em>result</em> array. The result array contains one or more message objects. These are the ones that are relevant for us. For each message object, we try to parse it as a command (i‚Äôll give you the runCommand() function is a second‚Ä¶) and depending on which command we got, execute the respective method. Afterwards the offset gets updates to the id of the latest message plus one (to not receive it again next time) and a new request gets opened.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* ... */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> dosth = <span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">message</span>) </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// to be implemented....</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> COMMANDS = &#123;</span><br><span class=\"line\">    <span class=\"string\">\"dosth\"</span> : dosth</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>Now we specify a map, which maps strings (representing the users‚Äô command input ‚Äì in this case <em>/dosth</em> to actual functions.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">var</span> msgtext = message.text;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (msgtext.indexOf(<span class=\"string\">\"/\"</span>) != <span class=\"number\">0</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>; <span class=\"comment\">// no slash at beginning?</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> command = msgtext.substring(<span class=\"number\">1</span>, msgtext.indexOf(<span class=\"string\">\" \"</span>));</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (COMMANDS[command] == <span class=\"literal\">null</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>; <span class=\"comment\">// not a valid command?</span></span><br><span class=\"line\">    COMMANDS[command](message);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>And this is the runCommand method. It takes the entire Telegram message object, which we got as a response from the Telegram API above and tries to parse its text as a command. A command string always starts with a slash. So if there is not slash in the beginning, we can be sure that we didn‚Äôt get a valid command. We simply return here, but we also could send a message to the user telling him ‚ÄúHey, please enter a valid command.‚Äù. But let‚Äôs keep it simple. In the following line we extract everything after the slash and before the first blank space (assuming a command mustn‚Äôt contain a blank space) as the command. Afterwards we look into our map if the command actually is a key for a method and if so, we just run this method, passing it the message object as a parameter as the function will need information out of it.</p>\n<p>What have we done so far? We wrote a program that requests the Telegram API for new messages, parses potentially contained commands and runs functions depending on these commands.</p>\n<p>All we still need is to implement the method belonging to the <em>/dosth</em> command.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">var</span> caps = message.text.toUpperCase();</span><br><span class=\"line\">    <span class=\"keyword\">var</span> answer = &#123;</span><br><span class=\"line\">        chat_id : message.chat.id,</span><br><span class=\"line\">        text : <span class=\"string\">\"You told be to do something, so I took your input and made it all caps. Look: \"</span> + caps</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    unirest.post(SEND_MESSAGE_URL)</span><br><span class=\"line\">        .send(answer)</span><br><span class=\"line\">        .end(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.status == <span class=\"number\">200</span>) <span class=\"built_in\">console</span>.log(<span class=\"string\">\"Successfully sent message to \"</span> + message.chat.id);</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Most times you‚Äôll want to send a message as response to your user. You could also send an image, an audio, a location, ‚Ä¶ (see <a href=\"https://core.telegram.org/bots/api#available-methods\" target=\"_blank\" rel=\"noopener\">https://core.telegram.org/bots/api#available-methods</a>). Every message object needs a <em>chat_id</em> field, containing a Telegram user id, so that Telegram knows which user to deliver your message to. We simply extract this id out of the message object‚Äôs chat object we received from the user. The second mandatory field in a message object is the <em>text</em>. This is up to you. After having set up the new message object (you could add other fields, e.g. to show bot-buttons to the user ‚Äì see <a href=\"https://core.telegram.org/bots/api#message\" target=\"_blank\" rel=\"noopener\">https://core.telegram.org/bots/api#message</a> for this), we just need to HTTP POST it to Telegram using Unirest again. Finished.</p>\n<p>This example was kept veeeery simple. Of course you could implement your bot to do really fancy things. You could process media, do location-specific operations, include a database (MongoDB suits really well!) and many, many other things. You could write any complex application you can imagine ‚Äì with a Telegram chat as the text i/o interface. Please tell be your ideas ‚Äì what would be a great bot?</p>\n<p>If you like to try by bot, simply write a message to <strong>@FavoriteBot</strong> and share it to your friends, if you like it.</p>\n<p>If you have any questions, contact me via mail to <em>ferdinand(at)<a href=\"http://muetsch.io\">muetsch.io</a>.</em></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Recently <a href=\"http://telegram.org\" title=\"Telegram\" target=\"_blank\" rel=\"noopener\">Telegram</a> has introduced a new feature, the bots. Basically the bots enable human Telegram users to talk to machines, i.e. customly written little programs. A bot could be a daily helper, be it a bot that you can ask for the current temperature, one that googles something for you, one to manage your todo‚Äôs or even a little text based game ‚Äì everything within the Telegram chat. The nice thing about them is that they‚Äôre really simple to create. You can read more about the bots in general here: <a href=\"https://telegram.org/blog/bot-revolution\" target=\"_blank\" rel=\"noopener\">https://telegram.org/blog/bot-revolution</a></p>\n<p>This article shouldn‚Äôt cover how to create and publish a bot (which actually is the same step), but how to write its actual functionality ‚Äì its backend. On how to initially set up one, please refer to this little guide: <a href=\"https://core.telegram.org/bots\" target=\"_blank\" rel=\"noopener\">https://core.telegram.org/bots</a>. It‚Äôs very easy, trust me. Everything you need for that is your Telegram app.</p>\n<p>What you need further to program your bot basically is a favorite programming language, an IDE or at least a text editor, <a href=\"https://core.telegram.org/bots/api\" title=\"this\" target=\"_blank\" rel=\"noopener\">this</a> page to be open and again your app. In this tutorial we will use Node.js as programming language (or more precisely as programming platform), but you really could use any other language, as well. Probably Python or PHP would do the job well, but you could even write a Java application.</p>\n<p>First some basic things to understand. For the most parts your bot is your own application which we will try to create here, running on your own local PC or server. Telegram won‚Äôt host any code. All functionality and data storage is kept in your program on your machine. Basically Telegram doesn‚Äôt provide more than kind of an interface between your users‚Äô Telegram client (app) and your bot application. The flow would be like:</p>\n<ol>\n<li>\n<p>You create a new bot with @BotFather and set its description and commands (the commands you set there are ‚Äì strictly speaking ‚Äì completely independent of which commands your program will actually accept ‚Äì they are just strings which the user gets suggested in a chat with your bot) so that the bot gets publicly accessible as any other (human) Telegram user is via her @ nickname.</p>\n</li>\n<li>\n<p>You write the backend and run it to be listening.</p>\n</li>\n<li>\n<p>A user sends a message to your bot.</p>\n</li>\n<li>\n<p>The (JSON formatted) Telegram message object gets passed to your backend via HTTPS by Telegram.</p>\n</li>\n<li>\n<p>Your backend parses the message text, extract the commands and arguments, processes some logic, possibly does some database actions and so on.</p>\n</li>\n<li>\n<p>Your backend passes a response message object to the Telegram API via HTTPS (the other way round now).</p>\n</li>\n<li>\n<p>Telegram shows the message to your user‚Äôs chat window.</p>\n</li>\n</ol>\n<p>Alright, now let‚Äôs get into the code. I require you to already have set up a bot and got its authentication token (all using @BotFather). As I said, we will make a little Node.js program here, but for those who don‚Äôt understand JavaScript too well, I‚Äôll try to explain everything as clear as you need to re-do this in your programming language.</p>\n<p>First, you need any library that can do HTTP requests, because basically your backend will be busy doing POSTs and GETs for the most time. For Node.js we‚Äôll use the <a href=\"http://unirest.io/nodejs.html\" title=\"Unirest\" target=\"_blank\" rel=\"noopener\">Unirest</a> library, which probably is a little bit overkill, but very simple for our purpose. The library is available for Python, PHP, Ruby, Java and many others as well.</p>\n<p>So first step is setting up a Node application and requiring Unirest (i assume you know how to set up a Node application and you‚Äôre familiar with npm).</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* app.js */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> unirest = <span class=\"built_in\">require</span>(<span class=\"string\">'unirest'</span>);</span><br></pre></td></tr></table></figure>\n<p>Next is to receive messages from your users. Telegram offers two ways to do this. The first way would be to use Webhooks, which basically means that you‚Äôre backend runs on a webserver (or for Node it actually is one itself), Telegram knows your HTTPS endpoint and does a request to your backend every time a message is sent by a user. I would consider this the more elegant way. But the minus about this is that you would need to have a valid SSL certificate (which costs monthly charge) to provide a secure HTTPS connection. Telegram won‚Äôt send any data to an unsecure or unverified endpoint. This is why we will take the other approach, which works kind of the other way round. Your backend will continuously request the Telegram API if there are new messages available. In detail your backend won‚Äôt do request after request after request (because this would be so inefficient!) but use <a href=\"http://www.pubnub.com/blog/http-long-polling/\" title=\"long polling\" target=\"_blank\" rel=\"noopener\">long polling</a>. To put it simple long polling means that a request won‚Äôt be answered instantly, but kept open until there is some data available.</p>\n<p>We need to specify some constants. As i said, everything between your backend and Telegram happens via their HTTPS interface. We set up constants for the base API url, containing your bot token, the URL for the <a href=\"https://core.telegram.org/bots/api#getupdates\" title=\"getUpdates\" target=\"_blank\" rel=\"noopener\">getUpdates</a> method endpoint and the URL for the <a href=\"https://core.telegram.org/bots/api#sendmessage\" title=\"sendMessage\" target=\"_blank\" rel=\"noopener\">sendMessage</a> method endpoint. The <em>:offset:</em> within the URL string will get replaced by a number, specifying, which new messages to fetch from Telegram later.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* app.js */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* ... */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> BASE_URL = <span class=\"string\">\"https://api.telegram.org/botYOUR_TOKEN_HERE/\"</span>;</span><br><span class=\"line\"><span class=\"keyword\">var</span> POLLING_URL = BASE_URL + <span class=\"string\">\"getUpdates?offset=:offset:&amp;timeout=60\"</span>;</span><br><span class=\"line\"><span class=\"keyword\">var</span> SEND_MESSAGE_URL = BASE_URL + <span class=\"string\">\"sendMessage\"</span>;</span><br></pre></td></tr></table></figure>\n<p>Now we‚Äôll introduce a function called <em>poll</em> (you can choose any other name), which basically is kind of the main loop of our program. Here‚Äôs the code for this method, explanation follows.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* ... */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">poll</span>(<span class=\"params\">offset</span>) </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> url = POLLING_URL.replace(<span class=\"string\">\":offset:\"</span>, offset);</span><br><span class=\"line\"></span><br><span class=\"line\">    unirest.get(url)</span><br><span class=\"line\">        .end(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">var</span> body = response.raw_body;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.status == <span class=\"number\">200</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">var</span> jsonData = <span class=\"built_in\">JSON</span>.parse(body);</span><br><span class=\"line\">                <span class=\"keyword\">var</span> result = jsonData.result;</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (result.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> (i <span class=\"keyword\">in</span> result) &#123;</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (runCommand(result[i].message)) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">                    max_offset = <span class=\"built_in\">parseInt</span>(result[result.length - <span class=\"number\">1</span>].update_id) + <span class=\"number\">1</span>; <span class=\"comment\">// update max offset</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                poll(max_offset);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>Alright. The function is recursive, meaning it will call itself ‚Äì namely each time, a request was answered (which is, when a new message was fetched). The http request to Telegram API to get updates will be pending until a message arrives. Then it gets answered, but has to be re-opened again instantly now to continue listening for message updates again. As a parameter it takes an offset number, which replaces the <em>:offset:</em> placeholder in the url string. To read more about this parameter, go to <a href=\"https://core.telegram.org/bots/api#getupdates\" target=\"_blank\" rel=\"noopener\">https://core.telegram.org/bots/api#getupdates</a>. Unirest opens an http request to the specified url and executes the callback function given to end(), if the request was answered. First, we extract the response body. Afterwards, we check if our request was successful. If this is the case, we parse the body (which is a JSON object, consisting of an <em>ok</em> field and a <em>result</em> array. The result array contains one or more message objects. These are the ones that are relevant for us. For each message object, we try to parse it as a command (i‚Äôll give you the runCommand() function is a second‚Ä¶) and depending on which command we got, execute the respective method. Afterwards the offset gets updates to the id of the latest message plus one (to not receive it again next time) and a new request gets opened.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* ... */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> dosth = <span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">message</span>) </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// to be implemented....</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> COMMANDS = &#123;</span><br><span class=\"line\">    <span class=\"string\">\"dosth\"</span> : dosth</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>Now we specify a map, which maps strings (representing the users‚Äô command input ‚Äì in this case <em>/dosth</em> to actual functions.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">var</span> msgtext = message.text;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (msgtext.indexOf(<span class=\"string\">\"/\"</span>) != <span class=\"number\">0</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>; <span class=\"comment\">// no slash at beginning?</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> command = msgtext.substring(<span class=\"number\">1</span>, msgtext.indexOf(<span class=\"string\">\" \"</span>));</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (COMMANDS[command] == <span class=\"literal\">null</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>; <span class=\"comment\">// not a valid command?</span></span><br><span class=\"line\">    COMMANDS[command](message);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>And this is the runCommand method. It takes the entire Telegram message object, which we got as a response from the Telegram API above and tries to parse its text as a command. A command string always starts with a slash. So if there is not slash in the beginning, we can be sure that we didn‚Äôt get a valid command. We simply return here, but we also could send a message to the user telling him ‚ÄúHey, please enter a valid command.‚Äù. But let‚Äôs keep it simple. In the following line we extract everything after the slash and before the first blank space (assuming a command mustn‚Äôt contain a blank space) as the command. Afterwards we look into our map if the command actually is a key for a method and if so, we just run this method, passing it the message object as a parameter as the function will need information out of it.</p>\n<p>What have we done so far? We wrote a program that requests the Telegram API for new messages, parses potentially contained commands and runs functions depending on these commands.</p>\n<p>All we still need is to implement the method belonging to the <em>/dosth</em> command.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">var</span> caps = message.text.toUpperCase();</span><br><span class=\"line\">    <span class=\"keyword\">var</span> answer = &#123;</span><br><span class=\"line\">        chat_id : message.chat.id,</span><br><span class=\"line\">        text : <span class=\"string\">\"You told be to do something, so I took your input and made it all caps. Look: \"</span> + caps</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    unirest.post(SEND_MESSAGE_URL)</span><br><span class=\"line\">        .send(answer)</span><br><span class=\"line\">        .end(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.status == <span class=\"number\">200</span>) <span class=\"built_in\">console</span>.log(<span class=\"string\">\"Successfully sent message to \"</span> + message.chat.id);</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Most times you‚Äôll want to send a message as response to your user. You could also send an image, an audio, a location, ‚Ä¶ (see <a href=\"https://core.telegram.org/bots/api#available-methods\" target=\"_blank\" rel=\"noopener\">https://core.telegram.org/bots/api#available-methods</a>). Every message object needs a <em>chat_id</em> field, containing a Telegram user id, so that Telegram knows which user to deliver your message to. We simply extract this id out of the message object‚Äôs chat object we received from the user. The second mandatory field in a message object is the <em>text</em>. This is up to you. After having set up the new message object (you could add other fields, e.g. to show bot-buttons to the user ‚Äì see <a href=\"https://core.telegram.org/bots/api#message\" target=\"_blank\" rel=\"noopener\">https://core.telegram.org/bots/api#message</a> for this), we just need to HTTP POST it to Telegram using Unirest again. Finished.</p>\n<p>This example was kept veeeery simple. Of course you could implement your bot to do really fancy things. You could process media, do location-specific operations, include a database (MongoDB suits really well!) and many, many other things. You could write any complex application you can imagine ‚Äì with a Telegram chat as the text i/o interface. Please tell be your ideas ‚Äì what would be a great bot?</p>\n<p>If you like to try by bot, simply write a message to <strong>@FavoriteBot</strong> and share it to your friends, if you like it.</p>\n<p>If you have any questions, contact me via mail to <em>ferdinand(at)<a href=\"http://muetsch.io\">muetsch.io</a>.</em></p>\n"},{"title":"How to receive sharing intents in Flutter?","date":"2019-03-11T05:56:28.000Z","_content":"\n# Use Case\nA common use case when building an Android app is to have it handle data shared from other apps. All Android users know this little dialog that pops up when you hit the _\"Share\"_ button in any app. It displays a list of applications, which are registered for receiving shared data. The user has to choose one, which is then opened up to handle the shared text, URL or whatever it is. On iOS, a similar concept exists. However, this article focused on **Android only**. \n\n![](https://cketti.de/img/share-url-to-clipboard/screenshot_share.png)\n_(Source: https://cketti.de/)_\n\nAs an example, let's imagine having a bookmark-manager written in Flutter. It is supposed to save **URLs** with their accompanying **titles**, shared from the smartphone's browser to the app, to one of your bookmark collection. This is exactly [what I just build](https://github.com/muety/anchr-android).\n\n# Background\nThe official Flutter docs already give [a good example](https://flutter.dev/docs/get-started/flutter-for/android-devs#how-do-i-handle-incoming-intents-from-external-applications-in-flutter) on how to achieve that functionality. However, I found that their piece of code only works, if you share data to an app that **is still closed**. If you had opened your app before and it idles in the background, it won't receive the [sharing intent](https://www.androidcode.ninja/android-share-intent-example/) when it is [resumed](https://developer.android.com/guide/components/activities/activity-lifecycle#onresume). Therefore, I extended the example. \n\n# Code\n## AndroidManifest.xml\nFirst, you have to add an `intent-filter` to your `AndroidManifest.xml` in the `android/` directory to register your app as a sharing target.\n\n```xml\n...\n<intent-filter>\n    <action android:name=\"android.intent.action.SEND\" />\n    <category android:name=\"android.intent.category.DEFAULT\" />\n    <data android:mimeType=\"text/plain\" />\n</intent-filter>\n...\n```\n\n## MainActivity.java\nSecondly, you will need to add some code to `MainActivity.java` (analogously for Kotlin projects). In the `onCreate()` lifecycle hook, you have to register a `MethodCallHandler()` to act as an interface between the underlying Android app and your flutter code. In addition, you have to override the `onNewIntent()` callback, which is triggered when a new sharing intent (`Intent.ACTION_SEND`) causes your app to change its lifecycle state. Lastly, you need a method to handle the actual content shared from the external app. It consists of two fields, a URL and a title, both represented as strings in a Map. In the end, your `MainActivity` looks like something like this.\n\n```java\nprivate Map<String, String> sharedData = new HashMap();\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        GeneratedPluginRegistrant.registerWith(this);\n\n        // Handle intent when app is initially opened\n        handleSendIntent(getIntent());\n\n        new MethodChannel(getFlutterView(), \"app.channel.shared.data\").setMethodCallHandler(\n            new MethodCallHandler() {\n                @Override\n                public void onMethodCall(MethodCall call, MethodChannel.Result result) {\n                    if (call.method.contentEquals(\"getSharedData\")) {\n                        result.success(sharedData);\n                        sharedData.clear();\n                    }\n                }\n            }\n        );\n    }\n\n    @Override\n    protected void onNewIntent(Intent intent) {\n        // Handle intent when app is resumed\n        super.onNewIntent(intent);\n        handleSendIntent(intent);\n    }\n\n    private void handleSendIntent(Intent intent) {\n        String action = intent.getAction();\n        String type = intent.getType();\n\n        // We only care about sharing intent that contain plain text\n        if (Intent.ACTION_SEND.equals(action) && type != null) {\n            if (\"text/plain\".equals(type)) {\n                sharedData.put(\"subject\", intent.getStringExtra(Intent.EXTRA_SUBJECT));\n                sharedData.put(\"text\", intent.getStringExtra(Intent.EXTRA_TEXT));\n            }\n        }\n    }\n}\n```\n\nNote that the shared data is \"cached\" on the Java side of your app until is it picked up by your Flutter code.\n\n## Your Flutter app\nEventually, you need to add a method to your Flutter code to interact with the native-Android `MethodHandler`. It will be called once during state initialization and ‚Äì with the help of a listener ‚Äì every time the underlying Android activity is resumed. \n\n```dart\nclass SampleAppPage extends StatefulWidget {\n  SampleAppPage({Key key}) : super(key: key);\n\n  @override\n  _SampleAppPageState createState() => _SampleAppPageState();\n}\n\nclass _SampleAppPageState extends State<SampleAppPage> {\n    static const platform = const MethodChannel('app.channel.shared.data');\n    Map<dynamic, dynamic> sharedData = Map();\n\n    @override\n    void initState() {\n        super.initState();\n        _init();\n    }\n\n    _init() async {\n        // Case 1: App is already running in background:\n        // Listen to lifecycle changes to subsequently call Java MethodHandler to check for shared data\n        SystemChannels.lifecycle.setMessageHandler((msg) {\n            if (msg.contains('resumed')) {\n                _getSharedData().then((d) {\n                    if (d.isEmpty) return;\n                    // Your logic here\n                    // E.g. at this place you might want to use Navigator to launch a new page and pass the shared data\n                });\n            }\n        });\n\n        // Case 2: App is started by the intent:\n        // Call Java MethodHandler on application start up to check for shared data\n        var data = await _getSharedData();\n        setState(() => sharedData = data);\n\n        // You can use sharedData in your build() method now\n    }\n\n    Future<Map> _getSharedData() async => await platform.invokeMethod('getSharedData');\n}\n```\n\nNow you're good to go! Once you extracted the sharing intent's contents, you can, for instance, show a pre-filled dialog to add the new link to one of your bookmark collections, just as [I did here](https://github.com/muety/anchr-android/blob/897395528532a03ce4e1bdba00fe4b3b35f5fe43/lib/app.dart#L39).\n\n# Conclusion\nThis approach might seem a little complicated, but in fact, it is the only working solution I could find. There is a plugin called [flutter-share](https://github.com/d-silveira/flutter-share), but unfortunately it did not work for me. Happy coding üòâ!","source":"_posts/how-to-receive-sharing-intents-in-flutter.md","raw":"---\ntitle: How to receive sharing intents in Flutter?\ndate: 2019-03-11 06:56:28\ntags:\n---\n\n# Use Case\nA common use case when building an Android app is to have it handle data shared from other apps. All Android users know this little dialog that pops up when you hit the _\"Share\"_ button in any app. It displays a list of applications, which are registered for receiving shared data. The user has to choose one, which is then opened up to handle the shared text, URL or whatever it is. On iOS, a similar concept exists. However, this article focused on **Android only**. \n\n![](https://cketti.de/img/share-url-to-clipboard/screenshot_share.png)\n_(Source: https://cketti.de/)_\n\nAs an example, let's imagine having a bookmark-manager written in Flutter. It is supposed to save **URLs** with their accompanying **titles**, shared from the smartphone's browser to the app, to one of your bookmark collection. This is exactly [what I just build](https://github.com/muety/anchr-android).\n\n# Background\nThe official Flutter docs already give [a good example](https://flutter.dev/docs/get-started/flutter-for/android-devs#how-do-i-handle-incoming-intents-from-external-applications-in-flutter) on how to achieve that functionality. However, I found that their piece of code only works, if you share data to an app that **is still closed**. If you had opened your app before and it idles in the background, it won't receive the [sharing intent](https://www.androidcode.ninja/android-share-intent-example/) when it is [resumed](https://developer.android.com/guide/components/activities/activity-lifecycle#onresume). Therefore, I extended the example. \n\n# Code\n## AndroidManifest.xml\nFirst, you have to add an `intent-filter` to your `AndroidManifest.xml` in the `android/` directory to register your app as a sharing target.\n\n```xml\n...\n<intent-filter>\n    <action android:name=\"android.intent.action.SEND\" />\n    <category android:name=\"android.intent.category.DEFAULT\" />\n    <data android:mimeType=\"text/plain\" />\n</intent-filter>\n...\n```\n\n## MainActivity.java\nSecondly, you will need to add some code to `MainActivity.java` (analogously for Kotlin projects). In the `onCreate()` lifecycle hook, you have to register a `MethodCallHandler()` to act as an interface between the underlying Android app and your flutter code. In addition, you have to override the `onNewIntent()` callback, which is triggered when a new sharing intent (`Intent.ACTION_SEND`) causes your app to change its lifecycle state. Lastly, you need a method to handle the actual content shared from the external app. It consists of two fields, a URL and a title, both represented as strings in a Map. In the end, your `MainActivity` looks like something like this.\n\n```java\nprivate Map<String, String> sharedData = new HashMap();\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        GeneratedPluginRegistrant.registerWith(this);\n\n        // Handle intent when app is initially opened\n        handleSendIntent(getIntent());\n\n        new MethodChannel(getFlutterView(), \"app.channel.shared.data\").setMethodCallHandler(\n            new MethodCallHandler() {\n                @Override\n                public void onMethodCall(MethodCall call, MethodChannel.Result result) {\n                    if (call.method.contentEquals(\"getSharedData\")) {\n                        result.success(sharedData);\n                        sharedData.clear();\n                    }\n                }\n            }\n        );\n    }\n\n    @Override\n    protected void onNewIntent(Intent intent) {\n        // Handle intent when app is resumed\n        super.onNewIntent(intent);\n        handleSendIntent(intent);\n    }\n\n    private void handleSendIntent(Intent intent) {\n        String action = intent.getAction();\n        String type = intent.getType();\n\n        // We only care about sharing intent that contain plain text\n        if (Intent.ACTION_SEND.equals(action) && type != null) {\n            if (\"text/plain\".equals(type)) {\n                sharedData.put(\"subject\", intent.getStringExtra(Intent.EXTRA_SUBJECT));\n                sharedData.put(\"text\", intent.getStringExtra(Intent.EXTRA_TEXT));\n            }\n        }\n    }\n}\n```\n\nNote that the shared data is \"cached\" on the Java side of your app until is it picked up by your Flutter code.\n\n## Your Flutter app\nEventually, you need to add a method to your Flutter code to interact with the native-Android `MethodHandler`. It will be called once during state initialization and ‚Äì with the help of a listener ‚Äì every time the underlying Android activity is resumed. \n\n```dart\nclass SampleAppPage extends StatefulWidget {\n  SampleAppPage({Key key}) : super(key: key);\n\n  @override\n  _SampleAppPageState createState() => _SampleAppPageState();\n}\n\nclass _SampleAppPageState extends State<SampleAppPage> {\n    static const platform = const MethodChannel('app.channel.shared.data');\n    Map<dynamic, dynamic> sharedData = Map();\n\n    @override\n    void initState() {\n        super.initState();\n        _init();\n    }\n\n    _init() async {\n        // Case 1: App is already running in background:\n        // Listen to lifecycle changes to subsequently call Java MethodHandler to check for shared data\n        SystemChannels.lifecycle.setMessageHandler((msg) {\n            if (msg.contains('resumed')) {\n                _getSharedData().then((d) {\n                    if (d.isEmpty) return;\n                    // Your logic here\n                    // E.g. at this place you might want to use Navigator to launch a new page and pass the shared data\n                });\n            }\n        });\n\n        // Case 2: App is started by the intent:\n        // Call Java MethodHandler on application start up to check for shared data\n        var data = await _getSharedData();\n        setState(() => sharedData = data);\n\n        // You can use sharedData in your build() method now\n    }\n\n    Future<Map> _getSharedData() async => await platform.invokeMethod('getSharedData');\n}\n```\n\nNow you're good to go! Once you extracted the sharing intent's contents, you can, for instance, show a pre-filled dialog to add the new link to one of your bookmark collections, just as [I did here](https://github.com/muety/anchr-android/blob/897395528532a03ce4e1bdba00fe4b3b35f5fe43/lib/app.dart#L39).\n\n# Conclusion\nThis approach might seem a little complicated, but in fact, it is the only working solution I could find. There is a plugin called [flutter-share](https://github.com/d-silveira/flutter-share), but unfortunately it did not work for me. Happy coding üòâ!","slug":"how-to-receive-sharing-intents-in-flutter","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhia000j40mq7h7jhj32","content":"<h1>Use Case</h1>\n<p>A common use case when building an Android app is to have it handle data shared from other apps. All Android users know this little dialog that pops up when you hit the <em>‚ÄúShare‚Äù</em> button in any app. It displays a list of applications, which are registered for receiving shared data. The user has to choose one, which is then opened up to handle the shared text, URL or whatever it is. On iOS, a similar concept exists. However, this article focused on <strong>Android only</strong>.</p>\n<p><img src=\"https://cketti.de/img/share-url-to-clipboard/screenshot_share.png\" alt><br>\n<em>(Source: <a href=\"https://cketti.de/\" target=\"_blank\" rel=\"noopener\">https://cketti.de/</a>)</em></p>\n<p>As an example, let‚Äôs imagine having a bookmark-manager written in Flutter. It is supposed to save <strong>URLs</strong> with their accompanying <strong>titles</strong>, shared from the smartphone‚Äôs browser to the app, to one of your bookmark collection. This is exactly <a href=\"https://github.com/muety/anchr-android\" target=\"_blank\" rel=\"noopener\">what I just build</a>.</p>\n<h1>Background</h1>\n<p>The official Flutter docs already give <a href=\"https://flutter.dev/docs/get-started/flutter-for/android-devs#how-do-i-handle-incoming-intents-from-external-applications-in-flutter\" target=\"_blank\" rel=\"noopener\">a good example</a> on how to achieve that functionality. However, I found that their piece of code only works, if you share data to an app that <strong>is still closed</strong>. If you had opened your app before and it idles in the background, it won‚Äôt receive the <a href=\"https://www.androidcode.ninja/android-share-intent-example/\" target=\"_blank\" rel=\"noopener\">sharing intent</a> when it is <a href=\"https://developer.android.com/guide/components/activities/activity-lifecycle#onresume\" target=\"_blank\" rel=\"noopener\">resumed</a>. Therefore, I extended the example.</p>\n<h1>Code</h1>\n<h2 id=\"AndroidManifest-xml\">AndroidManifest.xml</h2>\n<p>First, you have to add an <code>intent-filter</code> to your <code>AndroidManifest.xml</code> in the <code>android/</code> directory to register your app as a sharing target.</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">intent-filter</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">action</span> <span class=\"attr\">android:name</span>=<span class=\"string\">\"android.intent.action.SEND\"</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">category</span> <span class=\"attr\">android:name</span>=<span class=\"string\">\"android.intent.category.DEFAULT\"</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">data</span> <span class=\"attr\">android:mimeType</span>=<span class=\"string\">\"text/plain\"</span> /&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">intent-filter</span>&gt;</span></span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<h2 id=\"MainActivity-java\">MainActivity.java</h2>\n<p>Secondly, you will need to add some code to <code>MainActivity.java</code> (analogously for Kotlin projects). In the <code>onCreate()</code> lifecycle hook, you have to register a <code>MethodCallHandler()</code> to act as an interface between the underlying Android app and your flutter code. In addition, you have to override the <code>onNewIntent()</code> callback, which is triggered when a new sharing intent (<code>Intent.ACTION_SEND</code>) causes your app to change its lifecycle state. Lastly, you need a method to handle the actual content shared from the external app. It consists of two fields, a URL and a title, both represented as strings in a Map. In the end, your <code>MainActivity</code> looks like something like this.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> Map&lt;String, String&gt; sharedData = <span class=\"keyword\">new</span> HashMap();</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">onCreate</span><span class=\"params\">(Bundle savedInstanceState)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onCreate(savedInstanceState);</span><br><span class=\"line\">        GeneratedPluginRegistrant.registerWith(<span class=\"keyword\">this</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Handle intent when app is initially opened</span></span><br><span class=\"line\">        handleSendIntent(getIntent());</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">new</span> MethodChannel(getFlutterView(), <span class=\"string\">\"app.channel.shared.data\"</span>).setMethodCallHandler(</span><br><span class=\"line\">            <span class=\"keyword\">new</span> MethodCallHandler() &#123;</span><br><span class=\"line\">                <span class=\"meta\">@Override</span></span><br><span class=\"line\">                <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onMethodCall</span><span class=\"params\">(MethodCall call, MethodChannel.Result result)</span> </span>&#123;</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (call.method.contentEquals(<span class=\"string\">\"getSharedData\"</span>)) &#123;</span><br><span class=\"line\">                        result.success(sharedData);</span><br><span class=\"line\">                        sharedData.clear();</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">onNewIntent</span><span class=\"params\">(Intent intent)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// Handle intent when app is resumed</span></span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onNewIntent(intent);</span><br><span class=\"line\">        handleSendIntent(intent);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">handleSendIntent</span><span class=\"params\">(Intent intent)</span> </span>&#123;</span><br><span class=\"line\">        String action = intent.getAction();</span><br><span class=\"line\">        String type = intent.getType();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// We only care about sharing intent that contain plain text</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (Intent.ACTION_SEND.equals(action) &amp;&amp; type != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"string\">\"text/plain\"</span>.equals(type)) &#123;</span><br><span class=\"line\">                sharedData.put(<span class=\"string\">\"subject\"</span>, intent.getStringExtra(Intent.EXTRA_SUBJECT));</span><br><span class=\"line\">                sharedData.put(<span class=\"string\">\"text\"</span>, intent.getStringExtra(Intent.EXTRA_TEXT));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Note that the shared data is ‚Äúcached‚Äù on the Java side of your app until is it picked up by your Flutter code.</p>\n<h2 id=\"Your-Flutter-app\">Your Flutter app</h2>\n<p>Eventually, you need to add a method to your Flutter code to interact with the native-Android <code>MethodHandler</code>. It will be called once during state initialization and ‚Äì with the help of a listener ‚Äì every time the underlying Android activity is resumed.</p>\n<figure class=\"highlight dart\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SampleAppPage</span> <span class=\"keyword\">extends</span> <span class=\"title\">StatefulWidget</span> </span>&#123;</span><br><span class=\"line\">  SampleAppPage(&#123;Key key&#125;) : <span class=\"keyword\">super</span>(key: key);</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"meta\">@override</span></span><br><span class=\"line\">  _SampleAppPageState createState() =&gt; _SampleAppPageState();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">_SampleAppPageState</span> <span class=\"keyword\">extends</span> <span class=\"title\">State</span>&lt;<span class=\"title\">SampleAppPage</span>&gt; </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">const</span> platform = <span class=\"keyword\">const</span> MethodChannel(<span class=\"string\">'app.channel.shared.data'</span>);</span><br><span class=\"line\">    <span class=\"built_in\">Map</span>&lt;<span class=\"built_in\">dynamic</span>, <span class=\"built_in\">dynamic</span>&gt; sharedData = <span class=\"built_in\">Map</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@override</span></span><br><span class=\"line\">    <span class=\"keyword\">void</span> initState() &#123;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.initState();</span><br><span class=\"line\">        _init();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    _init() <span class=\"keyword\">async</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Case 1: App is already running in background:</span></span><br><span class=\"line\">        <span class=\"comment\">// Listen to lifecycle changes to subsequently call Java MethodHandler to check for shared data</span></span><br><span class=\"line\">        SystemChannels.lifecycle.setMessageHandler((msg) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (msg.contains(<span class=\"string\">'resumed'</span>)) &#123;</span><br><span class=\"line\">                _getSharedData().then((d) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (d.isEmpty) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">                    <span class=\"comment\">// Your logic here</span></span><br><span class=\"line\">                    <span class=\"comment\">// E.g. at this place you might want to use Navigator to launch a new page and pass the shared data</span></span><br><span class=\"line\">                &#125;);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Case 2: App is started by the intent:</span></span><br><span class=\"line\">        <span class=\"comment\">// Call Java MethodHandler on application start up to check for shared data</span></span><br><span class=\"line\">        <span class=\"keyword\">var</span> data = <span class=\"keyword\">await</span> _getSharedData();</span><br><span class=\"line\">        setState(() =&gt; sharedData = data);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// You can use sharedData in your build() method now</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    Future&lt;<span class=\"built_in\">Map</span>&gt; _getSharedData() <span class=\"keyword\">async</span> =&gt; <span class=\"keyword\">await</span> platform.invokeMethod(<span class=\"string\">'getSharedData'</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Now you‚Äôre good to go! Once you extracted the sharing intent‚Äôs contents, you can, for instance, show a pre-filled dialog to add the new link to one of your bookmark collections, just as <a href=\"https://github.com/muety/anchr-android/blob/897395528532a03ce4e1bdba00fe4b3b35f5fe43/lib/app.dart#L39\" target=\"_blank\" rel=\"noopener\">I did here</a>.</p>\n<h1>Conclusion</h1>\n<p>This approach might seem a little complicated, but in fact, it is the only working solution I could find. There is a plugin called <a href=\"https://github.com/d-silveira/flutter-share\" target=\"_blank\" rel=\"noopener\">flutter-share</a>, but unfortunately it did not work for me. Happy coding üòâ!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1>Use Case</h1>\n<p>A common use case when building an Android app is to have it handle data shared from other apps. All Android users know this little dialog that pops up when you hit the <em>‚ÄúShare‚Äù</em> button in any app. It displays a list of applications, which are registered for receiving shared data. The user has to choose one, which is then opened up to handle the shared text, URL or whatever it is. On iOS, a similar concept exists. However, this article focused on <strong>Android only</strong>.</p>\n<p><img src=\"https://cketti.de/img/share-url-to-clipboard/screenshot_share.png\" alt><br>\n<em>(Source: <a href=\"https://cketti.de/\" target=\"_blank\" rel=\"noopener\">https://cketti.de/</a>)</em></p>\n<p>As an example, let‚Äôs imagine having a bookmark-manager written in Flutter. It is supposed to save <strong>URLs</strong> with their accompanying <strong>titles</strong>, shared from the smartphone‚Äôs browser to the app, to one of your bookmark collection. This is exactly <a href=\"https://github.com/muety/anchr-android\" target=\"_blank\" rel=\"noopener\">what I just build</a>.</p>\n<h1>Background</h1>\n<p>The official Flutter docs already give <a href=\"https://flutter.dev/docs/get-started/flutter-for/android-devs#how-do-i-handle-incoming-intents-from-external-applications-in-flutter\" target=\"_blank\" rel=\"noopener\">a good example</a> on how to achieve that functionality. However, I found that their piece of code only works, if you share data to an app that <strong>is still closed</strong>. If you had opened your app before and it idles in the background, it won‚Äôt receive the <a href=\"https://www.androidcode.ninja/android-share-intent-example/\" target=\"_blank\" rel=\"noopener\">sharing intent</a> when it is <a href=\"https://developer.android.com/guide/components/activities/activity-lifecycle#onresume\" target=\"_blank\" rel=\"noopener\">resumed</a>. Therefore, I extended the example.</p>\n<h1>Code</h1>\n<h2 id=\"AndroidManifest-xml\">AndroidManifest.xml</h2>\n<p>First, you have to add an <code>intent-filter</code> to your <code>AndroidManifest.xml</code> in the <code>android/</code> directory to register your app as a sharing target.</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">intent-filter</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">action</span> <span class=\"attr\">android:name</span>=<span class=\"string\">\"android.intent.action.SEND\"</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">category</span> <span class=\"attr\">android:name</span>=<span class=\"string\">\"android.intent.category.DEFAULT\"</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">data</span> <span class=\"attr\">android:mimeType</span>=<span class=\"string\">\"text/plain\"</span> /&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">intent-filter</span>&gt;</span></span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<h2 id=\"MainActivity-java\">MainActivity.java</h2>\n<p>Secondly, you will need to add some code to <code>MainActivity.java</code> (analogously for Kotlin projects). In the <code>onCreate()</code> lifecycle hook, you have to register a <code>MethodCallHandler()</code> to act as an interface between the underlying Android app and your flutter code. In addition, you have to override the <code>onNewIntent()</code> callback, which is triggered when a new sharing intent (<code>Intent.ACTION_SEND</code>) causes your app to change its lifecycle state. Lastly, you need a method to handle the actual content shared from the external app. It consists of two fields, a URL and a title, both represented as strings in a Map. In the end, your <code>MainActivity</code> looks like something like this.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> Map&lt;String, String&gt; sharedData = <span class=\"keyword\">new</span> HashMap();</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">onCreate</span><span class=\"params\">(Bundle savedInstanceState)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onCreate(savedInstanceState);</span><br><span class=\"line\">        GeneratedPluginRegistrant.registerWith(<span class=\"keyword\">this</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Handle intent when app is initially opened</span></span><br><span class=\"line\">        handleSendIntent(getIntent());</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">new</span> MethodChannel(getFlutterView(), <span class=\"string\">\"app.channel.shared.data\"</span>).setMethodCallHandler(</span><br><span class=\"line\">            <span class=\"keyword\">new</span> MethodCallHandler() &#123;</span><br><span class=\"line\">                <span class=\"meta\">@Override</span></span><br><span class=\"line\">                <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onMethodCall</span><span class=\"params\">(MethodCall call, MethodChannel.Result result)</span> </span>&#123;</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (call.method.contentEquals(<span class=\"string\">\"getSharedData\"</span>)) &#123;</span><br><span class=\"line\">                        result.success(sharedData);</span><br><span class=\"line\">                        sharedData.clear();</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">onNewIntent</span><span class=\"params\">(Intent intent)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// Handle intent when app is resumed</span></span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onNewIntent(intent);</span><br><span class=\"line\">        handleSendIntent(intent);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">handleSendIntent</span><span class=\"params\">(Intent intent)</span> </span>&#123;</span><br><span class=\"line\">        String action = intent.getAction();</span><br><span class=\"line\">        String type = intent.getType();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// We only care about sharing intent that contain plain text</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (Intent.ACTION_SEND.equals(action) &amp;&amp; type != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"string\">\"text/plain\"</span>.equals(type)) &#123;</span><br><span class=\"line\">                sharedData.put(<span class=\"string\">\"subject\"</span>, intent.getStringExtra(Intent.EXTRA_SUBJECT));</span><br><span class=\"line\">                sharedData.put(<span class=\"string\">\"text\"</span>, intent.getStringExtra(Intent.EXTRA_TEXT));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Note that the shared data is ‚Äúcached‚Äù on the Java side of your app until is it picked up by your Flutter code.</p>\n<h2 id=\"Your-Flutter-app\">Your Flutter app</h2>\n<p>Eventually, you need to add a method to your Flutter code to interact with the native-Android <code>MethodHandler</code>. It will be called once during state initialization and ‚Äì with the help of a listener ‚Äì every time the underlying Android activity is resumed.</p>\n<figure class=\"highlight dart\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SampleAppPage</span> <span class=\"keyword\">extends</span> <span class=\"title\">StatefulWidget</span> </span>&#123;</span><br><span class=\"line\">  SampleAppPage(&#123;Key key&#125;) : <span class=\"keyword\">super</span>(key: key);</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"meta\">@override</span></span><br><span class=\"line\">  _SampleAppPageState createState() =&gt; _SampleAppPageState();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">_SampleAppPageState</span> <span class=\"keyword\">extends</span> <span class=\"title\">State</span>&lt;<span class=\"title\">SampleAppPage</span>&gt; </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">const</span> platform = <span class=\"keyword\">const</span> MethodChannel(<span class=\"string\">'app.channel.shared.data'</span>);</span><br><span class=\"line\">    <span class=\"built_in\">Map</span>&lt;<span class=\"built_in\">dynamic</span>, <span class=\"built_in\">dynamic</span>&gt; sharedData = <span class=\"built_in\">Map</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@override</span></span><br><span class=\"line\">    <span class=\"keyword\">void</span> initState() &#123;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.initState();</span><br><span class=\"line\">        _init();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    _init() <span class=\"keyword\">async</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Case 1: App is already running in background:</span></span><br><span class=\"line\">        <span class=\"comment\">// Listen to lifecycle changes to subsequently call Java MethodHandler to check for shared data</span></span><br><span class=\"line\">        SystemChannels.lifecycle.setMessageHandler((msg) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (msg.contains(<span class=\"string\">'resumed'</span>)) &#123;</span><br><span class=\"line\">                _getSharedData().then((d) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (d.isEmpty) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">                    <span class=\"comment\">// Your logic here</span></span><br><span class=\"line\">                    <span class=\"comment\">// E.g. at this place you might want to use Navigator to launch a new page and pass the shared data</span></span><br><span class=\"line\">                &#125;);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Case 2: App is started by the intent:</span></span><br><span class=\"line\">        <span class=\"comment\">// Call Java MethodHandler on application start up to check for shared data</span></span><br><span class=\"line\">        <span class=\"keyword\">var</span> data = <span class=\"keyword\">await</span> _getSharedData();</span><br><span class=\"line\">        setState(() =&gt; sharedData = data);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// You can use sharedData in your build() method now</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    Future&lt;<span class=\"built_in\">Map</span>&gt; _getSharedData() <span class=\"keyword\">async</span> =&gt; <span class=\"keyword\">await</span> platform.invokeMethod(<span class=\"string\">'getSharedData'</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Now you‚Äôre good to go! Once you extracted the sharing intent‚Äôs contents, you can, for instance, show a pre-filled dialog to add the new link to one of your bookmark collections, just as <a href=\"https://github.com/muety/anchr-android/blob/897395528532a03ce4e1bdba00fe4b3b35f5fe43/lib/app.dart#L39\" target=\"_blank\" rel=\"noopener\">I did here</a>.</p>\n<h1>Conclusion</h1>\n<p>This approach might seem a little complicated, but in fact, it is the only working solution I could find. There is a plugin called <a href=\"https://github.com/d-silveira/flutter-share\" target=\"_blank\" rel=\"noopener\">flutter-share</a>, but unfortunately it did not work for me. Happy coding üòâ!</p>\n"},{"title":"Http performance Java (Jersey) vs. Go vs. NodeJS","date":"2016-11-19T22:06:49.000Z","_content":"\nI developed a very basic benchmark suite to compare different HTTP server's performance. It is inspired by [arcadius/java-rest-api-web-container-benchmark](https://github.com/arcadius/java-rest-api-web-container-benchmark), but uses [h2load](https://github.com/nghttp2/nghttp2#benchmarking-tool) instead of [ab](http://httpd.apache.org/docs/2.4/programs/ab.html).\n\nI implemented four very basic REST APIs with exactly one route each, which exposes a small, static todo list as JSON.\n\n## Server Implementations\n* __Java:__ [Jersey](http://jersey.java.net/) with embedded [Grizzly](https://grizzly.java.net/)\n* __Go:__ Using plain `net/http` package\n* __NodeJS:__ Using plain `http` package\n* __NodeJS:__ Using de-facto standard [Express 4](http://expressjs.com/) framework\n\n## Setup\nMy machine, where the benchmark suite was executed on, has the following specifications.\n\n```\n===CPU:\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\n \n===RAM: \n             total       used       free     shared    buffers     cached\nMem:          7.7G       6.3G       1.4G       412M       527M       2.4G\n-/+ buffers/cache:       3.3G       4.3G\nSwap:         5.6G         0B       5.6G\n\n===Java version: \njava version \"1.8.0_101\"\nJava(TM) SE Runtime Environment (build 1.8.0_101-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)\n \n===OS: \nLinux ferdinand-notebook 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux\n\n===Node: \nv6.5.0\n\n=== Go:\ngo version go1.7.3 linux/amd64\n```\n\n## Test parameters\nBasically there are three parameters to be varied for the benchmark.\n* The __number of total reqests__ to be performed against the API. I chose to set this to __100,000__\n* The __number of concurrent__ client to make those requests. I chose to have __32__ concurrent clients, each of them making 3,125 requests.\n* The __number of threads__ to be used by _h2load_. I set this parameter to four, corresponding to the number of logical CPU cores of my machine.\n\n## Results\nRunning my [benchmark script](https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh) delivered the following results.\n\n![](images/benchmarks.svg)\n\n## Discussion\nFirst of all, please notice that this is definitely not a 100 % correct, scientifical evaluation. Rather it should give basic insights on the order of magnitute of the performance differences between different language's HTTP servers.\nAs we can clearly see, Go is the fastest candidate among my test subjects. However, my implementation only utilized the plain, built-in http package without any custom ServeMux or any router or middleware on top of it. In a real-world application, one would most likely not operate on such a low level, but use frameworks like [Iris](http://iris-go.com/) on top, which add additional overhead.\n\nSecond place is Java using Grizzly as an embedded server inside a Jersey application. The reason for me picking Grizzly was that it pointed out to be the fastest among the common servers in [this benchmark](http://menelic.com/2016/01/06/java-rest-api-benchmark-tomcat-vs-jetty-vs-grizzly-vs-undertow/).\n\nBoth of my Node implementations perform worst in this benchmark, whereas Express is even only as half as good as the plain http package. Evidently, it introduces quite an amount of overhead. However, one would most likely not implement an actual REST API without a higher-level framework like Express. Consequently, the Express benchmark is probably more representative. \n\nConclusing I can say that I was pretty surprised about how large the differences between various servers are. Go is almost six times as fast as Node with Express, even though [Express still has a very great performance](https://raygun.com/blog/2016/06/node-performance/).\n\nThe full benchmark results as well as the suite's source code can be found at my [GitHub project](https://github.com/muety/http-server-benchmarks).\n\n## EDIT\nAt January 1st 2017 I did some minor adjustments to my benchmark suite. A thoughtful reader has drawn my attention to the fact that my comparison was a little unfair in the way that Go's net/http as well as Grizzly use as many threads as the host system provides CPU core by default, while Node doesn't. Using Node's `cluster` module I made both Node-based webservers use four listener threads, too and actually the results have improved by around 45 %. Furthermore I did an adjustment to the Jersey + Grizzly server by changing the `IOStrategy` from the default `WorkerThreadIOStrategy` to `SameThreadIOStrategy`, which brought around 10 % in this specific case, where we don't have any blocking computations but only spit out static JSON. If you're interested in leaarning more about different io strategies, refer to [this official documentation page](https://grizzly.java.net/iostrategies.html). Here is my updated benchmark chart.\n\n![](images/benchmarks2.svg)","source":"_posts/http-performance-java-jersey-vs-go-vs-nodejs.md","raw":"---\ntitle: Http performance Java (Jersey) vs. Go vs. NodeJS\ndate: 2016-11-19 23:06:49\ntags:\n---\n\nI developed a very basic benchmark suite to compare different HTTP server's performance. It is inspired by [arcadius/java-rest-api-web-container-benchmark](https://github.com/arcadius/java-rest-api-web-container-benchmark), but uses [h2load](https://github.com/nghttp2/nghttp2#benchmarking-tool) instead of [ab](http://httpd.apache.org/docs/2.4/programs/ab.html).\n\nI implemented four very basic REST APIs with exactly one route each, which exposes a small, static todo list as JSON.\n\n## Server Implementations\n* __Java:__ [Jersey](http://jersey.java.net/) with embedded [Grizzly](https://grizzly.java.net/)\n* __Go:__ Using plain `net/http` package\n* __NodeJS:__ Using plain `http` package\n* __NodeJS:__ Using de-facto standard [Express 4](http://expressjs.com/) framework\n\n## Setup\nMy machine, where the benchmark suite was executed on, has the following specifications.\n\n```\n===CPU:\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\n \n===RAM: \n             total       used       free     shared    buffers     cached\nMem:          7.7G       6.3G       1.4G       412M       527M       2.4G\n-/+ buffers/cache:       3.3G       4.3G\nSwap:         5.6G         0B       5.6G\n\n===Java version: \njava version \"1.8.0_101\"\nJava(TM) SE Runtime Environment (build 1.8.0_101-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)\n \n===OS: \nLinux ferdinand-notebook 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux\n\n===Node: \nv6.5.0\n\n=== Go:\ngo version go1.7.3 linux/amd64\n```\n\n## Test parameters\nBasically there are three parameters to be varied for the benchmark.\n* The __number of total reqests__ to be performed against the API. I chose to set this to __100,000__\n* The __number of concurrent__ client to make those requests. I chose to have __32__ concurrent clients, each of them making 3,125 requests.\n* The __number of threads__ to be used by _h2load_. I set this parameter to four, corresponding to the number of logical CPU cores of my machine.\n\n## Results\nRunning my [benchmark script](https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh) delivered the following results.\n\n![](images/benchmarks.svg)\n\n## Discussion\nFirst of all, please notice that this is definitely not a 100 % correct, scientifical evaluation. Rather it should give basic insights on the order of magnitute of the performance differences between different language's HTTP servers.\nAs we can clearly see, Go is the fastest candidate among my test subjects. However, my implementation only utilized the plain, built-in http package without any custom ServeMux or any router or middleware on top of it. In a real-world application, one would most likely not operate on such a low level, but use frameworks like [Iris](http://iris-go.com/) on top, which add additional overhead.\n\nSecond place is Java using Grizzly as an embedded server inside a Jersey application. The reason for me picking Grizzly was that it pointed out to be the fastest among the common servers in [this benchmark](http://menelic.com/2016/01/06/java-rest-api-benchmark-tomcat-vs-jetty-vs-grizzly-vs-undertow/).\n\nBoth of my Node implementations perform worst in this benchmark, whereas Express is even only as half as good as the plain http package. Evidently, it introduces quite an amount of overhead. However, one would most likely not implement an actual REST API without a higher-level framework like Express. Consequently, the Express benchmark is probably more representative. \n\nConclusing I can say that I was pretty surprised about how large the differences between various servers are. Go is almost six times as fast as Node with Express, even though [Express still has a very great performance](https://raygun.com/blog/2016/06/node-performance/).\n\nThe full benchmark results as well as the suite's source code can be found at my [GitHub project](https://github.com/muety/http-server-benchmarks).\n\n## EDIT\nAt January 1st 2017 I did some minor adjustments to my benchmark suite. A thoughtful reader has drawn my attention to the fact that my comparison was a little unfair in the way that Go's net/http as well as Grizzly use as many threads as the host system provides CPU core by default, while Node doesn't. Using Node's `cluster` module I made both Node-based webservers use four listener threads, too and actually the results have improved by around 45 %. Furthermore I did an adjustment to the Jersey + Grizzly server by changing the `IOStrategy` from the default `WorkerThreadIOStrategy` to `SameThreadIOStrategy`, which brought around 10 % in this specific case, where we don't have any blocking computations but only spit out static JSON. If you're interested in leaarning more about different io strategies, refer to [this official documentation page](https://grizzly.java.net/iostrategies.html). Here is my updated benchmark chart.\n\n![](images/benchmarks2.svg)","slug":"http-performance-java-jersey-vs-go-vs-nodejs","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhid000k40mqyzgxdnkd","content":"<p>I developed a very basic benchmark suite to compare different HTTP server‚Äôs performance. It is inspired by <a href=\"https://github.com/arcadius/java-rest-api-web-container-benchmark\" target=\"_blank\" rel=\"noopener\">arcadius/java-rest-api-web-container-benchmark</a>, but uses <a href=\"https://github.com/nghttp2/nghttp2#benchmarking-tool\" target=\"_blank\" rel=\"noopener\">h2load</a> instead of <a href=\"http://httpd.apache.org/docs/2.4/programs/ab.html\" target=\"_blank\" rel=\"noopener\">ab</a>.</p>\n<p>I implemented four very basic REST APIs with exactly one route each, which exposes a small, static todo list as JSON.</p>\n<h2 id=\"Server-Implementations\">Server Implementations</h2>\n<ul>\n<li><strong>Java:</strong> <a href=\"http://jersey.java.net/\" target=\"_blank\" rel=\"noopener\">Jersey</a> with embedded <a href=\"https://grizzly.java.net/\" target=\"_blank\" rel=\"noopener\">Grizzly</a></li>\n<li><strong>Go:</strong> Using plain <code>net/http</code> package</li>\n<li><strong>NodeJS:</strong> Using plain <code>http</code> package</li>\n<li><strong>NodeJS:</strong> Using de-facto standard <a href=\"http://expressjs.com/\" target=\"_blank\" rel=\"noopener\">Express 4</a> framework</li>\n</ul>\n<h2 id=\"Setup\">Setup</h2>\n<p>My machine, where the benchmark suite was executed on, has the following specifications.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">===CPU:</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\"> </span><br><span class=\"line\">===RAM: </span><br><span class=\"line\">             total       used       free     shared    buffers     cached</span><br><span class=\"line\">Mem:          7.7G       6.3G       1.4G       412M       527M       2.4G</span><br><span class=\"line\">-/+ buffers/cache:       3.3G       4.3G</span><br><span class=\"line\">Swap:         5.6G         0B       5.6G</span><br><span class=\"line\"></span><br><span class=\"line\">===Java version: </span><br><span class=\"line\">java version &quot;1.8.0_101&quot;</span><br><span class=\"line\">Java(TM) SE Runtime Environment (build 1.8.0_101-b13)</span><br><span class=\"line\">Java HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)</span><br><span class=\"line\"> </span><br><span class=\"line\">===OS: </span><br><span class=\"line\">Linux ferdinand-notebook 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux</span><br><span class=\"line\"></span><br><span class=\"line\">===Node: </span><br><span class=\"line\">v6.5.0</span><br><span class=\"line\"></span><br><span class=\"line\">=== Go:</span><br><span class=\"line\">go version go1.7.3 linux/amd64</span><br></pre></td></tr></table></figure>\n<h2 id=\"Test-parameters\">Test parameters</h2>\n<p>Basically there are three parameters to be varied for the benchmark.</p>\n<ul>\n<li>The <strong>number of total reqests</strong> to be performed against the API. I chose to set this to <strong>100,000</strong></li>\n<li>The <strong>number of concurrent</strong> client to make those requests. I chose to have <strong>32</strong> concurrent clients, each of them making 3,125 requests.</li>\n<li>The <strong>number of threads</strong> to be used by <em>h2load</em>. I set this parameter to four, corresponding to the number of logical CPU cores of my machine.</li>\n</ul>\n<h2 id=\"Results\">Results</h2>\n<p>Running my <a href=\"https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh\" target=\"_blank\" rel=\"noopener\">benchmark script</a> delivered the following results.</p>\n<p><img src=\"images/benchmarks.svg\" alt></p>\n<h2 id=\"Discussion\">Discussion</h2>\n<p>First of all, please notice that this is definitely not a 100 % correct, scientifical evaluation. Rather it should give basic insights on the order of magnitute of the performance differences between different language‚Äôs HTTP servers.<br>\nAs we can clearly see, Go is the fastest candidate among my test subjects. However, my implementation only utilized the plain, built-in http package without any custom ServeMux or any router or middleware on top of it. In a real-world application, one would most likely not operate on such a low level, but use frameworks like <a href=\"http://iris-go.com/\" target=\"_blank\" rel=\"noopener\">Iris</a> on top, which add additional overhead.</p>\n<p>Second place is Java using Grizzly as an embedded server inside a Jersey application. The reason for me picking Grizzly was that it pointed out to be the fastest among the common servers in <a href=\"http://menelic.com/2016/01/06/java-rest-api-benchmark-tomcat-vs-jetty-vs-grizzly-vs-undertow/\" target=\"_blank\" rel=\"noopener\">this benchmark</a>.</p>\n<p>Both of my Node implementations perform worst in this benchmark, whereas Express is even only as half as good as the plain http package. Evidently, it introduces quite an amount of overhead. However, one would most likely not implement an actual REST API without a higher-level framework like Express. Consequently, the Express benchmark is probably more representative.</p>\n<p>Conclusing I can say that I was pretty surprised about how large the differences between various servers are. Go is almost six times as fast as Node with Express, even though <a href=\"https://raygun.com/blog/2016/06/node-performance/\" target=\"_blank\" rel=\"noopener\">Express still has a very great performance</a>.</p>\n<p>The full benchmark results as well as the suite‚Äôs source code can be found at my <a href=\"https://github.com/muety/http-server-benchmarks\" target=\"_blank\" rel=\"noopener\">GitHub project</a>.</p>\n<h2 id=\"EDIT\">EDIT</h2>\n<p>At January 1st 2017 I did some minor adjustments to my benchmark suite. A thoughtful reader has drawn my attention to the fact that my comparison was a little unfair in the way that Go‚Äôs net/http as well as Grizzly use as many threads as the host system provides CPU core by default, while Node doesn‚Äôt. Using Node‚Äôs <code>cluster</code> module I made both Node-based webservers use four listener threads, too and actually the results have improved by around 45 %. Furthermore I did an adjustment to the Jersey + Grizzly server by changing the <code>IOStrategy</code> from the default <code>WorkerThreadIOStrategy</code> to <code>SameThreadIOStrategy</code>, which brought around 10 % in this specific case, where we don‚Äôt have any blocking computations but only spit out static JSON. If you‚Äôre interested in leaarning more about different io strategies, refer to <a href=\"https://grizzly.java.net/iostrategies.html\" target=\"_blank\" rel=\"noopener\">this official documentation page</a>. Here is my updated benchmark chart.</p>\n<p><img src=\"images/benchmarks2.svg\" alt></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>I developed a very basic benchmark suite to compare different HTTP server‚Äôs performance. It is inspired by <a href=\"https://github.com/arcadius/java-rest-api-web-container-benchmark\" target=\"_blank\" rel=\"noopener\">arcadius/java-rest-api-web-container-benchmark</a>, but uses <a href=\"https://github.com/nghttp2/nghttp2#benchmarking-tool\" target=\"_blank\" rel=\"noopener\">h2load</a> instead of <a href=\"http://httpd.apache.org/docs/2.4/programs/ab.html\" target=\"_blank\" rel=\"noopener\">ab</a>.</p>\n<p>I implemented four very basic REST APIs with exactly one route each, which exposes a small, static todo list as JSON.</p>\n<h2 id=\"Server-Implementations\">Server Implementations</h2>\n<ul>\n<li><strong>Java:</strong> <a href=\"http://jersey.java.net/\" target=\"_blank\" rel=\"noopener\">Jersey</a> with embedded <a href=\"https://grizzly.java.net/\" target=\"_blank\" rel=\"noopener\">Grizzly</a></li>\n<li><strong>Go:</strong> Using plain <code>net/http</code> package</li>\n<li><strong>NodeJS:</strong> Using plain <code>http</code> package</li>\n<li><strong>NodeJS:</strong> Using de-facto standard <a href=\"http://expressjs.com/\" target=\"_blank\" rel=\"noopener\">Express 4</a> framework</li>\n</ul>\n<h2 id=\"Setup\">Setup</h2>\n<p>My machine, where the benchmark suite was executed on, has the following specifications.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">===CPU:</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\"> </span><br><span class=\"line\">===RAM: </span><br><span class=\"line\">             total       used       free     shared    buffers     cached</span><br><span class=\"line\">Mem:          7.7G       6.3G       1.4G       412M       527M       2.4G</span><br><span class=\"line\">-/+ buffers/cache:       3.3G       4.3G</span><br><span class=\"line\">Swap:         5.6G         0B       5.6G</span><br><span class=\"line\"></span><br><span class=\"line\">===Java version: </span><br><span class=\"line\">java version &quot;1.8.0_101&quot;</span><br><span class=\"line\">Java(TM) SE Runtime Environment (build 1.8.0_101-b13)</span><br><span class=\"line\">Java HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)</span><br><span class=\"line\"> </span><br><span class=\"line\">===OS: </span><br><span class=\"line\">Linux ferdinand-notebook 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux</span><br><span class=\"line\"></span><br><span class=\"line\">===Node: </span><br><span class=\"line\">v6.5.0</span><br><span class=\"line\"></span><br><span class=\"line\">=== Go:</span><br><span class=\"line\">go version go1.7.3 linux/amd64</span><br></pre></td></tr></table></figure>\n<h2 id=\"Test-parameters\">Test parameters</h2>\n<p>Basically there are three parameters to be varied for the benchmark.</p>\n<ul>\n<li>The <strong>number of total reqests</strong> to be performed against the API. I chose to set this to <strong>100,000</strong></li>\n<li>The <strong>number of concurrent</strong> client to make those requests. I chose to have <strong>32</strong> concurrent clients, each of them making 3,125 requests.</li>\n<li>The <strong>number of threads</strong> to be used by <em>h2load</em>. I set this parameter to four, corresponding to the number of logical CPU cores of my machine.</li>\n</ul>\n<h2 id=\"Results\">Results</h2>\n<p>Running my <a href=\"https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh\" target=\"_blank\" rel=\"noopener\">benchmark script</a> delivered the following results.</p>\n<p><img src=\"images/benchmarks.svg\" alt></p>\n<h2 id=\"Discussion\">Discussion</h2>\n<p>First of all, please notice that this is definitely not a 100 % correct, scientifical evaluation. Rather it should give basic insights on the order of magnitute of the performance differences between different language‚Äôs HTTP servers.<br>\nAs we can clearly see, Go is the fastest candidate among my test subjects. However, my implementation only utilized the plain, built-in http package without any custom ServeMux or any router or middleware on top of it. In a real-world application, one would most likely not operate on such a low level, but use frameworks like <a href=\"http://iris-go.com/\" target=\"_blank\" rel=\"noopener\">Iris</a> on top, which add additional overhead.</p>\n<p>Second place is Java using Grizzly as an embedded server inside a Jersey application. The reason for me picking Grizzly was that it pointed out to be the fastest among the common servers in <a href=\"http://menelic.com/2016/01/06/java-rest-api-benchmark-tomcat-vs-jetty-vs-grizzly-vs-undertow/\" target=\"_blank\" rel=\"noopener\">this benchmark</a>.</p>\n<p>Both of my Node implementations perform worst in this benchmark, whereas Express is even only as half as good as the plain http package. Evidently, it introduces quite an amount of overhead. However, one would most likely not implement an actual REST API without a higher-level framework like Express. Consequently, the Express benchmark is probably more representative.</p>\n<p>Conclusing I can say that I was pretty surprised about how large the differences between various servers are. Go is almost six times as fast as Node with Express, even though <a href=\"https://raygun.com/blog/2016/06/node-performance/\" target=\"_blank\" rel=\"noopener\">Express still has a very great performance</a>.</p>\n<p>The full benchmark results as well as the suite‚Äôs source code can be found at my <a href=\"https://github.com/muety/http-server-benchmarks\" target=\"_blank\" rel=\"noopener\">GitHub project</a>.</p>\n<h2 id=\"EDIT\">EDIT</h2>\n<p>At January 1st 2017 I did some minor adjustments to my benchmark suite. A thoughtful reader has drawn my attention to the fact that my comparison was a little unfair in the way that Go‚Äôs net/http as well as Grizzly use as many threads as the host system provides CPU core by default, while Node doesn‚Äôt. Using Node‚Äôs <code>cluster</code> module I made both Node-based webservers use four listener threads, too and actually the results have improved by around 45 %. Furthermore I did an adjustment to the Jersey + Grizzly server by changing the <code>IOStrategy</code> from the default <code>WorkerThreadIOStrategy</code> to <code>SameThreadIOStrategy</code>, which brought around 10 % in this specific case, where we don‚Äôt have any blocking computations but only spit out static JSON. If you‚Äôre interested in leaarning more about different io strategies, refer to <a href=\"https://grizzly.java.net/iostrategies.html\" target=\"_blank\" rel=\"noopener\">this official documentation page</a>. Here is my updated benchmark chart.</p>\n<p><img src=\"images/benchmarks2.svg\" alt></p>\n"},{"title":"HTTP/2.0 server push proxy","date":"2016-11-14T22:05:45.000Z","_content":"\nI just released a new, little Node project on GitHub and NPM, which is called [http2-serverpush-proxy](https://www.npmjs.com/package/http2-serverpush-proxy) and does exactly what the name suggests. It spawns a reverse proxy between a web application and its clients, that serves via HTTP/2 and automatically server-pushes assets contained in the HTML. It can be used as either standalone server or as _connect_ middleware for ExpressJS.\n\n### How it works\nUsually, websites consist of multiple assets, like CSS and JS files as well as images like PNGs, JPGs and SVGs. Traditionally, a user's browser fetches the HTML first, parses it and then downloads all linked assets. However, this is slow, since the assets can't be loaded before the HTML is completely fetched and parsed. With server push, your webserver can actively send those assets to the client browser even before it requested them. To prevent you from having to implement this functionality, _http2-serverpush-proxy_ sits as a proxy between your actual webserver and the user. In contrast to some other approaches like [http2-push-manifest](https://github.com/GoogleChrome/http2-push-manifest), where the assets to be pushed are declared statically, this library __dynamically parses the HTML__ files and extracts contained assets that should be pushed.\n\n\n![](/images/push_screenshot1.png)\nWithout server push\n![](/images/push_screenshot2.png)\nWith server push\n\nDetails on how to use this library are to be found on the [project site](https://github.com/muety/http2-serverpush-proxy). Please feel free to give me feedback!\n","source":"_posts/http20-server-push-proxy.md","raw":"---\ntitle: HTTP/2.0 server push proxy\ndate: 2016-11-14 23:05:45\ntags:\n---\n\nI just released a new, little Node project on GitHub and NPM, which is called [http2-serverpush-proxy](https://www.npmjs.com/package/http2-serverpush-proxy) and does exactly what the name suggests. It spawns a reverse proxy between a web application and its clients, that serves via HTTP/2 and automatically server-pushes assets contained in the HTML. It can be used as either standalone server or as _connect_ middleware for ExpressJS.\n\n### How it works\nUsually, websites consist of multiple assets, like CSS and JS files as well as images like PNGs, JPGs and SVGs. Traditionally, a user's browser fetches the HTML first, parses it and then downloads all linked assets. However, this is slow, since the assets can't be loaded before the HTML is completely fetched and parsed. With server push, your webserver can actively send those assets to the client browser even before it requested them. To prevent you from having to implement this functionality, _http2-serverpush-proxy_ sits as a proxy between your actual webserver and the user. In contrast to some other approaches like [http2-push-manifest](https://github.com/GoogleChrome/http2-push-manifest), where the assets to be pushed are declared statically, this library __dynamically parses the HTML__ files and extracts contained assets that should be pushed.\n\n\n![](/images/push_screenshot1.png)\nWithout server push\n![](/images/push_screenshot2.png)\nWith server push\n\nDetails on how to use this library are to be found on the [project site](https://github.com/muety/http2-serverpush-proxy). Please feel free to give me feedback!\n","slug":"http20-server-push-proxy","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhif000l40mq47j2u9j4","content":"<p>I just released a new, little Node project on GitHub and NPM, which is called <a href=\"https://www.npmjs.com/package/http2-serverpush-proxy\" target=\"_blank\" rel=\"noopener\">http2-serverpush-proxy</a> and does exactly what the name suggests. It spawns a reverse proxy between a web application and its clients, that serves via HTTP/2 and automatically server-pushes assets contained in the HTML. It can be used as either standalone server or as <em>connect</em> middleware for ExpressJS.</p>\n<h3 id=\"How-it-works\">How it works</h3>\n<p>Usually, websites consist of multiple assets, like CSS and JS files as well as images like PNGs, JPGs and SVGs. Traditionally, a user‚Äôs browser fetches the HTML first, parses it and then downloads all linked assets. However, this is slow, since the assets can‚Äôt be loaded before the HTML is completely fetched and parsed. With server push, your webserver can actively send those assets to the client browser even before it requested them. To prevent you from having to implement this functionality, <em>http2-serverpush-proxy</em> sits as a proxy between your actual webserver and the user. In contrast to some other approaches like <a href=\"https://github.com/GoogleChrome/http2-push-manifest\" target=\"_blank\" rel=\"noopener\">http2-push-manifest</a>, where the assets to be pushed are declared statically, this library <strong>dynamically parses the HTML</strong> files and extracts contained assets that should be pushed.</p>\n<p><img src=\"/images/push_screenshot1.png\" alt><br>\nWithout server push<br>\n<img src=\"/images/push_screenshot2.png\" alt><br>\nWith server push</p>\n<p>Details on how to use this library are to be found on the <a href=\"https://github.com/muety/http2-serverpush-proxy\" target=\"_blank\" rel=\"noopener\">project site</a>. Please feel free to give me feedback!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>I just released a new, little Node project on GitHub and NPM, which is called <a href=\"https://www.npmjs.com/package/http2-serverpush-proxy\" target=\"_blank\" rel=\"noopener\">http2-serverpush-proxy</a> and does exactly what the name suggests. It spawns a reverse proxy between a web application and its clients, that serves via HTTP/2 and automatically server-pushes assets contained in the HTML. It can be used as either standalone server or as <em>connect</em> middleware for ExpressJS.</p>\n<h3 id=\"How-it-works\">How it works</h3>\n<p>Usually, websites consist of multiple assets, like CSS and JS files as well as images like PNGs, JPGs and SVGs. Traditionally, a user‚Äôs browser fetches the HTML first, parses it and then downloads all linked assets. However, this is slow, since the assets can‚Äôt be loaded before the HTML is completely fetched and parsed. With server push, your webserver can actively send those assets to the client browser even before it requested them. To prevent you from having to implement this functionality, <em>http2-serverpush-proxy</em> sits as a proxy between your actual webserver and the user. In contrast to some other approaches like <a href=\"https://github.com/GoogleChrome/http2-push-manifest\" target=\"_blank\" rel=\"noopener\">http2-push-manifest</a>, where the assets to be pushed are declared statically, this library <strong>dynamically parses the HTML</strong> files and extracts contained assets that should be pushed.</p>\n<p><img src=\"/images/push_screenshot1.png\" alt><br>\nWithout server push<br>\n<img src=\"/images/push_screenshot2.png\" alt><br>\nWith server push</p>\n<p>Details on how to use this library are to be found on the <a href=\"https://github.com/muety/http2-serverpush-proxy\" target=\"_blank\" rel=\"noopener\">project site</a>. Please feel free to give me feedback!</p>\n"},{"title":"Innovation in Germany - not","date":"2016-05-19T21:00:53.000Z","_content":"\nOver last last while I got confronted with this topic quite frequently. Eventually [this german article](http://t3n.de/news/hotspots-unitymedia-abgemahnt-707512/) ‚Äì or more precisely the comments below it ‚Äì caused me to write my own little post on my personal perception of how innovation takes place in Germany. For the non-german readers among us I want to give a brief summary of that article. Recently the german internet provider ‚ÄúUnitymedia‚Äù came up with the idea to provide free WiFi-Hotspots for everyone, since internet connection in public places is pretty much a negative report in Germany. The key point with that is their intention of how to implement those so called ‚ÄúWifiSpots‚Äù. Every Unitymedia customer who has a Wifi-capable router in her home should become such a hotspot, while they promise that the hotspot network is completely isolated from the customer‚Äôs own network ‚Äì in terms of both security and bandwidth. Personally I like the idea, because I consider it quite efficient. Why put effort in distributing routers in public places if there already is full Wifi coverage? Most people‚Äôs routers are nowhere near working at capacity anyway. And if it‚Äôs guaranteed that the public internet traffic doesn‚Äôt influence you at all, why not? The article tells that the consumer protection center of North Rhine-Westphalia had admonished Unitymedia for their plans recently. One of the top comments below the article says about the following:\n\n*\"That‚Äôs exactly the reason why innovation isn‚Äôt possible in Germany. As soon as a company tries to solve people‚Äôs problems, everybody goes to the barricades. One gets punished for experiments ‚Äì not surprisingly nobody wants to found a company.\"*\n\nEven though the comment received a lot of bad write-up, that boils it down for me quite well. In my opinion Germany is ways too sluggish and conservative when it comes to adapting something new. Even though most representatives of the german manufacture consider themselves progressive, I actually don‚Äôt think they are ‚Äì at least not as much as they once were. I went to a conference on [Industry 4.0](https://en.wikipedia.org/wiki/Industry_4.0) last week where one speaker claimed that Germany was at least one to two years ahead of other countries at Industry 4.0 topics, but what I picked out between the lines is that quite the opposite is true. While the Germany are continuously trying to define standards, develop well-defined business-processes, clarify legal aspects and the like other countries are just doing it. They simply try it out, taking not too much risk, and if it fails, it fails. But if it succeeds ‚Äì and I claim that if not applied totally wrong, new technology is likely to ‚Äì they are given a competitive advantage, while the current big players‚Äô lead is melting. Of course, with this attitude you are more likely to fail as if you examine every little aspect fussily. But you‚Äôre also so much more likely to win big. As a professor at university had repeatedly said: ‚Äúthink big!‚Äù.\n\n***\"If your dreams do not scare you, they are not big enough! ‚Äì Ellen Johnson Sirleaf\"***\n\nThat‚Äôs what often is referred to as the Silicon valley mindset. In fact, as the diagram below shows, most startups are founded in the U.S.A. and I guess if there was a ranking of really big and successful startups (like Uber, WhatsApp, Tesla, ‚Ä¶) the contrast would be even more dramatic. Take Elon Musk ‚Äì the founder of Tesla Motors and SpaceX ‚Äì (I really recommend his biography) as an instance. He thought big and he obviously won (admittedly he took a really high risk). I don‚Äôt think they evaluate and plan new technology (like VR and stuff) that much at SpaceX ‚Äì they just do it.\n\n[![](/images/statista.png)](http://www.statista.com/statistics/268786/start-ups-in-leading-economic-nations/)\nSource: Statista.com\n\nAnother example for Germany‚Äôs innovation power is the following. I‚Äôve worked for two different companies as a working student ‚Äì both were software manufacturers. One was a typical german medium-sized company and the other was an american corporation. In one of them, the second-latest version of Internet Explorer was the only browser installed on every employee‚Äôs computer and if you desired another, you had to open a ticket to make the software distribution department install an outdated version of Firefox a few hours later. I don‚Äôt want to blame that company ‚Äì they did a great job at what they did. But the overall way of thinking there was old-fashioned, strict and not open-minded at all. Primarily it‚Äôs the people‚Äôs mindset that differentiates those two companies completely. I can‚Äôt really imagine that this first company is a workplace where you really feel comfortable and where you‚Äôre looking forward to a workday. At the time I worked there, they were about to release a little mobile app, whose development effort I estimate to only few weeks of intensive work of a small group of developers. The app‚Äôs purpose was great, but unfortunately, it was modern and innovative. Therefore there were only too few people to insistently support it. And there‚Äôs also the fact that processes were ways too sluggish to do a rapid development. As a result the app is still not released, but instead, two american countries each released an app with pretty much the exact same purpose. So much for that mindset in german companies ‚Äì of course and as always, there definitely are exceptions (a really big global player originated in Germany at which one department prints out document to hand it over to the other department, at which a trainee typewrites it to make it digital again, not being one of them).\n\nAnother alarming fact I want to mention in this context is that the [average internet speed in Germany is even far behind countries like Sri-Lanka and Vietnam](https://en.wikipedia.org/wiki/List_of_countries_by_Internet_connection_speeds#Akamai_Q3_2015_rankings).\n\n***\"Once a new technology rolls over you, if you‚Äôre not part of the steamroller, you‚Äôre part of the road.‚Ää‚Äî‚ÄäStewart Brand\"***\n\nWhat this all amounts to is that Germany should really watch out to not get passed by countries where the people are more ambitious and motivated and less conservative and formal. We should never rest on our laurels but try to permanently improve in a continuous evolution ‚Äì or to quote many speakers at the conference mentioned above: to not fall into the process of disruptive self-destruction.","source":"_posts/innovation-in-germany-not.md","raw":"---\ntitle: Innovation in Germany - not\ndate: 2016-05-19 23:00:53\ntags:\n---\n\nOver last last while I got confronted with this topic quite frequently. Eventually [this german article](http://t3n.de/news/hotspots-unitymedia-abgemahnt-707512/) ‚Äì or more precisely the comments below it ‚Äì caused me to write my own little post on my personal perception of how innovation takes place in Germany. For the non-german readers among us I want to give a brief summary of that article. Recently the german internet provider ‚ÄúUnitymedia‚Äù came up with the idea to provide free WiFi-Hotspots for everyone, since internet connection in public places is pretty much a negative report in Germany. The key point with that is their intention of how to implement those so called ‚ÄúWifiSpots‚Äù. Every Unitymedia customer who has a Wifi-capable router in her home should become such a hotspot, while they promise that the hotspot network is completely isolated from the customer‚Äôs own network ‚Äì in terms of both security and bandwidth. Personally I like the idea, because I consider it quite efficient. Why put effort in distributing routers in public places if there already is full Wifi coverage? Most people‚Äôs routers are nowhere near working at capacity anyway. And if it‚Äôs guaranteed that the public internet traffic doesn‚Äôt influence you at all, why not? The article tells that the consumer protection center of North Rhine-Westphalia had admonished Unitymedia for their plans recently. One of the top comments below the article says about the following:\n\n*\"That‚Äôs exactly the reason why innovation isn‚Äôt possible in Germany. As soon as a company tries to solve people‚Äôs problems, everybody goes to the barricades. One gets punished for experiments ‚Äì not surprisingly nobody wants to found a company.\"*\n\nEven though the comment received a lot of bad write-up, that boils it down for me quite well. In my opinion Germany is ways too sluggish and conservative when it comes to adapting something new. Even though most representatives of the german manufacture consider themselves progressive, I actually don‚Äôt think they are ‚Äì at least not as much as they once were. I went to a conference on [Industry 4.0](https://en.wikipedia.org/wiki/Industry_4.0) last week where one speaker claimed that Germany was at least one to two years ahead of other countries at Industry 4.0 topics, but what I picked out between the lines is that quite the opposite is true. While the Germany are continuously trying to define standards, develop well-defined business-processes, clarify legal aspects and the like other countries are just doing it. They simply try it out, taking not too much risk, and if it fails, it fails. But if it succeeds ‚Äì and I claim that if not applied totally wrong, new technology is likely to ‚Äì they are given a competitive advantage, while the current big players‚Äô lead is melting. Of course, with this attitude you are more likely to fail as if you examine every little aspect fussily. But you‚Äôre also so much more likely to win big. As a professor at university had repeatedly said: ‚Äúthink big!‚Äù.\n\n***\"If your dreams do not scare you, they are not big enough! ‚Äì Ellen Johnson Sirleaf\"***\n\nThat‚Äôs what often is referred to as the Silicon valley mindset. In fact, as the diagram below shows, most startups are founded in the U.S.A. and I guess if there was a ranking of really big and successful startups (like Uber, WhatsApp, Tesla, ‚Ä¶) the contrast would be even more dramatic. Take Elon Musk ‚Äì the founder of Tesla Motors and SpaceX ‚Äì (I really recommend his biography) as an instance. He thought big and he obviously won (admittedly he took a really high risk). I don‚Äôt think they evaluate and plan new technology (like VR and stuff) that much at SpaceX ‚Äì they just do it.\n\n[![](/images/statista.png)](http://www.statista.com/statistics/268786/start-ups-in-leading-economic-nations/)\nSource: Statista.com\n\nAnother example for Germany‚Äôs innovation power is the following. I‚Äôve worked for two different companies as a working student ‚Äì both were software manufacturers. One was a typical german medium-sized company and the other was an american corporation. In one of them, the second-latest version of Internet Explorer was the only browser installed on every employee‚Äôs computer and if you desired another, you had to open a ticket to make the software distribution department install an outdated version of Firefox a few hours later. I don‚Äôt want to blame that company ‚Äì they did a great job at what they did. But the overall way of thinking there was old-fashioned, strict and not open-minded at all. Primarily it‚Äôs the people‚Äôs mindset that differentiates those two companies completely. I can‚Äôt really imagine that this first company is a workplace where you really feel comfortable and where you‚Äôre looking forward to a workday. At the time I worked there, they were about to release a little mobile app, whose development effort I estimate to only few weeks of intensive work of a small group of developers. The app‚Äôs purpose was great, but unfortunately, it was modern and innovative. Therefore there were only too few people to insistently support it. And there‚Äôs also the fact that processes were ways too sluggish to do a rapid development. As a result the app is still not released, but instead, two american countries each released an app with pretty much the exact same purpose. So much for that mindset in german companies ‚Äì of course and as always, there definitely are exceptions (a really big global player originated in Germany at which one department prints out document to hand it over to the other department, at which a trainee typewrites it to make it digital again, not being one of them).\n\nAnother alarming fact I want to mention in this context is that the [average internet speed in Germany is even far behind countries like Sri-Lanka and Vietnam](https://en.wikipedia.org/wiki/List_of_countries_by_Internet_connection_speeds#Akamai_Q3_2015_rankings).\n\n***\"Once a new technology rolls over you, if you‚Äôre not part of the steamroller, you‚Äôre part of the road.‚Ää‚Äî‚ÄäStewart Brand\"***\n\nWhat this all amounts to is that Germany should really watch out to not get passed by countries where the people are more ambitious and motivated and less conservative and formal. We should never rest on our laurels but try to permanently improve in a continuous evolution ‚Äì or to quote many speakers at the conference mentioned above: to not fall into the process of disruptive self-destruction.","slug":"innovation-in-germany-not","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhik000m40mqv7q4h4t4","content":"<p>Over last last while I got confronted with this topic quite frequently. Eventually <a href=\"http://t3n.de/news/hotspots-unitymedia-abgemahnt-707512/\" target=\"_blank\" rel=\"noopener\">this german article</a> ‚Äì or more precisely the comments below it ‚Äì caused me to write my own little post on my personal perception of how innovation takes place in Germany. For the non-german readers among us I want to give a brief summary of that article. Recently the german internet provider ‚ÄúUnitymedia‚Äù came up with the idea to provide free WiFi-Hotspots for everyone, since internet connection in public places is pretty much a negative report in Germany. The key point with that is their intention of how to implement those so called ‚ÄúWifiSpots‚Äù. Every Unitymedia customer who has a Wifi-capable router in her home should become such a hotspot, while they promise that the hotspot network is completely isolated from the customer‚Äôs own network ‚Äì in terms of both security and bandwidth. Personally I like the idea, because I consider it quite efficient. Why put effort in distributing routers in public places if there already is full Wifi coverage? Most people‚Äôs routers are nowhere near working at capacity anyway. And if it‚Äôs guaranteed that the public internet traffic doesn‚Äôt influence you at all, why not? The article tells that the consumer protection center of North Rhine-Westphalia had admonished Unitymedia for their plans recently. One of the top comments below the article says about the following:</p>\n<p><em>‚ÄúThat‚Äôs exactly the reason why innovation isn‚Äôt possible in Germany. As soon as a company tries to solve people‚Äôs problems, everybody goes to the barricades. One gets punished for experiments ‚Äì not surprisingly nobody wants to found a company.‚Äù</em></p>\n<p>Even though the comment received a lot of bad write-up, that boils it down for me quite well. In my opinion Germany is ways too sluggish and conservative when it comes to adapting something new. Even though most representatives of the german manufacture consider themselves progressive, I actually don‚Äôt think they are ‚Äì at least not as much as they once were. I went to a conference on <a href=\"https://en.wikipedia.org/wiki/Industry_4.0\" target=\"_blank\" rel=\"noopener\">Industry 4.0</a> last week where one speaker claimed that Germany was at least one to two years ahead of other countries at Industry 4.0 topics, but what I picked out between the lines is that quite the opposite is true. While the Germany are continuously trying to define standards, develop well-defined business-processes, clarify legal aspects and the like other countries are just doing it. They simply try it out, taking not too much risk, and if it fails, it fails. But if it succeeds ‚Äì and I claim that if not applied totally wrong, new technology is likely to ‚Äì they are given a competitive advantage, while the current big players‚Äô lead is melting. Of course, with this attitude you are more likely to fail as if you examine every little aspect fussily. But you‚Äôre also so much more likely to win big. As a professor at university had repeatedly said: ‚Äúthink big!‚Äù.</p>\n<p><em><strong>&quot;If your dreams do not scare you, they are not big enough! ‚Äì Ellen Johnson Sirleaf&quot;</strong></em></p>\n<p>That‚Äôs what often is referred to as the Silicon valley mindset. In fact, as the diagram below shows, most startups are founded in the U.S.A. and I guess if there was a ranking of really big and successful startups (like Uber, WhatsApp, Tesla, ‚Ä¶) the contrast would be even more dramatic. Take Elon Musk ‚Äì the founder of Tesla Motors and SpaceX ‚Äì (I really recommend his biography) as an instance. He thought big and he obviously won (admittedly he took a really high risk). I don‚Äôt think they evaluate and plan new technology (like VR and stuff) that much at SpaceX ‚Äì they just do it.</p>\n<p><a href=\"http://www.statista.com/statistics/268786/start-ups-in-leading-economic-nations/\" target=\"_blank\" rel=\"noopener\"><img src=\"/images/statista.png\" alt></a><br>\nSource: <a href=\"http://Statista.com\" target=\"_blank\" rel=\"noopener\">Statista.com</a></p>\n<p>Another example for Germany‚Äôs innovation power is the following. I‚Äôve worked for two different companies as a working student ‚Äì both were software manufacturers. One was a typical german medium-sized company and the other was an american corporation. In one of them, the second-latest version of Internet Explorer was the only browser installed on every employee‚Äôs computer and if you desired another, you had to open a ticket to make the software distribution department install an outdated version of Firefox a few hours later. I don‚Äôt want to blame that company ‚Äì they did a great job at what they did. But the overall way of thinking there was old-fashioned, strict and not open-minded at all. Primarily it‚Äôs the people‚Äôs mindset that differentiates those two companies completely. I can‚Äôt really imagine that this first company is a workplace where you really feel comfortable and where you‚Äôre looking forward to a workday. At the time I worked there, they were about to release a little mobile app, whose development effort I estimate to only few weeks of intensive work of a small group of developers. The app‚Äôs purpose was great, but unfortunately, it was modern and innovative. Therefore there were only too few people to insistently support it. And there‚Äôs also the fact that processes were ways too sluggish to do a rapid development. As a result the app is still not released, but instead, two american countries each released an app with pretty much the exact same purpose. So much for that mindset in german companies ‚Äì of course and as always, there definitely are exceptions (a really big global player originated in Germany at which one department prints out document to hand it over to the other department, at which a trainee typewrites it to make it digital again, not being one of them).</p>\n<p>Another alarming fact I want to mention in this context is that the <a href=\"https://en.wikipedia.org/wiki/List_of_countries_by_Internet_connection_speeds#Akamai_Q3_2015_rankings\" target=\"_blank\" rel=\"noopener\">average internet speed in Germany is even far behind countries like Sri-Lanka and Vietnam</a>.</p>\n<p><em><strong>&quot;Once a new technology rolls over you, if you‚Äôre not part of the steamroller, you‚Äôre part of the road.‚Ää‚Äî‚ÄäStewart Brand&quot;</strong></em></p>\n<p>What this all amounts to is that Germany should really watch out to not get passed by countries where the people are more ambitious and motivated and less conservative and formal. We should never rest on our laurels but try to permanently improve in a continuous evolution ‚Äì or to quote many speakers at the conference mentioned above: to not fall into the process of disruptive self-destruction.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Over last last while I got confronted with this topic quite frequently. Eventually <a href=\"http://t3n.de/news/hotspots-unitymedia-abgemahnt-707512/\" target=\"_blank\" rel=\"noopener\">this german article</a> ‚Äì or more precisely the comments below it ‚Äì caused me to write my own little post on my personal perception of how innovation takes place in Germany. For the non-german readers among us I want to give a brief summary of that article. Recently the german internet provider ‚ÄúUnitymedia‚Äù came up with the idea to provide free WiFi-Hotspots for everyone, since internet connection in public places is pretty much a negative report in Germany. The key point with that is their intention of how to implement those so called ‚ÄúWifiSpots‚Äù. Every Unitymedia customer who has a Wifi-capable router in her home should become such a hotspot, while they promise that the hotspot network is completely isolated from the customer‚Äôs own network ‚Äì in terms of both security and bandwidth. Personally I like the idea, because I consider it quite efficient. Why put effort in distributing routers in public places if there already is full Wifi coverage? Most people‚Äôs routers are nowhere near working at capacity anyway. And if it‚Äôs guaranteed that the public internet traffic doesn‚Äôt influence you at all, why not? The article tells that the consumer protection center of North Rhine-Westphalia had admonished Unitymedia for their plans recently. One of the top comments below the article says about the following:</p>\n<p><em>‚ÄúThat‚Äôs exactly the reason why innovation isn‚Äôt possible in Germany. As soon as a company tries to solve people‚Äôs problems, everybody goes to the barricades. One gets punished for experiments ‚Äì not surprisingly nobody wants to found a company.‚Äù</em></p>\n<p>Even though the comment received a lot of bad write-up, that boils it down for me quite well. In my opinion Germany is ways too sluggish and conservative when it comes to adapting something new. Even though most representatives of the german manufacture consider themselves progressive, I actually don‚Äôt think they are ‚Äì at least not as much as they once were. I went to a conference on <a href=\"https://en.wikipedia.org/wiki/Industry_4.0\" target=\"_blank\" rel=\"noopener\">Industry 4.0</a> last week where one speaker claimed that Germany was at least one to two years ahead of other countries at Industry 4.0 topics, but what I picked out between the lines is that quite the opposite is true. While the Germany are continuously trying to define standards, develop well-defined business-processes, clarify legal aspects and the like other countries are just doing it. They simply try it out, taking not too much risk, and if it fails, it fails. But if it succeeds ‚Äì and I claim that if not applied totally wrong, new technology is likely to ‚Äì they are given a competitive advantage, while the current big players‚Äô lead is melting. Of course, with this attitude you are more likely to fail as if you examine every little aspect fussily. But you‚Äôre also so much more likely to win big. As a professor at university had repeatedly said: ‚Äúthink big!‚Äù.</p>\n<p><em><strong>&quot;If your dreams do not scare you, they are not big enough! ‚Äì Ellen Johnson Sirleaf&quot;</strong></em></p>\n<p>That‚Äôs what often is referred to as the Silicon valley mindset. In fact, as the diagram below shows, most startups are founded in the U.S.A. and I guess if there was a ranking of really big and successful startups (like Uber, WhatsApp, Tesla, ‚Ä¶) the contrast would be even more dramatic. Take Elon Musk ‚Äì the founder of Tesla Motors and SpaceX ‚Äì (I really recommend his biography) as an instance. He thought big and he obviously won (admittedly he took a really high risk). I don‚Äôt think they evaluate and plan new technology (like VR and stuff) that much at SpaceX ‚Äì they just do it.</p>\n<p><a href=\"http://www.statista.com/statistics/268786/start-ups-in-leading-economic-nations/\" target=\"_blank\" rel=\"noopener\"><img src=\"/images/statista.png\" alt></a><br>\nSource: <a href=\"http://Statista.com\" target=\"_blank\" rel=\"noopener\">Statista.com</a></p>\n<p>Another example for Germany‚Äôs innovation power is the following. I‚Äôve worked for two different companies as a working student ‚Äì both were software manufacturers. One was a typical german medium-sized company and the other was an american corporation. In one of them, the second-latest version of Internet Explorer was the only browser installed on every employee‚Äôs computer and if you desired another, you had to open a ticket to make the software distribution department install an outdated version of Firefox a few hours later. I don‚Äôt want to blame that company ‚Äì they did a great job at what they did. But the overall way of thinking there was old-fashioned, strict and not open-minded at all. Primarily it‚Äôs the people‚Äôs mindset that differentiates those two companies completely. I can‚Äôt really imagine that this first company is a workplace where you really feel comfortable and where you‚Äôre looking forward to a workday. At the time I worked there, they were about to release a little mobile app, whose development effort I estimate to only few weeks of intensive work of a small group of developers. The app‚Äôs purpose was great, but unfortunately, it was modern and innovative. Therefore there were only too few people to insistently support it. And there‚Äôs also the fact that processes were ways too sluggish to do a rapid development. As a result the app is still not released, but instead, two american countries each released an app with pretty much the exact same purpose. So much for that mindset in german companies ‚Äì of course and as always, there definitely are exceptions (a really big global player originated in Germany at which one department prints out document to hand it over to the other department, at which a trainee typewrites it to make it digital again, not being one of them).</p>\n<p>Another alarming fact I want to mention in this context is that the <a href=\"https://en.wikipedia.org/wiki/List_of_countries_by_Internet_connection_speeds#Akamai_Q3_2015_rankings\" target=\"_blank\" rel=\"noopener\">average internet speed in Germany is even far behind countries like Sri-Lanka and Vietnam</a>.</p>\n<p><em><strong>&quot;Once a new technology rolls over you, if you‚Äôre not part of the steamroller, you‚Äôre part of the road.‚Ää‚Äî‚ÄäStewart Brand&quot;</strong></em></p>\n<p>What this all amounts to is that Germany should really watch out to not get passed by countries where the people are more ambitious and motivated and less conservative and formal. We should never rest on our laurels but try to permanently improve in a continuous evolution ‚Äì or to quote many speakers at the conference mentioned above: to not fall into the process of disruptive self-destruction.</p>\n"},{"title":"Instant messenger security / encryption overview","date":"2016-02-01T21:48:46.000Z","_content":"\n---\n\n**[Update: Feb 28, 2020]**\nThis post is quite old now and things change rapdily, so please keep in mind that it might not be 100 % accurate anymore. Therefore, I would like to refer to a more recent article on [The best encrypted messaging apps (and their limitations) in 2020](https://www.comparitech.com/blog/information-security/best-encrypted-messaging-apps/). Also, there was a very interesting discussion about [Why do instant messengers not simply use PGP?](https://www.reddit.com/r/crypto/comments/f87asa/why_do_instant_messengers_not_simply_use_pgp/), that helps to better understand the topic of secure messaging.\n\n---\n\nI found a very nice page showing an overview of the security features of almost any instant messenger available. Nowadays where digital privacy observation is on everyones lips it is really interesting to see in how far the messengers we use every day are actually secure and who can potentially read or messages or not.\n\n[![](/images/scorecard.jpg)](https://www.eff.org/pages/secure-messaging-scorecard)\n\nThe aspects ‚ÄúEncrypted in transit?‚Äù and ‚ÄúEncrypted so the provider can‚Äôt read it?‚Äù are probably the most important ones and are a different way of saying if the application uses encrypted transmission (probably HTTPS for the most cases) and if it has end-to-end encryption. The last one means that keys are exchanged between sender and recipient which are used for asynchronous encryption so that nobody who potentially receives the message in the middle between them could read it ‚Äì neither the provider nor the government. To prove that this is actually implemented properly it is required to have an open code which can be reviewed by anyone, because if nobody has ever seen the code it could potentially be the case that your messenger might use end-to-end encryption but the provider grabs your private key too or things the like.\n\nWhat I miss about this ‚ÄúSecure Messaging Scorecard‚Äù is a specification of whether images (and audio recordings, videos, ‚Ä¶) get encrypted, too, by messengers that have a checkmark in the second column. Maybe I will do some research on this for some of the listed messengers.\n\nWhat I find alarming is that some commonly used apps like Skype don‚Äôt even have end-to-end encryption ‚Äì it is not that hard to integrate and for me this should be standard today. I don‚Äôt care that extremely much about online privacy because eventually I don‚Äôt have anything to hide but anyhow ‚Äì why should Microsoft employees potentially be able to read my messages and view my pics?","source":"_posts/instant-messenger-security-encryption-overview.md","raw":"---\ntitle: Instant messenger security / encryption overview\ndate: 2016-02-01 22:48:46\ntags:\n---\n\n---\n\n**[Update: Feb 28, 2020]**\nThis post is quite old now and things change rapdily, so please keep in mind that it might not be 100 % accurate anymore. Therefore, I would like to refer to a more recent article on [The best encrypted messaging apps (and their limitations) in 2020](https://www.comparitech.com/blog/information-security/best-encrypted-messaging-apps/). Also, there was a very interesting discussion about [Why do instant messengers not simply use PGP?](https://www.reddit.com/r/crypto/comments/f87asa/why_do_instant_messengers_not_simply_use_pgp/), that helps to better understand the topic of secure messaging.\n\n---\n\nI found a very nice page showing an overview of the security features of almost any instant messenger available. Nowadays where digital privacy observation is on everyones lips it is really interesting to see in how far the messengers we use every day are actually secure and who can potentially read or messages or not.\n\n[![](/images/scorecard.jpg)](https://www.eff.org/pages/secure-messaging-scorecard)\n\nThe aspects ‚ÄúEncrypted in transit?‚Äù and ‚ÄúEncrypted so the provider can‚Äôt read it?‚Äù are probably the most important ones and are a different way of saying if the application uses encrypted transmission (probably HTTPS for the most cases) and if it has end-to-end encryption. The last one means that keys are exchanged between sender and recipient which are used for asynchronous encryption so that nobody who potentially receives the message in the middle between them could read it ‚Äì neither the provider nor the government. To prove that this is actually implemented properly it is required to have an open code which can be reviewed by anyone, because if nobody has ever seen the code it could potentially be the case that your messenger might use end-to-end encryption but the provider grabs your private key too or things the like.\n\nWhat I miss about this ‚ÄúSecure Messaging Scorecard‚Äù is a specification of whether images (and audio recordings, videos, ‚Ä¶) get encrypted, too, by messengers that have a checkmark in the second column. Maybe I will do some research on this for some of the listed messengers.\n\nWhat I find alarming is that some commonly used apps like Skype don‚Äôt even have end-to-end encryption ‚Äì it is not that hard to integrate and for me this should be standard today. I don‚Äôt care that extremely much about online privacy because eventually I don‚Äôt have anything to hide but anyhow ‚Äì why should Microsoft employees potentially be able to read my messages and view my pics?","slug":"instant-messenger-security-encryption-overview","published":1,"updated":"2020-06-06T15:14:33.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhil000n40mqtauijo6i","content":"<hr>\n<p><strong>[Update: Feb 28, 2020]</strong><br>\nThis post is quite old now and things change rapdily, so please keep in mind that it might not be 100 % accurate anymore. Therefore, I would like to refer to a more recent article on <a href=\"https://www.comparitech.com/blog/information-security/best-encrypted-messaging-apps/\" target=\"_blank\" rel=\"noopener\">The best encrypted messaging apps (and their limitations) in 2020</a>. Also, there was a very interesting discussion about <a href=\"https://www.reddit.com/r/crypto/comments/f87asa/why_do_instant_messengers_not_simply_use_pgp/\" target=\"_blank\" rel=\"noopener\">Why do instant messengers not simply use PGP?</a>, that helps to better understand the topic of secure messaging.</p>\n<hr>\n<p>I found a very nice page showing an overview of the security features of almost any instant messenger available. Nowadays where digital privacy observation is on everyones lips it is really interesting to see in how far the messengers we use every day are actually secure and who can potentially read or messages or not.</p>\n<p><a href=\"https://www.eff.org/pages/secure-messaging-scorecard\" target=\"_blank\" rel=\"noopener\"><img src=\"/images/scorecard.jpg\" alt></a></p>\n<p>The aspects ‚ÄúEncrypted in transit?‚Äù and ‚ÄúEncrypted so the provider can‚Äôt read it?‚Äù are probably the most important ones and are a different way of saying if the application uses encrypted transmission (probably HTTPS for the most cases) and if it has end-to-end encryption. The last one means that keys are exchanged between sender and recipient which are used for asynchronous encryption so that nobody who potentially receives the message in the middle between them could read it ‚Äì neither the provider nor the government. To prove that this is actually implemented properly it is required to have an open code which can be reviewed by anyone, because if nobody has ever seen the code it could potentially be the case that your messenger might use end-to-end encryption but the provider grabs your private key too or things the like.</p>\n<p>What I miss about this ‚ÄúSecure Messaging Scorecard‚Äù is a specification of whether images (and audio recordings, videos, ‚Ä¶) get encrypted, too, by messengers that have a checkmark in the second column. Maybe I will do some research on this for some of the listed messengers.</p>\n<p>What I find alarming is that some commonly used apps like Skype don‚Äôt even have end-to-end encryption ‚Äì it is not that hard to integrate and for me this should be standard today. I don‚Äôt care that extremely much about online privacy because eventually I don‚Äôt have anything to hide but anyhow ‚Äì why should Microsoft employees potentially be able to read my messages and view my pics?</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<hr>\n<p><strong>[Update: Feb 28, 2020]</strong><br>\nThis post is quite old now and things change rapdily, so please keep in mind that it might not be 100 % accurate anymore. Therefore, I would like to refer to a more recent article on <a href=\"https://www.comparitech.com/blog/information-security/best-encrypted-messaging-apps/\" target=\"_blank\" rel=\"noopener\">The best encrypted messaging apps (and their limitations) in 2020</a>. Also, there was a very interesting discussion about <a href=\"https://www.reddit.com/r/crypto/comments/f87asa/why_do_instant_messengers_not_simply_use_pgp/\" target=\"_blank\" rel=\"noopener\">Why do instant messengers not simply use PGP?</a>, that helps to better understand the topic of secure messaging.</p>\n<hr>\n<p>I found a very nice page showing an overview of the security features of almost any instant messenger available. Nowadays where digital privacy observation is on everyones lips it is really interesting to see in how far the messengers we use every day are actually secure and who can potentially read or messages or not.</p>\n<p><a href=\"https://www.eff.org/pages/secure-messaging-scorecard\" target=\"_blank\" rel=\"noopener\"><img src=\"/images/scorecard.jpg\" alt></a></p>\n<p>The aspects ‚ÄúEncrypted in transit?‚Äù and ‚ÄúEncrypted so the provider can‚Äôt read it?‚Äù are probably the most important ones and are a different way of saying if the application uses encrypted transmission (probably HTTPS for the most cases) and if it has end-to-end encryption. The last one means that keys are exchanged between sender and recipient which are used for asynchronous encryption so that nobody who potentially receives the message in the middle between them could read it ‚Äì neither the provider nor the government. To prove that this is actually implemented properly it is required to have an open code which can be reviewed by anyone, because if nobody has ever seen the code it could potentially be the case that your messenger might use end-to-end encryption but the provider grabs your private key too or things the like.</p>\n<p>What I miss about this ‚ÄúSecure Messaging Scorecard‚Äù is a specification of whether images (and audio recordings, videos, ‚Ä¶) get encrypted, too, by messengers that have a checkmark in the second column. Maybe I will do some research on this for some of the listed messengers.</p>\n<p>What I find alarming is that some commonly used apps like Skype don‚Äôt even have end-to-end encryption ‚Äì it is not that hard to integrate and for me this should be standard today. I don‚Äôt care that extremely much about online privacy because eventually I don‚Äôt have anything to hide but anyhow ‚Äì why should Microsoft employees potentially be able to read my messages and view my pics?</p>\n"},{"title":"Learning Angular2: What is new?","date":"2016-02-17T21:51:59.000Z","_content":"\n![](/images/angular2_logo.png)\n\nA few days ago i started teaching myself [Angular 2](http://angular.io), which is the successor of the popular frontend web-framework [AngularJS 1.x](https://angularjs.org/). It is still in development and only released as a beta and the developers at Google recommend to not use it in production yet. But I‚Äôm sure it will come some time in the near future so why not take a step ahead and already learn it now? As Angular 1 has become very successful and wide-spread for web-applications‚Äô client side I have no doubts that Angular 2 will establish itself pretty quick too.\n\nFor those of you who are familiar with Angular 1 and have developed with it yet: according to what I‚Äôve seen so far you will definitely need to take some time for learning Angular 2 ‚Äì it is considerably different from the first version and got few major changes, at least in my eyes. Those changes include:\n\n*   **Different syntax for directives** in HTML: They have introduced parantheses (), brackets [], stars *, hashtags # and combinations of them to be used in your markup. E.g. parantheses () are used as attributes in HTML elements to bind to their events.\n*   There are no controllers anymore. Instead everything is based on (Web-)**Components** (as you may know them from [Google Polymer](https://www.polymer-project.org/1.0/) ‚Äì if you don‚Äôt check this out as well, it is pretty cool), which basically consist of the component‚Äôs logic and a view and define a new custom HTML element each. Almost everything in Angular 2 is a component, which enables the code to be even more structurable, more modular and more reusable. But it is a completely new way of thinking in comparison to Angular 1‚Äôs controllers.\n*   It is based on **ES6 and [TypeScript](http://www.typescriptlang.org/)**. ES6 is the latest JavaScript standard (or version so to say) and TypeScript is even a superset of that, which basically introduces types and modifiers for variables and functions (as you may know from strongly-typed languages like Java). This brings some completely [new features](https://github.com/lukehoban/es6features) and syntax you need to get familiar with. For instance you can define classes with attributes and methods like this now:\n```javascript\n    class Greeter {\n        greeting: string;\n        constructor(message: string) {\n            this.greeting = message;\n        }\n        greet() {\n            return \"Hello, \" + this.greeting;\n        }\n    }\n\n    var greeter = new Greeter(\"world\");</pre>\n```\n\nAlso there are interfaces, import statements, a shorthand way for anonymous functions called ‚Äúarrow functions‚Äù and many more. Before learning Angular 2 I really recommend to first learn JavaScript ES6 for which [these two videos](https://www.youtube.com/playlist?list=PLoYCgNOIyGACDQLaThEEKBAlgs4OIUGif) are really great.\n\n*   **Dependency injection** has also been reworked to be better understandable, easier to use and more modular now. You will no use @Inectable decorators for injectable services and other modules and provide in the modules by referencing to them in a providers property in components‚Äô @Component decorator.\n*   Two-way data-binding is still available but the focus is now on **one-way data-binding** (if I got it right the main reason are performance considerations). One-way data-binding means that data isn‚Äôt continuously updated between template and component but only based on events triggered.\n\nThose where just some of the major changes I got so far. If you want to learn Angular 2 I recommend you the following resources:\n\n * [Angular‚Äôs offical Getting Started](https://angular.io/docs/ts/latest/quickstart.html)\n * [Angular 2 Fundamentals](https://www.udemy.com/angular-2-fundamentals/) (a free video course on [Udemy](http://udemy.com) for the very basic concepts)\n * This video tutorial on YouTube which I found quite good (and I really had to laugh at 30:20 minutes). You can skip the first few minutes.  \n\n[![](http://img.youtube.com/vi/KL4Yi3WtymA/0.jpg)](http://www.youtube.com/watch?v=KL4Yi3WtymA)\n\nBeing able to develop the brand new, fancy Angular 2 with the brand new JavaScript ES6 (which is so new that most of today‚Äôs browsers won‚Äôt even understand it and as a result it needs to get transpiled to ES5 at present) will definitely give you benefits in web development and also in finding a job in that sector ‚Äì which is probably the fastest growing and most hyped one at the moment.","source":"_posts/learning-angular2-what-is-new.md","raw":"---\ntitle: 'Learning Angular2: What is new?'\ndate: 2016-02-17 22:51:59\ntags:\n---\n\n![](/images/angular2_logo.png)\n\nA few days ago i started teaching myself [Angular 2](http://angular.io), which is the successor of the popular frontend web-framework [AngularJS 1.x](https://angularjs.org/). It is still in development and only released as a beta and the developers at Google recommend to not use it in production yet. But I‚Äôm sure it will come some time in the near future so why not take a step ahead and already learn it now? As Angular 1 has become very successful and wide-spread for web-applications‚Äô client side I have no doubts that Angular 2 will establish itself pretty quick too.\n\nFor those of you who are familiar with Angular 1 and have developed with it yet: according to what I‚Äôve seen so far you will definitely need to take some time for learning Angular 2 ‚Äì it is considerably different from the first version and got few major changes, at least in my eyes. Those changes include:\n\n*   **Different syntax for directives** in HTML: They have introduced parantheses (), brackets [], stars *, hashtags # and combinations of them to be used in your markup. E.g. parantheses () are used as attributes in HTML elements to bind to their events.\n*   There are no controllers anymore. Instead everything is based on (Web-)**Components** (as you may know them from [Google Polymer](https://www.polymer-project.org/1.0/) ‚Äì if you don‚Äôt check this out as well, it is pretty cool), which basically consist of the component‚Äôs logic and a view and define a new custom HTML element each. Almost everything in Angular 2 is a component, which enables the code to be even more structurable, more modular and more reusable. But it is a completely new way of thinking in comparison to Angular 1‚Äôs controllers.\n*   It is based on **ES6 and [TypeScript](http://www.typescriptlang.org/)**. ES6 is the latest JavaScript standard (or version so to say) and TypeScript is even a superset of that, which basically introduces types and modifiers for variables and functions (as you may know from strongly-typed languages like Java). This brings some completely [new features](https://github.com/lukehoban/es6features) and syntax you need to get familiar with. For instance you can define classes with attributes and methods like this now:\n```javascript\n    class Greeter {\n        greeting: string;\n        constructor(message: string) {\n            this.greeting = message;\n        }\n        greet() {\n            return \"Hello, \" + this.greeting;\n        }\n    }\n\n    var greeter = new Greeter(\"world\");</pre>\n```\n\nAlso there are interfaces, import statements, a shorthand way for anonymous functions called ‚Äúarrow functions‚Äù and many more. Before learning Angular 2 I really recommend to first learn JavaScript ES6 for which [these two videos](https://www.youtube.com/playlist?list=PLoYCgNOIyGACDQLaThEEKBAlgs4OIUGif) are really great.\n\n*   **Dependency injection** has also been reworked to be better understandable, easier to use and more modular now. You will no use @Inectable decorators for injectable services and other modules and provide in the modules by referencing to them in a providers property in components‚Äô @Component decorator.\n*   Two-way data-binding is still available but the focus is now on **one-way data-binding** (if I got it right the main reason are performance considerations). One-way data-binding means that data isn‚Äôt continuously updated between template and component but only based on events triggered.\n\nThose where just some of the major changes I got so far. If you want to learn Angular 2 I recommend you the following resources:\n\n * [Angular‚Äôs offical Getting Started](https://angular.io/docs/ts/latest/quickstart.html)\n * [Angular 2 Fundamentals](https://www.udemy.com/angular-2-fundamentals/) (a free video course on [Udemy](http://udemy.com) for the very basic concepts)\n * This video tutorial on YouTube which I found quite good (and I really had to laugh at 30:20 minutes). You can skip the first few minutes.  \n\n[![](http://img.youtube.com/vi/KL4Yi3WtymA/0.jpg)](http://www.youtube.com/watch?v=KL4Yi3WtymA)\n\nBeing able to develop the brand new, fancy Angular 2 with the brand new JavaScript ES6 (which is so new that most of today‚Äôs browsers won‚Äôt even understand it and as a result it needs to get transpiled to ES5 at present) will definitely give you benefits in web development and also in finding a job in that sector ‚Äì which is probably the fastest growing and most hyped one at the moment.","slug":"learning-angular2-what-is-new","published":1,"updated":"2020-06-06T15:14:33.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhin000o40mq90v5g195","content":"<p><img src=\"/images/angular2_logo.png\" alt></p>\n<p>A few days ago i started teaching myself <a href=\"http://angular.io\" target=\"_blank\" rel=\"noopener\">Angular 2</a>, which is the successor of the popular frontend web-framework <a href=\"https://angularjs.org/\" target=\"_blank\" rel=\"noopener\">AngularJS 1.x</a>. It is still in development and only released as a beta and the developers at Google recommend to not use it in production yet. But I‚Äôm sure it will come some time in the near future so why not take a step ahead and already learn it now? As Angular 1 has become very successful and wide-spread for web-applications‚Äô client side I have no doubts that Angular 2 will establish itself pretty quick too.</p>\n<p>For those of you who are familiar with Angular 1 and have developed with it yet: according to what I‚Äôve seen so far you will definitely need to take some time for learning Angular 2 ‚Äì it is considerably different from the first version and got few major changes, at least in my eyes. Those changes include:</p>\n<ul>\n<li><strong>Different syntax for directives</strong> in HTML: They have introduced parantheses (), brackets [], stars *, hashtags # and combinations of them to be used in your markup. E.g. parantheses () are used as attributes in HTML elements to bind to their events.</li>\n<li>There are no controllers anymore. Instead everything is based on (Web-)<strong>Components</strong> (as you may know them from <a href=\"https://www.polymer-project.org/1.0/\" target=\"_blank\" rel=\"noopener\">Google Polymer</a> ‚Äì if you don‚Äôt check this out as well, it is pretty cool), which basically consist of the component‚Äôs logic and a view and define a new custom HTML element each. Almost everything in Angular 2 is a component, which enables the code to be even more structurable, more modular and more reusable. But it is a completely new way of thinking in comparison to Angular 1‚Äôs controllers.</li>\n<li>It is based on <strong>ES6 and <a href=\"http://www.typescriptlang.org/\" target=\"_blank\" rel=\"noopener\">TypeScript</a></strong>. ES6 is the latest JavaScript standard (or version so to say) and TypeScript is even a superset of that, which basically introduces types and modifiers for variables and functions (as you may know from strongly-typed languages like Java). This brings some completely <a href=\"https://github.com/lukehoban/es6features\" target=\"_blank\" rel=\"noopener\">new features</a> and syntax you need to get familiar with. For instance you can define classes with attributes and methods like this now:</li>\n</ul>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Greeter</span> </span>&#123;</span><br><span class=\"line\">    greeting: string;</span><br><span class=\"line\">    <span class=\"keyword\">constructor</span>(message: string) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.greeting = message;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    greet() &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">\"Hello, \"</span> + <span class=\"keyword\">this</span>.greeting;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> greeter = <span class=\"keyword\">new</span> Greeter(<span class=\"string\">\"world\"</span>);&lt;<span class=\"regexp\">/pre&gt;</span></span><br></pre></td></tr></table></figure>\n<p>Also there are interfaces, import statements, a shorthand way for anonymous functions called ‚Äúarrow functions‚Äù and many more. Before learning Angular 2 I really recommend to first learn JavaScript ES6 for which <a href=\"https://www.youtube.com/playlist?list=PLoYCgNOIyGACDQLaThEEKBAlgs4OIUGif\" target=\"_blank\" rel=\"noopener\">these two videos</a> are really great.</p>\n<ul>\n<li><strong>Dependency injection</strong> has also been reworked to be better understandable, easier to use and more modular now. You will no use @Inectable decorators for injectable services and other modules and provide in the modules by referencing to them in a providers property in components‚Äô @Component decorator.</li>\n<li>Two-way data-binding is still available but the focus is now on <strong>one-way data-binding</strong> (if I got it right the main reason are performance considerations). One-way data-binding means that data isn‚Äôt continuously updated between template and component but only based on events triggered.</li>\n</ul>\n<p>Those where just some of the major changes I got so far. If you want to learn Angular 2 I recommend you the following resources:</p>\n<ul>\n<li><a href=\"https://angular.io/docs/ts/latest/quickstart.html\" target=\"_blank\" rel=\"noopener\">Angular‚Äôs offical Getting Started</a></li>\n<li><a href=\"https://www.udemy.com/angular-2-fundamentals/\" target=\"_blank\" rel=\"noopener\">Angular 2 Fundamentals</a> (a free video course on <a href=\"http://udemy.com\" target=\"_blank\" rel=\"noopener\">Udemy</a> for the very basic concepts)</li>\n<li>This video tutorial on YouTube which I found quite good (and I really had to laugh at 30:20 minutes). You can skip the first few minutes.</li>\n</ul>\n<p><a href=\"http://www.youtube.com/watch?v=KL4Yi3WtymA\" target=\"_blank\" rel=\"noopener\"><img src=\"http://img.youtube.com/vi/KL4Yi3WtymA/0.jpg\" alt></a></p>\n<p>Being able to develop the brand new, fancy Angular 2 with the brand new JavaScript ES6 (which is so new that most of today‚Äôs browsers won‚Äôt even understand it and as a result it needs to get transpiled to ES5 at present) will definitely give you benefits in web development and also in finding a job in that sector ‚Äì which is probably the fastest growing and most hyped one at the moment.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"/images/angular2_logo.png\" alt></p>\n<p>A few days ago i started teaching myself <a href=\"http://angular.io\" target=\"_blank\" rel=\"noopener\">Angular 2</a>, which is the successor of the popular frontend web-framework <a href=\"https://angularjs.org/\" target=\"_blank\" rel=\"noopener\">AngularJS 1.x</a>. It is still in development and only released as a beta and the developers at Google recommend to not use it in production yet. But I‚Äôm sure it will come some time in the near future so why not take a step ahead and already learn it now? As Angular 1 has become very successful and wide-spread for web-applications‚Äô client side I have no doubts that Angular 2 will establish itself pretty quick too.</p>\n<p>For those of you who are familiar with Angular 1 and have developed with it yet: according to what I‚Äôve seen so far you will definitely need to take some time for learning Angular 2 ‚Äì it is considerably different from the first version and got few major changes, at least in my eyes. Those changes include:</p>\n<ul>\n<li><strong>Different syntax for directives</strong> in HTML: They have introduced parantheses (), brackets [], stars *, hashtags # and combinations of them to be used in your markup. E.g. parantheses () are used as attributes in HTML elements to bind to their events.</li>\n<li>There are no controllers anymore. Instead everything is based on (Web-)<strong>Components</strong> (as you may know them from <a href=\"https://www.polymer-project.org/1.0/\" target=\"_blank\" rel=\"noopener\">Google Polymer</a> ‚Äì if you don‚Äôt check this out as well, it is pretty cool), which basically consist of the component‚Äôs logic and a view and define a new custom HTML element each. Almost everything in Angular 2 is a component, which enables the code to be even more structurable, more modular and more reusable. But it is a completely new way of thinking in comparison to Angular 1‚Äôs controllers.</li>\n<li>It is based on <strong>ES6 and <a href=\"http://www.typescriptlang.org/\" target=\"_blank\" rel=\"noopener\">TypeScript</a></strong>. ES6 is the latest JavaScript standard (or version so to say) and TypeScript is even a superset of that, which basically introduces types and modifiers for variables and functions (as you may know from strongly-typed languages like Java). This brings some completely <a href=\"https://github.com/lukehoban/es6features\" target=\"_blank\" rel=\"noopener\">new features</a> and syntax you need to get familiar with. For instance you can define classes with attributes and methods like this now:</li>\n</ul>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Greeter</span> </span>&#123;</span><br><span class=\"line\">    greeting: string;</span><br><span class=\"line\">    <span class=\"keyword\">constructor</span>(message: string) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">this</span>.greeting = message;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    greet() &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">\"Hello, \"</span> + <span class=\"keyword\">this</span>.greeting;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> greeter = <span class=\"keyword\">new</span> Greeter(<span class=\"string\">\"world\"</span>);&lt;<span class=\"regexp\">/pre&gt;</span></span><br></pre></td></tr></table></figure>\n<p>Also there are interfaces, import statements, a shorthand way for anonymous functions called ‚Äúarrow functions‚Äù and many more. Before learning Angular 2 I really recommend to first learn JavaScript ES6 for which <a href=\"https://www.youtube.com/playlist?list=PLoYCgNOIyGACDQLaThEEKBAlgs4OIUGif\" target=\"_blank\" rel=\"noopener\">these two videos</a> are really great.</p>\n<ul>\n<li><strong>Dependency injection</strong> has also been reworked to be better understandable, easier to use and more modular now. You will no use @Inectable decorators for injectable services and other modules and provide in the modules by referencing to them in a providers property in components‚Äô @Component decorator.</li>\n<li>Two-way data-binding is still available but the focus is now on <strong>one-way data-binding</strong> (if I got it right the main reason are performance considerations). One-way data-binding means that data isn‚Äôt continuously updated between template and component but only based on events triggered.</li>\n</ul>\n<p>Those where just some of the major changes I got so far. If you want to learn Angular 2 I recommend you the following resources:</p>\n<ul>\n<li><a href=\"https://angular.io/docs/ts/latest/quickstart.html\" target=\"_blank\" rel=\"noopener\">Angular‚Äôs offical Getting Started</a></li>\n<li><a href=\"https://www.udemy.com/angular-2-fundamentals/\" target=\"_blank\" rel=\"noopener\">Angular 2 Fundamentals</a> (a free video course on <a href=\"http://udemy.com\" target=\"_blank\" rel=\"noopener\">Udemy</a> for the very basic concepts)</li>\n<li>This video tutorial on YouTube which I found quite good (and I really had to laugh at 30:20 minutes). You can skip the first few minutes.</li>\n</ul>\n<p><a href=\"http://www.youtube.com/watch?v=KL4Yi3WtymA\" target=\"_blank\" rel=\"noopener\"><img src=\"http://img.youtube.com/vi/KL4Yi3WtymA/0.jpg\" alt></a></p>\n<p>Being able to develop the brand new, fancy Angular 2 with the brand new JavaScript ES6 (which is so new that most of today‚Äôs browsers won‚Äôt even understand it and as a result it needs to get transpiled to ES5 at present) will definitely give you benefits in web development and also in finding a job in that sector ‚Äì which is probably the fastest growing and most hyped one at the moment.</p>\n"},{"title":"LinkedData Trivia Game","date":"2017-02-01T22:09:18.000Z","_content":"\nOriginally I got inspired by [this recent post](https://news.ycombinator.com/item?id=13677748) on HackerNews, where [alex_g](https://news.ycombinator.com/user?id=alex_g) has built a quiz, which automatically generates questions from Wikipedia articles using natural language processing (NLP). However, I found the results not that satisfying, yet, and decides to build my own dynamic quiz. Instead of NLP processing I decided to use Linked Data as a base for generating questions. More precisely I'm using the [DBPedia](https://dbpedia.org) knowledge base to retreive fact information from, which mostly originates in Wikipedia articles as well. The data is structured as an RDF graph and can be queried using SPARQL. Despite from the official DBPedia SPARQL endpoint this little proof-of-concept-like app uses another webservice, [kit-lod16-knowledge-panel](https://github.com/muety/kit-lod16-knowledge-panel), which I developed in the context of the Linked Open Data seminar at university. It is responsible for ranking RDF properties for specific RDF entities by relevance in order to decide, which one to display to an end-user (or include to a quiz).\n\n[Code on GitHub](https://github.com/muety/linkeddata-trivia)\n\n![](images/trivia.jpg)\n\n### Limitations\nThis project is __not a production-ready__ app at all, but rather a __proof-of-concept__ to experiment with. Currently, the __major issue is performance__. Since the app fires a bunch of rather expensive, non-optimized SPARQL queries at the public DBPedia endpoint, the whole process of generating a quiz question takes several seconds on average, sometimes even up to a minute. This could be optimized to a certain extent (e.g. currently there are at least 8 separate HTTP requests from this app plus a few more from the ranking webservice), but all in all querying RDF data is still pretty slow. \n\nAnother limitation is the way \"wrong\" answer options are generated. Currently, random values within a certain interval around the \"correct\" answer are generated for dates and numbers. For properties, whose _rdfs:range_ are entities of a class, a random set of other entities from the same class is fetched from DBPedia and shown as alternative answers. However, __string-valued answers, among others, are ignored__ completely, because it's hard to auto-generate an alternative value for a plain string. There's room for enhancement here.\n\nA third way for improvement would be to include not only DBPedia, but also [Yago](https://yago-knowledge.org), Wikidata and other sources. ","source":"_posts/linkeddata-trivia-game.md","raw":"---\ntitle: LinkedData Trivia Game\ndate: 2017-02-01 23:09:18\ntags:\n---\n\nOriginally I got inspired by [this recent post](https://news.ycombinator.com/item?id=13677748) on HackerNews, where [alex_g](https://news.ycombinator.com/user?id=alex_g) has built a quiz, which automatically generates questions from Wikipedia articles using natural language processing (NLP). However, I found the results not that satisfying, yet, and decides to build my own dynamic quiz. Instead of NLP processing I decided to use Linked Data as a base for generating questions. More precisely I'm using the [DBPedia](https://dbpedia.org) knowledge base to retreive fact information from, which mostly originates in Wikipedia articles as well. The data is structured as an RDF graph and can be queried using SPARQL. Despite from the official DBPedia SPARQL endpoint this little proof-of-concept-like app uses another webservice, [kit-lod16-knowledge-panel](https://github.com/muety/kit-lod16-knowledge-panel), which I developed in the context of the Linked Open Data seminar at university. It is responsible for ranking RDF properties for specific RDF entities by relevance in order to decide, which one to display to an end-user (or include to a quiz).\n\n[Code on GitHub](https://github.com/muety/linkeddata-trivia)\n\n![](images/trivia.jpg)\n\n### Limitations\nThis project is __not a production-ready__ app at all, but rather a __proof-of-concept__ to experiment with. Currently, the __major issue is performance__. Since the app fires a bunch of rather expensive, non-optimized SPARQL queries at the public DBPedia endpoint, the whole process of generating a quiz question takes several seconds on average, sometimes even up to a minute. This could be optimized to a certain extent (e.g. currently there are at least 8 separate HTTP requests from this app plus a few more from the ranking webservice), but all in all querying RDF data is still pretty slow. \n\nAnother limitation is the way \"wrong\" answer options are generated. Currently, random values within a certain interval around the \"correct\" answer are generated for dates and numbers. For properties, whose _rdfs:range_ are entities of a class, a random set of other entities from the same class is fetched from DBPedia and shown as alternative answers. However, __string-valued answers, among others, are ignored__ completely, because it's hard to auto-generate an alternative value for a plain string. There's room for enhancement here.\n\nA third way for improvement would be to include not only DBPedia, but also [Yago](https://yago-knowledge.org), Wikidata and other sources. ","slug":"linkeddata-trivia-game","published":1,"updated":"2020-06-06T15:14:33.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhio000p40mqcfcp985u","content":"<p>Originally I got inspired by <a href=\"https://news.ycombinator.com/item?id=13677748\" target=\"_blank\" rel=\"noopener\">this recent post</a> on HackerNews, where <a href=\"https://news.ycombinator.com/user?id=alex_g\" target=\"_blank\" rel=\"noopener\">alex_g</a> has built a quiz, which automatically generates questions from Wikipedia articles using natural language processing (NLP). However, I found the results not that satisfying, yet, and decides to build my own dynamic quiz. Instead of NLP processing I decided to use Linked Data as a base for generating questions. More precisely I‚Äôm using the <a href=\"https://dbpedia.org\" target=\"_blank\" rel=\"noopener\">DBPedia</a> knowledge base to retreive fact information from, which mostly originates in Wikipedia articles as well. The data is structured as an RDF graph and can be queried using SPARQL. Despite from the official DBPedia SPARQL endpoint this little proof-of-concept-like app uses another webservice, <a href=\"https://github.com/muety/kit-lod16-knowledge-panel\" target=\"_blank\" rel=\"noopener\">kit-lod16-knowledge-panel</a>, which I developed in the context of the Linked Open Data seminar at university. It is responsible for ranking RDF properties for specific RDF entities by relevance in order to decide, which one to display to an end-user (or include to a quiz).</p>\n<p><a href=\"https://github.com/muety/linkeddata-trivia\" target=\"_blank\" rel=\"noopener\">Code on GitHub</a></p>\n<p><img src=\"images/trivia.jpg\" alt></p>\n<h3 id=\"Limitations\">Limitations</h3>\n<p>This project is <strong>not a production-ready</strong> app at all, but rather a <strong>proof-of-concept</strong> to experiment with. Currently, the <strong>major issue is performance</strong>. Since the app fires a bunch of rather expensive, non-optimized SPARQL queries at the public DBPedia endpoint, the whole process of generating a quiz question takes several seconds on average, sometimes even up to a minute. This could be optimized to a certain extent (e.g. currently there are at least 8 separate HTTP requests from this app plus a few more from the ranking webservice), but all in all querying RDF data is still pretty slow.</p>\n<p>Another limitation is the way ‚Äúwrong‚Äù answer options are generated. Currently, random values within a certain interval around the ‚Äúcorrect‚Äù answer are generated for dates and numbers. For properties, whose <em>rdfs:range</em> are entities of a class, a random set of other entities from the same class is fetched from DBPedia and shown as alternative answers. However, <strong>string-valued answers, among others, are ignored</strong> completely, because it‚Äôs hard to auto-generate an alternative value for a plain string. There‚Äôs room for enhancement here.</p>\n<p>A third way for improvement would be to include not only DBPedia, but also <a href=\"https://yago-knowledge.org\" target=\"_blank\" rel=\"noopener\">Yago</a>, Wikidata and other sources.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Originally I got inspired by <a href=\"https://news.ycombinator.com/item?id=13677748\" target=\"_blank\" rel=\"noopener\">this recent post</a> on HackerNews, where <a href=\"https://news.ycombinator.com/user?id=alex_g\" target=\"_blank\" rel=\"noopener\">alex_g</a> has built a quiz, which automatically generates questions from Wikipedia articles using natural language processing (NLP). However, I found the results not that satisfying, yet, and decides to build my own dynamic quiz. Instead of NLP processing I decided to use Linked Data as a base for generating questions. More precisely I‚Äôm using the <a href=\"https://dbpedia.org\" target=\"_blank\" rel=\"noopener\">DBPedia</a> knowledge base to retreive fact information from, which mostly originates in Wikipedia articles as well. The data is structured as an RDF graph and can be queried using SPARQL. Despite from the official DBPedia SPARQL endpoint this little proof-of-concept-like app uses another webservice, <a href=\"https://github.com/muety/kit-lod16-knowledge-panel\" target=\"_blank\" rel=\"noopener\">kit-lod16-knowledge-panel</a>, which I developed in the context of the Linked Open Data seminar at university. It is responsible for ranking RDF properties for specific RDF entities by relevance in order to decide, which one to display to an end-user (or include to a quiz).</p>\n<p><a href=\"https://github.com/muety/linkeddata-trivia\" target=\"_blank\" rel=\"noopener\">Code on GitHub</a></p>\n<p><img src=\"images/trivia.jpg\" alt></p>\n<h3 id=\"Limitations\">Limitations</h3>\n<p>This project is <strong>not a production-ready</strong> app at all, but rather a <strong>proof-of-concept</strong> to experiment with. Currently, the <strong>major issue is performance</strong>. Since the app fires a bunch of rather expensive, non-optimized SPARQL queries at the public DBPedia endpoint, the whole process of generating a quiz question takes several seconds on average, sometimes even up to a minute. This could be optimized to a certain extent (e.g. currently there are at least 8 separate HTTP requests from this app plus a few more from the ranking webservice), but all in all querying RDF data is still pretty slow.</p>\n<p>Another limitation is the way ‚Äúwrong‚Äù answer options are generated. Currently, random values within a certain interval around the ‚Äúcorrect‚Äù answer are generated for dates and numbers. For properties, whose <em>rdfs:range</em> are entities of a class, a random set of other entities from the same class is fetched from DBPedia and shown as alternative answers. However, <strong>string-valued answers, among others, are ignored</strong> completely, because it‚Äôs hard to auto-generate an alternative value for a plain string. There‚Äôs room for enhancement here.</p>\n<p>A third way for improvement would be to include not only DBPedia, but also <a href=\"https://yago-knowledge.org\" target=\"_blank\" rel=\"noopener\">Yago</a>, Wikidata and other sources.</p>\n"},{"title":"Linux - Cache information bash script","date":"2015-02-27T21:31:53.000Z","_content":"\nThis is a little bash script to get the CPU cache ratios on Ubuntu.\n\n```\nCache Level: (1, 2 or 3)\nCache Type: (data-, instruction or general cache)\nCapacity: of the respective cache\nAssociativity: (set size)\nBlock capacity: / capacity of a cache line\nNumber of sets: ((total capacity / block capacity) / associativity)\n```\n\nConcerning the associativity, see [https://en.wikipedia.org/wiki/CPU_cache#Associativity](https://en.wikipedia.org/wiki/CPU_cache#Associativity).\n\n```bash\nfor DIR0 in /sys/devices/system/cpu/cpu0/cache/*\n    do\n        LEVEL0=$(sudo cat $DIR0\\/level)\n        TYPE0=$(sudo cat $DIR0\\/type)\n        SIZE0=$(sudo cat $DIR0\\/size)\n        ASSOC0=$(sudo cat $DIR0\\/ways_of_associativity)\n        BLOCK0=$(sudo cat $DIR0\\/coherency_line_size)\n        SETS0=$(sudo cat $DIR0\\/number_of_sets)\n\n        printf &quot;Cache level: %s\\nCache type: %s\\nCapacity: %s Bytes\\nAssociativity: %s\\nSets: %s\\nBlock size: %s Bytes\\n\\n&quot; &quot;$LEVEL0&quot; &quot;$TYPE0&quot; &quot;$SIZE0&quot; &quot;$ASSOC0&quot; &quot;$SETS0&quot; &quot;$BLOCK0&quot;\ndone\n```\n\n**Usage:**\n\n1.  Save code to file, e.g. _~/cacheinfo.sh_\n2.  Make it executable: _chmod +x cacheinfo.sh_\n3.  Execute: _sudo sh cacheinfo.sh_","source":"_posts/linux-cache-information-bash-script.md","raw":"---\ntitle: Linux - Cache information bash script\ndate: 2015-02-27 22:31:53\ntags:\n---\n\nThis is a little bash script to get the CPU cache ratios on Ubuntu.\n\n```\nCache Level: (1, 2 or 3)\nCache Type: (data-, instruction or general cache)\nCapacity: of the respective cache\nAssociativity: (set size)\nBlock capacity: / capacity of a cache line\nNumber of sets: ((total capacity / block capacity) / associativity)\n```\n\nConcerning the associativity, see [https://en.wikipedia.org/wiki/CPU_cache#Associativity](https://en.wikipedia.org/wiki/CPU_cache#Associativity).\n\n```bash\nfor DIR0 in /sys/devices/system/cpu/cpu0/cache/*\n    do\n        LEVEL0=$(sudo cat $DIR0\\/level)\n        TYPE0=$(sudo cat $DIR0\\/type)\n        SIZE0=$(sudo cat $DIR0\\/size)\n        ASSOC0=$(sudo cat $DIR0\\/ways_of_associativity)\n        BLOCK0=$(sudo cat $DIR0\\/coherency_line_size)\n        SETS0=$(sudo cat $DIR0\\/number_of_sets)\n\n        printf &quot;Cache level: %s\\nCache type: %s\\nCapacity: %s Bytes\\nAssociativity: %s\\nSets: %s\\nBlock size: %s Bytes\\n\\n&quot; &quot;$LEVEL0&quot; &quot;$TYPE0&quot; &quot;$SIZE0&quot; &quot;$ASSOC0&quot; &quot;$SETS0&quot; &quot;$BLOCK0&quot;\ndone\n```\n\n**Usage:**\n\n1.  Save code to file, e.g. _~/cacheinfo.sh_\n2.  Make it executable: _chmod +x cacheinfo.sh_\n3.  Execute: _sudo sh cacheinfo.sh_","slug":"linux-cache-information-bash-script","published":1,"updated":"2020-06-06T15:14:33.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhiq000q40mqg8zwxsu1","content":"<p>This is a little bash script to get the CPU cache ratios on Ubuntu.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Cache Level: (1, 2 or 3)</span><br><span class=\"line\">Cache Type: (data-, instruction or general cache)</span><br><span class=\"line\">Capacity: of the respective cache</span><br><span class=\"line\">Associativity: (set size)</span><br><span class=\"line\">Block capacity: / capacity of a cache line</span><br><span class=\"line\">Number of sets: ((total capacity / block capacity) / associativity)</span><br></pre></td></tr></table></figure>\n<p>Concerning the associativity, see <a href=\"https://en.wikipedia.org/wiki/CPU_cache#Associativity\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/CPU_cache#Associativity</a>.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> DIR0 <span class=\"keyword\">in</span> /sys/devices/system/cpu/cpu0/cache/*</span><br><span class=\"line\">    <span class=\"keyword\">do</span></span><br><span class=\"line\">        LEVEL0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/level)</span><br><span class=\"line\">        TYPE0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/<span class=\"built_in\">type</span>)</span><br><span class=\"line\">        SIZE0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/size)</span><br><span class=\"line\">        ASSOC0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/ways_of_associativity)</span><br><span class=\"line\">        BLOCK0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/coherency_line_size)</span><br><span class=\"line\">        SETS0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/number_of_sets)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">printf</span> &amp;quot;Cache level: %s\\nCache <span class=\"built_in\">type</span>: %s\\nCapacity: %s Bytes\\nAssociativity: %s\\nSets: %s\\nBlock size: %s Bytes\\n\\n&amp;quot; &amp;quot;<span class=\"variable\">$LEVEL0</span>&amp;quot; &amp;quot;<span class=\"variable\">$TYPE0</span>&amp;quot; &amp;quot;<span class=\"variable\">$SIZE0</span>&amp;quot; &amp;quot;<span class=\"variable\">$ASSOC0</span>&amp;quot; &amp;quot;<span class=\"variable\">$SETS0</span>&amp;quot; &amp;quot;<span class=\"variable\">$BLOCK0</span>&amp;quot;</span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n<p><strong>Usage:</strong></p>\n<ol>\n<li>Save code to file, e.g. <em>~/cacheinfo.sh</em></li>\n<li>Make it executable: <em>chmod +x <a href=\"http://cacheinfo.sh\" target=\"_blank\" rel=\"noopener\">cacheinfo.sh</a></em></li>\n<li>Execute: <em>sudo sh <a href=\"http://cacheinfo.sh\" target=\"_blank\" rel=\"noopener\">cacheinfo.sh</a></em></li>\n</ol>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>This is a little bash script to get the CPU cache ratios on Ubuntu.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Cache Level: (1, 2 or 3)</span><br><span class=\"line\">Cache Type: (data-, instruction or general cache)</span><br><span class=\"line\">Capacity: of the respective cache</span><br><span class=\"line\">Associativity: (set size)</span><br><span class=\"line\">Block capacity: / capacity of a cache line</span><br><span class=\"line\">Number of sets: ((total capacity / block capacity) / associativity)</span><br></pre></td></tr></table></figure>\n<p>Concerning the associativity, see <a href=\"https://en.wikipedia.org/wiki/CPU_cache#Associativity\" target=\"_blank\" rel=\"noopener\">https://en.wikipedia.org/wiki/CPU_cache#Associativity</a>.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> DIR0 <span class=\"keyword\">in</span> /sys/devices/system/cpu/cpu0/cache/*</span><br><span class=\"line\">    <span class=\"keyword\">do</span></span><br><span class=\"line\">        LEVEL0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/level)</span><br><span class=\"line\">        TYPE0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/<span class=\"built_in\">type</span>)</span><br><span class=\"line\">        SIZE0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/size)</span><br><span class=\"line\">        ASSOC0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/ways_of_associativity)</span><br><span class=\"line\">        BLOCK0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/coherency_line_size)</span><br><span class=\"line\">        SETS0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/number_of_sets)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">printf</span> &amp;quot;Cache level: %s\\nCache <span class=\"built_in\">type</span>: %s\\nCapacity: %s Bytes\\nAssociativity: %s\\nSets: %s\\nBlock size: %s Bytes\\n\\n&amp;quot; &amp;quot;<span class=\"variable\">$LEVEL0</span>&amp;quot; &amp;quot;<span class=\"variable\">$TYPE0</span>&amp;quot; &amp;quot;<span class=\"variable\">$SIZE0</span>&amp;quot; &amp;quot;<span class=\"variable\">$ASSOC0</span>&amp;quot; &amp;quot;<span class=\"variable\">$SETS0</span>&amp;quot; &amp;quot;<span class=\"variable\">$BLOCK0</span>&amp;quot;</span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n<p><strong>Usage:</strong></p>\n<ol>\n<li>Save code to file, e.g. <em>~/cacheinfo.sh</em></li>\n<li>Make it executable: <em>chmod +x <a href=\"http://cacheinfo.sh\" target=\"_blank\" rel=\"noopener\">cacheinfo.sh</a></em></li>\n<li>Execute: <em>sudo sh <a href=\"http://cacheinfo.sh\" target=\"_blank\" rel=\"noopener\">cacheinfo.sh</a></em></li>\n</ol>\n"},{"title":"Migrate Maildir to new server using imapsync","date":"2016-07-23T21:01:44.000Z","_content":"\n\nThis is a little tutorial for mailserver administrators, who want to *migrate* to a new server while *keeping all e-mails*. This works for mailservers whose MDA uses the [Maildir](https://en.wikipedia.org/wiki/Maildir) format ‚Äì like Dovecot by default ‚Äì and have *IMAP* enabled.  \nThis tutorial does *not* cover how to set up and configure a new mailserver on a new machine, based on the old one‚Äôs configuration, but only how to migrate the e-mails. Simply *tar*ing the Maildir folder and un_tar_ing it on the new machine again usually won‚Äôt work. But don‚Äôt worry, there is a cleaner way that abstracts away any actual mailserver or file-level considerations by only using the IMAP protocol‚Äôs methods. Therefore, we use a tool *imapsync*, which is written Perl. It acts as an ordinary IMAP client ‚Äì just as Outlook or Thunderbird ‚Äì that connects to both mailservers, the old and the new one. All information needed is how to authenticate the respective user with both servers. Actually one ‚Äúmanual‚Äù way to migrate the mails would be to set up both mail accounts in Outlook or Thunderbird, let download the mails via IMAP from the old one and Ctrl+A and Drag&Drop them over to the new one. imapsync does just that ‚Äì yet automatically and without Outlook or Thunderbird.\n\nFirst we need to *install imapsync*. You could install imapsync on your local PC, just as you would with Outlook or Thunderbird, but then there would be a unnecessary detour from server 1 over your PC to server 2\\. And since your local internet connection is probably ways slower then the servers‚Äô, your PC would be a bottleneck. So I recommend to install imapsync on either the old or the new mailserver‚Äôs host machine. Let‚Äôs do it.\n\n1.  Clone the imapsync repository to any folder on your machine, e.g. `/opt/imapsync`: `git clone https://github.com/imapsync/imapsync`\n\n2.  Read the installation notes for your specific operation system at [https://github.com/imapsync/imapsync/tree/master/INSTALL.d](https://github.com/imapsync/imapsync/tree/master/INSTALL.d) and do exactly what‚Äôs described there. Usually, you will need to install some dependencies and the like.\n\n3.  Now you should be able to execute `./imapsync` from within the directory where you have cloned it to, e.g. `/opt/imapsync`. You should see a description on how to use the program.\n\nLet‚Äôs now assume that you want to migrate mails from your old server with ip *12.34.45.78* for user ‚Äú*foo@example.org‚Äù* with password ‚Äú*suchsecret‚Äù* to your new server with ip *98.76.54.32*. A prerequisite is that on both machines the mailserver is up and running and the respective user is configured. Further, let‚Äôs assume that on the new machine the user, as it makes sense, is called ‚Äú*foo@example.org‚Äù* again, but his password is ‚Äú*ssshhhhh‚Äù* now and that both MDAs require a *TLS*-secured connection, use standard *PLAIN* login method and are listening on *port 143*.\n\nTo perform the migration now, run the following command:\n\n```bash\n./imapsync --host1 12.34.45.78 --user1 foo@example.org --password1 suchsecret --authmech1 PLAIN --tls1 --host2 98.76.54.32 --user2 foo@example.org --password2 ssshhhhh --authmech2 PLAIN --tls2\n```\n\nNow all mails should be transferred from `host1` through the imapsync client to `host2`, using nothing but the IMAP protocol. If you want to test if everything is working fine first, before actually transferring data, you could add the `--dry` option to the above command.\n\nTo migrate multiple accounts at once, you could write a small scripts that takes username-password combinations from a text file, as described here: [https://wiki.ubuntuusers.de/imapsync/#Massenmigration](https://wiki.ubuntuusers.de/imapsync/#Massenmigration) (although that article is in German, the code should be clear).","source":"_posts/migrate-maildir-to-new-server-using-imapsync.md","raw":"---\ntitle: Migrate Maildir to new server using imapsync\ndate: 2016-07-23 23:01:44\ntags:\n---\n\n\nThis is a little tutorial for mailserver administrators, who want to *migrate* to a new server while *keeping all e-mails*. This works for mailservers whose MDA uses the [Maildir](https://en.wikipedia.org/wiki/Maildir) format ‚Äì like Dovecot by default ‚Äì and have *IMAP* enabled.  \nThis tutorial does *not* cover how to set up and configure a new mailserver on a new machine, based on the old one‚Äôs configuration, but only how to migrate the e-mails. Simply *tar*ing the Maildir folder and un_tar_ing it on the new machine again usually won‚Äôt work. But don‚Äôt worry, there is a cleaner way that abstracts away any actual mailserver or file-level considerations by only using the IMAP protocol‚Äôs methods. Therefore, we use a tool *imapsync*, which is written Perl. It acts as an ordinary IMAP client ‚Äì just as Outlook or Thunderbird ‚Äì that connects to both mailservers, the old and the new one. All information needed is how to authenticate the respective user with both servers. Actually one ‚Äúmanual‚Äù way to migrate the mails would be to set up both mail accounts in Outlook or Thunderbird, let download the mails via IMAP from the old one and Ctrl+A and Drag&Drop them over to the new one. imapsync does just that ‚Äì yet automatically and without Outlook or Thunderbird.\n\nFirst we need to *install imapsync*. You could install imapsync on your local PC, just as you would with Outlook or Thunderbird, but then there would be a unnecessary detour from server 1 over your PC to server 2\\. And since your local internet connection is probably ways slower then the servers‚Äô, your PC would be a bottleneck. So I recommend to install imapsync on either the old or the new mailserver‚Äôs host machine. Let‚Äôs do it.\n\n1.  Clone the imapsync repository to any folder on your machine, e.g. `/opt/imapsync`: `git clone https://github.com/imapsync/imapsync`\n\n2.  Read the installation notes for your specific operation system at [https://github.com/imapsync/imapsync/tree/master/INSTALL.d](https://github.com/imapsync/imapsync/tree/master/INSTALL.d) and do exactly what‚Äôs described there. Usually, you will need to install some dependencies and the like.\n\n3.  Now you should be able to execute `./imapsync` from within the directory where you have cloned it to, e.g. `/opt/imapsync`. You should see a description on how to use the program.\n\nLet‚Äôs now assume that you want to migrate mails from your old server with ip *12.34.45.78* for user ‚Äú*foo@example.org‚Äù* with password ‚Äú*suchsecret‚Äù* to your new server with ip *98.76.54.32*. A prerequisite is that on both machines the mailserver is up and running and the respective user is configured. Further, let‚Äôs assume that on the new machine the user, as it makes sense, is called ‚Äú*foo@example.org‚Äù* again, but his password is ‚Äú*ssshhhhh‚Äù* now and that both MDAs require a *TLS*-secured connection, use standard *PLAIN* login method and are listening on *port 143*.\n\nTo perform the migration now, run the following command:\n\n```bash\n./imapsync --host1 12.34.45.78 --user1 foo@example.org --password1 suchsecret --authmech1 PLAIN --tls1 --host2 98.76.54.32 --user2 foo@example.org --password2 ssshhhhh --authmech2 PLAIN --tls2\n```\n\nNow all mails should be transferred from `host1` through the imapsync client to `host2`, using nothing but the IMAP protocol. If you want to test if everything is working fine first, before actually transferring data, you could add the `--dry` option to the above command.\n\nTo migrate multiple accounts at once, you could write a small scripts that takes username-password combinations from a text file, as described here: [https://wiki.ubuntuusers.de/imapsync/#Massenmigration](https://wiki.ubuntuusers.de/imapsync/#Massenmigration) (although that article is in German, the code should be clear).","slug":"migrate-maildir-to-new-server-using-imapsync","published":1,"updated":"2020-06-06T15:14:33.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhis000r40mqcj64sa8y","content":"<p>This is a little tutorial for mailserver administrators, who want to <em>migrate</em> to a new server while <em>keeping all e-mails</em>. This works for mailservers whose MDA uses the <a href=\"https://en.wikipedia.org/wiki/Maildir\" target=\"_blank\" rel=\"noopener\">Maildir</a> format ‚Äì like Dovecot by default ‚Äì and have <em>IMAP</em> enabled.<br>\nThis tutorial does <em>not</em> cover how to set up and configure a new mailserver on a new machine, based on the old one‚Äôs configuration, but only how to migrate the e-mails. Simply <em>tar</em>ing the Maildir folder and un_tar_ing it on the new machine again usually won‚Äôt work. But don‚Äôt worry, there is a cleaner way that abstracts away any actual mailserver or file-level considerations by only using the IMAP protocol‚Äôs methods. Therefore, we use a tool <em>imapsync</em>, which is written Perl. It acts as an ordinary IMAP client ‚Äì just as Outlook or Thunderbird ‚Äì that connects to both mailservers, the old and the new one. All information needed is how to authenticate the respective user with both servers. Actually one ‚Äúmanual‚Äù way to migrate the mails would be to set up both mail accounts in Outlook or Thunderbird, let download the mails via IMAP from the old one and Ctrl+A and Drag&amp;Drop them over to the new one. imapsync does just that ‚Äì yet automatically and without Outlook or Thunderbird.</p>\n<p>First we need to <em>install imapsync</em>. You could install imapsync on your local PC, just as you would with Outlook or Thunderbird, but then there would be a unnecessary detour from server 1 over your PC to server 2. And since your local internet connection is probably ways slower then the servers‚Äô, your PC would be a bottleneck. So I recommend to install imapsync on either the old or the new mailserver‚Äôs host machine. Let‚Äôs do it.</p>\n<ol>\n<li>\n<p>Clone the imapsync repository to any folder on your machine, e.g. <code>/opt/imapsync</code>: <code>git clone https://github.com/imapsync/imapsync</code></p>\n</li>\n<li>\n<p>Read the installation notes for your specific operation system at <a href=\"https://github.com/imapsync/imapsync/tree/master/INSTALL.d\" target=\"_blank\" rel=\"noopener\">https://github.com/imapsync/imapsync/tree/master/INSTALL.d</a> and do exactly what‚Äôs described there. Usually, you will need to install some dependencies and the like.</p>\n</li>\n<li>\n<p>Now you should be able to execute <code>./imapsync</code> from within the directory where you have cloned it to, e.g. <code>/opt/imapsync</code>. You should see a description on how to use the program.</p>\n</li>\n</ol>\n<p>Let‚Äôs now assume that you want to migrate mails from your old server with ip <em>12.34.45.78</em> for user ‚Äú<em><a href=\"mailto:foo@example.org\" target=\"_blank\" rel=\"noopener\">foo@example.org</a>‚Äù</em> with password ‚Äú<em>suchsecret‚Äù</em> to your new server with ip <em>98.76.54.32</em>. A prerequisite is that on both machines the mailserver is up and running and the respective user is configured. Further, let‚Äôs assume that on the new machine the user, as it makes sense, is called ‚Äú<em><a href=\"mailto:foo@example.org\" target=\"_blank\" rel=\"noopener\">foo@example.org</a>‚Äù</em> again, but his password is ‚Äú<em>ssshhhhh‚Äù</em> now and that both MDAs require a <em>TLS</em>-secured connection, use standard <em>PLAIN</em> login method and are listening on <em>port 143</em>.</p>\n<p>To perform the migration now, run the following command:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./imapsync --host1 12.34.45.78 --user1 foo@example.org --password1 suchsecret --authmech1 PLAIN --tls1 --host2 98.76.54.32 --user2 foo@example.org --password2 ssshhhhh --authmech2 PLAIN --tls2</span><br></pre></td></tr></table></figure>\n<p>Now all mails should be transferred from <code>host1</code> through the imapsync client to <code>host2</code>, using nothing but the IMAP protocol. If you want to test if everything is working fine first, before actually transferring data, you could add the <code>--dry</code> option to the above command.</p>\n<p>To migrate multiple accounts at once, you could write a small scripts that takes username-password combinations from a text file, as described here: <a href=\"https://wiki.ubuntuusers.de/imapsync/#Massenmigration\" target=\"_blank\" rel=\"noopener\">https://wiki.ubuntuusers.de/imapsync/#Massenmigration</a> (although that article is in German, the code should be clear).</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>This is a little tutorial for mailserver administrators, who want to <em>migrate</em> to a new server while <em>keeping all e-mails</em>. This works for mailservers whose MDA uses the <a href=\"https://en.wikipedia.org/wiki/Maildir\" target=\"_blank\" rel=\"noopener\">Maildir</a> format ‚Äì like Dovecot by default ‚Äì and have <em>IMAP</em> enabled.<br>\nThis tutorial does <em>not</em> cover how to set up and configure a new mailserver on a new machine, based on the old one‚Äôs configuration, but only how to migrate the e-mails. Simply <em>tar</em>ing the Maildir folder and un_tar_ing it on the new machine again usually won‚Äôt work. But don‚Äôt worry, there is a cleaner way that abstracts away any actual mailserver or file-level considerations by only using the IMAP protocol‚Äôs methods. Therefore, we use a tool <em>imapsync</em>, which is written Perl. It acts as an ordinary IMAP client ‚Äì just as Outlook or Thunderbird ‚Äì that connects to both mailservers, the old and the new one. All information needed is how to authenticate the respective user with both servers. Actually one ‚Äúmanual‚Äù way to migrate the mails would be to set up both mail accounts in Outlook or Thunderbird, let download the mails via IMAP from the old one and Ctrl+A and Drag&amp;Drop them over to the new one. imapsync does just that ‚Äì yet automatically and without Outlook or Thunderbird.</p>\n<p>First we need to <em>install imapsync</em>. You could install imapsync on your local PC, just as you would with Outlook or Thunderbird, but then there would be a unnecessary detour from server 1 over your PC to server 2. And since your local internet connection is probably ways slower then the servers‚Äô, your PC would be a bottleneck. So I recommend to install imapsync on either the old or the new mailserver‚Äôs host machine. Let‚Äôs do it.</p>\n<ol>\n<li>\n<p>Clone the imapsync repository to any folder on your machine, e.g. <code>/opt/imapsync</code>: <code>git clone https://github.com/imapsync/imapsync</code></p>\n</li>\n<li>\n<p>Read the installation notes for your specific operation system at <a href=\"https://github.com/imapsync/imapsync/tree/master/INSTALL.d\" target=\"_blank\" rel=\"noopener\">https://github.com/imapsync/imapsync/tree/master/INSTALL.d</a> and do exactly what‚Äôs described there. Usually, you will need to install some dependencies and the like.</p>\n</li>\n<li>\n<p>Now you should be able to execute <code>./imapsync</code> from within the directory where you have cloned it to, e.g. <code>/opt/imapsync</code>. You should see a description on how to use the program.</p>\n</li>\n</ol>\n<p>Let‚Äôs now assume that you want to migrate mails from your old server with ip <em>12.34.45.78</em> for user ‚Äú<em><a href=\"mailto:foo@example.org\" target=\"_blank\" rel=\"noopener\">foo@example.org</a>‚Äù</em> with password ‚Äú<em>suchsecret‚Äù</em> to your new server with ip <em>98.76.54.32</em>. A prerequisite is that on both machines the mailserver is up and running and the respective user is configured. Further, let‚Äôs assume that on the new machine the user, as it makes sense, is called ‚Äú<em><a href=\"mailto:foo@example.org\" target=\"_blank\" rel=\"noopener\">foo@example.org</a>‚Äù</em> again, but his password is ‚Äú<em>ssshhhhh‚Äù</em> now and that both MDAs require a <em>TLS</em>-secured connection, use standard <em>PLAIN</em> login method and are listening on <em>port 143</em>.</p>\n<p>To perform the migration now, run the following command:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./imapsync --host1 12.34.45.78 --user1 foo@example.org --password1 suchsecret --authmech1 PLAIN --tls1 --host2 98.76.54.32 --user2 foo@example.org --password2 ssshhhhh --authmech2 PLAIN --tls2</span><br></pre></td></tr></table></figure>\n<p>Now all mails should be transferred from <code>host1</code> through the imapsync client to <code>host2</code>, using nothing but the IMAP protocol. If you want to test if everything is working fine first, before actually transferring data, you could add the <code>--dry</code> option to the above command.</p>\n<p>To migrate multiple accounts at once, you could write a small scripts that takes username-password combinations from a text file, as described here: <a href=\"https://wiki.ubuntuusers.de/imapsync/#Massenmigration\" target=\"_blank\" rel=\"noopener\">https://wiki.ubuntuusers.de/imapsync/#Massenmigration</a> (although that article is in German, the code should be clear).</p>\n"},{"title":"ML: Telegram chat message classification","date":"2017-02-28T22:10:05.000Z","_content":"\n### Intro\nFirst of all, a short disclaimer: I'm not an expert in machine learning at all. In fact I'm in a rather early stage of the learning process, have basic knowledge and this project is kind of my first practical hands-on ML. I've done the [machine learning course](https://www.youtube.com/user/UWCSE/playlists?sort=dd&shelf_id=16&view=50) by [Pedro Domingos](https://homes.cs.washington.edu/~pedrod/) at University of Washington, [Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120) by Udacity and Google and the [Machine Learning 1 lecture at Karlsruhe Institute Of Technology](https://his.anthropomatik.kit.edu/english/28_315.php), all of which I can really recommend.\nAfter having gathered all that theoretical knowledge, I wanted to try something practical on my own. I decided to learn a simple classifier for chat messages from my [Telegram](https://telegram.com) messenger history. I wanted to learn a program that can, given a chat message, tell who the sender of that message is. I further got inspired after having read the papers related to Facebook's [fastText](https://github.com/facebookresearch/fastText) text classification algorithm. In their examples they classify Wikipedia abstracts / descriptions to [DBPedia](https://dbpedia.org) classes or news article headlines to their respective news categories, only based on plain, natural words. Basically these problems are very similar to mine, so I decided to give it a try. Since I found that many text classifiers are learned using the Naive Bayes algorithm (especially popular in spam detection and part of [SpamAssassin](http://spamassassin.apache.org/)) and it's really easy to understand, I decided to go for that one, too. Inspired by [this article](http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/), where the sentiment of tweets is analyzed, I chose to also use the [natural language toolkit](http://www.nltk.org/) for Python. Another option would have been [sklearn](http://scikit-learn.org/), but NLTK also provided some useful utilities beyond the pure ML scope. \n\nAll of my __code is [available on GitHub](https://github.com/muety/tg-chat-classification/)__.\n\n### Basic Steps\n1. The very first step was to download the data, namely the chat messages. Luckily, Telegram has an open API. However it's not a classical REST API, but instead they're using the [MTProto](https://core.telegram.org/mtproto) protocol. I found [vysheng/tg](https://github.com/vysheng/tg) as a cool C++-written commandline client on GitHub as well as [tvdstaaij/telegram-history-dump](https://github.com/tvdstaaij/telegram-history-dump) as a Ruby script to automate the history download for a set of users / chat partners. I told the script (ran in a Docker container, since I didn't want to install Ruby) to fetch at max 40,000 messages for my top three chat partners (let's call them _M_, _P_ and _J_). The outcome were three [JSON Lines](http://jsonlines.org/) files.\n2. To pre-process these files as needed for my learning algorithm, I wrote a Python script that extracted only message text and sender from all incoming messages and dumped these data to a JSON file. Additionally I also extracted the same information for all outgoing messages, i.e. all messages where the sender was me. Consequently, there are four classes: __C = { _M_, _P_, _J_, _F_ }__\n3. Another data-preprocessing step was to convert the JSON objects with class names as keys for message-arrays to one large list of tuples of the form _(text, label)_, where _label_ is the name of the message's sender and _text_ is the respective message text. In this step I also discarded words with a length of less than 2 characters and converted everything to lower case.\n4. Next step was to extract the features. In text classification, there is often one binary (_contains_ / _contains not_) feature for every possible word. So if all messages in total comprise X different words, there will be a X-dimensional feature vector.\n5. Last step before actually training the classifier is to compute the feature vector for every messages. For examples if the total feature set is `['in', 'case', 'of', 'fire', 'coffee', 'we', 'trust']`, the resultung feature vector for a message _\"in coffee we trust\"_ would be `('in'=True, 'case'=False, 'of'=False, 'fire'=False, 'coffee'=True, 'we'=True, 'trust'=True)`.\n6. One more minor thing: shuffle the feature set so that the order of messages and message senders is random. Also divide the feature set into training- and test data, where test data contains about 10 % of the number of messages in the train data.\n7. Train [nltk.NaiveBayesClassifier](http://www.nltk.org/api/nltk.classify.html) classifier. This is really just one line of code.\n8. Use the returned classifier to predict classes for the test messages, validate them and compute the accuracy.\n\nUsing that basic initial setup on a set of __37257 messages__, (7931 from M, 9795 from P, 9314 from F and 10217 from J), I ended up with an __accuracy of 0.58__. There seemed to be room for optimization.\n\n### Optimizations\n* Inspired by _fastText_, I decided to include n-grams. This seemed resonable to me, because intuitively I'd say that single words a way less characteristic for a person's writing style than certain phrases. I extended the feature list from step 4 by all possible __bi- and tri-grams__, which are easy to compute with NLTK. Actually I'm not taking ALL bi- and tri-grams and I'm not even take all single words as features. Reason for that is that there were approx. 35k different words in the dataset. Plus the n-grams this would make an extremely multi-dimensional feature vector and as it turned out, it was way to complex for my 16 GB MacBook Pro to compute. Consequently, I only took the __top 5000 single words, bigrams and trigrams__, ranked descending by their overall frequency. \n* Since NLTK already provides a corpus of __stopwords__ (like \"in\", \"and\", \"of\", etc.), which are obviously not characteristic for a person's style of chatting, I decided to remove them (the German ones) from the message set in step 2.\n\nWith these optimizations, I ended up with an __accuracy of 0.61__ after a training time of 348 seconds (I didn't log testing time at that point).\n\n### Conclusion\nCertainly 61 % accuracy isn't really a good classifier, but at least significantly better than random guessing (chance of 1/4 in this case). However, I trained a __fastText__ classifier on my data as a comparison baseline and it even only reached __60 % accuracy__ (but with a much better __training time of only 0.66 seconds__). \nMy intuitive explanation for these rather bad results is the complexity of the problem itself. Given only a set of words without any context and semantics, it's not only hard for a machine to predict the message's sender but also for a human. \nMoreover, given more training data (I'd need a longer message history) and more computing power to handle larger feature sets, the accuracy might further improve slightly.\nActually, the practical relevance of this project isn't quit high anyway, but it was a good practice for me to get into the basics of ML and it's really fun!\n\nPlease leave me feedback if you like to.","source":"_posts/ml-telegram-chat-message-classification.md","raw":"---\ntitle: 'ML: Telegram chat message classification'\ndate: 2017-02-28 23:10:05\ntags:\n---\n\n### Intro\nFirst of all, a short disclaimer: I'm not an expert in machine learning at all. In fact I'm in a rather early stage of the learning process, have basic knowledge and this project is kind of my first practical hands-on ML. I've done the [machine learning course](https://www.youtube.com/user/UWCSE/playlists?sort=dd&shelf_id=16&view=50) by [Pedro Domingos](https://homes.cs.washington.edu/~pedrod/) at University of Washington, [Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120) by Udacity and Google and the [Machine Learning 1 lecture at Karlsruhe Institute Of Technology](https://his.anthropomatik.kit.edu/english/28_315.php), all of which I can really recommend.\nAfter having gathered all that theoretical knowledge, I wanted to try something practical on my own. I decided to learn a simple classifier for chat messages from my [Telegram](https://telegram.com) messenger history. I wanted to learn a program that can, given a chat message, tell who the sender of that message is. I further got inspired after having read the papers related to Facebook's [fastText](https://github.com/facebookresearch/fastText) text classification algorithm. In their examples they classify Wikipedia abstracts / descriptions to [DBPedia](https://dbpedia.org) classes or news article headlines to their respective news categories, only based on plain, natural words. Basically these problems are very similar to mine, so I decided to give it a try. Since I found that many text classifiers are learned using the Naive Bayes algorithm (especially popular in spam detection and part of [SpamAssassin](http://spamassassin.apache.org/)) and it's really easy to understand, I decided to go for that one, too. Inspired by [this article](http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/), where the sentiment of tweets is analyzed, I chose to also use the [natural language toolkit](http://www.nltk.org/) for Python. Another option would have been [sklearn](http://scikit-learn.org/), but NLTK also provided some useful utilities beyond the pure ML scope. \n\nAll of my __code is [available on GitHub](https://github.com/muety/tg-chat-classification/)__.\n\n### Basic Steps\n1. The very first step was to download the data, namely the chat messages. Luckily, Telegram has an open API. However it's not a classical REST API, but instead they're using the [MTProto](https://core.telegram.org/mtproto) protocol. I found [vysheng/tg](https://github.com/vysheng/tg) as a cool C++-written commandline client on GitHub as well as [tvdstaaij/telegram-history-dump](https://github.com/tvdstaaij/telegram-history-dump) as a Ruby script to automate the history download for a set of users / chat partners. I told the script (ran in a Docker container, since I didn't want to install Ruby) to fetch at max 40,000 messages for my top three chat partners (let's call them _M_, _P_ and _J_). The outcome were three [JSON Lines](http://jsonlines.org/) files.\n2. To pre-process these files as needed for my learning algorithm, I wrote a Python script that extracted only message text and sender from all incoming messages and dumped these data to a JSON file. Additionally I also extracted the same information for all outgoing messages, i.e. all messages where the sender was me. Consequently, there are four classes: __C = { _M_, _P_, _J_, _F_ }__\n3. Another data-preprocessing step was to convert the JSON objects with class names as keys for message-arrays to one large list of tuples of the form _(text, label)_, where _label_ is the name of the message's sender and _text_ is the respective message text. In this step I also discarded words with a length of less than 2 characters and converted everything to lower case.\n4. Next step was to extract the features. In text classification, there is often one binary (_contains_ / _contains not_) feature for every possible word. So if all messages in total comprise X different words, there will be a X-dimensional feature vector.\n5. Last step before actually training the classifier is to compute the feature vector for every messages. For examples if the total feature set is `['in', 'case', 'of', 'fire', 'coffee', 'we', 'trust']`, the resultung feature vector for a message _\"in coffee we trust\"_ would be `('in'=True, 'case'=False, 'of'=False, 'fire'=False, 'coffee'=True, 'we'=True, 'trust'=True)`.\n6. One more minor thing: shuffle the feature set so that the order of messages and message senders is random. Also divide the feature set into training- and test data, where test data contains about 10 % of the number of messages in the train data.\n7. Train [nltk.NaiveBayesClassifier](http://www.nltk.org/api/nltk.classify.html) classifier. This is really just one line of code.\n8. Use the returned classifier to predict classes for the test messages, validate them and compute the accuracy.\n\nUsing that basic initial setup on a set of __37257 messages__, (7931 from M, 9795 from P, 9314 from F and 10217 from J), I ended up with an __accuracy of 0.58__. There seemed to be room for optimization.\n\n### Optimizations\n* Inspired by _fastText_, I decided to include n-grams. This seemed resonable to me, because intuitively I'd say that single words a way less characteristic for a person's writing style than certain phrases. I extended the feature list from step 4 by all possible __bi- and tri-grams__, which are easy to compute with NLTK. Actually I'm not taking ALL bi- and tri-grams and I'm not even take all single words as features. Reason for that is that there were approx. 35k different words in the dataset. Plus the n-grams this would make an extremely multi-dimensional feature vector and as it turned out, it was way to complex for my 16 GB MacBook Pro to compute. Consequently, I only took the __top 5000 single words, bigrams and trigrams__, ranked descending by their overall frequency. \n* Since NLTK already provides a corpus of __stopwords__ (like \"in\", \"and\", \"of\", etc.), which are obviously not characteristic for a person's style of chatting, I decided to remove them (the German ones) from the message set in step 2.\n\nWith these optimizations, I ended up with an __accuracy of 0.61__ after a training time of 348 seconds (I didn't log testing time at that point).\n\n### Conclusion\nCertainly 61 % accuracy isn't really a good classifier, but at least significantly better than random guessing (chance of 1/4 in this case). However, I trained a __fastText__ classifier on my data as a comparison baseline and it even only reached __60 % accuracy__ (but with a much better __training time of only 0.66 seconds__). \nMy intuitive explanation for these rather bad results is the complexity of the problem itself. Given only a set of words without any context and semantics, it's not only hard for a machine to predict the message's sender but also for a human. \nMoreover, given more training data (I'd need a longer message history) and more computing power to handle larger feature sets, the accuracy might further improve slightly.\nActually, the practical relevance of this project isn't quit high anyway, but it was a good practice for me to get into the basics of ML and it's really fun!\n\nPlease leave me feedback if you like to.","slug":"ml-telegram-chat-message-classification","published":1,"updated":"2020-06-06T15:14:33.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhit000s40mqqxgbp50s","content":"<h3 id=\"Intro\">Intro</h3>\n<p>First of all, a short disclaimer: I‚Äôm not an expert in machine learning at all. In fact I‚Äôm in a rather early stage of the learning process, have basic knowledge and this project is kind of my first practical hands-on ML. I‚Äôve done the <a href=\"https://www.youtube.com/user/UWCSE/playlists?sort=dd&amp;shelf_id=16&amp;view=50\" target=\"_blank\" rel=\"noopener\">machine learning course</a> by <a href=\"https://homes.cs.washington.edu/~pedrod/\" target=\"_blank\" rel=\"noopener\">Pedro Domingos</a> at University of Washington, <a href=\"https://www.udacity.com/course/intro-to-machine-learning--ud120\" target=\"_blank\" rel=\"noopener\">Intro to Machine Learning</a> by Udacity and Google and the <a href=\"https://his.anthropomatik.kit.edu/english/28_315.php\" target=\"_blank\" rel=\"noopener\">Machine Learning 1 lecture at Karlsruhe Institute Of Technology</a>, all of which I can really recommend.<br>\nAfter having gathered all that theoretical knowledge, I wanted to try something practical on my own. I decided to learn a simple classifier for chat messages from my <a href=\"https://telegram.com\" target=\"_blank\" rel=\"noopener\">Telegram</a> messenger history. I wanted to learn a program that can, given a chat message, tell who the sender of that message is. I further got inspired after having read the papers related to Facebook‚Äôs <a href=\"https://github.com/facebookresearch/fastText\" target=\"_blank\" rel=\"noopener\">fastText</a> text classification algorithm. In their examples they classify Wikipedia abstracts / descriptions to <a href=\"https://dbpedia.org\" target=\"_blank\" rel=\"noopener\">DBPedia</a> classes or news article headlines to their respective news categories, only based on plain, natural words. Basically these problems are very similar to mine, so I decided to give it a try. Since I found that many text classifiers are learned using the Naive Bayes algorithm (especially popular in spam detection and part of <a href=\"http://spamassassin.apache.org/\" target=\"_blank\" rel=\"noopener\">SpamAssassin</a>) and it‚Äôs really easy to understand, I decided to go for that one, too. Inspired by <a href=\"http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/\" target=\"_blank\" rel=\"noopener\">this article</a>, where the sentiment of tweets is analyzed, I chose to also use the <a href=\"http://www.nltk.org/\" target=\"_blank\" rel=\"noopener\">natural language toolkit</a> for Python. Another option would have been <a href=\"http://scikit-learn.org/\" target=\"_blank\" rel=\"noopener\">sklearn</a>, but NLTK also provided some useful utilities beyond the pure ML scope.</p>\n<p>All of my <strong>code is <a href=\"https://github.com/muety/tg-chat-classification/\" target=\"_blank\" rel=\"noopener\">available on GitHub</a></strong>.</p>\n<h3 id=\"Basic-Steps\">Basic Steps</h3>\n<ol>\n<li>The very first step was to download the data, namely the chat messages. Luckily, Telegram has an open API. However it‚Äôs not a classical REST API, but instead they‚Äôre using the <a href=\"https://core.telegram.org/mtproto\" target=\"_blank\" rel=\"noopener\">MTProto</a> protocol. I found <a href=\"https://github.com/vysheng/tg\" target=\"_blank\" rel=\"noopener\">vysheng/tg</a> as a cool C+¬±written commandline client on GitHub as well as <a href=\"https://github.com/tvdstaaij/telegram-history-dump\" target=\"_blank\" rel=\"noopener\">tvdstaaij/telegram-history-dump</a> as a Ruby script to automate the history download for a set of users / chat partners. I told the script (ran in a Docker container, since I didn‚Äôt want to install Ruby) to fetch at max 40,000 messages for my top three chat partners (let‚Äôs call them <em>M</em>, <em>P</em> and <em>J</em>). The outcome were three <a href=\"http://jsonlines.org/\" target=\"_blank\" rel=\"noopener\">JSON Lines</a> files.</li>\n<li>To pre-process these files as needed for my learning algorithm, I wrote a Python script that extracted only message text and sender from all incoming messages and dumped these data to a JSON file. Additionally I also extracted the same information for all outgoing messages, i.e. all messages where the sender was me. Consequently, there are four classes: <strong>C = { <em>M</em>, <em>P</em>, <em>J</em>, <em>F</em> }</strong></li>\n<li>Another data-preprocessing step was to convert the JSON objects with class names as keys for message-arrays to one large list of tuples of the form <em>(text, label)</em>, where <em>label</em> is the name of the message‚Äôs sender and <em>text</em> is the respective message text. In this step I also discarded words with a length of less than 2 characters and converted everything to lower case.</li>\n<li>Next step was to extract the features. In text classification, there is often one binary (<em>contains</em> / <em>contains not</em>) feature for every possible word. So if all messages in total comprise X different words, there will be a X-dimensional feature vector.</li>\n<li>Last step before actually training the classifier is to compute the feature vector for every messages. For examples if the total feature set is <code>['in', 'case', 'of', 'fire', 'coffee', 'we', 'trust']</code>, the resultung feature vector for a message <em>‚Äúin coffee we trust‚Äù</em> would be <code>('in'=True, 'case'=False, 'of'=False, 'fire'=False, 'coffee'=True, 'we'=True, 'trust'=True)</code>.</li>\n<li>One more minor thing: shuffle the feature set so that the order of messages and message senders is random. Also divide the feature set into training- and test data, where test data contains about 10 % of the number of messages in the train data.</li>\n<li>Train <a href=\"http://www.nltk.org/api/nltk.classify.html\" target=\"_blank\" rel=\"noopener\">nltk.NaiveBayesClassifier</a> classifier. This is really just one line of code.</li>\n<li>Use the returned classifier to predict classes for the test messages, validate them and compute the accuracy.</li>\n</ol>\n<p>Using that basic initial setup on a set of <strong>37257 messages</strong>, (7931 from M, 9795 from P, 9314 from F and 10217 from J), I ended up with an <strong>accuracy of 0.58</strong>. There seemed to be room for optimization.</p>\n<h3 id=\"Optimizations\">Optimizations</h3>\n<ul>\n<li>Inspired by <em>fastText</em>, I decided to include n-grams. This seemed resonable to me, because intuitively I‚Äôd say that single words a way less characteristic for a person‚Äôs writing style than certain phrases. I extended the feature list from step 4 by all possible <strong>bi- and tri-grams</strong>, which are easy to compute with NLTK. Actually I‚Äôm not taking ALL bi- and tri-grams and I‚Äôm not even take all single words as features. Reason for that is that there were approx. 35k different words in the dataset. Plus the n-grams this would make an extremely multi-dimensional feature vector and as it turned out, it was way to complex for my 16 GB MacBook Pro to compute. Consequently, I only took the <strong>top 5000 single words, bigrams and trigrams</strong>, ranked descending by their overall frequency.</li>\n<li>Since NLTK already provides a corpus of <strong>stopwords</strong> (like ‚Äúin‚Äù, ‚Äúand‚Äù, ‚Äúof‚Äù, etc.), which are obviously not characteristic for a person‚Äôs style of chatting, I decided to remove them (the German ones) from the message set in step 2.</li>\n</ul>\n<p>With these optimizations, I ended up with an <strong>accuracy of 0.61</strong> after a training time of 348 seconds (I didn‚Äôt log testing time at that point).</p>\n<h3 id=\"Conclusion\">Conclusion</h3>\n<p>Certainly 61 % accuracy isn‚Äôt really a good classifier, but at least significantly better than random guessing (chance of 1/4 in this case). However, I trained a <strong>fastText</strong> classifier on my data as a comparison baseline and it even only reached <strong>60 % accuracy</strong> (but with a much better <strong>training time of only 0.66 seconds</strong>).<br>\nMy intuitive explanation for these rather bad results is the complexity of the problem itself. Given only a set of words without any context and semantics, it‚Äôs not only hard for a machine to predict the message‚Äôs sender but also for a human.<br>\nMoreover, given more training data (I‚Äôd need a longer message history) and more computing power to handle larger feature sets, the accuracy might further improve slightly.<br>\nActually, the practical relevance of this project isn‚Äôt quit high anyway, but it was a good practice for me to get into the basics of ML and it‚Äôs really fun!</p>\n<p>Please leave me feedback if you like to.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h3 id=\"Intro\">Intro</h3>\n<p>First of all, a short disclaimer: I‚Äôm not an expert in machine learning at all. In fact I‚Äôm in a rather early stage of the learning process, have basic knowledge and this project is kind of my first practical hands-on ML. I‚Äôve done the <a href=\"https://www.youtube.com/user/UWCSE/playlists?sort=dd&amp;shelf_id=16&amp;view=50\" target=\"_blank\" rel=\"noopener\">machine learning course</a> by <a href=\"https://homes.cs.washington.edu/~pedrod/\" target=\"_blank\" rel=\"noopener\">Pedro Domingos</a> at University of Washington, <a href=\"https://www.udacity.com/course/intro-to-machine-learning--ud120\" target=\"_blank\" rel=\"noopener\">Intro to Machine Learning</a> by Udacity and Google and the <a href=\"https://his.anthropomatik.kit.edu/english/28_315.php\" target=\"_blank\" rel=\"noopener\">Machine Learning 1 lecture at Karlsruhe Institute Of Technology</a>, all of which I can really recommend.<br>\nAfter having gathered all that theoretical knowledge, I wanted to try something practical on my own. I decided to learn a simple classifier for chat messages from my <a href=\"https://telegram.com\" target=\"_blank\" rel=\"noopener\">Telegram</a> messenger history. I wanted to learn a program that can, given a chat message, tell who the sender of that message is. I further got inspired after having read the papers related to Facebook‚Äôs <a href=\"https://github.com/facebookresearch/fastText\" target=\"_blank\" rel=\"noopener\">fastText</a> text classification algorithm. In their examples they classify Wikipedia abstracts / descriptions to <a href=\"https://dbpedia.org\" target=\"_blank\" rel=\"noopener\">DBPedia</a> classes or news article headlines to their respective news categories, only based on plain, natural words. Basically these problems are very similar to mine, so I decided to give it a try. Since I found that many text classifiers are learned using the Naive Bayes algorithm (especially popular in spam detection and part of <a href=\"http://spamassassin.apache.org/\" target=\"_blank\" rel=\"noopener\">SpamAssassin</a>) and it‚Äôs really easy to understand, I decided to go for that one, too. Inspired by <a href=\"http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/\" target=\"_blank\" rel=\"noopener\">this article</a>, where the sentiment of tweets is analyzed, I chose to also use the <a href=\"http://www.nltk.org/\" target=\"_blank\" rel=\"noopener\">natural language toolkit</a> for Python. Another option would have been <a href=\"http://scikit-learn.org/\" target=\"_blank\" rel=\"noopener\">sklearn</a>, but NLTK also provided some useful utilities beyond the pure ML scope.</p>\n<p>All of my <strong>code is <a href=\"https://github.com/muety/tg-chat-classification/\" target=\"_blank\" rel=\"noopener\">available on GitHub</a></strong>.</p>\n<h3 id=\"Basic-Steps\">Basic Steps</h3>\n<ol>\n<li>The very first step was to download the data, namely the chat messages. Luckily, Telegram has an open API. However it‚Äôs not a classical REST API, but instead they‚Äôre using the <a href=\"https://core.telegram.org/mtproto\" target=\"_blank\" rel=\"noopener\">MTProto</a> protocol. I found <a href=\"https://github.com/vysheng/tg\" target=\"_blank\" rel=\"noopener\">vysheng/tg</a> as a cool C+¬±written commandline client on GitHub as well as <a href=\"https://github.com/tvdstaaij/telegram-history-dump\" target=\"_blank\" rel=\"noopener\">tvdstaaij/telegram-history-dump</a> as a Ruby script to automate the history download for a set of users / chat partners. I told the script (ran in a Docker container, since I didn‚Äôt want to install Ruby) to fetch at max 40,000 messages for my top three chat partners (let‚Äôs call them <em>M</em>, <em>P</em> and <em>J</em>). The outcome were three <a href=\"http://jsonlines.org/\" target=\"_blank\" rel=\"noopener\">JSON Lines</a> files.</li>\n<li>To pre-process these files as needed for my learning algorithm, I wrote a Python script that extracted only message text and sender from all incoming messages and dumped these data to a JSON file. Additionally I also extracted the same information for all outgoing messages, i.e. all messages where the sender was me. Consequently, there are four classes: <strong>C = { <em>M</em>, <em>P</em>, <em>J</em>, <em>F</em> }</strong></li>\n<li>Another data-preprocessing step was to convert the JSON objects with class names as keys for message-arrays to one large list of tuples of the form <em>(text, label)</em>, where <em>label</em> is the name of the message‚Äôs sender and <em>text</em> is the respective message text. In this step I also discarded words with a length of less than 2 characters and converted everything to lower case.</li>\n<li>Next step was to extract the features. In text classification, there is often one binary (<em>contains</em> / <em>contains not</em>) feature for every possible word. So if all messages in total comprise X different words, there will be a X-dimensional feature vector.</li>\n<li>Last step before actually training the classifier is to compute the feature vector for every messages. For examples if the total feature set is <code>['in', 'case', 'of', 'fire', 'coffee', 'we', 'trust']</code>, the resultung feature vector for a message <em>‚Äúin coffee we trust‚Äù</em> would be <code>('in'=True, 'case'=False, 'of'=False, 'fire'=False, 'coffee'=True, 'we'=True, 'trust'=True)</code>.</li>\n<li>One more minor thing: shuffle the feature set so that the order of messages and message senders is random. Also divide the feature set into training- and test data, where test data contains about 10 % of the number of messages in the train data.</li>\n<li>Train <a href=\"http://www.nltk.org/api/nltk.classify.html\" target=\"_blank\" rel=\"noopener\">nltk.NaiveBayesClassifier</a> classifier. This is really just one line of code.</li>\n<li>Use the returned classifier to predict classes for the test messages, validate them and compute the accuracy.</li>\n</ol>\n<p>Using that basic initial setup on a set of <strong>37257 messages</strong>, (7931 from M, 9795 from P, 9314 from F and 10217 from J), I ended up with an <strong>accuracy of 0.58</strong>. There seemed to be room for optimization.</p>\n<h3 id=\"Optimizations\">Optimizations</h3>\n<ul>\n<li>Inspired by <em>fastText</em>, I decided to include n-grams. This seemed resonable to me, because intuitively I‚Äôd say that single words a way less characteristic for a person‚Äôs writing style than certain phrases. I extended the feature list from step 4 by all possible <strong>bi- and tri-grams</strong>, which are easy to compute with NLTK. Actually I‚Äôm not taking ALL bi- and tri-grams and I‚Äôm not even take all single words as features. Reason for that is that there were approx. 35k different words in the dataset. Plus the n-grams this would make an extremely multi-dimensional feature vector and as it turned out, it was way to complex for my 16 GB MacBook Pro to compute. Consequently, I only took the <strong>top 5000 single words, bigrams and trigrams</strong>, ranked descending by their overall frequency.</li>\n<li>Since NLTK already provides a corpus of <strong>stopwords</strong> (like ‚Äúin‚Äù, ‚Äúand‚Äù, ‚Äúof‚Äù, etc.), which are obviously not characteristic for a person‚Äôs style of chatting, I decided to remove them (the German ones) from the message set in step 2.</li>\n</ul>\n<p>With these optimizations, I ended up with an <strong>accuracy of 0.61</strong> after a training time of 348 seconds (I didn‚Äôt log testing time at that point).</p>\n<h3 id=\"Conclusion\">Conclusion</h3>\n<p>Certainly 61 % accuracy isn‚Äôt really a good classifier, but at least significantly better than random guessing (chance of 1/4 in this case). However, I trained a <strong>fastText</strong> classifier on my data as a comparison baseline and it even only reached <strong>60 % accuracy</strong> (but with a much better <strong>training time of only 0.66 seconds</strong>).<br>\nMy intuitive explanation for these rather bad results is the complexity of the problem itself. Given only a set of words without any context and semantics, it‚Äôs not only hard for a machine to predict the message‚Äôs sender but also for a human.<br>\nMoreover, given more training data (I‚Äôd need a longer message history) and more computing power to handle larger feature sets, the accuracy might further improve slightly.<br>\nActually, the practical relevance of this project isn‚Äôt quit high anyway, but it was a good practice for me to get into the basics of ML and it‚Äôs really fun!</p>\n<p>Please leave me feedback if you like to.</p>\n"},{"title":"Modern, reactive web APIs with GraphQL, Go and Server-Sent Events ‚Äì Part 1","date":"2020-06-06T09:59:23.000Z","_content":"\n![](images/graphql_cover.png)\n\n# Introduction\nIn the course of this two-part article, the interested reader is briefly introduced to the basic of GraphQL and how it compares to traditional approaches. In the second part, an example single-page web application (SPA) is built to demonstrate the use of GraphQL in combination with further modern web technologies. The [final project](https://github.com/muety/go-graphql-sse-example) provides a clean, opinionated code- and project structure for both backend and frontend and constitutes a good starting point for new apps based on the presented tech stack.\n\n\\> **Code**: [muety/go-graphql-sse-example](https://github.com/muety/go-graphql-sse-example)\n\n# What is GraphQL?\n[GraphQL](https://engineering.fb.com/core-data/graphql-a-data-query-language/) is a relatively new (proposed in 2015 by Facebook engineers) approach to designing APIs for (web) backend applications and can be considered an alternative to [REST](https://developer.mozilla.org/en-US/docs/Glossary/REST) or remote-procedure-call (RPC) mechanisms like [JSON-RPC](https://www.jsonrpc.org/). In other words, it's *\"an open-source data query and manipulation language for APIs\"* [[1]](https://en.wikipedia.org/wiki/GraphQL). The specification is open-source and actively evolving on [GitHub](https://github.com/graphql/graphql-spec). While REST APIs are currently the de-facto standard on the web (although not necessarily all of them being fully [mature](https://www.martinfowler.com/articles/richardsonMaturityModel.html)) ‚Äì GraphQL starts to [gain traction](https://trends.google.com/trends/explore?date=2018-05-06%202020-06-06&gprop=youtube&q=graphql).\n\n## Comparison with REST and RPC\n\nIn contrast to REST, which is primarily structured around resources or entities and RPC-based APIs, which focus on actions or methods, GraphQL is all about the underlying data itself. The consumer of an API ‚Äì usually the frontend / client-side part of a SPA ‚Äì only has to know the schema and structure of the data provided by the API to [CRUD](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete) it. Compared to REST APIs, where the consumer heavily depends on the fixed data structure delivered by the backend API, this is especially beneficial as it introduced a lot more flexibility and decoupling and can save the developer some time making the client-side application [tolerant](https://martinfowler.com/bliki/TolerantReader.html). Also, you will probably not need [backends for frontends](https://docs.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends) anymore.\n\nEssentially, with GraphQL, **the consumer asks exactly for what it needs and how it needs it**, i.e. your client application tells the backend exactly what to return and in which format. **Consuming a GraphQL API is like querying a database, but with more guidance and control.**\n\n## Example\nLet's look at an example to get a better idea of how GraphQL works, especially in comparison to the REST principles. \n\nImagine you have an e-commerce application with products and orders. Every order consists, among others, of a set of products. As the operator of the web shop, you might want to get a list of all orders. With a more or less RESTful API (we neglect the hypermedia controls in the example though), your request-response pair could look like this:\n\n<details>\n<summary>Click to view</summary>\n```\nRequest\n-------\nGET /api/orders\n\nResponse Body\n-------------\n[\n    {\n        \"id\": 125,\n        \"customerId\": 8977,\n        \"createdAt\": \"2020-06-06T13:40:49.038Z\",\n        \"productIds\": [ 49863176 ]\n    }\n]\n```\n</details>\n\nSo far so good, but potentially you will also want to view the actual products right away. What you got are only ids, for each of which you would have to issue another API call to retrieve it. Alternatively, the API could also return nested objects, like so:\n\n<details>\n<summary>Click to view</summary>\n```\n[\n    {\n        \"id\": 125,\n        \"customerId\": 8977,\n        \"createdAt\": \"2020-06-06T13:40:49.038Z\",\n        \"products\": [\n            {\n                \"id\": 49863176,\n                \"name\": \"Slim T-Shirt navy-blue\",\n                \"price\": 17.90,\n                \"options\": [\n                    {\n                        \"id\": \"size\",\n                        \"name\": \"Size\",\n                        \"description\": \"T-Shirt size\",\n                        \"values\": [\n                            {\n                                \"id\": \"s\",\n                                \"name\": \"Size S\",\n                            },\n                            {\n                                \"id\": \"m\",\n                                \"name\": \"Size M\",\n                            },\n                            {\n                                \"id\": \"l\",\n                                \"name\": \"Size L\",\n                            }\n                        ]\n                    }\n                ]\n            },\n        ]\n    }\n]\n```\n</details>\n\nHowever, that is ‚Äî to my understanding ‚Äì not truly RESTful anymore. Also, while the above example is still quite straightforward, things get ugly as nested objects include other nested objects, that include other nested objects, that... Quickly you get JSON responses of several tens or hundreds of kilobytes, although you're potentially only interested in two or three attributes. Moreover, on some pages of your shop you may be interested in all possible options (e.g. \"size\") of a product, but not on others. Should your API define different [view models](https://www.infoq.com/articles/View-Model-Definition/) now and expose different endpoints? Or a single endpoints with query flags like `?expanded=true`? Soon you might be catching yourself **tailoring your API specifically to the needs of your client** while neglecting REST conventions and a straightforward design. \n\nWith GraphQL, things are different. Your API is a bit **dumber and less opinionated** now and does not deliver data in a fixed structure, according to a specified [GQL query](https://graphql.org/learn/queries/), which looks a lot like JSON. The above example might look like this, now:\n\n<details>\n<summary>Click to view</summary>\n```\nRequest\n-------\nPOST /api/graphql/query\n\n{\n    \"query\": \"\\{\n        orders {\n            id\n            customerId\n            products {\n                name\n                price\n                options {\n                    name\n                }\n            }\n        }\n    \\}\"\n}\n\nResponse Body\n-------------\n{\n    \"data\": {\n        \"orders\": [\n            {\n                \"id\": 125,\n                \"customerId\": 8977,\n                \"products\": [\n                    {\n                        \"name\": \"Slim T-Shirt navy-blue\",\n                        \"price\": 17.90,\n                        \"options\": [\n                            {\n                                \"name\": \"Size\",\n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    }\n}\n```\n</details>\n\nThis way, you get only the data you want. All your API has to know is how to fetch every piece of data. All your client has to know is how the data schema itself looks like.\n\n## Try it out\n![](images/graphql_github.png)\n\nGitHub's official API offers GraphQL query endpoints. You can try it out using their [GraphQL explorer](https://developer.github.com/v4/explorer/).\n\n# GraphQL Basic\nSince this article does not aim to be another introduction to GraphQL, you can read most of the basics about fields, data types, etc. in the [official docs](https://graphql.org/learn/queries/). However, it is worth mentioning that GraphQL supports three types of queries:\n\n* **`Query`**: \"Standard\" type of queries, used for fetching data (see above). Similar to what you would do with a `GET` in REST.\n* **`Mutation`**: Query type used to modify data. Similar to what you would do with a `PUT`, `POST`, `PATCH` or `DELETE` in REST.\n* **`Subscription`**: Query type to communicate your intent to subscribe to live data updates. \n\n## Subscriptions\n\nWhile a basic GraphQL application will at least use the former two types, the latter is especially interesting in the context of this article. Using subscriptions, you can have your web frontend be notified when new data arrives at the server or existing data changes. For instance, the operator of the above web shop could have a live-updating dashboard, that shows new orders just as they are placed. \n\nFor subscriptions, the GraphQL standard does not define a lot more than their plain existence and purpose. Especially, it is not defined how and which technology to implement them. On the web, any [publish/subscribe](https://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber)-like mechanism that provides bi-directional or uni-directional server-to-client communication is appropriate. For the sake of simplicity, [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events) are used in this article.\n\n# What's next?\nThis part gave a brief introduction to GraphQL. The next part is about actual code. We're going to build an example web app with live-updates using GraphQL, Go, MongoDB and VueJs. Stay tuned!\n\n\\> [**Part 2**](https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.html)","source":"_posts/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.md","raw":"---\ntitle: 'Modern, reactive web APIs with GraphQL, Go and Server-Sent Events ‚Äì Part 1'\ndate: 2020-06-06 11:59:23\ntags:\n---\n\n![](images/graphql_cover.png)\n\n# Introduction\nIn the course of this two-part article, the interested reader is briefly introduced to the basic of GraphQL and how it compares to traditional approaches. In the second part, an example single-page web application (SPA) is built to demonstrate the use of GraphQL in combination with further modern web technologies. The [final project](https://github.com/muety/go-graphql-sse-example) provides a clean, opinionated code- and project structure for both backend and frontend and constitutes a good starting point for new apps based on the presented tech stack.\n\n\\> **Code**: [muety/go-graphql-sse-example](https://github.com/muety/go-graphql-sse-example)\n\n# What is GraphQL?\n[GraphQL](https://engineering.fb.com/core-data/graphql-a-data-query-language/) is a relatively new (proposed in 2015 by Facebook engineers) approach to designing APIs for (web) backend applications and can be considered an alternative to [REST](https://developer.mozilla.org/en-US/docs/Glossary/REST) or remote-procedure-call (RPC) mechanisms like [JSON-RPC](https://www.jsonrpc.org/). In other words, it's *\"an open-source data query and manipulation language for APIs\"* [[1]](https://en.wikipedia.org/wiki/GraphQL). The specification is open-source and actively evolving on [GitHub](https://github.com/graphql/graphql-spec). While REST APIs are currently the de-facto standard on the web (although not necessarily all of them being fully [mature](https://www.martinfowler.com/articles/richardsonMaturityModel.html)) ‚Äì GraphQL starts to [gain traction](https://trends.google.com/trends/explore?date=2018-05-06%202020-06-06&gprop=youtube&q=graphql).\n\n## Comparison with REST and RPC\n\nIn contrast to REST, which is primarily structured around resources or entities and RPC-based APIs, which focus on actions or methods, GraphQL is all about the underlying data itself. The consumer of an API ‚Äì usually the frontend / client-side part of a SPA ‚Äì only has to know the schema and structure of the data provided by the API to [CRUD](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete) it. Compared to REST APIs, where the consumer heavily depends on the fixed data structure delivered by the backend API, this is especially beneficial as it introduced a lot more flexibility and decoupling and can save the developer some time making the client-side application [tolerant](https://martinfowler.com/bliki/TolerantReader.html). Also, you will probably not need [backends for frontends](https://docs.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends) anymore.\n\nEssentially, with GraphQL, **the consumer asks exactly for what it needs and how it needs it**, i.e. your client application tells the backend exactly what to return and in which format. **Consuming a GraphQL API is like querying a database, but with more guidance and control.**\n\n## Example\nLet's look at an example to get a better idea of how GraphQL works, especially in comparison to the REST principles. \n\nImagine you have an e-commerce application with products and orders. Every order consists, among others, of a set of products. As the operator of the web shop, you might want to get a list of all orders. With a more or less RESTful API (we neglect the hypermedia controls in the example though), your request-response pair could look like this:\n\n<details>\n<summary>Click to view</summary>\n```\nRequest\n-------\nGET /api/orders\n\nResponse Body\n-------------\n[\n    {\n        \"id\": 125,\n        \"customerId\": 8977,\n        \"createdAt\": \"2020-06-06T13:40:49.038Z\",\n        \"productIds\": [ 49863176 ]\n    }\n]\n```\n</details>\n\nSo far so good, but potentially you will also want to view the actual products right away. What you got are only ids, for each of which you would have to issue another API call to retrieve it. Alternatively, the API could also return nested objects, like so:\n\n<details>\n<summary>Click to view</summary>\n```\n[\n    {\n        \"id\": 125,\n        \"customerId\": 8977,\n        \"createdAt\": \"2020-06-06T13:40:49.038Z\",\n        \"products\": [\n            {\n                \"id\": 49863176,\n                \"name\": \"Slim T-Shirt navy-blue\",\n                \"price\": 17.90,\n                \"options\": [\n                    {\n                        \"id\": \"size\",\n                        \"name\": \"Size\",\n                        \"description\": \"T-Shirt size\",\n                        \"values\": [\n                            {\n                                \"id\": \"s\",\n                                \"name\": \"Size S\",\n                            },\n                            {\n                                \"id\": \"m\",\n                                \"name\": \"Size M\",\n                            },\n                            {\n                                \"id\": \"l\",\n                                \"name\": \"Size L\",\n                            }\n                        ]\n                    }\n                ]\n            },\n        ]\n    }\n]\n```\n</details>\n\nHowever, that is ‚Äî to my understanding ‚Äì not truly RESTful anymore. Also, while the above example is still quite straightforward, things get ugly as nested objects include other nested objects, that include other nested objects, that... Quickly you get JSON responses of several tens or hundreds of kilobytes, although you're potentially only interested in two or three attributes. Moreover, on some pages of your shop you may be interested in all possible options (e.g. \"size\") of a product, but not on others. Should your API define different [view models](https://www.infoq.com/articles/View-Model-Definition/) now and expose different endpoints? Or a single endpoints with query flags like `?expanded=true`? Soon you might be catching yourself **tailoring your API specifically to the needs of your client** while neglecting REST conventions and a straightforward design. \n\nWith GraphQL, things are different. Your API is a bit **dumber and less opinionated** now and does not deliver data in a fixed structure, according to a specified [GQL query](https://graphql.org/learn/queries/), which looks a lot like JSON. The above example might look like this, now:\n\n<details>\n<summary>Click to view</summary>\n```\nRequest\n-------\nPOST /api/graphql/query\n\n{\n    \"query\": \"\\{\n        orders {\n            id\n            customerId\n            products {\n                name\n                price\n                options {\n                    name\n                }\n            }\n        }\n    \\}\"\n}\n\nResponse Body\n-------------\n{\n    \"data\": {\n        \"orders\": [\n            {\n                \"id\": 125,\n                \"customerId\": 8977,\n                \"products\": [\n                    {\n                        \"name\": \"Slim T-Shirt navy-blue\",\n                        \"price\": 17.90,\n                        \"options\": [\n                            {\n                                \"name\": \"Size\",\n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    }\n}\n```\n</details>\n\nThis way, you get only the data you want. All your API has to know is how to fetch every piece of data. All your client has to know is how the data schema itself looks like.\n\n## Try it out\n![](images/graphql_github.png)\n\nGitHub's official API offers GraphQL query endpoints. You can try it out using their [GraphQL explorer](https://developer.github.com/v4/explorer/).\n\n# GraphQL Basic\nSince this article does not aim to be another introduction to GraphQL, you can read most of the basics about fields, data types, etc. in the [official docs](https://graphql.org/learn/queries/). However, it is worth mentioning that GraphQL supports three types of queries:\n\n* **`Query`**: \"Standard\" type of queries, used for fetching data (see above). Similar to what you would do with a `GET` in REST.\n* **`Mutation`**: Query type used to modify data. Similar to what you would do with a `PUT`, `POST`, `PATCH` or `DELETE` in REST.\n* **`Subscription`**: Query type to communicate your intent to subscribe to live data updates. \n\n## Subscriptions\n\nWhile a basic GraphQL application will at least use the former two types, the latter is especially interesting in the context of this article. Using subscriptions, you can have your web frontend be notified when new data arrives at the server or existing data changes. For instance, the operator of the above web shop could have a live-updating dashboard, that shows new orders just as they are placed. \n\nFor subscriptions, the GraphQL standard does not define a lot more than their plain existence and purpose. Especially, it is not defined how and which technology to implement them. On the web, any [publish/subscribe](https://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber)-like mechanism that provides bi-directional or uni-directional server-to-client communication is appropriate. For the sake of simplicity, [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events) are used in this article.\n\n# What's next?\nThis part gave a brief introduction to GraphQL. The next part is about actual code. We're going to build an example web app with live-updates using GraphQL, Go, MongoDB and VueJs. Stay tuned!\n\n\\> [**Part 2**](https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.html)","slug":"modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1","published":1,"updated":"2020-06-08T08:01:59.253Z","_id":"ckb3slhiv000t40mqyb8wxyjv","comments":1,"layout":"post","photos":[],"link":"","content":"<p><img src=\"images/graphql_cover.png\" alt></p>\n<h1>Introduction</h1>\n<p>In the course of this two-part article, the interested reader is briefly introduced to the basic of GraphQL and how it compares to traditional approaches. In the second part, an example single-page web application (SPA) is built to demonstrate the use of GraphQL in combination with further modern web technologies. The <a href=\"https://github.com/muety/go-graphql-sse-example\" target=\"_blank\" rel=\"noopener\">final project</a> provides a clean, opinionated code- and project structure for both backend and frontend and constitutes a good starting point for new apps based on the presented tech stack.</p>\n<p>&gt; <strong>Code</strong>: <a href=\"https://github.com/muety/go-graphql-sse-example\" target=\"_blank\" rel=\"noopener\">muety/go-graphql-sse-example</a></p>\n<h1>What is GraphQL?</h1>\n<p><a href=\"https://engineering.fb.com/core-data/graphql-a-data-query-language/\" target=\"_blank\" rel=\"noopener\">GraphQL</a> is a relatively new (proposed in 2015 by Facebook engineers) approach to designing APIs for (web) backend applications and can be considered an alternative to <a href=\"https://developer.mozilla.org/en-US/docs/Glossary/REST\" target=\"_blank\" rel=\"noopener\">REST</a> or remote-procedure-call (RPC) mechanisms like <a href=\"https://www.jsonrpc.org/\" target=\"_blank\" rel=\"noopener\">JSON-RPC</a>. In other words, it‚Äôs <em>‚Äúan open-source data query and manipulation language for APIs‚Äù</em> <a href=\"https://en.wikipedia.org/wiki/GraphQL\" target=\"_blank\" rel=\"noopener\">[1]</a>. The specification is open-source and actively evolving on <a href=\"https://github.com/graphql/graphql-spec\" target=\"_blank\" rel=\"noopener\">GitHub</a>. While REST APIs are currently the de-facto standard on the web (although not necessarily all of them being fully <a href=\"https://www.martinfowler.com/articles/richardsonMaturityModel.html\" target=\"_blank\" rel=\"noopener\">mature</a>) ‚Äì GraphQL starts to <a href=\"https://trends.google.com/trends/explore?date=2018-05-06%202020-06-06&amp;gprop=youtube&amp;q=graphql\" target=\"_blank\" rel=\"noopener\">gain traction</a>.</p>\n<h2 id=\"Comparison-with-REST-and-RPC\">Comparison with REST and RPC</h2>\n<p>In contrast to REST, which is primarily structured around resources or entities and RPC-based APIs, which focus on actions or methods, GraphQL is all about the underlying data itself. The consumer of an API ‚Äì usually the frontend / client-side part of a SPA ‚Äì only has to know the schema and structure of the data provided by the API to <a href=\"https://en.wikipedia.org/wiki/Create,_read,_update_and_delete\" target=\"_blank\" rel=\"noopener\">CRUD</a> it. Compared to REST APIs, where the consumer heavily depends on the fixed data structure delivered by the backend API, this is especially beneficial as it introduced a lot more flexibility and decoupling and can save the developer some time making the client-side application <a href=\"https://martinfowler.com/bliki/TolerantReader.html\" target=\"_blank\" rel=\"noopener\">tolerant</a>. Also, you will probably not need <a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends\" target=\"_blank\" rel=\"noopener\">backends for frontends</a> anymore.</p>\n<p>Essentially, with GraphQL, <strong>the consumer asks exactly for what it needs and how it needs it</strong>, i.e. your client application tells the backend exactly what to return and in which format. <strong>Consuming a GraphQL API is like querying a database, but with more guidance and control.</strong></p>\n<h2 id=\"Example\">Example</h2>\n<p>Let‚Äôs look at an example to get a better idea of how GraphQL works, especially in comparison to the REST principles.</p>\n<p>Imagine you have an e-commerce application with products and orders. Every order consists, among others, of a set of products. As the operator of the web shop, you might want to get a list of all orders. With a more or less RESTful API (we neglect the hypermedia controls in the example though), your request-response pair could look like this:</p>\n<details>\n<summary>Click to view</summary>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Request</span><br><span class=\"line\">-------</span><br><span class=\"line\">GET /api/orders</span><br><span class=\"line\"></span><br><span class=\"line\">Response Body</span><br><span class=\"line\">-------------</span><br><span class=\"line\">[</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        &quot;id&quot;: 125,</span><br><span class=\"line\">        &quot;customerId&quot;: 8977,</span><br><span class=\"line\">        &quot;createdAt&quot;: &quot;2020-06-06T13:40:49.038Z&quot;,</span><br><span class=\"line\">        &quot;productIds&quot;: [ 49863176 ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n</details>\n<p>So far so good, but potentially you will also want to view the actual products right away. What you got are only ids, for each of which you would have to issue another API call to retrieve it. Alternatively, the API could also return nested objects, like so:</p>\n<details>\n<summary>Click to view</summary>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        &quot;id&quot;: 125,</span><br><span class=\"line\">        &quot;customerId&quot;: 8977,</span><br><span class=\"line\">        &quot;createdAt&quot;: &quot;2020-06-06T13:40:49.038Z&quot;,</span><br><span class=\"line\">        &quot;products&quot;: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                &quot;id&quot;: 49863176,</span><br><span class=\"line\">                &quot;name&quot;: &quot;Slim T-Shirt navy-blue&quot;,</span><br><span class=\"line\">                &quot;price&quot;: 17.90,</span><br><span class=\"line\">                &quot;options&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;id&quot;: &quot;size&quot;,</span><br><span class=\"line\">                        &quot;name&quot;: &quot;Size&quot;,</span><br><span class=\"line\">                        &quot;description&quot;: &quot;T-Shirt size&quot;,</span><br><span class=\"line\">                        &quot;values&quot;: [</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;id&quot;: &quot;s&quot;,</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size S&quot;,</span><br><span class=\"line\">                            &#125;,</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;id&quot;: &quot;m&quot;,</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size M&quot;,</span><br><span class=\"line\">                            &#125;,</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;id&quot;: &quot;l&quot;,</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size L&quot;,</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        ]</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n</details>\n<p>However, that is ‚Äî to my understanding ‚Äì not truly RESTful anymore. Also, while the above example is still quite straightforward, things get ugly as nested objects include other nested objects, that include other nested objects, that‚Ä¶ Quickly you get JSON responses of several tens or hundreds of kilobytes, although you‚Äôre potentially only interested in two or three attributes. Moreover, on some pages of your shop you may be interested in all possible options (e.g. ‚Äúsize‚Äù) of a product, but not on others. Should your API define different <a href=\"https://www.infoq.com/articles/View-Model-Definition/\" target=\"_blank\" rel=\"noopener\">view models</a> now and expose different endpoints? Or a single endpoints with query flags like <code>?expanded=true</code>? Soon you might be catching yourself <strong>tailoring your API specifically to the needs of your client</strong> while neglecting REST conventions and a straightforward design.</p>\n<p>With GraphQL, things are different. Your API is a bit <strong>dumber and less opinionated</strong> now and does not deliver data in a fixed structure, according to a specified <a href=\"https://graphql.org/learn/queries/\" target=\"_blank\" rel=\"noopener\">GQL query</a>, which looks a lot like JSON. The above example might look like this, now:</p>\n<details>\n<summary>Click to view</summary>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Request</span><br><span class=\"line\">-------</span><br><span class=\"line\">POST /api/graphql/query</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;query&quot;: &quot;\\&#123;</span><br><span class=\"line\">        orders &#123;</span><br><span class=\"line\">            id</span><br><span class=\"line\">            customerId</span><br><span class=\"line\">            products &#123;</span><br><span class=\"line\">                name</span><br><span class=\"line\">                price</span><br><span class=\"line\">                options &#123;</span><br><span class=\"line\">                    name</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    \\&#125;&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">Response Body</span><br><span class=\"line\">-------------</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;data&quot;: &#123;</span><br><span class=\"line\">        &quot;orders&quot;: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                &quot;id&quot;: 125,</span><br><span class=\"line\">                &quot;customerId&quot;: 8977,</span><br><span class=\"line\">                &quot;products&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;name&quot;: &quot;Slim T-Shirt navy-blue&quot;,</span><br><span class=\"line\">                        &quot;price&quot;: 17.90,</span><br><span class=\"line\">                        &quot;options&quot;: [</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size&quot;,</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        ]</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</details>\n<p>This way, you get only the data you want. All your API has to know is how to fetch every piece of data. All your client has to know is how the data schema itself looks like.</p>\n<h2 id=\"Try-it-out\">Try it out</h2>\n<p><img src=\"images/graphql_github.png\" alt></p>\n<p>GitHub‚Äôs official API offers GraphQL query endpoints. You can try it out using their <a href=\"https://developer.github.com/v4/explorer/\" target=\"_blank\" rel=\"noopener\">GraphQL explorer</a>.</p>\n<h1>GraphQL Basic</h1>\n<p>Since this article does not aim to be another introduction to GraphQL, you can read most of the basics about fields, data types, etc. in the <a href=\"https://graphql.org/learn/queries/\" target=\"_blank\" rel=\"noopener\">official docs</a>. However, it is worth mentioning that GraphQL supports three types of queries:</p>\n<ul>\n<li><strong><code>Query</code></strong>: ‚ÄúStandard‚Äù type of queries, used for fetching data (see above). Similar to what you would do with a <code>GET</code> in REST.</li>\n<li><strong><code>Mutation</code></strong>: Query type used to modify data. Similar to what you would do with a <code>PUT</code>, <code>POST</code>, <code>PATCH</code> or <code>DELETE</code> in REST.</li>\n<li><strong><code>Subscription</code></strong>: Query type to communicate your intent to subscribe to live data updates.</li>\n</ul>\n<h2 id=\"Subscriptions\">Subscriptions</h2>\n<p>While a basic GraphQL application will at least use the former two types, the latter is especially interesting in the context of this article. Using subscriptions, you can have your web frontend be notified when new data arrives at the server or existing data changes. For instance, the operator of the above web shop could have a live-updating dashboard, that shows new orders just as they are placed.</p>\n<p>For subscriptions, the GraphQL standard does not define a lot more than their plain existence and purpose. Especially, it is not defined how and which technology to implement them. On the web, any <a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber\" target=\"_blank\" rel=\"noopener\">publish/subscribe</a>-like mechanism that provides bi-directional or uni-directional server-to-client communication is appropriate. For the sake of simplicity, <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\" target=\"_blank\" rel=\"noopener\">Server-Sent Events</a> are used in this article.</p>\n<h1>What‚Äôs next?</h1>\n<p>This part gave a brief introduction to GraphQL. The next part is about actual code. We‚Äôre going to build an example web app with live-updates using GraphQL, Go, MongoDB and VueJs. Stay tuned!</p>\n<p>&gt; <a href=\"https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.html\"><strong>Part 2</strong></a></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"images/graphql_cover.png\" alt></p>\n<h1>Introduction</h1>\n<p>In the course of this two-part article, the interested reader is briefly introduced to the basic of GraphQL and how it compares to traditional approaches. In the second part, an example single-page web application (SPA) is built to demonstrate the use of GraphQL in combination with further modern web technologies. The <a href=\"https://github.com/muety/go-graphql-sse-example\" target=\"_blank\" rel=\"noopener\">final project</a> provides a clean, opinionated code- and project structure for both backend and frontend and constitutes a good starting point for new apps based on the presented tech stack.</p>\n<p>&gt; <strong>Code</strong>: <a href=\"https://github.com/muety/go-graphql-sse-example\" target=\"_blank\" rel=\"noopener\">muety/go-graphql-sse-example</a></p>\n<h1>What is GraphQL?</h1>\n<p><a href=\"https://engineering.fb.com/core-data/graphql-a-data-query-language/\" target=\"_blank\" rel=\"noopener\">GraphQL</a> is a relatively new (proposed in 2015 by Facebook engineers) approach to designing APIs for (web) backend applications and can be considered an alternative to <a href=\"https://developer.mozilla.org/en-US/docs/Glossary/REST\" target=\"_blank\" rel=\"noopener\">REST</a> or remote-procedure-call (RPC) mechanisms like <a href=\"https://www.jsonrpc.org/\" target=\"_blank\" rel=\"noopener\">JSON-RPC</a>. In other words, it‚Äôs <em>‚Äúan open-source data query and manipulation language for APIs‚Äù</em> <a href=\"https://en.wikipedia.org/wiki/GraphQL\" target=\"_blank\" rel=\"noopener\">[1]</a>. The specification is open-source and actively evolving on <a href=\"https://github.com/graphql/graphql-spec\" target=\"_blank\" rel=\"noopener\">GitHub</a>. While REST APIs are currently the de-facto standard on the web (although not necessarily all of them being fully <a href=\"https://www.martinfowler.com/articles/richardsonMaturityModel.html\" target=\"_blank\" rel=\"noopener\">mature</a>) ‚Äì GraphQL starts to <a href=\"https://trends.google.com/trends/explore?date=2018-05-06%202020-06-06&amp;gprop=youtube&amp;q=graphql\" target=\"_blank\" rel=\"noopener\">gain traction</a>.</p>\n<h2 id=\"Comparison-with-REST-and-RPC\">Comparison with REST and RPC</h2>\n<p>In contrast to REST, which is primarily structured around resources or entities and RPC-based APIs, which focus on actions or methods, GraphQL is all about the underlying data itself. The consumer of an API ‚Äì usually the frontend / client-side part of a SPA ‚Äì only has to know the schema and structure of the data provided by the API to <a href=\"https://en.wikipedia.org/wiki/Create,_read,_update_and_delete\" target=\"_blank\" rel=\"noopener\">CRUD</a> it. Compared to REST APIs, where the consumer heavily depends on the fixed data structure delivered by the backend API, this is especially beneficial as it introduced a lot more flexibility and decoupling and can save the developer some time making the client-side application <a href=\"https://martinfowler.com/bliki/TolerantReader.html\" target=\"_blank\" rel=\"noopener\">tolerant</a>. Also, you will probably not need <a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends\" target=\"_blank\" rel=\"noopener\">backends for frontends</a> anymore.</p>\n<p>Essentially, with GraphQL, <strong>the consumer asks exactly for what it needs and how it needs it</strong>, i.e. your client application tells the backend exactly what to return and in which format. <strong>Consuming a GraphQL API is like querying a database, but with more guidance and control.</strong></p>\n<h2 id=\"Example\">Example</h2>\n<p>Let‚Äôs look at an example to get a better idea of how GraphQL works, especially in comparison to the REST principles.</p>\n<p>Imagine you have an e-commerce application with products and orders. Every order consists, among others, of a set of products. As the operator of the web shop, you might want to get a list of all orders. With a more or less RESTful API (we neglect the hypermedia controls in the example though), your request-response pair could look like this:</p>\n<details>\n<summary>Click to view</summary>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Request</span><br><span class=\"line\">-------</span><br><span class=\"line\">GET /api/orders</span><br><span class=\"line\"></span><br><span class=\"line\">Response Body</span><br><span class=\"line\">-------------</span><br><span class=\"line\">[</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        &quot;id&quot;: 125,</span><br><span class=\"line\">        &quot;customerId&quot;: 8977,</span><br><span class=\"line\">        &quot;createdAt&quot;: &quot;2020-06-06T13:40:49.038Z&quot;,</span><br><span class=\"line\">        &quot;productIds&quot;: [ 49863176 ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n</details>\n<p>So far so good, but potentially you will also want to view the actual products right away. What you got are only ids, for each of which you would have to issue another API call to retrieve it. Alternatively, the API could also return nested objects, like so:</p>\n<details>\n<summary>Click to view</summary>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        &quot;id&quot;: 125,</span><br><span class=\"line\">        &quot;customerId&quot;: 8977,</span><br><span class=\"line\">        &quot;createdAt&quot;: &quot;2020-06-06T13:40:49.038Z&quot;,</span><br><span class=\"line\">        &quot;products&quot;: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                &quot;id&quot;: 49863176,</span><br><span class=\"line\">                &quot;name&quot;: &quot;Slim T-Shirt navy-blue&quot;,</span><br><span class=\"line\">                &quot;price&quot;: 17.90,</span><br><span class=\"line\">                &quot;options&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;id&quot;: &quot;size&quot;,</span><br><span class=\"line\">                        &quot;name&quot;: &quot;Size&quot;,</span><br><span class=\"line\">                        &quot;description&quot;: &quot;T-Shirt size&quot;,</span><br><span class=\"line\">                        &quot;values&quot;: [</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;id&quot;: &quot;s&quot;,</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size S&quot;,</span><br><span class=\"line\">                            &#125;,</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;id&quot;: &quot;m&quot;,</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size M&quot;,</span><br><span class=\"line\">                            &#125;,</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;id&quot;: &quot;l&quot;,</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size L&quot;,</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        ]</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n</details>\n<p>However, that is ‚Äî to my understanding ‚Äì not truly RESTful anymore. Also, while the above example is still quite straightforward, things get ugly as nested objects include other nested objects, that include other nested objects, that‚Ä¶ Quickly you get JSON responses of several tens or hundreds of kilobytes, although you‚Äôre potentially only interested in two or three attributes. Moreover, on some pages of your shop you may be interested in all possible options (e.g. ‚Äúsize‚Äù) of a product, but not on others. Should your API define different <a href=\"https://www.infoq.com/articles/View-Model-Definition/\" target=\"_blank\" rel=\"noopener\">view models</a> now and expose different endpoints? Or a single endpoints with query flags like <code>?expanded=true</code>? Soon you might be catching yourself <strong>tailoring your API specifically to the needs of your client</strong> while neglecting REST conventions and a straightforward design.</p>\n<p>With GraphQL, things are different. Your API is a bit <strong>dumber and less opinionated</strong> now and does not deliver data in a fixed structure, according to a specified <a href=\"https://graphql.org/learn/queries/\" target=\"_blank\" rel=\"noopener\">GQL query</a>, which looks a lot like JSON. The above example might look like this, now:</p>\n<details>\n<summary>Click to view</summary>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Request</span><br><span class=\"line\">-------</span><br><span class=\"line\">POST /api/graphql/query</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;query&quot;: &quot;\\&#123;</span><br><span class=\"line\">        orders &#123;</span><br><span class=\"line\">            id</span><br><span class=\"line\">            customerId</span><br><span class=\"line\">            products &#123;</span><br><span class=\"line\">                name</span><br><span class=\"line\">                price</span><br><span class=\"line\">                options &#123;</span><br><span class=\"line\">                    name</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    \\&#125;&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">Response Body</span><br><span class=\"line\">-------------</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;data&quot;: &#123;</span><br><span class=\"line\">        &quot;orders&quot;: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                &quot;id&quot;: 125,</span><br><span class=\"line\">                &quot;customerId&quot;: 8977,</span><br><span class=\"line\">                &quot;products&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;name&quot;: &quot;Slim T-Shirt navy-blue&quot;,</span><br><span class=\"line\">                        &quot;price&quot;: 17.90,</span><br><span class=\"line\">                        &quot;options&quot;: [</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size&quot;,</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        ]</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</details>\n<p>This way, you get only the data you want. All your API has to know is how to fetch every piece of data. All your client has to know is how the data schema itself looks like.</p>\n<h2 id=\"Try-it-out\">Try it out</h2>\n<p><img src=\"images/graphql_github.png\" alt></p>\n<p>GitHub‚Äôs official API offers GraphQL query endpoints. You can try it out using their <a href=\"https://developer.github.com/v4/explorer/\" target=\"_blank\" rel=\"noopener\">GraphQL explorer</a>.</p>\n<h1>GraphQL Basic</h1>\n<p>Since this article does not aim to be another introduction to GraphQL, you can read most of the basics about fields, data types, etc. in the <a href=\"https://graphql.org/learn/queries/\" target=\"_blank\" rel=\"noopener\">official docs</a>. However, it is worth mentioning that GraphQL supports three types of queries:</p>\n<ul>\n<li><strong><code>Query</code></strong>: ‚ÄúStandard‚Äù type of queries, used for fetching data (see above). Similar to what you would do with a <code>GET</code> in REST.</li>\n<li><strong><code>Mutation</code></strong>: Query type used to modify data. Similar to what you would do with a <code>PUT</code>, <code>POST</code>, <code>PATCH</code> or <code>DELETE</code> in REST.</li>\n<li><strong><code>Subscription</code></strong>: Query type to communicate your intent to subscribe to live data updates.</li>\n</ul>\n<h2 id=\"Subscriptions\">Subscriptions</h2>\n<p>While a basic GraphQL application will at least use the former two types, the latter is especially interesting in the context of this article. Using subscriptions, you can have your web frontend be notified when new data arrives at the server or existing data changes. For instance, the operator of the above web shop could have a live-updating dashboard, that shows new orders just as they are placed.</p>\n<p>For subscriptions, the GraphQL standard does not define a lot more than their plain existence and purpose. Especially, it is not defined how and which technology to implement them. On the web, any <a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber\" target=\"_blank\" rel=\"noopener\">publish/subscribe</a>-like mechanism that provides bi-directional or uni-directional server-to-client communication is appropriate. For the sake of simplicity, <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\" target=\"_blank\" rel=\"noopener\">Server-Sent Events</a> are used in this article.</p>\n<h1>What‚Äôs next?</h1>\n<p>This part gave a brief introduction to GraphQL. The next part is about actual code. We‚Äôre going to build an example web app with live-updates using GraphQL, Go, MongoDB and VueJs. Stay tuned!</p>\n<p>&gt; <a href=\"https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.html\"><strong>Part 2</strong></a></p>\n"},{"title":"My teck stack if I had to build an app today","date":"2016-11-11T22:04:56.000Z","_content":"\nWhat technology stack would I choose, if I had to develop a web application completely from scratch? That's the question this article will cover.\n\nFirst of all: by saying web application I'm referring to something between a plain static HTML page and an entire Facebook. Basically, an application that fulfills a certain domain of tasks for the user and that requires the usual features like user management, a backend database, multiple UI views and controls, etc. The size of application I'm thinking of could be a browser-based chat app, password-manager or something similar. Neither too simple, nor too complex.\n\nBack to the topic. Choosing the right technology for a web app feels much like customizing a new PC or even a new car. There are nearly endless options to be weighed to finally pick a bunch of them for a new web application. This super famous article [How it feels to learn JavaScript in 2016](https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f#.m1nodqu6f) complains about the confusing and ever-growing, chaotic jungle of new JavaScript frameworks in an ironical way. Indeed, I hear similar arguments from many developers these days. Many of them claim that code quality was getting worse in the web and that every newbie JavaScript programmer threw out his own new framework on yet another .io domain. Although that might be true to a certain extent, I personally still like the great technological variety and innovation. I love to browse GitHub, Reddit, Hackernews and Co. to discover new cool libraries to try out in a project some day. And here's what I would pick if I had to realize such a project right today and if there weren't any restrictions. \n\nOf course, the technology choice depends on the concrete project requirements to a certain extent, but not completely. Consequently, a new project is always a chance to try something new. ThoughtWorks just published their new [technology radar for 2016](https://www.thoughtworks.com/de/radar), where they separate into categories _adopt_, _trial_, _assess_ and _hold_. Of course, _hold_-techs are not an option for new projects and I actually pretty much agree with their views on what has to be in the _hold_ category. _Adopt_ basically are things that are modern, but also well-established enough to avoid too much risk. _Trial_-techs are more experimental and _assess_ are the latest fancy s***, so to say. Since I'm extremely eager to try out new things, my stack would probably mostly consist of technologies from the last category. So what would my stack now look like? Actually, I couldn't decide on one stack, but set up two: _the fancy one_ and _the super fancy one_. Additionally, I define their intersection as _the base stack_, which consists of fundamental tools etc. that both have in common.\n\n### The base stack\nFirst of all, I'd use __Git__ for version control, __Visual Studio Code__ as code editor and __GitLab__ for repository hosting and as build server. If I didn't had to implement user management myself, I'd pick __Auth0__ for that. For deployment, I'd use containers with __Docker__ on __DigitalOcean__ machines and if I needed multiple instances, __Rancher__ would help me to manage them. As reverse proxy in front of the backend I'd choose __nginx__ since it's extremely efficient, performant and has __HTTP/2.0__ support. For bundling, __Webpack__ would be my choice and task automation would be done using plain __npm scripts__. For styling the UI, I'd simply use __Bootstrap 4__ and __SCSS__.  \n\n### The super fancy stack\nThe key point here is that I'd want to abandon a traditional REST API in my project and use __GraphQL__ instead. The backend would be written in __NodeJS__ with [__Graffiti__](https://github.com/RisingStack/graffiti) as GraphQL implementation. I don't know much about the latter one, yet, except for that it's the de-facto GraphQL solution for Node. Why Node? Because it's simply the best choice for the web (my view...). It's performant, comfortable to develop and especially brings consistency by having JS in front- and backend. By always being quite up-to-date with the latest ES* features, Node doesn't get boring. Since GraphQL is told to work best with other Facebook technology, I'd not be that experimental here and build the frontend on __React__ plus __Relay__ (which is still completely new for me). Database would probably be a __MongoDB__ (JSON everywhere!) with [__Waterline__](https://github.com/balderdashy/waterline) ORM. To put a cherry on the cake, I'd also introduce __Redux__ in addition. I haven't worked with it much, yet, and I heard that it's kind of mind-blowing in the beginning. However, I consider its concept to cover a large potential to manage consistency in my app. The last thing here is that I desperately want is ES6 syntax. It isn't supported by the React compiler afaik (please correct me, if I'm wrong), so I'd use __Babel__ to have latest JavaScript features. If having to go mobile, __React Native__ would be the rational choice.\n\n### The fancy stack\nThis stack differs from the _super fancy stack_ in a few points. A key point is that it would not use GraphQL, but a good old REST API. This API would be written in __Go__, since I like the language - especially its efficiency and its good suitability for web development. More precisely, I'd use the [__Iris__](http://iris-go.com/) framework. I've read the documentation and it looked incredibly powerful to me (in terms of both functionality and perfomance). _(__EDIT:__ It's not what it seems! Please see my comment below!)._ For the frontend I'm balancing between __Angular 2__ and [__Aurelia__](http://aurelia.io/). Angular 2 is guaranteed to work for any potential case, is extremely powerful and has great community- and library support. However, Aurelia look promising, too, and probably is even more clear and less boilerplate code. Consequently, I'd give it a try. But if having to go mobile, I'd still favor Angular 2, since it perfectly aligns with __Ionic 2__.\n\nTwo other options, which look really interesting to me are __Meteor__ and __HorizonJS__. However, I'm not sure, if it's a good idea to commit to only one comprehensive framework through the full stack.\n\nSo these are my two alternative ways through the webdev jungle - btw, [this good article](https://medium.freecodecamp.com/a-study-plan-to-cure-javascript-fatigue-8ad3a54f2eb1#.3ri3a1fdv) describes another one, especially for newcomer web developers. Sorry, that I haven't justified all choices. Actually, as you probably know, if you're a developer, subjective views like these often aren't even based on pure rational considerations, but are rather emotional and spontaneous.\n\nPlease feel free to give me feedback on my tech stack of choice! \n\n__EDIT:__ Another framework I‚Äôd really like to try out is [InfernoJS](https://infernojs.org/), because it claims to be extremely lightweight and performant. However, before using Inferno, one should probably be familiar with React, since it uses very similar concepts and syntaxes.\n\n__EDIT 2:__ After having read [this article](http://www.florinpatan.ro/2016/10/why-you-should-not-use-iris-for-your-go.html) and having done some further research on the Iris framework I really have to retract my above statement that I‚Äôd use it as a web backend. While it looks nice on paper, after diving a little deeper I really have to admit that it‚Äôd be morally tenable to support the authors of that project. So please forget about Iris and take a look at [Beego](https://beego.me/) instead.","source":"_posts/my-teck-stack-if-i-had-to-build-an-app-today.md","raw":"---\ntitle: My teck stack if I had to build an app today\ndate: 2016-11-11 23:04:56\ntags:\n---\n\nWhat technology stack would I choose, if I had to develop a web application completely from scratch? That's the question this article will cover.\n\nFirst of all: by saying web application I'm referring to something between a plain static HTML page and an entire Facebook. Basically, an application that fulfills a certain domain of tasks for the user and that requires the usual features like user management, a backend database, multiple UI views and controls, etc. The size of application I'm thinking of could be a browser-based chat app, password-manager or something similar. Neither too simple, nor too complex.\n\nBack to the topic. Choosing the right technology for a web app feels much like customizing a new PC or even a new car. There are nearly endless options to be weighed to finally pick a bunch of them for a new web application. This super famous article [How it feels to learn JavaScript in 2016](https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f#.m1nodqu6f) complains about the confusing and ever-growing, chaotic jungle of new JavaScript frameworks in an ironical way. Indeed, I hear similar arguments from many developers these days. Many of them claim that code quality was getting worse in the web and that every newbie JavaScript programmer threw out his own new framework on yet another .io domain. Although that might be true to a certain extent, I personally still like the great technological variety and innovation. I love to browse GitHub, Reddit, Hackernews and Co. to discover new cool libraries to try out in a project some day. And here's what I would pick if I had to realize such a project right today and if there weren't any restrictions. \n\nOf course, the technology choice depends on the concrete project requirements to a certain extent, but not completely. Consequently, a new project is always a chance to try something new. ThoughtWorks just published their new [technology radar for 2016](https://www.thoughtworks.com/de/radar), where they separate into categories _adopt_, _trial_, _assess_ and _hold_. Of course, _hold_-techs are not an option for new projects and I actually pretty much agree with their views on what has to be in the _hold_ category. _Adopt_ basically are things that are modern, but also well-established enough to avoid too much risk. _Trial_-techs are more experimental and _assess_ are the latest fancy s***, so to say. Since I'm extremely eager to try out new things, my stack would probably mostly consist of technologies from the last category. So what would my stack now look like? Actually, I couldn't decide on one stack, but set up two: _the fancy one_ and _the super fancy one_. Additionally, I define their intersection as _the base stack_, which consists of fundamental tools etc. that both have in common.\n\n### The base stack\nFirst of all, I'd use __Git__ for version control, __Visual Studio Code__ as code editor and __GitLab__ for repository hosting and as build server. If I didn't had to implement user management myself, I'd pick __Auth0__ for that. For deployment, I'd use containers with __Docker__ on __DigitalOcean__ machines and if I needed multiple instances, __Rancher__ would help me to manage them. As reverse proxy in front of the backend I'd choose __nginx__ since it's extremely efficient, performant and has __HTTP/2.0__ support. For bundling, __Webpack__ would be my choice and task automation would be done using plain __npm scripts__. For styling the UI, I'd simply use __Bootstrap 4__ and __SCSS__.  \n\n### The super fancy stack\nThe key point here is that I'd want to abandon a traditional REST API in my project and use __GraphQL__ instead. The backend would be written in __NodeJS__ with [__Graffiti__](https://github.com/RisingStack/graffiti) as GraphQL implementation. I don't know much about the latter one, yet, except for that it's the de-facto GraphQL solution for Node. Why Node? Because it's simply the best choice for the web (my view...). It's performant, comfortable to develop and especially brings consistency by having JS in front- and backend. By always being quite up-to-date with the latest ES* features, Node doesn't get boring. Since GraphQL is told to work best with other Facebook technology, I'd not be that experimental here and build the frontend on __React__ plus __Relay__ (which is still completely new for me). Database would probably be a __MongoDB__ (JSON everywhere!) with [__Waterline__](https://github.com/balderdashy/waterline) ORM. To put a cherry on the cake, I'd also introduce __Redux__ in addition. I haven't worked with it much, yet, and I heard that it's kind of mind-blowing in the beginning. However, I consider its concept to cover a large potential to manage consistency in my app. The last thing here is that I desperately want is ES6 syntax. It isn't supported by the React compiler afaik (please correct me, if I'm wrong), so I'd use __Babel__ to have latest JavaScript features. If having to go mobile, __React Native__ would be the rational choice.\n\n### The fancy stack\nThis stack differs from the _super fancy stack_ in a few points. A key point is that it would not use GraphQL, but a good old REST API. This API would be written in __Go__, since I like the language - especially its efficiency and its good suitability for web development. More precisely, I'd use the [__Iris__](http://iris-go.com/) framework. I've read the documentation and it looked incredibly powerful to me (in terms of both functionality and perfomance). _(__EDIT:__ It's not what it seems! Please see my comment below!)._ For the frontend I'm balancing between __Angular 2__ and [__Aurelia__](http://aurelia.io/). Angular 2 is guaranteed to work for any potential case, is extremely powerful and has great community- and library support. However, Aurelia look promising, too, and probably is even more clear and less boilerplate code. Consequently, I'd give it a try. But if having to go mobile, I'd still favor Angular 2, since it perfectly aligns with __Ionic 2__.\n\nTwo other options, which look really interesting to me are __Meteor__ and __HorizonJS__. However, I'm not sure, if it's a good idea to commit to only one comprehensive framework through the full stack.\n\nSo these are my two alternative ways through the webdev jungle - btw, [this good article](https://medium.freecodecamp.com/a-study-plan-to-cure-javascript-fatigue-8ad3a54f2eb1#.3ri3a1fdv) describes another one, especially for newcomer web developers. Sorry, that I haven't justified all choices. Actually, as you probably know, if you're a developer, subjective views like these often aren't even based on pure rational considerations, but are rather emotional and spontaneous.\n\nPlease feel free to give me feedback on my tech stack of choice! \n\n__EDIT:__ Another framework I‚Äôd really like to try out is [InfernoJS](https://infernojs.org/), because it claims to be extremely lightweight and performant. However, before using Inferno, one should probably be familiar with React, since it uses very similar concepts and syntaxes.\n\n__EDIT 2:__ After having read [this article](http://www.florinpatan.ro/2016/10/why-you-should-not-use-iris-for-your-go.html) and having done some further research on the Iris framework I really have to retract my above statement that I‚Äôd use it as a web backend. While it looks nice on paper, after diving a little deeper I really have to admit that it‚Äôd be morally tenable to support the authors of that project. So please forget about Iris and take a look at [Beego](https://beego.me/) instead.","slug":"my-teck-stack-if-i-had-to-build-an-app-today","published":1,"updated":"2020-06-06T15:14:33.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhiw000u40mqhfjpnkoi","content":"<p>What technology stack would I choose, if I had to develop a web application completely from scratch? That‚Äôs the question this article will cover.</p>\n<p>First of all: by saying web application I‚Äôm referring to something between a plain static HTML page and an entire Facebook. Basically, an application that fulfills a certain domain of tasks for the user and that requires the usual features like user management, a backend database, multiple UI views and controls, etc. The size of application I‚Äôm thinking of could be a browser-based chat app, password-manager or something similar. Neither too simple, nor too complex.</p>\n<p>Back to the topic. Choosing the right technology for a web app feels much like customizing a new PC or even a new car. There are nearly endless options to be weighed to finally pick a bunch of them for a new web application. This super famous article <a href=\"https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f#.m1nodqu6f\" target=\"_blank\" rel=\"noopener\">How it feels to learn JavaScript in 2016</a> complains about the confusing and ever-growing, chaotic jungle of new JavaScript frameworks in an ironical way. Indeed, I hear similar arguments from many developers these days. Many of them claim that code quality was getting worse in the web and that every newbie JavaScript programmer threw out his own new framework on yet another .io domain. Although that might be true to a certain extent, I personally still like the great technological variety and innovation. I love to browse GitHub, Reddit, Hackernews and Co. to discover new cool libraries to try out in a project some day. And here‚Äôs what I would pick if I had to realize such a project right today and if there weren‚Äôt any restrictions.</p>\n<p>Of course, the technology choice depends on the concrete project requirements to a certain extent, but not completely. Consequently, a new project is always a chance to try something new. ThoughtWorks just published their new <a href=\"https://www.thoughtworks.com/de/radar\" target=\"_blank\" rel=\"noopener\">technology radar for 2016</a>, where they separate into categories <em>adopt</em>, <em>trial</em>, <em>assess</em> and <em>hold</em>. Of course, <em>hold</em>-techs are not an option for new projects and I actually pretty much agree with their views on what has to be in the <em>hold</em> category. <em>Adopt</em> basically are things that are modern, but also well-established enough to avoid too much risk. <em>Trial</em>-techs are more experimental and <em>assess</em> are the latest fancy s***, so to say. Since I‚Äôm extremely eager to try out new things, my stack would probably mostly consist of technologies from the last category. So what would my stack now look like? Actually, I couldn‚Äôt decide on one stack, but set up two: <em>the fancy one</em> and <em>the super fancy one</em>. Additionally, I define their intersection as <em>the base stack</em>, which consists of fundamental tools etc. that both have in common.</p>\n<h3 id=\"The-base-stack\">The base stack</h3>\n<p>First of all, I‚Äôd use <strong>Git</strong> for version control, <strong>Visual Studio Code</strong> as code editor and <strong>GitLab</strong> for repository hosting and as build server. If I didn‚Äôt had to implement user management myself, I‚Äôd pick <strong>Auth0</strong> for that. For deployment, I‚Äôd use containers with <strong>Docker</strong> on <strong>DigitalOcean</strong> machines and if I needed multiple instances, <strong>Rancher</strong> would help me to manage them. As reverse proxy in front of the backend I‚Äôd choose <strong>nginx</strong> since it‚Äôs extremely efficient, performant and has <strong>HTTP/2.0</strong> support. For bundling, <strong>Webpack</strong> would be my choice and task automation would be done using plain <strong>npm scripts</strong>. For styling the UI, I‚Äôd simply use <strong>Bootstrap 4</strong> and <strong>SCSS</strong>.</p>\n<h3 id=\"The-super-fancy-stack\">The super fancy stack</h3>\n<p>The key point here is that I‚Äôd want to abandon a traditional REST API in my project and use <strong>GraphQL</strong> instead. The backend would be written in <strong>NodeJS</strong> with <a href=\"https://github.com/RisingStack/graffiti\" target=\"_blank\" rel=\"noopener\"><strong>Graffiti</strong></a> as GraphQL implementation. I don‚Äôt know much about the latter one, yet, except for that it‚Äôs the de-facto GraphQL solution for Node. Why Node? Because it‚Äôs simply the best choice for the web (my view‚Ä¶). It‚Äôs performant, comfortable to develop and especially brings consistency by having JS in front- and backend. By always being quite up-to-date with the latest ES* features, Node doesn‚Äôt get boring. Since GraphQL is told to work best with other Facebook technology, I‚Äôd not be that experimental here and build the frontend on <strong>React</strong> plus <strong>Relay</strong> (which is still completely new for me). Database would probably be a <strong>MongoDB</strong> (JSON everywhere!) with <a href=\"https://github.com/balderdashy/waterline\" target=\"_blank\" rel=\"noopener\"><strong>Waterline</strong></a> ORM. To put a cherry on the cake, I‚Äôd also introduce <strong>Redux</strong> in addition. I haven‚Äôt worked with it much, yet, and I heard that it‚Äôs kind of mind-blowing in the beginning. However, I consider its concept to cover a large potential to manage consistency in my app. The last thing here is that I desperately want is ES6 syntax. It isn‚Äôt supported by the React compiler afaik (please correct me, if I‚Äôm wrong), so I‚Äôd use <strong>Babel</strong> to have latest JavaScript features. If having to go mobile, <strong>React Native</strong> would be the rational choice.</p>\n<h3 id=\"The-fancy-stack\">The fancy stack</h3>\n<p>This stack differs from the <em>super fancy stack</em> in a few points. A key point is that it would not use GraphQL, but a good old REST API. This API would be written in <strong>Go</strong>, since I like the language - especially its efficiency and its good suitability for web development. More precisely, I‚Äôd use the <a href=\"http://iris-go.com/\" target=\"_blank\" rel=\"noopener\"><strong>Iris</strong></a> framework. I‚Äôve read the documentation and it looked incredibly powerful to me (in terms of both functionality and perfomance). <em>(<strong>EDIT:</strong> It‚Äôs not what it seems! Please see my comment below!).</em> For the frontend I‚Äôm balancing between <strong>Angular 2</strong> and <a href=\"http://aurelia.io/\" target=\"_blank\" rel=\"noopener\"><strong>Aurelia</strong></a>. Angular 2 is guaranteed to work for any potential case, is extremely powerful and has great community- and library support. However, Aurelia look promising, too, and probably is even more clear and less boilerplate code. Consequently, I‚Äôd give it a try. But if having to go mobile, I‚Äôd still favor Angular 2, since it perfectly aligns with <strong>Ionic 2</strong>.</p>\n<p>Two other options, which look really interesting to me are <strong>Meteor</strong> and <strong>HorizonJS</strong>. However, I‚Äôm not sure, if it‚Äôs a good idea to commit to only one comprehensive framework through the full stack.</p>\n<p>So these are my two alternative ways through the webdev jungle - btw, <a href=\"https://medium.freecodecamp.com/a-study-plan-to-cure-javascript-fatigue-8ad3a54f2eb1#.3ri3a1fdv\" target=\"_blank\" rel=\"noopener\">this good article</a> describes another one, especially for newcomer web developers. Sorry, that I haven‚Äôt justified all choices. Actually, as you probably know, if you‚Äôre a developer, subjective views like these often aren‚Äôt even based on pure rational considerations, but are rather emotional and spontaneous.</p>\n<p>Please feel free to give me feedback on my tech stack of choice!</p>\n<p><strong>EDIT:</strong> Another framework I‚Äôd really like to try out is <a href=\"https://infernojs.org/\" target=\"_blank\" rel=\"noopener\">InfernoJS</a>, because it claims to be extremely lightweight and performant. However, before using Inferno, one should probably be familiar with React, since it uses very similar concepts and syntaxes.</p>\n<p><strong>EDIT 2:</strong> After having read <a href=\"http://www.florinpatan.ro/2016/10/why-you-should-not-use-iris-for-your-go.html\" target=\"_blank\" rel=\"noopener\">this article</a> and having done some further research on the Iris framework I really have to retract my above statement that I‚Äôd use it as a web backend. While it looks nice on paper, after diving a little deeper I really have to admit that it‚Äôd be morally tenable to support the authors of that project. So please forget about Iris and take a look at <a href=\"https://beego.me/\" target=\"_blank\" rel=\"noopener\">Beego</a> instead.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>What technology stack would I choose, if I had to develop a web application completely from scratch? That‚Äôs the question this article will cover.</p>\n<p>First of all: by saying web application I‚Äôm referring to something between a plain static HTML page and an entire Facebook. Basically, an application that fulfills a certain domain of tasks for the user and that requires the usual features like user management, a backend database, multiple UI views and controls, etc. The size of application I‚Äôm thinking of could be a browser-based chat app, password-manager or something similar. Neither too simple, nor too complex.</p>\n<p>Back to the topic. Choosing the right technology for a web app feels much like customizing a new PC or even a new car. There are nearly endless options to be weighed to finally pick a bunch of them for a new web application. This super famous article <a href=\"https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f#.m1nodqu6f\" target=\"_blank\" rel=\"noopener\">How it feels to learn JavaScript in 2016</a> complains about the confusing and ever-growing, chaotic jungle of new JavaScript frameworks in an ironical way. Indeed, I hear similar arguments from many developers these days. Many of them claim that code quality was getting worse in the web and that every newbie JavaScript programmer threw out his own new framework on yet another .io domain. Although that might be true to a certain extent, I personally still like the great technological variety and innovation. I love to browse GitHub, Reddit, Hackernews and Co. to discover new cool libraries to try out in a project some day. And here‚Äôs what I would pick if I had to realize such a project right today and if there weren‚Äôt any restrictions.</p>\n<p>Of course, the technology choice depends on the concrete project requirements to a certain extent, but not completely. Consequently, a new project is always a chance to try something new. ThoughtWorks just published their new <a href=\"https://www.thoughtworks.com/de/radar\" target=\"_blank\" rel=\"noopener\">technology radar for 2016</a>, where they separate into categories <em>adopt</em>, <em>trial</em>, <em>assess</em> and <em>hold</em>. Of course, <em>hold</em>-techs are not an option for new projects and I actually pretty much agree with their views on what has to be in the <em>hold</em> category. <em>Adopt</em> basically are things that are modern, but also well-established enough to avoid too much risk. <em>Trial</em>-techs are more experimental and <em>assess</em> are the latest fancy s***, so to say. Since I‚Äôm extremely eager to try out new things, my stack would probably mostly consist of technologies from the last category. So what would my stack now look like? Actually, I couldn‚Äôt decide on one stack, but set up two: <em>the fancy one</em> and <em>the super fancy one</em>. Additionally, I define their intersection as <em>the base stack</em>, which consists of fundamental tools etc. that both have in common.</p>\n<h3 id=\"The-base-stack\">The base stack</h3>\n<p>First of all, I‚Äôd use <strong>Git</strong> for version control, <strong>Visual Studio Code</strong> as code editor and <strong>GitLab</strong> for repository hosting and as build server. If I didn‚Äôt had to implement user management myself, I‚Äôd pick <strong>Auth0</strong> for that. For deployment, I‚Äôd use containers with <strong>Docker</strong> on <strong>DigitalOcean</strong> machines and if I needed multiple instances, <strong>Rancher</strong> would help me to manage them. As reverse proxy in front of the backend I‚Äôd choose <strong>nginx</strong> since it‚Äôs extremely efficient, performant and has <strong>HTTP/2.0</strong> support. For bundling, <strong>Webpack</strong> would be my choice and task automation would be done using plain <strong>npm scripts</strong>. For styling the UI, I‚Äôd simply use <strong>Bootstrap 4</strong> and <strong>SCSS</strong>.</p>\n<h3 id=\"The-super-fancy-stack\">The super fancy stack</h3>\n<p>The key point here is that I‚Äôd want to abandon a traditional REST API in my project and use <strong>GraphQL</strong> instead. The backend would be written in <strong>NodeJS</strong> with <a href=\"https://github.com/RisingStack/graffiti\" target=\"_blank\" rel=\"noopener\"><strong>Graffiti</strong></a> as GraphQL implementation. I don‚Äôt know much about the latter one, yet, except for that it‚Äôs the de-facto GraphQL solution for Node. Why Node? Because it‚Äôs simply the best choice for the web (my view‚Ä¶). It‚Äôs performant, comfortable to develop and especially brings consistency by having JS in front- and backend. By always being quite up-to-date with the latest ES* features, Node doesn‚Äôt get boring. Since GraphQL is told to work best with other Facebook technology, I‚Äôd not be that experimental here and build the frontend on <strong>React</strong> plus <strong>Relay</strong> (which is still completely new for me). Database would probably be a <strong>MongoDB</strong> (JSON everywhere!) with <a href=\"https://github.com/balderdashy/waterline\" target=\"_blank\" rel=\"noopener\"><strong>Waterline</strong></a> ORM. To put a cherry on the cake, I‚Äôd also introduce <strong>Redux</strong> in addition. I haven‚Äôt worked with it much, yet, and I heard that it‚Äôs kind of mind-blowing in the beginning. However, I consider its concept to cover a large potential to manage consistency in my app. The last thing here is that I desperately want is ES6 syntax. It isn‚Äôt supported by the React compiler afaik (please correct me, if I‚Äôm wrong), so I‚Äôd use <strong>Babel</strong> to have latest JavaScript features. If having to go mobile, <strong>React Native</strong> would be the rational choice.</p>\n<h3 id=\"The-fancy-stack\">The fancy stack</h3>\n<p>This stack differs from the <em>super fancy stack</em> in a few points. A key point is that it would not use GraphQL, but a good old REST API. This API would be written in <strong>Go</strong>, since I like the language - especially its efficiency and its good suitability for web development. More precisely, I‚Äôd use the <a href=\"http://iris-go.com/\" target=\"_blank\" rel=\"noopener\"><strong>Iris</strong></a> framework. I‚Äôve read the documentation and it looked incredibly powerful to me (in terms of both functionality and perfomance). <em>(<strong>EDIT:</strong> It‚Äôs not what it seems! Please see my comment below!).</em> For the frontend I‚Äôm balancing between <strong>Angular 2</strong> and <a href=\"http://aurelia.io/\" target=\"_blank\" rel=\"noopener\"><strong>Aurelia</strong></a>. Angular 2 is guaranteed to work for any potential case, is extremely powerful and has great community- and library support. However, Aurelia look promising, too, and probably is even more clear and less boilerplate code. Consequently, I‚Äôd give it a try. But if having to go mobile, I‚Äôd still favor Angular 2, since it perfectly aligns with <strong>Ionic 2</strong>.</p>\n<p>Two other options, which look really interesting to me are <strong>Meteor</strong> and <strong>HorizonJS</strong>. However, I‚Äôm not sure, if it‚Äôs a good idea to commit to only one comprehensive framework through the full stack.</p>\n<p>So these are my two alternative ways through the webdev jungle - btw, <a href=\"https://medium.freecodecamp.com/a-study-plan-to-cure-javascript-fatigue-8ad3a54f2eb1#.3ri3a1fdv\" target=\"_blank\" rel=\"noopener\">this good article</a> describes another one, especially for newcomer web developers. Sorry, that I haven‚Äôt justified all choices. Actually, as you probably know, if you‚Äôre a developer, subjective views like these often aren‚Äôt even based on pure rational considerations, but are rather emotional and spontaneous.</p>\n<p>Please feel free to give me feedback on my tech stack of choice!</p>\n<p><strong>EDIT:</strong> Another framework I‚Äôd really like to try out is <a href=\"https://infernojs.org/\" target=\"_blank\" rel=\"noopener\">InfernoJS</a>, because it claims to be extremely lightweight and performant. However, before using Inferno, one should probably be familiar with React, since it uses very similar concepts and syntaxes.</p>\n<p><strong>EDIT 2:</strong> After having read <a href=\"http://www.florinpatan.ro/2016/10/why-you-should-not-use-iris-for-your-go.html\" target=\"_blank\" rel=\"noopener\">this article</a> and having done some further research on the Iris framework I really have to retract my above statement that I‚Äôd use it as a web backend. While it looks nice on paper, after diving a little deeper I really have to admit that it‚Äôd be morally tenable to support the authors of that project. So please forget about Iris and take a look at <a href=\"https://beego.me/\" target=\"_blank\" rel=\"noopener\">Beego</a> instead.</p>\n"},{"title":"OverGrive not starting on Ubuntu 18.04","date":"2018-06-18T05:37:13.000Z","_content":"# Problem\nFirst of all: I am really happy that it seems like I finally found a working Google Drive client for Linux, [overGrive](https://www.thefanclub.co.za/overgrive). It seems to be written in Python and, unfortunately is not open source, but rather costs $4.99. But that is fine as long as it actually works.\nHowever, I had some issues when trying to install overGrive for the first time. I am using Ubuntu 18.04, so I followed the [official installation instructions](https://www.thefanclub.co.za/overgrive/installation-instructions-ubuntu), including downloaing the `.deb` package and installing it with _dpkg_. I could see overGrive in Gnome's app launcher afterwards, but when I clicked it, nothing happened. It did not start without any error message.\n\n# Solution\nThe solution was the following. I figured out that overGrive is by default installed to `/opt/thefanclub/overgrive` and the binary can be run with Python (2.7), like `python /opt/thefanclub/overgrive/overgrive`. This gave me the error message that `No module named oauth2client.client`. After googling I found that I manually had to install these Python modules in addition.\n\n* `sudo pip install google-api-python-client`\n* `sudo pip install oauth2client`\n\nIf pip complains about further missing dependencies while installing one of the above packages, simply `pip install` them, too.\n\nGood luck! And again, thanks to [thefanclub](https://www.thefanclub.co.za/) for this Google Drive sync client.\n\n**EDIT:** After a few days I found that OverGrive did not work well at all, unfortunately. Syncs produced inconsistent states and the client \"logged out\" randomly after rebooting. Eventually, I set up a [NextCloud](https://nextcloud.com/) and migrated from Google Drive. I was already pleased with NextCloud a few months ago and only switched to GDrive because of Google Docs, but that's not worth the cr****y clients (the official Windows client is not quite better). NextCloud has a great web interface, sync clients for all major platform, a decent Android app and is open-source :-)","source":"_posts/overgrive-not-starting-on-ubuntu-18-04.md","raw":"---\ntitle: OverGrive not starting on Ubuntu 18.04\ndate: 2018-06-18 07:37:13\ntags:\n---\n# Problem\nFirst of all: I am really happy that it seems like I finally found a working Google Drive client for Linux, [overGrive](https://www.thefanclub.co.za/overgrive). It seems to be written in Python and, unfortunately is not open source, but rather costs $4.99. But that is fine as long as it actually works.\nHowever, I had some issues when trying to install overGrive for the first time. I am using Ubuntu 18.04, so I followed the [official installation instructions](https://www.thefanclub.co.za/overgrive/installation-instructions-ubuntu), including downloaing the `.deb` package and installing it with _dpkg_. I could see overGrive in Gnome's app launcher afterwards, but when I clicked it, nothing happened. It did not start without any error message.\n\n# Solution\nThe solution was the following. I figured out that overGrive is by default installed to `/opt/thefanclub/overgrive` and the binary can be run with Python (2.7), like `python /opt/thefanclub/overgrive/overgrive`. This gave me the error message that `No module named oauth2client.client`. After googling I found that I manually had to install these Python modules in addition.\n\n* `sudo pip install google-api-python-client`\n* `sudo pip install oauth2client`\n\nIf pip complains about further missing dependencies while installing one of the above packages, simply `pip install` them, too.\n\nGood luck! And again, thanks to [thefanclub](https://www.thefanclub.co.za/) for this Google Drive sync client.\n\n**EDIT:** After a few days I found that OverGrive did not work well at all, unfortunately. Syncs produced inconsistent states and the client \"logged out\" randomly after rebooting. Eventually, I set up a [NextCloud](https://nextcloud.com/) and migrated from Google Drive. I was already pleased with NextCloud a few months ago and only switched to GDrive because of Google Docs, but that's not worth the cr****y clients (the official Windows client is not quite better). NextCloud has a great web interface, sync clients for all major platform, a decent Android app and is open-source :-)","slug":"overgrive-not-starting-on-ubuntu-18-04","published":1,"updated":"2020-06-06T15:14:33.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhiy000v40mq8kjp1sfb","content":"<h1>Problem</h1>\n<p>First of all: I am really happy that it seems like I finally found a working Google Drive client for Linux, <a href=\"https://www.thefanclub.co.za/overgrive\" target=\"_blank\" rel=\"noopener\">overGrive</a>. It seems to be written in Python and, unfortunately is not open source, but rather costs $4.99. But that is fine as long as it actually works.<br>\nHowever, I had some issues when trying to install overGrive for the first time. I am using Ubuntu 18.04, so I followed the <a href=\"https://www.thefanclub.co.za/overgrive/installation-instructions-ubuntu\" target=\"_blank\" rel=\"noopener\">official installation instructions</a>, including downloaing the <code>.deb</code> package and installing it with <em>dpkg</em>. I could see overGrive in Gnome‚Äôs app launcher afterwards, but when I clicked it, nothing happened. It did not start without any error message.</p>\n<h1>Solution</h1>\n<p>The solution was the following. I figured out that overGrive is by default installed to <code>/opt/thefanclub/overgrive</code> and the binary can be run with Python (2.7), like <code>python /opt/thefanclub/overgrive/overgrive</code>. This gave me the error message that <code>No module named oauth2client.client</code>. After googling I found that I manually had to install these Python modules in addition.</p>\n<ul>\n<li><code>sudo pip install google-api-python-client</code></li>\n<li><code>sudo pip install oauth2client</code></li>\n</ul>\n<p>If pip complains about further missing dependencies while installing one of the above packages, simply <code>pip install</code> them, too.</p>\n<p>Good luck! And again, thanks to <a href=\"https://www.thefanclub.co.za/\" target=\"_blank\" rel=\"noopener\">thefanclub</a> for this Google Drive sync client.</p>\n<p><strong>EDIT:</strong> After a few days I found that OverGrive did not work well at all, unfortunately. Syncs produced inconsistent states and the client ‚Äúlogged out‚Äù randomly after rebooting. Eventually, I set up a <a href=\"https://nextcloud.com/\" target=\"_blank\" rel=\"noopener\">NextCloud</a> and migrated from Google Drive. I was already pleased with NextCloud a few months ago and only switched to GDrive because of Google Docs, but that‚Äôs not worth the cr****y clients (the official Windows client is not quite better). NextCloud has a great web interface, sync clients for all major platform, a decent Android app and is open-source :-)</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1>Problem</h1>\n<p>First of all: I am really happy that it seems like I finally found a working Google Drive client for Linux, <a href=\"https://www.thefanclub.co.za/overgrive\" target=\"_blank\" rel=\"noopener\">overGrive</a>. It seems to be written in Python and, unfortunately is not open source, but rather costs $4.99. But that is fine as long as it actually works.<br>\nHowever, I had some issues when trying to install overGrive for the first time. I am using Ubuntu 18.04, so I followed the <a href=\"https://www.thefanclub.co.za/overgrive/installation-instructions-ubuntu\" target=\"_blank\" rel=\"noopener\">official installation instructions</a>, including downloaing the <code>.deb</code> package and installing it with <em>dpkg</em>. I could see overGrive in Gnome‚Äôs app launcher afterwards, but when I clicked it, nothing happened. It did not start without any error message.</p>\n<h1>Solution</h1>\n<p>The solution was the following. I figured out that overGrive is by default installed to <code>/opt/thefanclub/overgrive</code> and the binary can be run with Python (2.7), like <code>python /opt/thefanclub/overgrive/overgrive</code>. This gave me the error message that <code>No module named oauth2client.client</code>. After googling I found that I manually had to install these Python modules in addition.</p>\n<ul>\n<li><code>sudo pip install google-api-python-client</code></li>\n<li><code>sudo pip install oauth2client</code></li>\n</ul>\n<p>If pip complains about further missing dependencies while installing one of the above packages, simply <code>pip install</code> them, too.</p>\n<p>Good luck! And again, thanks to <a href=\"https://www.thefanclub.co.za/\" target=\"_blank\" rel=\"noopener\">thefanclub</a> for this Google Drive sync client.</p>\n<p><strong>EDIT:</strong> After a few days I found that OverGrive did not work well at all, unfortunately. Syncs produced inconsistent states and the client ‚Äúlogged out‚Äù randomly after rebooting. Eventually, I set up a <a href=\"https://nextcloud.com/\" target=\"_blank\" rel=\"noopener\">NextCloud</a> and migrated from Google Drive. I was already pleased with NextCloud a few months ago and only switched to GDrive because of Google Docs, but that‚Äôs not worth the cr****y clients (the official Windows client is not quite better). NextCloud has a great web interface, sync clients for all major platform, a decent Android app and is open-source :-)</p>\n"},{"title":"My experiences with the Android Developer Nanodegree","date":"2019-01-26T08:01:50.000Z","_content":"\nLast year from March to August I participated in Udacity's [Android Developer Nanodegree](https://www.udacity.com/course/android-developer-nanodegree-by-google--nd801) program and here I want to share my experiences. \n\nOriginally, I got an ad on Facebook to apply for a scholarship offered by Google and Udacity for the Nanodegree program, which normally costs ~ 900 ‚Ç¨. I had done very few Android development before and although I did not particularly want to prepare for a career as an Android developer, I was definitely interested in learning Android more thoroughly. However, as we all know, it is very hard to find motivation to learn a new technology without having a real, serious project to build. Consequently, I applied for the scholarship and I was lucky (thanks to Google and Udacity for this opportunity!). \n\n# The Nanodegree\n## Time Management\nWhen participating in a Nanodegree program, you usually have six months to complete it. Due to classes at university, I only had time to start in my, so essentially I had three months to complete it. But since I was not a complete beginner, that was totally manageable.\n\nUdacity suggests a schedule for when to do which module of the course as well as soft deadlines for the finals projects at the end of every learning section. However, that is only a suggestion. The only hard deadline is the submission of the very final project. Personally, I decided to take two days in a week to fully focus on learning and coding for the Nanodegree. \n\n## Structure \nThis specific Nanodegree program consisted of five major sections.\n\n1. Developing Android Apps\n2. Advanced Android App Development\n3. Gradle for Android and Java\n4. Material Design for Android Developers\n6. Capstone Project\n\nEvery section consists of several **(1) video lessons**, in which Udacity developers explain concepts and do live coding. Between lessons, there are **(2) quizzes** to test your gained knowledge, however, the quizzes were usually quite easy and obvious. In addition to that, there are several **(3) coding tasks**, which require you to practically apply the newly learned concepts. Every coding task starts with an unfinished, small toy app and a list of TODOs you have to fulfill in order to finish it. The TODOs tell you quite precisely what to do, so sometimes it was not really a challenge. Also, you do not have to do the coding tasks at all, if you do not want to, because nobody checks your results. But obviously it makes sense to do them and it helps a lot for the **(4) project app** at the end of each section (sometimes there are more than one). For this project, you are told to implement an app with a certain functionality (e.g. a cooking recipe manager, a movie collection manager, an RSS reader, etc.). Usually it starts off with a raw scaffolded app skeleton which you have to finish - this time without specific TODOs or instructions. At the end, you submit your code either via a GitHub repository or as a ZIP file and a Udacity mentor will review your code and give you helpful feedback regarding functionality, design and code style. \n\nHere are three of my section-end project apps:\n\n1. **[popular-movies-android](https://github.com/muety/popular-movies-android):** App for displaying movie information fetched from an online movie database. Focus was on interacting with an external, third-party web API.\n2. **[baking-time-android](https://github.com/muety/baking-time-android):** App for showing baking recipes and instructions. Focus was on widgets, responsive design and integrating a video player.\n3. **[xyz-reader-android](https://github.com/muety/xyz-reader-android):** Basic reading app for text articles. Focus was on properly implementing Material Design, animations and UX.\n\n## Community\nProbably the best thing about the whole course was the community. There is an official Slack channel and a forum full of like-minded developers from literally all around the world who are going through the same experience like you. People ask questions, discuss about certain tasks or technology in general and you immediately feel extremely welcome. Whether you are unsure about a task or cannot get a certain error fixed, there are people who will help you. Also, a lot of Udacity mentors hang out on Slack and in the forum and provide support as well, e.g. in form of weekly AMA sessions. In addition to that, every participant of the course is assigned a personal mentor, who is a Udacity mentor that you can contact directly of you have questions. Actually, I did never contact mine, but I am sure they are willing to help.\n\n## Career Boost\nDespite the fact that having a Nanodegree looks quite good on your resum√© anyway, Udacity also provides a lot of support to help you building a successful career with your newly gained Android knowledge. They provide information and support for your application, review your CV and more.\n\n## The Capstone project\nAt the very end of the course, there is the so called **Capstone project** and this was the most fun part during the whole Nanodegree (and also the greatest effort). You are given the task to freely realize any app you like, given some requirements, e.g. that you use at least three third-party libraries, follow Material Design guidelines, provide a homescreen widget and a few more. \n\nThe final project consists of two parts. First, you have to submit a design proposal, which includes your app idea, some mock-ups and details on how you plan to implement it. After your design has been approved by Udacity mentors, you can start with phase 2, the actual implementation.\n\n![](images/cert.png)\n\n# QuizNerd\n![](images/qn_feature.png)\n\nAt that time I had a few coding interviews, so I came to the idea to implement a multi-player coding quiz game as a final project. Although that was probably a more comprehensive project than most of the others, I still wanted to do it, especially because it was an app I really wanted to have for myself, not only for the Nanodegree. \n\nI spent approximately two weeks of nearly full-time coding on that final project and finally came up with my app called QuizNerd. It is implemented in pure Android (using Java) without any structural frameworks (e.g. like [Dagger](http://square.github.io/dagger/)) and uses Google's [Firebase](https://firebase.google.com/) as a backend. More precisely I used Firebase Authentication for user management, [Firestore](https://firebase.google.com/docs/firestore/) as a real-time document database, FCM for notifications and and Firebase [Cloud Functions](https://firebase.google.com/docs/functions/) as a Serverless framework for backend-side logic. \n\nIf you are a developer who likes games like [QuizClash](https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite), I would love to have you try out QuizNerd! You can find it on the [Play Store](https://play.google.com/store/apps/details?id=com.github.n1try.quiznerd) and it has several hundred questions for Android, C++, C#, HTML, Java, JavaScript, PHP, Python and Swift. Feel free to share your feedback with me üôÇ.\n\n![](images/qn2.png)\n\n# Conclusion\nIt was fun! I learned a lot during the Nanodegree and I am kind of confident calling myself an Android developer now. Most of the concepts were explained very detailed. Fore instance one of my favorite chapters was the one about Gradle, where they precisely explained how Gradle works, how to write own Gradle tasks and how to apply that to Android.\n\nIf you keep on motivating yourself to work through the lessons and especially through the final project, it is going to pay off. Moreover, in addition to precious programming knowledge, you also get to know a lot of interesting people from the community all over the world. \n\nHowever, there is two things I would want to criticize. Firstly, nowadays Java is rapidly becoming less popular for Android development, while Kotlin is considered the future. Many professional developers who I have talked to claim that it does not make a lot of sense today to still start a new Android project with Java instead of Kotlin, so I wish the Nandogree was based on Kotlin in order to be even more future-proof. Also, de-facto standard frameworks like Dagger were not mentioned throughout the course, while (in my opinion) less useful things like homescreen widget had been pushed by Udacity. Maybe that is going to change in newer versions of the course.\n\nThe second thing is that, as I mentioned earlier, the TODO-tasks in the final projects of every section were too specific and too fine-grained. Sometimes I found myself just stupidly doing exactly what the TODOs wanted me to do instead of trying to capture a more high-level picture and solve design problems by myself. \n\nThat being said, I would recommend the Udacity Nanodegree to everyone who is interested in becoming an Android developer. Have fun!\n\n# Links\n* [QuizNerd on Google Play](https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite)\n* [Collection of Nanodegree 2018 final projects](https://google-udacity-scholars18.github.io/and/)","source":"_posts/quiznerd-my-experiences-with-the-android-developer-nanodegree.md","raw":"---\ntitle: My experiences with the Android Developer Nanodegree\ndate: 2019-01-26 09:01:50\ntags:\n---\n\nLast year from March to August I participated in Udacity's [Android Developer Nanodegree](https://www.udacity.com/course/android-developer-nanodegree-by-google--nd801) program and here I want to share my experiences. \n\nOriginally, I got an ad on Facebook to apply for a scholarship offered by Google and Udacity for the Nanodegree program, which normally costs ~ 900 ‚Ç¨. I had done very few Android development before and although I did not particularly want to prepare for a career as an Android developer, I was definitely interested in learning Android more thoroughly. However, as we all know, it is very hard to find motivation to learn a new technology without having a real, serious project to build. Consequently, I applied for the scholarship and I was lucky (thanks to Google and Udacity for this opportunity!). \n\n# The Nanodegree\n## Time Management\nWhen participating in a Nanodegree program, you usually have six months to complete it. Due to classes at university, I only had time to start in my, so essentially I had three months to complete it. But since I was not a complete beginner, that was totally manageable.\n\nUdacity suggests a schedule for when to do which module of the course as well as soft deadlines for the finals projects at the end of every learning section. However, that is only a suggestion. The only hard deadline is the submission of the very final project. Personally, I decided to take two days in a week to fully focus on learning and coding for the Nanodegree. \n\n## Structure \nThis specific Nanodegree program consisted of five major sections.\n\n1. Developing Android Apps\n2. Advanced Android App Development\n3. Gradle for Android and Java\n4. Material Design for Android Developers\n6. Capstone Project\n\nEvery section consists of several **(1) video lessons**, in which Udacity developers explain concepts and do live coding. Between lessons, there are **(2) quizzes** to test your gained knowledge, however, the quizzes were usually quite easy and obvious. In addition to that, there are several **(3) coding tasks**, which require you to practically apply the newly learned concepts. Every coding task starts with an unfinished, small toy app and a list of TODOs you have to fulfill in order to finish it. The TODOs tell you quite precisely what to do, so sometimes it was not really a challenge. Also, you do not have to do the coding tasks at all, if you do not want to, because nobody checks your results. But obviously it makes sense to do them and it helps a lot for the **(4) project app** at the end of each section (sometimes there are more than one). For this project, you are told to implement an app with a certain functionality (e.g. a cooking recipe manager, a movie collection manager, an RSS reader, etc.). Usually it starts off with a raw scaffolded app skeleton which you have to finish - this time without specific TODOs or instructions. At the end, you submit your code either via a GitHub repository or as a ZIP file and a Udacity mentor will review your code and give you helpful feedback regarding functionality, design and code style. \n\nHere are three of my section-end project apps:\n\n1. **[popular-movies-android](https://github.com/muety/popular-movies-android):** App for displaying movie information fetched from an online movie database. Focus was on interacting with an external, third-party web API.\n2. **[baking-time-android](https://github.com/muety/baking-time-android):** App for showing baking recipes and instructions. Focus was on widgets, responsive design and integrating a video player.\n3. **[xyz-reader-android](https://github.com/muety/xyz-reader-android):** Basic reading app for text articles. Focus was on properly implementing Material Design, animations and UX.\n\n## Community\nProbably the best thing about the whole course was the community. There is an official Slack channel and a forum full of like-minded developers from literally all around the world who are going through the same experience like you. People ask questions, discuss about certain tasks or technology in general and you immediately feel extremely welcome. Whether you are unsure about a task or cannot get a certain error fixed, there are people who will help you. Also, a lot of Udacity mentors hang out on Slack and in the forum and provide support as well, e.g. in form of weekly AMA sessions. In addition to that, every participant of the course is assigned a personal mentor, who is a Udacity mentor that you can contact directly of you have questions. Actually, I did never contact mine, but I am sure they are willing to help.\n\n## Career Boost\nDespite the fact that having a Nanodegree looks quite good on your resum√© anyway, Udacity also provides a lot of support to help you building a successful career with your newly gained Android knowledge. They provide information and support for your application, review your CV and more.\n\n## The Capstone project\nAt the very end of the course, there is the so called **Capstone project** and this was the most fun part during the whole Nanodegree (and also the greatest effort). You are given the task to freely realize any app you like, given some requirements, e.g. that you use at least three third-party libraries, follow Material Design guidelines, provide a homescreen widget and a few more. \n\nThe final project consists of two parts. First, you have to submit a design proposal, which includes your app idea, some mock-ups and details on how you plan to implement it. After your design has been approved by Udacity mentors, you can start with phase 2, the actual implementation.\n\n![](images/cert.png)\n\n# QuizNerd\n![](images/qn_feature.png)\n\nAt that time I had a few coding interviews, so I came to the idea to implement a multi-player coding quiz game as a final project. Although that was probably a more comprehensive project than most of the others, I still wanted to do it, especially because it was an app I really wanted to have for myself, not only for the Nanodegree. \n\nI spent approximately two weeks of nearly full-time coding on that final project and finally came up with my app called QuizNerd. It is implemented in pure Android (using Java) without any structural frameworks (e.g. like [Dagger](http://square.github.io/dagger/)) and uses Google's [Firebase](https://firebase.google.com/) as a backend. More precisely I used Firebase Authentication for user management, [Firestore](https://firebase.google.com/docs/firestore/) as a real-time document database, FCM for notifications and and Firebase [Cloud Functions](https://firebase.google.com/docs/functions/) as a Serverless framework for backend-side logic. \n\nIf you are a developer who likes games like [QuizClash](https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite), I would love to have you try out QuizNerd! You can find it on the [Play Store](https://play.google.com/store/apps/details?id=com.github.n1try.quiznerd) and it has several hundred questions for Android, C++, C#, HTML, Java, JavaScript, PHP, Python and Swift. Feel free to share your feedback with me üôÇ.\n\n![](images/qn2.png)\n\n# Conclusion\nIt was fun! I learned a lot during the Nanodegree and I am kind of confident calling myself an Android developer now. Most of the concepts were explained very detailed. Fore instance one of my favorite chapters was the one about Gradle, where they precisely explained how Gradle works, how to write own Gradle tasks and how to apply that to Android.\n\nIf you keep on motivating yourself to work through the lessons and especially through the final project, it is going to pay off. Moreover, in addition to precious programming knowledge, you also get to know a lot of interesting people from the community all over the world. \n\nHowever, there is two things I would want to criticize. Firstly, nowadays Java is rapidly becoming less popular for Android development, while Kotlin is considered the future. Many professional developers who I have talked to claim that it does not make a lot of sense today to still start a new Android project with Java instead of Kotlin, so I wish the Nandogree was based on Kotlin in order to be even more future-proof. Also, de-facto standard frameworks like Dagger were not mentioned throughout the course, while (in my opinion) less useful things like homescreen widget had been pushed by Udacity. Maybe that is going to change in newer versions of the course.\n\nThe second thing is that, as I mentioned earlier, the TODO-tasks in the final projects of every section were too specific and too fine-grained. Sometimes I found myself just stupidly doing exactly what the TODOs wanted me to do instead of trying to capture a more high-level picture and solve design problems by myself. \n\nThat being said, I would recommend the Udacity Nanodegree to everyone who is interested in becoming an Android developer. Have fun!\n\n# Links\n* [QuizNerd on Google Play](https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite)\n* [Collection of Nanodegree 2018 final projects](https://google-udacity-scholars18.github.io/and/)","slug":"quiznerd-my-experiences-with-the-android-developer-nanodegree","published":1,"updated":"2020-06-06T15:14:33.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhiz000w40mqlma1ebrn","content":"<p>Last year from March to August I participated in Udacity‚Äôs <a href=\"https://www.udacity.com/course/android-developer-nanodegree-by-google--nd801\" target=\"_blank\" rel=\"noopener\">Android Developer Nanodegree</a> program and here I want to share my experiences.</p>\n<p>Originally, I got an ad on Facebook to apply for a scholarship offered by Google and Udacity for the Nanodegree program, which normally costs ~ 900 ‚Ç¨. I had done very few Android development before and although I did not particularly want to prepare for a career as an Android developer, I was definitely interested in learning Android more thoroughly. However, as we all know, it is very hard to find motivation to learn a new technology without having a real, serious project to build. Consequently, I applied for the scholarship and I was lucky (thanks to Google and Udacity for this opportunity!).</p>\n<h1>The Nanodegree</h1>\n<h2 id=\"Time-Management\">Time Management</h2>\n<p>When participating in a Nanodegree program, you usually have six months to complete it. Due to classes at university, I only had time to start in my, so essentially I had three months to complete it. But since I was not a complete beginner, that was totally manageable.</p>\n<p>Udacity suggests a schedule for when to do which module of the course as well as soft deadlines for the finals projects at the end of every learning section. However, that is only a suggestion. The only hard deadline is the submission of the very final project. Personally, I decided to take two days in a week to fully focus on learning and coding for the Nanodegree.</p>\n<h2 id=\"Structure\">Structure</h2>\n<p>This specific Nanodegree program consisted of five major sections.</p>\n<ol>\n<li>Developing Android Apps</li>\n<li>Advanced Android App Development</li>\n<li>Gradle for Android and Java</li>\n<li>Material Design for Android Developers</li>\n<li>Capstone Project</li>\n</ol>\n<p>Every section consists of several <strong>(1) video lessons</strong>, in which Udacity developers explain concepts and do live coding. Between lessons, there are <strong>(2) quizzes</strong> to test your gained knowledge, however, the quizzes were usually quite easy and obvious. In addition to that, there are several <strong>(3) coding tasks</strong>, which require you to practically apply the newly learned concepts. Every coding task starts with an unfinished, small toy app and a list of TODOs you have to fulfill in order to finish it. The TODOs tell you quite precisely what to do, so sometimes it was not really a challenge. Also, you do not have to do the coding tasks at all, if you do not want to, because nobody checks your results. But obviously it makes sense to do them and it helps a lot for the <strong>(4) project app</strong> at the end of each section (sometimes there are more than one). For this project, you are told to implement an app with a certain functionality (e.g. a cooking recipe manager, a movie collection manager, an RSS reader, etc.). Usually it starts off with a raw scaffolded app skeleton which you have to finish - this time without specific TODOs or instructions. At the end, you submit your code either via a GitHub repository or as a ZIP file and a Udacity mentor will review your code and give you helpful feedback regarding functionality, design and code style.</p>\n<p>Here are three of my section-end project apps:</p>\n<ol>\n<li><strong><a href=\"https://github.com/muety/popular-movies-android\" target=\"_blank\" rel=\"noopener\">popular-movies-android</a>:</strong> App for displaying movie information fetched from an online movie database. Focus was on interacting with an external, third-party web API.</li>\n<li><strong><a href=\"https://github.com/muety/baking-time-android\" target=\"_blank\" rel=\"noopener\">baking-time-android</a>:</strong> App for showing baking recipes and instructions. Focus was on widgets, responsive design and integrating a video player.</li>\n<li><strong><a href=\"https://github.com/muety/xyz-reader-android\" target=\"_blank\" rel=\"noopener\">xyz-reader-android</a>:</strong> Basic reading app for text articles. Focus was on properly implementing Material Design, animations and UX.</li>\n</ol>\n<h2 id=\"Community\">Community</h2>\n<p>Probably the best thing about the whole course was the community. There is an official Slack channel and a forum full of like-minded developers from literally all around the world who are going through the same experience like you. People ask questions, discuss about certain tasks or technology in general and you immediately feel extremely welcome. Whether you are unsure about a task or cannot get a certain error fixed, there are people who will help you. Also, a lot of Udacity mentors hang out on Slack and in the forum and provide support as well, e.g. in form of weekly AMA sessions. In addition to that, every participant of the course is assigned a personal mentor, who is a Udacity mentor that you can contact directly of you have questions. Actually, I did never contact mine, but I am sure they are willing to help.</p>\n<h2 id=\"Career-Boost\">Career Boost</h2>\n<p>Despite the fact that having a Nanodegree looks quite good on your resum√© anyway, Udacity also provides a lot of support to help you building a successful career with your newly gained Android knowledge. They provide information and support for your application, review your CV and more.</p>\n<h2 id=\"The-Capstone-project\">The Capstone project</h2>\n<p>At the very end of the course, there is the so called <strong>Capstone project</strong> and this was the most fun part during the whole Nanodegree (and also the greatest effort). You are given the task to freely realize any app you like, given some requirements, e.g. that you use at least three third-party libraries, follow Material Design guidelines, provide a homescreen widget and a few more.</p>\n<p>The final project consists of two parts. First, you have to submit a design proposal, which includes your app idea, some mock-ups and details on how you plan to implement it. After your design has been approved by Udacity mentors, you can start with phase 2, the actual implementation.</p>\n<p><img src=\"images/cert.png\" alt></p>\n<h1>QuizNerd</h1>\n<p><img src=\"images/qn_feature.png\" alt></p>\n<p>At that time I had a few coding interviews, so I came to the idea to implement a multi-player coding quiz game as a final project. Although that was probably a more comprehensive project than most of the others, I still wanted to do it, especially because it was an app I really wanted to have for myself, not only for the Nanodegree.</p>\n<p>I spent approximately two weeks of nearly full-time coding on that final project and finally came up with my app called QuizNerd. It is implemented in pure Android (using Java) without any structural frameworks (e.g. like <a href=\"http://square.github.io/dagger/\" target=\"_blank\" rel=\"noopener\">Dagger</a>) and uses Google‚Äôs <a href=\"https://firebase.google.com/\" target=\"_blank\" rel=\"noopener\">Firebase</a> as a backend. More precisely I used Firebase Authentication for user management, <a href=\"https://firebase.google.com/docs/firestore/\" target=\"_blank\" rel=\"noopener\">Firestore</a> as a real-time document database, FCM for notifications and and Firebase <a href=\"https://firebase.google.com/docs/functions/\" target=\"_blank\" rel=\"noopener\">Cloud Functions</a> as a Serverless framework for backend-side logic.</p>\n<p>If you are a developer who likes games like <a href=\"https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite\" target=\"_blank\" rel=\"noopener\">QuizClash</a>, I would love to have you try out QuizNerd! You can find it on the <a href=\"https://play.google.com/store/apps/details?id=com.github.n1try.quiznerd\" target=\"_blank\" rel=\"noopener\">Play Store</a> and it has several hundred questions for Android, C++, C#, HTML, Java, JavaScript, PHP, Python and Swift. Feel free to share your feedback with me üôÇ.</p>\n<p><img src=\"images/qn2.png\" alt></p>\n<h1>Conclusion</h1>\n<p>It was fun! I learned a lot during the Nanodegree and I am kind of confident calling myself an Android developer now. Most of the concepts were explained very detailed. Fore instance one of my favorite chapters was the one about Gradle, where they precisely explained how Gradle works, how to write own Gradle tasks and how to apply that to Android.</p>\n<p>If you keep on motivating yourself to work through the lessons and especially through the final project, it is going to pay off. Moreover, in addition to precious programming knowledge, you also get to know a lot of interesting people from the community all over the world.</p>\n<p>However, there is two things I would want to criticize. Firstly, nowadays Java is rapidly becoming less popular for Android development, while Kotlin is considered the future. Many professional developers who I have talked to claim that it does not make a lot of sense today to still start a new Android project with Java instead of Kotlin, so I wish the Nandogree was based on Kotlin in order to be even more future-proof. Also, de-facto standard frameworks like Dagger were not mentioned throughout the course, while (in my opinion) less useful things like homescreen widget had been pushed by Udacity. Maybe that is going to change in newer versions of the course.</p>\n<p>The second thing is that, as I mentioned earlier, the TODO-tasks in the final projects of every section were too specific and too fine-grained. Sometimes I found myself just stupidly doing exactly what the TODOs wanted me to do instead of trying to capture a more high-level picture and solve design problems by myself.</p>\n<p>That being said, I would recommend the Udacity Nanodegree to everyone who is interested in becoming an Android developer. Have fun!</p>\n<h1>Links</h1>\n<ul>\n<li><a href=\"https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite\" target=\"_blank\" rel=\"noopener\">QuizNerd on Google Play</a></li>\n<li><a href=\"https://google-udacity-scholars18.github.io/and/\" target=\"_blank\" rel=\"noopener\">Collection of Nanodegree 2018 final projects</a></li>\n</ul>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Last year from March to August I participated in Udacity‚Äôs <a href=\"https://www.udacity.com/course/android-developer-nanodegree-by-google--nd801\" target=\"_blank\" rel=\"noopener\">Android Developer Nanodegree</a> program and here I want to share my experiences.</p>\n<p>Originally, I got an ad on Facebook to apply for a scholarship offered by Google and Udacity for the Nanodegree program, which normally costs ~ 900 ‚Ç¨. I had done very few Android development before and although I did not particularly want to prepare for a career as an Android developer, I was definitely interested in learning Android more thoroughly. However, as we all know, it is very hard to find motivation to learn a new technology without having a real, serious project to build. Consequently, I applied for the scholarship and I was lucky (thanks to Google and Udacity for this opportunity!).</p>\n<h1>The Nanodegree</h1>\n<h2 id=\"Time-Management\">Time Management</h2>\n<p>When participating in a Nanodegree program, you usually have six months to complete it. Due to classes at university, I only had time to start in my, so essentially I had three months to complete it. But since I was not a complete beginner, that was totally manageable.</p>\n<p>Udacity suggests a schedule for when to do which module of the course as well as soft deadlines for the finals projects at the end of every learning section. However, that is only a suggestion. The only hard deadline is the submission of the very final project. Personally, I decided to take two days in a week to fully focus on learning and coding for the Nanodegree.</p>\n<h2 id=\"Structure\">Structure</h2>\n<p>This specific Nanodegree program consisted of five major sections.</p>\n<ol>\n<li>Developing Android Apps</li>\n<li>Advanced Android App Development</li>\n<li>Gradle for Android and Java</li>\n<li>Material Design for Android Developers</li>\n<li>Capstone Project</li>\n</ol>\n<p>Every section consists of several <strong>(1) video lessons</strong>, in which Udacity developers explain concepts and do live coding. Between lessons, there are <strong>(2) quizzes</strong> to test your gained knowledge, however, the quizzes were usually quite easy and obvious. In addition to that, there are several <strong>(3) coding tasks</strong>, which require you to practically apply the newly learned concepts. Every coding task starts with an unfinished, small toy app and a list of TODOs you have to fulfill in order to finish it. The TODOs tell you quite precisely what to do, so sometimes it was not really a challenge. Also, you do not have to do the coding tasks at all, if you do not want to, because nobody checks your results. But obviously it makes sense to do them and it helps a lot for the <strong>(4) project app</strong> at the end of each section (sometimes there are more than one). For this project, you are told to implement an app with a certain functionality (e.g. a cooking recipe manager, a movie collection manager, an RSS reader, etc.). Usually it starts off with a raw scaffolded app skeleton which you have to finish - this time without specific TODOs or instructions. At the end, you submit your code either via a GitHub repository or as a ZIP file and a Udacity mentor will review your code and give you helpful feedback regarding functionality, design and code style.</p>\n<p>Here are three of my section-end project apps:</p>\n<ol>\n<li><strong><a href=\"https://github.com/muety/popular-movies-android\" target=\"_blank\" rel=\"noopener\">popular-movies-android</a>:</strong> App for displaying movie information fetched from an online movie database. Focus was on interacting with an external, third-party web API.</li>\n<li><strong><a href=\"https://github.com/muety/baking-time-android\" target=\"_blank\" rel=\"noopener\">baking-time-android</a>:</strong> App for showing baking recipes and instructions. Focus was on widgets, responsive design and integrating a video player.</li>\n<li><strong><a href=\"https://github.com/muety/xyz-reader-android\" target=\"_blank\" rel=\"noopener\">xyz-reader-android</a>:</strong> Basic reading app for text articles. Focus was on properly implementing Material Design, animations and UX.</li>\n</ol>\n<h2 id=\"Community\">Community</h2>\n<p>Probably the best thing about the whole course was the community. There is an official Slack channel and a forum full of like-minded developers from literally all around the world who are going through the same experience like you. People ask questions, discuss about certain tasks or technology in general and you immediately feel extremely welcome. Whether you are unsure about a task or cannot get a certain error fixed, there are people who will help you. Also, a lot of Udacity mentors hang out on Slack and in the forum and provide support as well, e.g. in form of weekly AMA sessions. In addition to that, every participant of the course is assigned a personal mentor, who is a Udacity mentor that you can contact directly of you have questions. Actually, I did never contact mine, but I am sure they are willing to help.</p>\n<h2 id=\"Career-Boost\">Career Boost</h2>\n<p>Despite the fact that having a Nanodegree looks quite good on your resum√© anyway, Udacity also provides a lot of support to help you building a successful career with your newly gained Android knowledge. They provide information and support for your application, review your CV and more.</p>\n<h2 id=\"The-Capstone-project\">The Capstone project</h2>\n<p>At the very end of the course, there is the so called <strong>Capstone project</strong> and this was the most fun part during the whole Nanodegree (and also the greatest effort). You are given the task to freely realize any app you like, given some requirements, e.g. that you use at least three third-party libraries, follow Material Design guidelines, provide a homescreen widget and a few more.</p>\n<p>The final project consists of two parts. First, you have to submit a design proposal, which includes your app idea, some mock-ups and details on how you plan to implement it. After your design has been approved by Udacity mentors, you can start with phase 2, the actual implementation.</p>\n<p><img src=\"images/cert.png\" alt></p>\n<h1>QuizNerd</h1>\n<p><img src=\"images/qn_feature.png\" alt></p>\n<p>At that time I had a few coding interviews, so I came to the idea to implement a multi-player coding quiz game as a final project. Although that was probably a more comprehensive project than most of the others, I still wanted to do it, especially because it was an app I really wanted to have for myself, not only for the Nanodegree.</p>\n<p>I spent approximately two weeks of nearly full-time coding on that final project and finally came up with my app called QuizNerd. It is implemented in pure Android (using Java) without any structural frameworks (e.g. like <a href=\"http://square.github.io/dagger/\" target=\"_blank\" rel=\"noopener\">Dagger</a>) and uses Google‚Äôs <a href=\"https://firebase.google.com/\" target=\"_blank\" rel=\"noopener\">Firebase</a> as a backend. More precisely I used Firebase Authentication for user management, <a href=\"https://firebase.google.com/docs/firestore/\" target=\"_blank\" rel=\"noopener\">Firestore</a> as a real-time document database, FCM for notifications and and Firebase <a href=\"https://firebase.google.com/docs/functions/\" target=\"_blank\" rel=\"noopener\">Cloud Functions</a> as a Serverless framework for backend-side logic.</p>\n<p>If you are a developer who likes games like <a href=\"https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite\" target=\"_blank\" rel=\"noopener\">QuizClash</a>, I would love to have you try out QuizNerd! You can find it on the <a href=\"https://play.google.com/store/apps/details?id=com.github.n1try.quiznerd\" target=\"_blank\" rel=\"noopener\">Play Store</a> and it has several hundred questions for Android, C++, C#, HTML, Java, JavaScript, PHP, Python and Swift. Feel free to share your feedback with me üôÇ.</p>\n<p><img src=\"images/qn2.png\" alt></p>\n<h1>Conclusion</h1>\n<p>It was fun! I learned a lot during the Nanodegree and I am kind of confident calling myself an Android developer now. Most of the concepts were explained very detailed. Fore instance one of my favorite chapters was the one about Gradle, where they precisely explained how Gradle works, how to write own Gradle tasks and how to apply that to Android.</p>\n<p>If you keep on motivating yourself to work through the lessons and especially through the final project, it is going to pay off. Moreover, in addition to precious programming knowledge, you also get to know a lot of interesting people from the community all over the world.</p>\n<p>However, there is two things I would want to criticize. Firstly, nowadays Java is rapidly becoming less popular for Android development, while Kotlin is considered the future. Many professional developers who I have talked to claim that it does not make a lot of sense today to still start a new Android project with Java instead of Kotlin, so I wish the Nandogree was based on Kotlin in order to be even more future-proof. Also, de-facto standard frameworks like Dagger were not mentioned throughout the course, while (in my opinion) less useful things like homescreen widget had been pushed by Udacity. Maybe that is going to change in newer versions of the course.</p>\n<p>The second thing is that, as I mentioned earlier, the TODO-tasks in the final projects of every section were too specific and too fine-grained. Sometimes I found myself just stupidly doing exactly what the TODOs wanted me to do instead of trying to capture a more high-level picture and solve design problems by myself.</p>\n<p>That being said, I would recommend the Udacity Nanodegree to everyone who is interested in becoming an Android developer. Have fun!</p>\n<h1>Links</h1>\n<ul>\n<li><a href=\"https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite\" target=\"_blank\" rel=\"noopener\">QuizNerd on Google Play</a></li>\n<li><a href=\"https://google-udacity-scholars18.github.io/and/\" target=\"_blank\" rel=\"noopener\">Collection of Nanodegree 2018 final projects</a></li>\n</ul>\n"},{"title":"Telegram bot example code in Node.js","date":"2015-12-01T21:42:05.000Z","_content":"\nAs a response to my [latest article](how-to-make-telegram-bots.html) on how to make Telegram bots, I got many requests from guys who had problems with actually getting such a bot set up with my tutorial and they asked for a complete, working code example. So I made one. It is limited to the very basics, but it works and may serve as inspiration for you. What most of you would probably like to have further in your bots, but what is not covered in my example, is a database connection. Because in most cases your bot will probably need to persist data. To realize this you should have a little rather advanced skills in Node as well as basic knowledge of the database system of your choice, which is MongoDB with Mongoose Node module for me.\n\nAlright, here‚Äôs my sample bot: [http://github.com/muety/telegram-bot-node-sample](http://github.com/muety/telegram-bot-node-sample/)\n\nAlso check out my _@FavoriteBot_.  \nHave fun and good luck...","source":"_posts/telegram-bot-example-code-in-nodejs.md","raw":"---\ntitle: Telegram bot example code in Node.js\ndate: 2015-12-01 22:42:05\ntags:\n---\n\nAs a response to my [latest article](how-to-make-telegram-bots.html) on how to make Telegram bots, I got many requests from guys who had problems with actually getting such a bot set up with my tutorial and they asked for a complete, working code example. So I made one. It is limited to the very basics, but it works and may serve as inspiration for you. What most of you would probably like to have further in your bots, but what is not covered in my example, is a database connection. Because in most cases your bot will probably need to persist data. To realize this you should have a little rather advanced skills in Node as well as basic knowledge of the database system of your choice, which is MongoDB with Mongoose Node module for me.\n\nAlright, here‚Äôs my sample bot: [http://github.com/muety/telegram-bot-node-sample](http://github.com/muety/telegram-bot-node-sample/)\n\nAlso check out my _@FavoriteBot_.  \nHave fun and good luck...","slug":"telegram-bot-example-code-in-nodejs","published":1,"updated":"2020-06-06T15:14:33.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhj0000x40mquc0hbxqz","content":"<p>As a response to my <a href=\"how-to-make-telegram-bots.html\">latest article</a> on how to make Telegram bots, I got many requests from guys who had problems with actually getting such a bot set up with my tutorial and they asked for a complete, working code example. So I made one. It is limited to the very basics, but it works and may serve as inspiration for you. What most of you would probably like to have further in your bots, but what is not covered in my example, is a database connection. Because in most cases your bot will probably need to persist data. To realize this you should have a little rather advanced skills in Node as well as basic knowledge of the database system of your choice, which is MongoDB with Mongoose Node module for me.</p>\n<p>Alright, here‚Äôs my sample bot: <a href=\"http://github.com/muety/telegram-bot-node-sample/\" target=\"_blank\" rel=\"noopener\">http://github.com/muety/telegram-bot-node-sample</a></p>\n<p>Also check out my <em>@FavoriteBot</em>.<br>\nHave fun and good luck‚Ä¶</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>As a response to my <a href=\"how-to-make-telegram-bots.html\">latest article</a> on how to make Telegram bots, I got many requests from guys who had problems with actually getting such a bot set up with my tutorial and they asked for a complete, working code example. So I made one. It is limited to the very basics, but it works and may serve as inspiration for you. What most of you would probably like to have further in your bots, but what is not covered in my example, is a database connection. Because in most cases your bot will probably need to persist data. To realize this you should have a little rather advanced skills in Node as well as basic knowledge of the database system of your choice, which is MongoDB with Mongoose Node module for me.</p>\n<p>Alright, here‚Äôs my sample bot: <a href=\"http://github.com/muety/telegram-bot-node-sample/\" target=\"_blank\" rel=\"noopener\">http://github.com/muety/telegram-bot-node-sample</a></p>\n<p>Also check out my <em>@FavoriteBot</em>.<br>\nHave fun and good luck‚Ä¶</p>\n"},{"title":"Telegram: ExpenseBot & DoodlerBot","date":"2016-05-08T20:59:33.000Z","_content":"\n\nIn 2015, the [Telegram](https://telegram.org) messenger announced their [Bots](https://core.telegram.org/bots). Basically they are pieces of software that act like a normal chat user in many ways. They could have any functionality, from being helpful at daily tasks to even simple games or trivias ‚Äì all within an ordinary Telegram chat. You send them message, they give answers ‚Äì some more and some less intelligent. Recently, also other companies ‚Äì like [Facebook](http://techcrunch.com/2016/04/07/facebook-chatbots/) or [Microsoft](https://dev.botframework.com/) ‚Äì announced such bots for their messaging apps. Sometimes bots are even considered kind of the next step after native (web-) applications in the future.\n\nFrom a developer‚Äôs perspective, making a bot is fun, because there are almost no restrictions on how to develop your bot. All communication with Telegram works by requesting a single REST API provided by them. Choices like which programming language and -framework to use and how to structure the code are completely up to the developer. A Telegram bot can theoretically be built in any programming language. The only requirements are to be able to make HTTP request from the application and to have a server to host the bot on.\n\nI‚Äôve recently created two bots for Telegram that should each help with a daily task.\n\n### ExpenseBot ‚Äì Keep track of your finances\n\n![1461614801_Money-Increase](/images/expensebot_icon.png)\n\nThis bot‚Äôs purpose is to help people manage their daily expenses and keep track of their financial situation. Users can add expenses from wherever they are using a few simple commands from within the chat and have an eye on how much they have spent in a month or a day. This obviates the need for confusing Excel spreadsheets or paper notes. You can reach the bot by sending a Message to *[@ExpenseBot](https://telegram.me/ExpenseBot)* in Telegram.\n\n### DoodlerBot ‚Äì Coordinate group appointments\n\n![1462726473_calendar](/images/doodlerbot_icon.png)\n\nMy second bot helps users coordinate a group of people and find the right date for a common appointment, just like you might know from [doodle.com](http://doodle.com) (even though it doesn‚Äôt have anything to do with that commercial service, except for fulfilling the same need). Open a new doodle and let your mates in the group chat vote for their preferred date to finally make the best decision for everyone. You can reach the bot by sending a Message to *[@DoodlerBot](https://telegram.me/DoodlerBot)* in Telegram.\n\nBoth projects are completely independent, non-commercial and privately operated. If you have any questions our found a bug (both bots are still in beta phase and therefore might show some unexpected behavior), please contact me at @n1try or via e-mail. In both cases, you should first read a basic introduction on how to use the respective bot, by sending a message with */help* to it.\n\nIn case you like my bots, I‚Äôd be really happy if you rated them at [https://storebot.me/bot/expensebot](https://storebot.me/bot/expensebot) and [https://storebot.me/bot/doodlerbot](https://storebot.me/bot/doodlerbot). Have fun!","source":"_posts/telegram-expensebot-doodlerbot.md","raw":"---\ntitle: 'Telegram: ExpenseBot & DoodlerBot'\ndate: 2016-05-08 22:59:33\ntags:\n---\n\n\nIn 2015, the [Telegram](https://telegram.org) messenger announced their [Bots](https://core.telegram.org/bots). Basically they are pieces of software that act like a normal chat user in many ways. They could have any functionality, from being helpful at daily tasks to even simple games or trivias ‚Äì all within an ordinary Telegram chat. You send them message, they give answers ‚Äì some more and some less intelligent. Recently, also other companies ‚Äì like [Facebook](http://techcrunch.com/2016/04/07/facebook-chatbots/) or [Microsoft](https://dev.botframework.com/) ‚Äì announced such bots for their messaging apps. Sometimes bots are even considered kind of the next step after native (web-) applications in the future.\n\nFrom a developer‚Äôs perspective, making a bot is fun, because there are almost no restrictions on how to develop your bot. All communication with Telegram works by requesting a single REST API provided by them. Choices like which programming language and -framework to use and how to structure the code are completely up to the developer. A Telegram bot can theoretically be built in any programming language. The only requirements are to be able to make HTTP request from the application and to have a server to host the bot on.\n\nI‚Äôve recently created two bots for Telegram that should each help with a daily task.\n\n### ExpenseBot ‚Äì Keep track of your finances\n\n![1461614801_Money-Increase](/images/expensebot_icon.png)\n\nThis bot‚Äôs purpose is to help people manage their daily expenses and keep track of their financial situation. Users can add expenses from wherever they are using a few simple commands from within the chat and have an eye on how much they have spent in a month or a day. This obviates the need for confusing Excel spreadsheets or paper notes. You can reach the bot by sending a Message to *[@ExpenseBot](https://telegram.me/ExpenseBot)* in Telegram.\n\n### DoodlerBot ‚Äì Coordinate group appointments\n\n![1462726473_calendar](/images/doodlerbot_icon.png)\n\nMy second bot helps users coordinate a group of people and find the right date for a common appointment, just like you might know from [doodle.com](http://doodle.com) (even though it doesn‚Äôt have anything to do with that commercial service, except for fulfilling the same need). Open a new doodle and let your mates in the group chat vote for their preferred date to finally make the best decision for everyone. You can reach the bot by sending a Message to *[@DoodlerBot](https://telegram.me/DoodlerBot)* in Telegram.\n\nBoth projects are completely independent, non-commercial and privately operated. If you have any questions our found a bug (both bots are still in beta phase and therefore might show some unexpected behavior), please contact me at @n1try or via e-mail. In both cases, you should first read a basic introduction on how to use the respective bot, by sending a message with */help* to it.\n\nIn case you like my bots, I‚Äôd be really happy if you rated them at [https://storebot.me/bot/expensebot](https://storebot.me/bot/expensebot) and [https://storebot.me/bot/doodlerbot](https://storebot.me/bot/doodlerbot). Have fun!","slug":"telegram-expensebot-doodlerbot","published":1,"updated":"2020-06-06T15:14:33.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhj2000y40mq3d0uk1qa","content":"<p>In 2015, the <a href=\"https://telegram.org\" target=\"_blank\" rel=\"noopener\">Telegram</a> messenger announced their <a href=\"https://core.telegram.org/bots\" target=\"_blank\" rel=\"noopener\">Bots</a>. Basically they are pieces of software that act like a normal chat user in many ways. They could have any functionality, from being helpful at daily tasks to even simple games or trivias ‚Äì all within an ordinary Telegram chat. You send them message, they give answers ‚Äì some more and some less intelligent. Recently, also other companies ‚Äì like <a href=\"http://techcrunch.com/2016/04/07/facebook-chatbots/\" target=\"_blank\" rel=\"noopener\">Facebook</a> or <a href=\"https://dev.botframework.com/\" target=\"_blank\" rel=\"noopener\">Microsoft</a> ‚Äì announced such bots for their messaging apps. Sometimes bots are even considered kind of the next step after native (web-) applications in the future.</p>\n<p>From a developer‚Äôs perspective, making a bot is fun, because there are almost no restrictions on how to develop your bot. All communication with Telegram works by requesting a single REST API provided by them. Choices like which programming language and -framework to use and how to structure the code are completely up to the developer. A Telegram bot can theoretically be built in any programming language. The only requirements are to be able to make HTTP request from the application and to have a server to host the bot on.</p>\n<p>I‚Äôve recently created two bots for Telegram that should each help with a daily task.</p>\n<h3 id=\"ExpenseBot-‚Äì-Keep-track-of-your-finances\">ExpenseBot ‚Äì Keep track of your finances</h3>\n<p><img src=\"/images/expensebot_icon.png\" alt=\"1461614801_Money-Increase\"></p>\n<p>This bot‚Äôs purpose is to help people manage their daily expenses and keep track of their financial situation. Users can add expenses from wherever they are using a few simple commands from within the chat and have an eye on how much they have spent in a month or a day. This obviates the need for confusing Excel spreadsheets or paper notes. You can reach the bot by sending a Message to <em><a href=\"https://telegram.me/ExpenseBot\" target=\"_blank\" rel=\"noopener\">@ExpenseBot</a></em> in Telegram.</p>\n<h3 id=\"DoodlerBot-‚Äì-Coordinate-group-appointments\">DoodlerBot ‚Äì Coordinate group appointments</h3>\n<p><img src=\"/images/doodlerbot_icon.png\" alt=\"1462726473_calendar\"></p>\n<p>My second bot helps users coordinate a group of people and find the right date for a common appointment, just like you might know from <a href=\"http://doodle.com\" target=\"_blank\" rel=\"noopener\">doodle.com</a> (even though it doesn‚Äôt have anything to do with that commercial service, except for fulfilling the same need). Open a new doodle and let your mates in the group chat vote for their preferred date to finally make the best decision for everyone. You can reach the bot by sending a Message to <em><a href=\"https://telegram.me/DoodlerBot\" target=\"_blank\" rel=\"noopener\">@DoodlerBot</a></em> in Telegram.</p>\n<p>Both projects are completely independent, non-commercial and privately operated. If you have any questions our found a bug (both bots are still in beta phase and therefore might show some unexpected behavior), please contact me at @n1try or via e-mail. In both cases, you should first read a basic introduction on how to use the respective bot, by sending a message with <em>/help</em> to it.</p>\n<p>In case you like my bots, I‚Äôd be really happy if you rated them at <a href=\"https://storebot.me/bot/expensebot\" target=\"_blank\" rel=\"noopener\">https://storebot.me/bot/expensebot</a> and <a href=\"https://storebot.me/bot/doodlerbot\" target=\"_blank\" rel=\"noopener\">https://storebot.me/bot/doodlerbot</a>. Have fun!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>In 2015, the <a href=\"https://telegram.org\" target=\"_blank\" rel=\"noopener\">Telegram</a> messenger announced their <a href=\"https://core.telegram.org/bots\" target=\"_blank\" rel=\"noopener\">Bots</a>. Basically they are pieces of software that act like a normal chat user in many ways. They could have any functionality, from being helpful at daily tasks to even simple games or trivias ‚Äì all within an ordinary Telegram chat. You send them message, they give answers ‚Äì some more and some less intelligent. Recently, also other companies ‚Äì like <a href=\"http://techcrunch.com/2016/04/07/facebook-chatbots/\" target=\"_blank\" rel=\"noopener\">Facebook</a> or <a href=\"https://dev.botframework.com/\" target=\"_blank\" rel=\"noopener\">Microsoft</a> ‚Äì announced such bots for their messaging apps. Sometimes bots are even considered kind of the next step after native (web-) applications in the future.</p>\n<p>From a developer‚Äôs perspective, making a bot is fun, because there are almost no restrictions on how to develop your bot. All communication with Telegram works by requesting a single REST API provided by them. Choices like which programming language and -framework to use and how to structure the code are completely up to the developer. A Telegram bot can theoretically be built in any programming language. The only requirements are to be able to make HTTP request from the application and to have a server to host the bot on.</p>\n<p>I‚Äôve recently created two bots for Telegram that should each help with a daily task.</p>\n<h3 id=\"ExpenseBot-‚Äì-Keep-track-of-your-finances\">ExpenseBot ‚Äì Keep track of your finances</h3>\n<p><img src=\"/images/expensebot_icon.png\" alt=\"1461614801_Money-Increase\"></p>\n<p>This bot‚Äôs purpose is to help people manage their daily expenses and keep track of their financial situation. Users can add expenses from wherever they are using a few simple commands from within the chat and have an eye on how much they have spent in a month or a day. This obviates the need for confusing Excel spreadsheets or paper notes. You can reach the bot by sending a Message to <em><a href=\"https://telegram.me/ExpenseBot\" target=\"_blank\" rel=\"noopener\">@ExpenseBot</a></em> in Telegram.</p>\n<h3 id=\"DoodlerBot-‚Äì-Coordinate-group-appointments\">DoodlerBot ‚Äì Coordinate group appointments</h3>\n<p><img src=\"/images/doodlerbot_icon.png\" alt=\"1462726473_calendar\"></p>\n<p>My second bot helps users coordinate a group of people and find the right date for a common appointment, just like you might know from <a href=\"http://doodle.com\" target=\"_blank\" rel=\"noopener\">doodle.com</a> (even though it doesn‚Äôt have anything to do with that commercial service, except for fulfilling the same need). Open a new doodle and let your mates in the group chat vote for their preferred date to finally make the best decision for everyone. You can reach the bot by sending a Message to <em><a href=\"https://telegram.me/DoodlerBot\" target=\"_blank\" rel=\"noopener\">@DoodlerBot</a></em> in Telegram.</p>\n<p>Both projects are completely independent, non-commercial and privately operated. If you have any questions our found a bug (both bots are still in beta phase and therefore might show some unexpected behavior), please contact me at @n1try or via e-mail. In both cases, you should first read a basic introduction on how to use the respective bot, by sending a message with <em>/help</em> to it.</p>\n<p>In case you like my bots, I‚Äôd be really happy if you rated them at <a href=\"https://storebot.me/bot/expensebot\" target=\"_blank\" rel=\"noopener\">https://storebot.me/bot/expensebot</a> and <a href=\"https://storebot.me/bot/doodlerbot\" target=\"_blank\" rel=\"noopener\">https://storebot.me/bot/doodlerbot</a>. Have fun!</p>\n"},{"title":"Middleman Bot - Push notifications as easy as POST","date":"2017-07-04T20:13:57.000Z","_content":"\n![](images/middleman.png)\n\n## E-Mails are so 2010\nE-Mails are everywhere. Literally anybody who gets in contact with a computer in some way will use the electronic mail system, which dates back to 1971. And certainly the E-Mail still has its right to exist, even in 2017. Especially in a business context it simply makes sense for many use cases - however not for all of those it is nowadays used for. The smaller the piece of information you want to communicate is, the less efficient e-mails are. Due to tons of header rows and other heavy load, a mail has a large overhead. Just look at a small random sample of mail from your inbox, find out the core information you get from it and look at the file size. Often you will find an overhead of more than 70 %. Great examples are subscription mails from online forum posts. Their essential information is basically only one bit in size, namely that something new has happened. However, they usually contain lots of HTML code, CSS, attachment pictures and some boilerplate text. Another example is those chatty-like conversations between two or more people, where each of them responds with only a few words to his conversation partner's last question or the like. Although only few words are added, the whole previous mail is replicated again, enriched with some headers and then sent to the server, where it is stored and delivered to the recipients. I'm glad that for those cases apps like Slack are gradually establishing and I hope they will replace the e-mail some day. Anyway, I get off the topic. This shouldn't be a hate letter against the e-mail. Rather I want to present another bot for the __Telegram__ instant messenger.\n\n## The pain I had\nIt mainly aims at developers and sysadmins, but is not limited to these addressees. Once I had the problem that I wanted to code a little watcher that regularly visits my university's website to check whether an announcement about the exam results has been made and notify me, if that was the case. The only way for realizing that notification that came to my mind was to send an e-mail. However, I had to integrate an SMTP library into my script then, which authenticates against my mail server using my account credentials - comparatively high effort. Then I wanted to write a little script that runs on my server in regular intervals to gather some basic statistics from log files and databases and send them to me so I could passively consume them (instead of me having to actively go to some webpage or so). Again, same problem. \n\n## The simple solution I created\nFor these reasons I created the Middleman Bot. It takes a simple, small JSON fragment as an input via HTTP call and forwards it to my Telegram chat. There's no need for an additional library (nearly every programming language has a built-in API for HTTP calls or you could use cURL) or authentication process. Simply register at the bot once and fire lightweight HTTP calls afterwards. \n\n## Example\nTo make my webserver send me a notification when its load is above a certain threshold, the only thing it would need to do is a `POST https://apps.muetsch.io/middleman/api/messages` with this body: \n\n```json\n{\n\t\"recipient_token\": \"3edf633a-eab0-45ea-9721-16c07bb8f245\",\n\t\"text\": \"Watch out! Average load in the last 10 minutes is >= 10000 requests per second.\",\n\t\"origin\": \"Caddy webserver @ muetsch.io\"\n}\n```\n\nThe token is your unique identifier the bot uses internally to decide to which chat / recipient to route an incoming message to. (For the trolls among you: I've invalidated the token from the example, so nobody could spam me.)\n\nAnd there you go. \n\n![](images/middleman2.png)\n\n\n## Give it a try\nI've pushed the code as well as some introductions on how to run and use that bot to GitHub at  [muety/telegram-middleman-bot](https://github.com/muety/telegram-middleman-bot). You can either run your own instance of the bot or use mine, which is running at _https://apps.muetsch.io_. Let me know what you think!","source":"_posts/telegram-middleman-bot-push-notifications-as-easy-as-post.md","raw":"---\ntitle: Middleman Bot - Push notifications as easy as POST\ndate: 2017-07-04 22:13:57\ntags:\n---\n\n![](images/middleman.png)\n\n## E-Mails are so 2010\nE-Mails are everywhere. Literally anybody who gets in contact with a computer in some way will use the electronic mail system, which dates back to 1971. And certainly the E-Mail still has its right to exist, even in 2017. Especially in a business context it simply makes sense for many use cases - however not for all of those it is nowadays used for. The smaller the piece of information you want to communicate is, the less efficient e-mails are. Due to tons of header rows and other heavy load, a mail has a large overhead. Just look at a small random sample of mail from your inbox, find out the core information you get from it and look at the file size. Often you will find an overhead of more than 70 %. Great examples are subscription mails from online forum posts. Their essential information is basically only one bit in size, namely that something new has happened. However, they usually contain lots of HTML code, CSS, attachment pictures and some boilerplate text. Another example is those chatty-like conversations between two or more people, where each of them responds with only a few words to his conversation partner's last question or the like. Although only few words are added, the whole previous mail is replicated again, enriched with some headers and then sent to the server, where it is stored and delivered to the recipients. I'm glad that for those cases apps like Slack are gradually establishing and I hope they will replace the e-mail some day. Anyway, I get off the topic. This shouldn't be a hate letter against the e-mail. Rather I want to present another bot for the __Telegram__ instant messenger.\n\n## The pain I had\nIt mainly aims at developers and sysadmins, but is not limited to these addressees. Once I had the problem that I wanted to code a little watcher that regularly visits my university's website to check whether an announcement about the exam results has been made and notify me, if that was the case. The only way for realizing that notification that came to my mind was to send an e-mail. However, I had to integrate an SMTP library into my script then, which authenticates against my mail server using my account credentials - comparatively high effort. Then I wanted to write a little script that runs on my server in regular intervals to gather some basic statistics from log files and databases and send them to me so I could passively consume them (instead of me having to actively go to some webpage or so). Again, same problem. \n\n## The simple solution I created\nFor these reasons I created the Middleman Bot. It takes a simple, small JSON fragment as an input via HTTP call and forwards it to my Telegram chat. There's no need for an additional library (nearly every programming language has a built-in API for HTTP calls or you could use cURL) or authentication process. Simply register at the bot once and fire lightweight HTTP calls afterwards. \n\n## Example\nTo make my webserver send me a notification when its load is above a certain threshold, the only thing it would need to do is a `POST https://apps.muetsch.io/middleman/api/messages` with this body: \n\n```json\n{\n\t\"recipient_token\": \"3edf633a-eab0-45ea-9721-16c07bb8f245\",\n\t\"text\": \"Watch out! Average load in the last 10 minutes is >= 10000 requests per second.\",\n\t\"origin\": \"Caddy webserver @ muetsch.io\"\n}\n```\n\nThe token is your unique identifier the bot uses internally to decide to which chat / recipient to route an incoming message to. (For the trolls among you: I've invalidated the token from the example, so nobody could spam me.)\n\nAnd there you go. \n\n![](images/middleman2.png)\n\n\n## Give it a try\nI've pushed the code as well as some introductions on how to run and use that bot to GitHub at  [muety/telegram-middleman-bot](https://github.com/muety/telegram-middleman-bot). You can either run your own instance of the bot or use mine, which is running at _https://apps.muetsch.io_. Let me know what you think!","slug":"telegram-middleman-bot-push-notifications-as-easy-as-post","published":1,"updated":"2020-06-06T15:14:33.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhj3000z40mqsxhw5l5d","content":"<p><img src=\"images/middleman.png\" alt></p>\n<h2 id=\"E-Mails-are-so-2010\">E-Mails are so 2010</h2>\n<p>E-Mails are everywhere. Literally anybody who gets in contact with a computer in some way will use the electronic mail system, which dates back to 1971. And certainly the E-Mail still has its right to exist, even in 2017. Especially in a business context it simply makes sense for many use cases - however not for all of those it is nowadays used for. The smaller the piece of information you want to communicate is, the less efficient e-mails are. Due to tons of header rows and other heavy load, a mail has a large overhead. Just look at a small random sample of mail from your inbox, find out the core information you get from it and look at the file size. Often you will find an overhead of more than 70 %. Great examples are subscription mails from online forum posts. Their essential information is basically only one bit in size, namely that something new has happened. However, they usually contain lots of HTML code, CSS, attachment pictures and some boilerplate text. Another example is those chatty-like conversations between two or more people, where each of them responds with only a few words to his conversation partner‚Äôs last question or the like. Although only few words are added, the whole previous mail is replicated again, enriched with some headers and then sent to the server, where it is stored and delivered to the recipients. I‚Äôm glad that for those cases apps like Slack are gradually establishing and I hope they will replace the e-mail some day. Anyway, I get off the topic. This shouldn‚Äôt be a hate letter against the e-mail. Rather I want to present another bot for the <strong>Telegram</strong> instant messenger.</p>\n<h2 id=\"The-pain-I-had\">The pain I had</h2>\n<p>It mainly aims at developers and sysadmins, but is not limited to these addressees. Once I had the problem that I wanted to code a little watcher that regularly visits my university‚Äôs website to check whether an announcement about the exam results has been made and notify me, if that was the case. The only way for realizing that notification that came to my mind was to send an e-mail. However, I had to integrate an SMTP library into my script then, which authenticates against my mail server using my account credentials - comparatively high effort. Then I wanted to write a little script that runs on my server in regular intervals to gather some basic statistics from log files and databases and send them to me so I could passively consume them (instead of me having to actively go to some webpage or so). Again, same problem.</p>\n<h2 id=\"The-simple-solution-I-created\">The simple solution I created</h2>\n<p>For these reasons I created the Middleman Bot. It takes a simple, small JSON fragment as an input via HTTP call and forwards it to my Telegram chat. There‚Äôs no need for an additional library (nearly every programming language has a built-in API for HTTP calls or you could use cURL) or authentication process. Simply register at the bot once and fire lightweight HTTP calls afterwards.</p>\n<h2 id=\"Example\">Example</h2>\n<p>To make my webserver send me a notification when its load is above a certain threshold, the only thing it would need to do is a <code>POST https://apps.muetsch.io/middleman/api/messages</code> with this body:</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"attr\">\"recipient_token\"</span>: <span class=\"string\">\"3edf633a-eab0-45ea-9721-16c07bb8f245\"</span>,</span><br><span class=\"line\">\t<span class=\"attr\">\"text\"</span>: <span class=\"string\">\"Watch out! Average load in the last 10 minutes is &gt;= 10000 requests per second.\"</span>,</span><br><span class=\"line\">\t<span class=\"attr\">\"origin\"</span>: <span class=\"string\">\"Caddy webserver @ muetsch.io\"</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>The token is your unique identifier the bot uses internally to decide to which chat / recipient to route an incoming message to. (For the trolls among you: I‚Äôve invalidated the token from the example, so nobody could spam me.)</p>\n<p>And there you go.</p>\n<p><img src=\"images/middleman2.png\" alt></p>\n<h2 id=\"Give-it-a-try\">Give it a try</h2>\n<p>I‚Äôve pushed the code as well as some introductions on how to run and use that bot to GitHub at  <a href=\"https://github.com/muety/telegram-middleman-bot\" target=\"_blank\" rel=\"noopener\">muety/telegram-middleman-bot</a>. You can either run your own instance of the bot or use mine, which is running at <em><a href=\"https://apps.muetsch.io\" target=\"_blank\" rel=\"noopener\">https://apps.muetsch.io</a></em>. Let me know what you think!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"images/middleman.png\" alt></p>\n<h2 id=\"E-Mails-are-so-2010\">E-Mails are so 2010</h2>\n<p>E-Mails are everywhere. Literally anybody who gets in contact with a computer in some way will use the electronic mail system, which dates back to 1971. And certainly the E-Mail still has its right to exist, even in 2017. Especially in a business context it simply makes sense for many use cases - however not for all of those it is nowadays used for. The smaller the piece of information you want to communicate is, the less efficient e-mails are. Due to tons of header rows and other heavy load, a mail has a large overhead. Just look at a small random sample of mail from your inbox, find out the core information you get from it and look at the file size. Often you will find an overhead of more than 70 %. Great examples are subscription mails from online forum posts. Their essential information is basically only one bit in size, namely that something new has happened. However, they usually contain lots of HTML code, CSS, attachment pictures and some boilerplate text. Another example is those chatty-like conversations between two or more people, where each of them responds with only a few words to his conversation partner‚Äôs last question or the like. Although only few words are added, the whole previous mail is replicated again, enriched with some headers and then sent to the server, where it is stored and delivered to the recipients. I‚Äôm glad that for those cases apps like Slack are gradually establishing and I hope they will replace the e-mail some day. Anyway, I get off the topic. This shouldn‚Äôt be a hate letter against the e-mail. Rather I want to present another bot for the <strong>Telegram</strong> instant messenger.</p>\n<h2 id=\"The-pain-I-had\">The pain I had</h2>\n<p>It mainly aims at developers and sysadmins, but is not limited to these addressees. Once I had the problem that I wanted to code a little watcher that regularly visits my university‚Äôs website to check whether an announcement about the exam results has been made and notify me, if that was the case. The only way for realizing that notification that came to my mind was to send an e-mail. However, I had to integrate an SMTP library into my script then, which authenticates against my mail server using my account credentials - comparatively high effort. Then I wanted to write a little script that runs on my server in regular intervals to gather some basic statistics from log files and databases and send them to me so I could passively consume them (instead of me having to actively go to some webpage or so). Again, same problem.</p>\n<h2 id=\"The-simple-solution-I-created\">The simple solution I created</h2>\n<p>For these reasons I created the Middleman Bot. It takes a simple, small JSON fragment as an input via HTTP call and forwards it to my Telegram chat. There‚Äôs no need for an additional library (nearly every programming language has a built-in API for HTTP calls or you could use cURL) or authentication process. Simply register at the bot once and fire lightweight HTTP calls afterwards.</p>\n<h2 id=\"Example\">Example</h2>\n<p>To make my webserver send me a notification when its load is above a certain threshold, the only thing it would need to do is a <code>POST https://apps.muetsch.io/middleman/api/messages</code> with this body:</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"attr\">\"recipient_token\"</span>: <span class=\"string\">\"3edf633a-eab0-45ea-9721-16c07bb8f245\"</span>,</span><br><span class=\"line\">\t<span class=\"attr\">\"text\"</span>: <span class=\"string\">\"Watch out! Average load in the last 10 minutes is &gt;= 10000 requests per second.\"</span>,</span><br><span class=\"line\">\t<span class=\"attr\">\"origin\"</span>: <span class=\"string\">\"Caddy webserver @ muetsch.io\"</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>The token is your unique identifier the bot uses internally to decide to which chat / recipient to route an incoming message to. (For the trolls among you: I‚Äôve invalidated the token from the example, so nobody could spam me.)</p>\n<p>And there you go.</p>\n<p><img src=\"images/middleman2.png\" alt></p>\n<h2 id=\"Give-it-a-try\">Give it a try</h2>\n<p>I‚Äôve pushed the code as well as some introductions on how to run and use that bot to GitHub at  <a href=\"https://github.com/muety/telegram-middleman-bot\" target=\"_blank\" rel=\"noopener\">muety/telegram-middleman-bot</a>. You can either run your own instance of the bot or use mine, which is running at <em><a href=\"https://apps.muetsch.io\" target=\"_blank\" rel=\"noopener\">https://apps.muetsch.io</a></em>. Let me know what you think!</p>\n"},{"title":"Transfer Learning for Multi-Digit Recognition using TensorFlow Object Detection and SVHN Classifier","date":"2019-09-05T13:41:49.000Z","description":"This article presents how to use convolutional neural networks and TensorFlow object detection to localize and recognize multi-digit labels from pictures of runners at sports events.","_content":"![Dublin City Marathon 2016](https://miro.medium.com/max/1024/1*ZrlYBlKXiADNJC6OsOKTMg.jpeg)\n(Dublin City Marathon 2016, {% link Source https://commons.wikimedia.org/wiki/File:Dublin_City_Marathon_2006_(283653500).jpg Wikimedia Commons %})\n\n# Introduction\nThis summer I attended a run in my home town, where each of the 6,000 runners was assigned a certain bib number to wear on their shirt for time tracking. During the run, several photographers took pictures of each runner, which were made available online afterward. To find oneself among tens of thousands of pictures, the web portal offered an option to search by one's bib number. However, the images are tagged manually by volunteer users, so only a very small fraction of all photos was actually searchable by number.\n\nI wondered if this might not be a perfectly suitable task for machine learning-based image processing and so I took it as a challenge to build a system that automatically tags pictures with the bib numbers they contain.\n\n# Problem Statement\nGiven high-resolution RGB images like the above, which contain one to N persons, each of them wearing 0 to 1 numbers at the front of their bodies, I want to output each of these numbers in text form and associate them to the picture. I figured the numbers to be between 1 and 5 digits long.\n\nBefore starting, I conducted some research and found that, surprisingly, the problem seems to be less trivial than it seemed. First, classical OCR methods didn't seem to work at all, even if the image is precisely cropped only to the number. Second, while one-digit recognition with machine learning is trivial (MNIST, etc.), multi-digit is a much harder problem. Usually, it can't just be solved as a simple classification, because there are not 10 possible output classes anymore, but several thousand. Some other solution was required.\n\n# Approach\n## Multi-Digit Recognition \nAs a starting point, I discovered a paper called [\"Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks\"](http://arxiv.org/pdf/1312.6082.pdf), which presents a multi-digit classifier for house numbers ‚Äì using convolutional neural nets ‚Äì that was trained on [Stanford's SVHN dataset](http://ufldl.stanford.edu/housenumbers/). Recognizing house numbers is a quite similar problem to recognizing bib numbers, so I decided to take this approach as a basis.\nLuckily, I found an [open-source PyTorch implementation](https://github.com/potterhsu/SVHNClassifier-PyTorch) of the neural net on GitHub. I needed to do several tweaks and change some code to make it fit my needs, but it was a good start.\n\nEventually, I hoped that I could take the pre-trained SVHN model and use transfer-learning to fit it to my problem. \n\nHowever, before I could get started with the actual classification, there was another problem to solve. The input for the digit classifier is, as one could expect, not a high-res image with a lot of noise and distraction in it, but rather only a very precise excerpt from that image, that exactly contains one single number and not much more. \n\n\n## Object Detection for Localization\nI needed to find a way to localize the 2D-coordinates of all number signs within a picture. To solve that, I decided to utilize TensorFlow's [object detection framework](https://github.com/tensorflow/models/tree/master/research/object_detection), whose purpose is exactly that; recognizing certain objects in an image and outputting a 2-dimensional bounding box for it. \n\n## Outline\n\nIn summary, I planned to build a two-step classification system.\n\n![](images/svhn_steps.png)\n\n**Step 1:** Recognize bib numbers and crop them out\n**Step 2:** Use first step's output as input for a fine-tuned SVHN classifier\n\n# Data Preparation\n## Data Acquisition and Labeling for Step 1\nFirst, I started collecting 1,000 images from the web portal mentioned above. I manually labeled them for step 1 by drawing bounding boxes around each number, using [labelImg](https://github.com/tzutalin/labelImg) and wrote a short script to separate them into training, test and validation sets.\n\n![](images/svhn_labelimg.png)\n\nThe output of this step is an XML file for every image, containing information about the respective labels and their bounding boxes. Using a script called [`generate_tfrecord.py`](https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py), those XML files can be combined together with their corresponding images into one big _TFRecord_ file for each set (training, test, validation), that is the format required as input for TensorFlow object detection.\n\n## Training the Object Detector\nFor training the object detection model to recognize bib numbers, I decided not to train it completely from scratch, but fine-tune the pre-trained `ssd_mobilenet_v1_pets` set to my needs. The TensorFlow object detection framework provides a quite convenient way to do so by simply adjusting a few config files. If you're interested in more details about training a custom object detector, there's a very interesting [article on pythonprogramming.net](https://pythonprogramming.net/introduction-use-tensorflow-object-detection-api-tutorial/) on this. \n\nAfter training for ~ 100,000 episodes, I ended up with a model ‚Äì represented as a so-called `frozen_inference_graph.pb` binary file - that was able to find bib number signs in sports imagery. \n\nLetting the model run on my data yields quite reliable results of bounding boxes of bib numbers, which I could then use to crop the original images to smaller ones with another small script. \n\n![](images/svhn_cropped_images.png)\n\n## Labeling for Step 2\nTo produce training data for the second step ‚Äì digit recognition ‚Äì I needed to do another round of labeling. This time, the little cropped images of numbers had to be assigned their actual numbers in text form. I did this manually and using a simple CSV table. \n\n![](images/svhn_labels.png) \n\n## Adding Data Augmentation\nTo (1.) overcome my lack of training data and (2.) hopefully make the model generalize better, I considered it a good idea to introduce some image augmentation. I extended the given [`DataLoader`](https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/dataset.py) in a way that a specified fraction of the number of raw training images is artificially added to the data set in a slightly transformed form. To be more precise, I used [PyTorch's TorchVision Transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) to introduce (a) color jitter (variance in brightness, contrast, saturation, and hue), (b) [affine transformations](https://en.wikipedia.org/wiki/Affine_transformation) and (c) rotation.\n\nAnother thing I changed from the original implementation is the way input images are transformed. The net's 64 x 64 x 3 input layer expects square RGB images. However, obviously, barely any of the training images are actually square. Whereas the original implementation essentially squeezes or tugs the images to match the required dimensions, I considered this unfavorable, especially for wide numbers, e.g. 5-digit numbers. Instead, I changed the input transformation in a way, that images are \"thumbnailed\". \n\nHowever, so far I didn't evaluate which way of pre-processing yields better performance.\n\n![](images/svhn_cropping.png)\n\n \n# Fine-Tuning using Transfer Learning\nEventually, after all data massaging and pre-processing was done, I could start with the interesting part: the actual digit recognition.\nDue to my lack of large amounts of high-variance, representative training data, I decided that it might not be a good idea to train the CNN model-based classifier completely from scratch. Instead, I used a [pre-trained model](https://github.com/potterhsu/SVHNClassifier-PyTorch#results) (trained on SVHN dataset) with an accuracy of 95 % for house numbers as a feature extractor and fine-tune it to work with bib numbers. To do so, I conveniently used the given [`train.py`](https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/train.py) script. However, in my understanding, it does not train only the net's very last classification layer while keeping all previous convolutional- and normalization layers frozen, but re-trains every layer. This is not exactly what I wanted, but it turned out to work quite well.\n\n# Results\nAfter training for 72,000 episodes with a batch size of 256, a learning rate of 10^-3 and an augmentation factor of 1.5, I eventually evaluated my two-step classification system on a set of 120 test images and reached an accuracy of **~ 76 %**. That is, about 3/4 of all numbers among all images were detected and classified correctly.\n\nHowever, there is still room for improvements. First, using a lot more training data would probably boost accuracy. Second, I didn't do any hyper-parameter tuning, which would probably also improve performance by a few percentage points.","source":"_posts/transfer-learning-for-multi-digit-recognition-using-tensorflow-object-detection-and-svhn-classifier.md","raw":"---\ntitle: Transfer Learning for Multi-Digit Recognition using TensorFlow Object Detection and SVHN Classifier\ndate: 2019-09-05 15:41:49\ntags:\ndescription: This article presents how to use convolutional neural networks and TensorFlow object detection to localize and recognize multi-digit labels from pictures of runners at sports events.\n---\n![Dublin City Marathon 2016](https://miro.medium.com/max/1024/1*ZrlYBlKXiADNJC6OsOKTMg.jpeg)\n(Dublin City Marathon 2016, {% link Source https://commons.wikimedia.org/wiki/File:Dublin_City_Marathon_2006_(283653500).jpg Wikimedia Commons %})\n\n# Introduction\nThis summer I attended a run in my home town, where each of the 6,000 runners was assigned a certain bib number to wear on their shirt for time tracking. During the run, several photographers took pictures of each runner, which were made available online afterward. To find oneself among tens of thousands of pictures, the web portal offered an option to search by one's bib number. However, the images are tagged manually by volunteer users, so only a very small fraction of all photos was actually searchable by number.\n\nI wondered if this might not be a perfectly suitable task for machine learning-based image processing and so I took it as a challenge to build a system that automatically tags pictures with the bib numbers they contain.\n\n# Problem Statement\nGiven high-resolution RGB images like the above, which contain one to N persons, each of them wearing 0 to 1 numbers at the front of their bodies, I want to output each of these numbers in text form and associate them to the picture. I figured the numbers to be between 1 and 5 digits long.\n\nBefore starting, I conducted some research and found that, surprisingly, the problem seems to be less trivial than it seemed. First, classical OCR methods didn't seem to work at all, even if the image is precisely cropped only to the number. Second, while one-digit recognition with machine learning is trivial (MNIST, etc.), multi-digit is a much harder problem. Usually, it can't just be solved as a simple classification, because there are not 10 possible output classes anymore, but several thousand. Some other solution was required.\n\n# Approach\n## Multi-Digit Recognition \nAs a starting point, I discovered a paper called [\"Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks\"](http://arxiv.org/pdf/1312.6082.pdf), which presents a multi-digit classifier for house numbers ‚Äì using convolutional neural nets ‚Äì that was trained on [Stanford's SVHN dataset](http://ufldl.stanford.edu/housenumbers/). Recognizing house numbers is a quite similar problem to recognizing bib numbers, so I decided to take this approach as a basis.\nLuckily, I found an [open-source PyTorch implementation](https://github.com/potterhsu/SVHNClassifier-PyTorch) of the neural net on GitHub. I needed to do several tweaks and change some code to make it fit my needs, but it was a good start.\n\nEventually, I hoped that I could take the pre-trained SVHN model and use transfer-learning to fit it to my problem. \n\nHowever, before I could get started with the actual classification, there was another problem to solve. The input for the digit classifier is, as one could expect, not a high-res image with a lot of noise and distraction in it, but rather only a very precise excerpt from that image, that exactly contains one single number and not much more. \n\n\n## Object Detection for Localization\nI needed to find a way to localize the 2D-coordinates of all number signs within a picture. To solve that, I decided to utilize TensorFlow's [object detection framework](https://github.com/tensorflow/models/tree/master/research/object_detection), whose purpose is exactly that; recognizing certain objects in an image and outputting a 2-dimensional bounding box for it. \n\n## Outline\n\nIn summary, I planned to build a two-step classification system.\n\n![](images/svhn_steps.png)\n\n**Step 1:** Recognize bib numbers and crop them out\n**Step 2:** Use first step's output as input for a fine-tuned SVHN classifier\n\n# Data Preparation\n## Data Acquisition and Labeling for Step 1\nFirst, I started collecting 1,000 images from the web portal mentioned above. I manually labeled them for step 1 by drawing bounding boxes around each number, using [labelImg](https://github.com/tzutalin/labelImg) and wrote a short script to separate them into training, test and validation sets.\n\n![](images/svhn_labelimg.png)\n\nThe output of this step is an XML file for every image, containing information about the respective labels and their bounding boxes. Using a script called [`generate_tfrecord.py`](https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py), those XML files can be combined together with their corresponding images into one big _TFRecord_ file for each set (training, test, validation), that is the format required as input for TensorFlow object detection.\n\n## Training the Object Detector\nFor training the object detection model to recognize bib numbers, I decided not to train it completely from scratch, but fine-tune the pre-trained `ssd_mobilenet_v1_pets` set to my needs. The TensorFlow object detection framework provides a quite convenient way to do so by simply adjusting a few config files. If you're interested in more details about training a custom object detector, there's a very interesting [article on pythonprogramming.net](https://pythonprogramming.net/introduction-use-tensorflow-object-detection-api-tutorial/) on this. \n\nAfter training for ~ 100,000 episodes, I ended up with a model ‚Äì represented as a so-called `frozen_inference_graph.pb` binary file - that was able to find bib number signs in sports imagery. \n\nLetting the model run on my data yields quite reliable results of bounding boxes of bib numbers, which I could then use to crop the original images to smaller ones with another small script. \n\n![](images/svhn_cropped_images.png)\n\n## Labeling for Step 2\nTo produce training data for the second step ‚Äì digit recognition ‚Äì I needed to do another round of labeling. This time, the little cropped images of numbers had to be assigned their actual numbers in text form. I did this manually and using a simple CSV table. \n\n![](images/svhn_labels.png) \n\n## Adding Data Augmentation\nTo (1.) overcome my lack of training data and (2.) hopefully make the model generalize better, I considered it a good idea to introduce some image augmentation. I extended the given [`DataLoader`](https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/dataset.py) in a way that a specified fraction of the number of raw training images is artificially added to the data set in a slightly transformed form. To be more precise, I used [PyTorch's TorchVision Transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) to introduce (a) color jitter (variance in brightness, contrast, saturation, and hue), (b) [affine transformations](https://en.wikipedia.org/wiki/Affine_transformation) and (c) rotation.\n\nAnother thing I changed from the original implementation is the way input images are transformed. The net's 64 x 64 x 3 input layer expects square RGB images. However, obviously, barely any of the training images are actually square. Whereas the original implementation essentially squeezes or tugs the images to match the required dimensions, I considered this unfavorable, especially for wide numbers, e.g. 5-digit numbers. Instead, I changed the input transformation in a way, that images are \"thumbnailed\". \n\nHowever, so far I didn't evaluate which way of pre-processing yields better performance.\n\n![](images/svhn_cropping.png)\n\n \n# Fine-Tuning using Transfer Learning\nEventually, after all data massaging and pre-processing was done, I could start with the interesting part: the actual digit recognition.\nDue to my lack of large amounts of high-variance, representative training data, I decided that it might not be a good idea to train the CNN model-based classifier completely from scratch. Instead, I used a [pre-trained model](https://github.com/potterhsu/SVHNClassifier-PyTorch#results) (trained on SVHN dataset) with an accuracy of 95 % for house numbers as a feature extractor and fine-tune it to work with bib numbers. To do so, I conveniently used the given [`train.py`](https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/train.py) script. However, in my understanding, it does not train only the net's very last classification layer while keeping all previous convolutional- and normalization layers frozen, but re-trains every layer. This is not exactly what I wanted, but it turned out to work quite well.\n\n# Results\nAfter training for 72,000 episodes with a batch size of 256, a learning rate of 10^-3 and an augmentation factor of 1.5, I eventually evaluated my two-step classification system on a set of 120 test images and reached an accuracy of **~ 76 %**. That is, about 3/4 of all numbers among all images were detected and classified correctly.\n\nHowever, there is still room for improvements. First, using a lot more training data would probably boost accuracy. Second, I didn't do any hyper-parameter tuning, which would probably also improve performance by a few percentage points.","slug":"transfer-learning-for-multi-digit-recognition-using-tensorflow-object-detection-and-svhn-classifier","published":1,"updated":"2020-06-06T15:14:33.459Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhj4001040mq20zd068f","content":"<p><img src=\"https://miro.medium.com/max/1024/1*ZrlYBlKXiADNJC6OsOKTMg.jpeg\" alt=\"Dublin City Marathon 2016\"><br>\n(Dublin City Marathon 2016, <a href=\"https://commons.wikimedia.org/wiki/File:Dublin_City_Marathon_2006_(283653500).jpg\" title=\"Wikimedia Commons\" target=\"_blank\" rel=\"noopener\">Source</a>)</p>\n<h1>Introduction</h1>\n<p>This summer I attended a run in my home town, where each of the 6,000 runners was assigned a certain bib number to wear on their shirt for time tracking. During the run, several photographers took pictures of each runner, which were made available online afterward. To find oneself among tens of thousands of pictures, the web portal offered an option to search by one‚Äôs bib number. However, the images are tagged manually by volunteer users, so only a very small fraction of all photos was actually searchable by number.</p>\n<p>I wondered if this might not be a perfectly suitable task for machine learning-based image processing and so I took it as a challenge to build a system that automatically tags pictures with the bib numbers they contain.</p>\n<h1>Problem Statement</h1>\n<p>Given high-resolution RGB images like the above, which contain one to N persons, each of them wearing 0 to 1 numbers at the front of their bodies, I want to output each of these numbers in text form and associate them to the picture. I figured the numbers to be between 1 and 5 digits long.</p>\n<p>Before starting, I conducted some research and found that, surprisingly, the problem seems to be less trivial than it seemed. First, classical OCR methods didn‚Äôt seem to work at all, even if the image is precisely cropped only to the number. Second, while one-digit recognition with machine learning is trivial (MNIST, etc.), multi-digit is a much harder problem. Usually, it can‚Äôt just be solved as a simple classification, because there are not 10 possible output classes anymore, but several thousand. Some other solution was required.</p>\n<h1>Approach</h1>\n<h2 id=\"Multi-Digit-Recognition\">Multi-Digit Recognition</h2>\n<p>As a starting point, I discovered a paper called <a href=\"http://arxiv.org/pdf/1312.6082.pdf\" target=\"_blank\" rel=\"noopener\">‚ÄúMulti-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks‚Äù</a>, which presents a multi-digit classifier for house numbers ‚Äì using convolutional neural nets ‚Äì that was trained on <a href=\"http://ufldl.stanford.edu/housenumbers/\" target=\"_blank\" rel=\"noopener\">Stanford‚Äôs SVHN dataset</a>. Recognizing house numbers is a quite similar problem to recognizing bib numbers, so I decided to take this approach as a basis.<br>\nLuckily, I found an <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch\" target=\"_blank\" rel=\"noopener\">open-source PyTorch implementation</a> of the neural net on GitHub. I needed to do several tweaks and change some code to make it fit my needs, but it was a good start.</p>\n<p>Eventually, I hoped that I could take the pre-trained SVHN model and use transfer-learning to fit it to my problem.</p>\n<p>However, before I could get started with the actual classification, there was another problem to solve. The input for the digit classifier is, as one could expect, not a high-res image with a lot of noise and distraction in it, but rather only a very precise excerpt from that image, that exactly contains one single number and not much more.</p>\n<h2 id=\"Object-Detection-for-Localization\">Object Detection for Localization</h2>\n<p>I needed to find a way to localize the 2D-coordinates of all number signs within a picture. To solve that, I decided to utilize TensorFlow‚Äôs <a href=\"https://github.com/tensorflow/models/tree/master/research/object_detection\" target=\"_blank\" rel=\"noopener\">object detection framework</a>, whose purpose is exactly that; recognizing certain objects in an image and outputting a 2-dimensional bounding box for it.</p>\n<h2 id=\"Outline\">Outline</h2>\n<p>In summary, I planned to build a two-step classification system.</p>\n<p><img src=\"images/svhn_steps.png\" alt></p>\n<p><strong>Step 1:</strong> Recognize bib numbers and crop them out<br>\n<strong>Step 2:</strong> Use first step‚Äôs output as input for a fine-tuned SVHN classifier</p>\n<h1>Data Preparation</h1>\n<h2 id=\"Data-Acquisition-and-Labeling-for-Step-1\">Data Acquisition and Labeling for Step 1</h2>\n<p>First, I started collecting 1,000 images from the web portal mentioned above. I manually labeled them for step 1 by drawing bounding boxes around each number, using <a href=\"https://github.com/tzutalin/labelImg\" target=\"_blank\" rel=\"noopener\">labelImg</a> and wrote a short script to separate them into training, test and validation sets.</p>\n<p><img src=\"images/svhn_labelimg.png\" alt></p>\n<p>The output of this step is an XML file for every image, containing information about the respective labels and their bounding boxes. Using a script called <a href=\"https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py\" target=\"_blank\" rel=\"noopener\"><code>generate_tfrecord.py</code></a>, those XML files can be combined together with their corresponding images into one big <em>TFRecord</em> file for each set (training, test, validation), that is the format required as input for TensorFlow object detection.</p>\n<h2 id=\"Training-the-Object-Detector\">Training the Object Detector</h2>\n<p>For training the object detection model to recognize bib numbers, I decided not to train it completely from scratch, but fine-tune the pre-trained <code>ssd_mobilenet_v1_pets</code> set to my needs. The TensorFlow object detection framework provides a quite convenient way to do so by simply adjusting a few config files. If you‚Äôre interested in more details about training a custom object detector, there‚Äôs a very interesting <a href=\"https://pythonprogramming.net/introduction-use-tensorflow-object-detection-api-tutorial/\" target=\"_blank\" rel=\"noopener\">article on pythonprogramming.net</a> on this.</p>\n<p>After training for ~ 100,000 episodes, I ended up with a model ‚Äì represented as a so-called <code>frozen_inference_graph.pb</code> binary file - that was able to find bib number signs in sports imagery.</p>\n<p>Letting the model run on my data yields quite reliable results of bounding boxes of bib numbers, which I could then use to crop the original images to smaller ones with another small script.</p>\n<p><img src=\"images/svhn_cropped_images.png\" alt></p>\n<h2 id=\"Labeling-for-Step-2\">Labeling for Step 2</h2>\n<p>To produce training data for the second step ‚Äì digit recognition ‚Äì I needed to do another round of labeling. This time, the little cropped images of numbers had to be assigned their actual numbers in text form. I did this manually and using a simple CSV table.</p>\n<p><img src=\"images/svhn_labels.png\" alt></p>\n<h2 id=\"Adding-Data-Augmentation\">Adding Data Augmentation</h2>\n<p>To (1.) overcome my lack of training data and (2.) hopefully make the model generalize better, I considered it a good idea to introduce some image augmentation. I extended the given <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/dataset.py\" target=\"_blank\" rel=\"noopener\"><code>DataLoader</code></a> in a way that a specified fraction of the number of raw training images is artificially added to the data set in a slightly transformed form. To be more precise, I used <a href=\"https://pytorch.org/docs/stable/torchvision/transforms.html\" target=\"_blank\" rel=\"noopener\">PyTorch‚Äôs TorchVision Transforms</a> to introduce (a) color jitter (variance in brightness, contrast, saturation, and hue), (b) <a href=\"https://en.wikipedia.org/wiki/Affine_transformation\" target=\"_blank\" rel=\"noopener\">affine transformations</a> and ¬© rotation.</p>\n<p>Another thing I changed from the original implementation is the way input images are transformed. The net‚Äôs 64 x 64 x 3 input layer expects square RGB images. However, obviously, barely any of the training images are actually square. Whereas the original implementation essentially squeezes or tugs the images to match the required dimensions, I considered this unfavorable, especially for wide numbers, e.g. 5-digit numbers. Instead, I changed the input transformation in a way, that images are ‚Äúthumbnailed‚Äù.</p>\n<p>However, so far I didn‚Äôt evaluate which way of pre-processing yields better performance.</p>\n<p><img src=\"images/svhn_cropping.png\" alt></p>\n<h1>Fine-Tuning using Transfer Learning</h1>\n<p>Eventually, after all data massaging and pre-processing was done, I could start with the interesting part: the actual digit recognition.<br>\nDue to my lack of large amounts of high-variance, representative training data, I decided that it might not be a good idea to train the CNN model-based classifier completely from scratch. Instead, I used a <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch#results\" target=\"_blank\" rel=\"noopener\">pre-trained model</a> (trained on SVHN dataset) with an accuracy of 95 % for house numbers as a feature extractor and fine-tune it to work with bib numbers. To do so, I conveniently used the given <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/train.py\" target=\"_blank\" rel=\"noopener\"><code>train.py</code></a> script. However, in my understanding, it does not train only the net‚Äôs very last classification layer while keeping all previous convolutional- and normalization layers frozen, but re-trains every layer. This is not exactly what I wanted, but it turned out to work quite well.</p>\n<h1>Results</h1>\n<p>After training for 72,000 episodes with a batch size of 256, a learning rate of 10^-3 and an augmentation factor of 1.5, I eventually evaluated my two-step classification system on a set of 120 test images and reached an accuracy of <strong>~ 76 %</strong>. That is, about 3/4 of all numbers among all images were detected and classified correctly.</p>\n<p>However, there is still room for improvements. First, using a lot more training data would probably boost accuracy. Second, I didn‚Äôt do any hyper-parameter tuning, which would probably also improve performance by a few percentage points.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"https://miro.medium.com/max/1024/1*ZrlYBlKXiADNJC6OsOKTMg.jpeg\" alt=\"Dublin City Marathon 2016\"><br>\n(Dublin City Marathon 2016, <a href=\"https://commons.wikimedia.org/wiki/File:Dublin_City_Marathon_2006_(283653500).jpg\" title=\"Wikimedia Commons\" target=\"_blank\" rel=\"noopener\">Source</a>)</p>\n<h1>Introduction</h1>\n<p>This summer I attended a run in my home town, where each of the 6,000 runners was assigned a certain bib number to wear on their shirt for time tracking. During the run, several photographers took pictures of each runner, which were made available online afterward. To find oneself among tens of thousands of pictures, the web portal offered an option to search by one‚Äôs bib number. However, the images are tagged manually by volunteer users, so only a very small fraction of all photos was actually searchable by number.</p>\n<p>I wondered if this might not be a perfectly suitable task for machine learning-based image processing and so I took it as a challenge to build a system that automatically tags pictures with the bib numbers they contain.</p>\n<h1>Problem Statement</h1>\n<p>Given high-resolution RGB images like the above, which contain one to N persons, each of them wearing 0 to 1 numbers at the front of their bodies, I want to output each of these numbers in text form and associate them to the picture. I figured the numbers to be between 1 and 5 digits long.</p>\n<p>Before starting, I conducted some research and found that, surprisingly, the problem seems to be less trivial than it seemed. First, classical OCR methods didn‚Äôt seem to work at all, even if the image is precisely cropped only to the number. Second, while one-digit recognition with machine learning is trivial (MNIST, etc.), multi-digit is a much harder problem. Usually, it can‚Äôt just be solved as a simple classification, because there are not 10 possible output classes anymore, but several thousand. Some other solution was required.</p>\n<h1>Approach</h1>\n<h2 id=\"Multi-Digit-Recognition\">Multi-Digit Recognition</h2>\n<p>As a starting point, I discovered a paper called <a href=\"http://arxiv.org/pdf/1312.6082.pdf\" target=\"_blank\" rel=\"noopener\">‚ÄúMulti-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks‚Äù</a>, which presents a multi-digit classifier for house numbers ‚Äì using convolutional neural nets ‚Äì that was trained on <a href=\"http://ufldl.stanford.edu/housenumbers/\" target=\"_blank\" rel=\"noopener\">Stanford‚Äôs SVHN dataset</a>. Recognizing house numbers is a quite similar problem to recognizing bib numbers, so I decided to take this approach as a basis.<br>\nLuckily, I found an <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch\" target=\"_blank\" rel=\"noopener\">open-source PyTorch implementation</a> of the neural net on GitHub. I needed to do several tweaks and change some code to make it fit my needs, but it was a good start.</p>\n<p>Eventually, I hoped that I could take the pre-trained SVHN model and use transfer-learning to fit it to my problem.</p>\n<p>However, before I could get started with the actual classification, there was another problem to solve. The input for the digit classifier is, as one could expect, not a high-res image with a lot of noise and distraction in it, but rather only a very precise excerpt from that image, that exactly contains one single number and not much more.</p>\n<h2 id=\"Object-Detection-for-Localization\">Object Detection for Localization</h2>\n<p>I needed to find a way to localize the 2D-coordinates of all number signs within a picture. To solve that, I decided to utilize TensorFlow‚Äôs <a href=\"https://github.com/tensorflow/models/tree/master/research/object_detection\" target=\"_blank\" rel=\"noopener\">object detection framework</a>, whose purpose is exactly that; recognizing certain objects in an image and outputting a 2-dimensional bounding box for it.</p>\n<h2 id=\"Outline\">Outline</h2>\n<p>In summary, I planned to build a two-step classification system.</p>\n<p><img src=\"images/svhn_steps.png\" alt></p>\n<p><strong>Step 1:</strong> Recognize bib numbers and crop them out<br>\n<strong>Step 2:</strong> Use first step‚Äôs output as input for a fine-tuned SVHN classifier</p>\n<h1>Data Preparation</h1>\n<h2 id=\"Data-Acquisition-and-Labeling-for-Step-1\">Data Acquisition and Labeling for Step 1</h2>\n<p>First, I started collecting 1,000 images from the web portal mentioned above. I manually labeled them for step 1 by drawing bounding boxes around each number, using <a href=\"https://github.com/tzutalin/labelImg\" target=\"_blank\" rel=\"noopener\">labelImg</a> and wrote a short script to separate them into training, test and validation sets.</p>\n<p><img src=\"images/svhn_labelimg.png\" alt></p>\n<p>The output of this step is an XML file for every image, containing information about the respective labels and their bounding boxes. Using a script called <a href=\"https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py\" target=\"_blank\" rel=\"noopener\"><code>generate_tfrecord.py</code></a>, those XML files can be combined together with their corresponding images into one big <em>TFRecord</em> file for each set (training, test, validation), that is the format required as input for TensorFlow object detection.</p>\n<h2 id=\"Training-the-Object-Detector\">Training the Object Detector</h2>\n<p>For training the object detection model to recognize bib numbers, I decided not to train it completely from scratch, but fine-tune the pre-trained <code>ssd_mobilenet_v1_pets</code> set to my needs. The TensorFlow object detection framework provides a quite convenient way to do so by simply adjusting a few config files. If you‚Äôre interested in more details about training a custom object detector, there‚Äôs a very interesting <a href=\"https://pythonprogramming.net/introduction-use-tensorflow-object-detection-api-tutorial/\" target=\"_blank\" rel=\"noopener\">article on pythonprogramming.net</a> on this.</p>\n<p>After training for ~ 100,000 episodes, I ended up with a model ‚Äì represented as a so-called <code>frozen_inference_graph.pb</code> binary file - that was able to find bib number signs in sports imagery.</p>\n<p>Letting the model run on my data yields quite reliable results of bounding boxes of bib numbers, which I could then use to crop the original images to smaller ones with another small script.</p>\n<p><img src=\"images/svhn_cropped_images.png\" alt></p>\n<h2 id=\"Labeling-for-Step-2\">Labeling for Step 2</h2>\n<p>To produce training data for the second step ‚Äì digit recognition ‚Äì I needed to do another round of labeling. This time, the little cropped images of numbers had to be assigned their actual numbers in text form. I did this manually and using a simple CSV table.</p>\n<p><img src=\"images/svhn_labels.png\" alt></p>\n<h2 id=\"Adding-Data-Augmentation\">Adding Data Augmentation</h2>\n<p>To (1.) overcome my lack of training data and (2.) hopefully make the model generalize better, I considered it a good idea to introduce some image augmentation. I extended the given <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/dataset.py\" target=\"_blank\" rel=\"noopener\"><code>DataLoader</code></a> in a way that a specified fraction of the number of raw training images is artificially added to the data set in a slightly transformed form. To be more precise, I used <a href=\"https://pytorch.org/docs/stable/torchvision/transforms.html\" target=\"_blank\" rel=\"noopener\">PyTorch‚Äôs TorchVision Transforms</a> to introduce (a) color jitter (variance in brightness, contrast, saturation, and hue), (b) <a href=\"https://en.wikipedia.org/wiki/Affine_transformation\" target=\"_blank\" rel=\"noopener\">affine transformations</a> and ¬© rotation.</p>\n<p>Another thing I changed from the original implementation is the way input images are transformed. The net‚Äôs 64 x 64 x 3 input layer expects square RGB images. However, obviously, barely any of the training images are actually square. Whereas the original implementation essentially squeezes or tugs the images to match the required dimensions, I considered this unfavorable, especially for wide numbers, e.g. 5-digit numbers. Instead, I changed the input transformation in a way, that images are ‚Äúthumbnailed‚Äù.</p>\n<p>However, so far I didn‚Äôt evaluate which way of pre-processing yields better performance.</p>\n<p><img src=\"images/svhn_cropping.png\" alt></p>\n<h1>Fine-Tuning using Transfer Learning</h1>\n<p>Eventually, after all data massaging and pre-processing was done, I could start with the interesting part: the actual digit recognition.<br>\nDue to my lack of large amounts of high-variance, representative training data, I decided that it might not be a good idea to train the CNN model-based classifier completely from scratch. Instead, I used a <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch#results\" target=\"_blank\" rel=\"noopener\">pre-trained model</a> (trained on SVHN dataset) with an accuracy of 95 % for house numbers as a feature extractor and fine-tune it to work with bib numbers. To do so, I conveniently used the given <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/train.py\" target=\"_blank\" rel=\"noopener\"><code>train.py</code></a> script. However, in my understanding, it does not train only the net‚Äôs very last classification layer while keeping all previous convolutional- and normalization layers frozen, but re-trains every layer. This is not exactly what I wanted, but it turned out to work quite well.</p>\n<h1>Results</h1>\n<p>After training for 72,000 episodes with a batch size of 256, a learning rate of 10^-3 and an augmentation factor of 1.5, I eventually evaluated my two-step classification system on a set of 120 test images and reached an accuracy of <strong>~ 76 %</strong>. That is, about 3/4 of all numbers among all images were detected and classified correctly.</p>\n<p>However, there is still room for improvements. First, using a lot more training data would probably boost accuracy. Second, I didn‚Äôt do any hyper-parameter tuning, which would probably also improve performance by a few percentage points.</p>\n"},{"title":"Unhosted.org applications with remoteStorage.io and WebFinger.net","date":"2016-04-12T20:57:43.000Z","_content":"\nLately you as an interested web developer might have heard or read about a thing called **unhosted applications**, mostly with a reference to [unhosted.org](http://unhosted.org/). This basically means web-apps running in your browser which do not rely on any kind of backend. Most apps you‚Äôre using on the web store data to a backend service at the provider‚Äôs host server, regardless of them being Google Docs, Evernote, Wunderlist or also [Anchr.io](https://anchr.io). Obviously this makes you dependent on the provider in terms of availability and security. Also these apps are usually online-only apps, meaning that you can only use them with an internet connection. Another type of apps are those without a specific backend but still with a central, cloud-based data store, e.g. Firebase. In this case your data is still anywhere out there in the cloud at a provider you potentially could not trust (even though not on any kind of dubious private-hosted server but on a certified platform of one of the big players ‚Äì deciding whether that is better or worse it‚Äôs up to you, actually). These apps are usually offline-capable, so you can use them without internet connection and if the connection comes back again, the data is synced to the data platform. And then there‚Äôs this third kind of apps, which they call unhosted ones. These are intended to be completely static, without needing any kind of backend. Theoretically you could download a zip of their HTML, JavaScript and CSS files to your computer and perfectly run the app without a webserver and a database. Well, ok, probably you‚Äôll need a webserver, but it only needs to server static files (like an Apache2, nginx or [http-server](https://www.npmjs.com/package/http-server)), nothing else ‚Äì no PHP, node Node.js‚Ä¶ Those apps (e.g. a simple todo-list) store all their data to your browser‚Äôs localStorage or IndexedDB. As a result it obviously is only available on your local computer and only until you clear you browser data. After all you might still want to sync your data to other devices now ‚Äì but without giving your data away to a untrusted provider.\n\n![](/images/unhosted.jpg)\n\nThis is where quite a new thing called [RemoteStorage](https://remotestorage.io) comes in. It‚Äôs a protocol for storing per-user data on the web in an unhosted fasion. Anything implementing the remoteStorage protocol can be your data-repository, which basically works as a simple key-value store. As a result you can implement your own remoteStorage or e.g. take the ‚Äúofficial‚Äù PHP library to host one on your own server. There are also a few providers (like [5apps](http://5apps.com)) out there, yet, which you can use, but don‚Äôt have to, if you don‚Äôt trust them. An unhosted-app that is remoteStorage-capable can now connect to your remoteStorage, e.g. _https://rs.yourserver.com/user1/appXyz_, and sync its data there. RemoteStorage works together with [WebFinger](https://webfinger.net). What is WebFinger now? WebFinger is kind of a registry on the web, where unique keys (usually in email-address style, like user1@yourserver.com), are mapped to URL‚Äôs for a certain type of relation. In this case you would tell your unhosted app such an email-address-like identifier, which maps to your remoteStorage-endpoint for the relation-type ‚Äúremotestorage‚Äù. The app queries WebFinger for that key and follows the returned registered URL to the datastore. In [this example](https://client.webfinger.net/lookup?resource=tony%405apps.com) the identifier *tony@5apps.com* maps to a *remotestorage* located at _https://storage.5apps.com/tony_. This makes the entire thing as decentralized as possible. Note that you could change your remoteStorage anytime by simply registering a new URL at WebFinger (you usually wouldn‚Äôt have to do this registration on your own, but the remoteStorage server implementation handles that for you). The authorization ‚Äì like which app may access which subkeys on the remoteStorage ‚Äì is handled by OAuth at your remoteStorage server implementation, where you can grant or revoke access for certain apps to certain store keys.\n\nTwo apps you can try are [litewrite.net](https://litewrite.net/) and [Grouptabs](http://grouptabs.5apps.com). If you just want to play around with remoteStorage it might be the easiest way to use [5apps](http://5apps.com)‚Äò remoteStorage for this.","source":"_posts/unhostedorg-applications-with-remotestorageio-and-webfingernet.md","raw":"---\ntitle: Unhosted.org applications with remoteStorage.io and WebFinger.net\ndate: 2016-04-12 22:57:43\ntags:\n---\n\nLately you as an interested web developer might have heard or read about a thing called **unhosted applications**, mostly with a reference to [unhosted.org](http://unhosted.org/). This basically means web-apps running in your browser which do not rely on any kind of backend. Most apps you‚Äôre using on the web store data to a backend service at the provider‚Äôs host server, regardless of them being Google Docs, Evernote, Wunderlist or also [Anchr.io](https://anchr.io). Obviously this makes you dependent on the provider in terms of availability and security. Also these apps are usually online-only apps, meaning that you can only use them with an internet connection. Another type of apps are those without a specific backend but still with a central, cloud-based data store, e.g. Firebase. In this case your data is still anywhere out there in the cloud at a provider you potentially could not trust (even though not on any kind of dubious private-hosted server but on a certified platform of one of the big players ‚Äì deciding whether that is better or worse it‚Äôs up to you, actually). These apps are usually offline-capable, so you can use them without internet connection and if the connection comes back again, the data is synced to the data platform. And then there‚Äôs this third kind of apps, which they call unhosted ones. These are intended to be completely static, without needing any kind of backend. Theoretically you could download a zip of their HTML, JavaScript and CSS files to your computer and perfectly run the app without a webserver and a database. Well, ok, probably you‚Äôll need a webserver, but it only needs to server static files (like an Apache2, nginx or [http-server](https://www.npmjs.com/package/http-server)), nothing else ‚Äì no PHP, node Node.js‚Ä¶ Those apps (e.g. a simple todo-list) store all their data to your browser‚Äôs localStorage or IndexedDB. As a result it obviously is only available on your local computer and only until you clear you browser data. After all you might still want to sync your data to other devices now ‚Äì but without giving your data away to a untrusted provider.\n\n![](/images/unhosted.jpg)\n\nThis is where quite a new thing called [RemoteStorage](https://remotestorage.io) comes in. It‚Äôs a protocol for storing per-user data on the web in an unhosted fasion. Anything implementing the remoteStorage protocol can be your data-repository, which basically works as a simple key-value store. As a result you can implement your own remoteStorage or e.g. take the ‚Äúofficial‚Äù PHP library to host one on your own server. There are also a few providers (like [5apps](http://5apps.com)) out there, yet, which you can use, but don‚Äôt have to, if you don‚Äôt trust them. An unhosted-app that is remoteStorage-capable can now connect to your remoteStorage, e.g. _https://rs.yourserver.com/user1/appXyz_, and sync its data there. RemoteStorage works together with [WebFinger](https://webfinger.net). What is WebFinger now? WebFinger is kind of a registry on the web, where unique keys (usually in email-address style, like user1@yourserver.com), are mapped to URL‚Äôs for a certain type of relation. In this case you would tell your unhosted app such an email-address-like identifier, which maps to your remoteStorage-endpoint for the relation-type ‚Äúremotestorage‚Äù. The app queries WebFinger for that key and follows the returned registered URL to the datastore. In [this example](https://client.webfinger.net/lookup?resource=tony%405apps.com) the identifier *tony@5apps.com* maps to a *remotestorage* located at _https://storage.5apps.com/tony_. This makes the entire thing as decentralized as possible. Note that you could change your remoteStorage anytime by simply registering a new URL at WebFinger (you usually wouldn‚Äôt have to do this registration on your own, but the remoteStorage server implementation handles that for you). The authorization ‚Äì like which app may access which subkeys on the remoteStorage ‚Äì is handled by OAuth at your remoteStorage server implementation, where you can grant or revoke access for certain apps to certain store keys.\n\nTwo apps you can try are [litewrite.net](https://litewrite.net/) and [Grouptabs](http://grouptabs.5apps.com). If you just want to play around with remoteStorage it might be the easiest way to use [5apps](http://5apps.com)‚Äò remoteStorage for this.","slug":"unhostedorg-applications-with-remotestorageio-and-webfingernet","published":1,"updated":"2020-06-06T15:14:33.460Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhj6001140mqx1d2z8cv","content":"<p>Lately you as an interested web developer might have heard or read about a thing called <strong>unhosted applications</strong>, mostly with a reference to <a href=\"http://unhosted.org/\" target=\"_blank\" rel=\"noopener\">unhosted.org</a>. This basically means web-apps running in your browser which do not rely on any kind of backend. Most apps you‚Äôre using on the web store data to a backend service at the provider‚Äôs host server, regardless of them being Google Docs, Evernote, Wunderlist or also <a href=\"https://anchr.io\" target=\"_blank\" rel=\"noopener\">Anchr.io</a>. Obviously this makes you dependent on the provider in terms of availability and security. Also these apps are usually online-only apps, meaning that you can only use them with an internet connection. Another type of apps are those without a specific backend but still with a central, cloud-based data store, e.g. Firebase. In this case your data is still anywhere out there in the cloud at a provider you potentially could not trust (even though not on any kind of dubious private-hosted server but on a certified platform of one of the big players ‚Äì deciding whether that is better or worse it‚Äôs up to you, actually). These apps are usually offline-capable, so you can use them without internet connection and if the connection comes back again, the data is synced to the data platform. And then there‚Äôs this third kind of apps, which they call unhosted ones. These are intended to be completely static, without needing any kind of backend. Theoretically you could download a zip of their HTML, JavaScript and CSS files to your computer and perfectly run the app without a webserver and a database. Well, ok, probably you‚Äôll need a webserver, but it only needs to server static files (like an Apache2, nginx or <a href=\"https://www.npmjs.com/package/http-server\" target=\"_blank\" rel=\"noopener\">http-server</a>), nothing else ‚Äì no PHP, node Node.js‚Ä¶ Those apps (e.g. a simple todo-list) store all their data to your browser‚Äôs localStorage or IndexedDB. As a result it obviously is only available on your local computer and only until you clear you browser data. After all you might still want to sync your data to other devices now ‚Äì but without giving your data away to a untrusted provider.</p>\n<p><img src=\"/images/unhosted.jpg\" alt></p>\n<p>This is where quite a new thing called <a href=\"https://remotestorage.io\" target=\"_blank\" rel=\"noopener\">RemoteStorage</a> comes in. It‚Äôs a protocol for storing per-user data on the web in an unhosted fasion. Anything implementing the remoteStorage protocol can be your data-repository, which basically works as a simple key-value store. As a result you can implement your own remoteStorage or e.g. take the ‚Äúofficial‚Äù PHP library to host one on your own server. There are also a few providers (like <a href=\"http://5apps.com\" target=\"_blank\" rel=\"noopener\">5apps</a>) out there, yet, which you can use, but don‚Äôt have to, if you don‚Äôt trust them. An unhosted-app that is remoteStorage-capable can now connect to your remoteStorage, e.g. <em><a href=\"https://rs.yourserver.com/user1/appXyz\" target=\"_blank\" rel=\"noopener\">https://rs.yourserver.com/user1/appXyz</a></em>, and sync its data there. RemoteStorage works together with <a href=\"https://webfinger.net\" target=\"_blank\" rel=\"noopener\">WebFinger</a>. What is WebFinger now? WebFinger is kind of a registry on the web, where unique keys (usually in email-address style, like <a href=\"mailto:user1@yourserver.com\" target=\"_blank\" rel=\"noopener\">user1@yourserver.com</a>), are mapped to URL‚Äôs for a certain type of relation. In this case you would tell your unhosted app such an email-address-like identifier, which maps to your remoteStorage-endpoint for the relation-type ‚Äúremotestorage‚Äù. The app queries WebFinger for that key and follows the returned registered URL to the datastore. In <a href=\"https://client.webfinger.net/lookup?resource=tony%405apps.com\" target=\"_blank\" rel=\"noopener\">this example</a> the identifier <em><a href=\"mailto:tony@5apps.com\" target=\"_blank\" rel=\"noopener\">tony@5apps.com</a></em> maps to a <em>remotestorage</em> located at <em><a href=\"https://storage.5apps.com/tony\" target=\"_blank\" rel=\"noopener\">https://storage.5apps.com/tony</a></em>. This makes the entire thing as decentralized as possible. Note that you could change your remoteStorage anytime by simply registering a new URL at WebFinger (you usually wouldn‚Äôt have to do this registration on your own, but the remoteStorage server implementation handles that for you). The authorization ‚Äì like which app may access which subkeys on the remoteStorage ‚Äì is handled by OAuth at your remoteStorage server implementation, where you can grant or revoke access for certain apps to certain store keys.</p>\n<p>Two apps you can try are <a href=\"https://litewrite.net/\" target=\"_blank\" rel=\"noopener\">litewrite.net</a> and <a href=\"http://grouptabs.5apps.com\" target=\"_blank\" rel=\"noopener\">Grouptabs</a>. If you just want to play around with remoteStorage it might be the easiest way to use <a href=\"http://5apps.com\" target=\"_blank\" rel=\"noopener\">5apps</a>‚Äò remoteStorage for this.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Lately you as an interested web developer might have heard or read about a thing called <strong>unhosted applications</strong>, mostly with a reference to <a href=\"http://unhosted.org/\" target=\"_blank\" rel=\"noopener\">unhosted.org</a>. This basically means web-apps running in your browser which do not rely on any kind of backend. Most apps you‚Äôre using on the web store data to a backend service at the provider‚Äôs host server, regardless of them being Google Docs, Evernote, Wunderlist or also <a href=\"https://anchr.io\" target=\"_blank\" rel=\"noopener\">Anchr.io</a>. Obviously this makes you dependent on the provider in terms of availability and security. Also these apps are usually online-only apps, meaning that you can only use them with an internet connection. Another type of apps are those without a specific backend but still with a central, cloud-based data store, e.g. Firebase. In this case your data is still anywhere out there in the cloud at a provider you potentially could not trust (even though not on any kind of dubious private-hosted server but on a certified platform of one of the big players ‚Äì deciding whether that is better or worse it‚Äôs up to you, actually). These apps are usually offline-capable, so you can use them without internet connection and if the connection comes back again, the data is synced to the data platform. And then there‚Äôs this third kind of apps, which they call unhosted ones. These are intended to be completely static, without needing any kind of backend. Theoretically you could download a zip of their HTML, JavaScript and CSS files to your computer and perfectly run the app without a webserver and a database. Well, ok, probably you‚Äôll need a webserver, but it only needs to server static files (like an Apache2, nginx or <a href=\"https://www.npmjs.com/package/http-server\" target=\"_blank\" rel=\"noopener\">http-server</a>), nothing else ‚Äì no PHP, node Node.js‚Ä¶ Those apps (e.g. a simple todo-list) store all their data to your browser‚Äôs localStorage or IndexedDB. As a result it obviously is only available on your local computer and only until you clear you browser data. After all you might still want to sync your data to other devices now ‚Äì but without giving your data away to a untrusted provider.</p>\n<p><img src=\"/images/unhosted.jpg\" alt></p>\n<p>This is where quite a new thing called <a href=\"https://remotestorage.io\" target=\"_blank\" rel=\"noopener\">RemoteStorage</a> comes in. It‚Äôs a protocol for storing per-user data on the web in an unhosted fasion. Anything implementing the remoteStorage protocol can be your data-repository, which basically works as a simple key-value store. As a result you can implement your own remoteStorage or e.g. take the ‚Äúofficial‚Äù PHP library to host one on your own server. There are also a few providers (like <a href=\"http://5apps.com\" target=\"_blank\" rel=\"noopener\">5apps</a>) out there, yet, which you can use, but don‚Äôt have to, if you don‚Äôt trust them. An unhosted-app that is remoteStorage-capable can now connect to your remoteStorage, e.g. <em><a href=\"https://rs.yourserver.com/user1/appXyz\" target=\"_blank\" rel=\"noopener\">https://rs.yourserver.com/user1/appXyz</a></em>, and sync its data there. RemoteStorage works together with <a href=\"https://webfinger.net\" target=\"_blank\" rel=\"noopener\">WebFinger</a>. What is WebFinger now? WebFinger is kind of a registry on the web, where unique keys (usually in email-address style, like <a href=\"mailto:user1@yourserver.com\" target=\"_blank\" rel=\"noopener\">user1@yourserver.com</a>), are mapped to URL‚Äôs for a certain type of relation. In this case you would tell your unhosted app such an email-address-like identifier, which maps to your remoteStorage-endpoint for the relation-type ‚Äúremotestorage‚Äù. The app queries WebFinger for that key and follows the returned registered URL to the datastore. In <a href=\"https://client.webfinger.net/lookup?resource=tony%405apps.com\" target=\"_blank\" rel=\"noopener\">this example</a> the identifier <em><a href=\"mailto:tony@5apps.com\" target=\"_blank\" rel=\"noopener\">tony@5apps.com</a></em> maps to a <em>remotestorage</em> located at <em><a href=\"https://storage.5apps.com/tony\" target=\"_blank\" rel=\"noopener\">https://storage.5apps.com/tony</a></em>. This makes the entire thing as decentralized as possible. Note that you could change your remoteStorage anytime by simply registering a new URL at WebFinger (you usually wouldn‚Äôt have to do this registration on your own, but the remoteStorage server implementation handles that for you). The authorization ‚Äì like which app may access which subkeys on the remoteStorage ‚Äì is handled by OAuth at your remoteStorage server implementation, where you can grant or revoke access for certain apps to certain store keys.</p>\n<p>Two apps you can try are <a href=\"https://litewrite.net/\" target=\"_blank\" rel=\"noopener\">litewrite.net</a> and <a href=\"http://grouptabs.5apps.com\" target=\"_blank\" rel=\"noopener\">Grouptabs</a>. If you just want to play around with remoteStorage it might be the easiest way to use <a href=\"http://5apps.com\" target=\"_blank\" rel=\"noopener\">5apps</a>‚Äò remoteStorage for this.</p>\n"},{"title":"Web Development Technology Stack","date":"2016-03-15T21:54:04.000Z","_content":"\nI tried to give a comprehensive overview of all important and fashionable technologies concerning web- and cross-platform development, so I created kind of a mind map outlining relevant technologies in terms of languages, frameworks, libraries, webservices and tools. I consider them kind of must-haves for JavaScript fullstack web-developers. You might have already heard of most of them and probably you have even worked with the majority.\n\nFeel free to give your opinion on this collection and maybe add something you‚Äôre missing (in the comments after having logged in, via mail or via [telegram.me/n1try](http://telegram.me/n1try)).\n\n[![Web Tech Stack](/imgages/webdev_techstack.png)](/imgages/webdev_techstack_large.png)","source":"_posts/web-development-technology-stack.md","raw":"---\ntitle: Web Development Technology Stack\ndate: 2016-03-15 22:54:04\ntags:\n---\n\nI tried to give a comprehensive overview of all important and fashionable technologies concerning web- and cross-platform development, so I created kind of a mind map outlining relevant technologies in terms of languages, frameworks, libraries, webservices and tools. I consider them kind of must-haves for JavaScript fullstack web-developers. You might have already heard of most of them and probably you have even worked with the majority.\n\nFeel free to give your opinion on this collection and maybe add something you‚Äôre missing (in the comments after having logged in, via mail or via [telegram.me/n1try](http://telegram.me/n1try)).\n\n[![Web Tech Stack](/imgages/webdev_techstack.png)](/imgages/webdev_techstack_large.png)","slug":"web-development-technology-stack","published":1,"updated":"2020-06-06T15:14:33.460Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhj9001240mqqvih2uee","content":"<p>I tried to give a comprehensive overview of all important and fashionable technologies concerning web- and cross-platform development, so I created kind of a mind map outlining relevant technologies in terms of languages, frameworks, libraries, webservices and tools. I consider them kind of must-haves for JavaScript fullstack web-developers. You might have already heard of most of them and probably you have even worked with the majority.</p>\n<p>Feel free to give your opinion on this collection and maybe add something you‚Äôre missing (in the comments after having logged in, via mail or via <a href=\"http://telegram.me/n1try\" target=\"_blank\" rel=\"noopener\">telegram.me/n1try</a>).</p>\n<p><a href=\"/imgages/webdev_techstack_large.png\"><img src=\"/imgages/webdev_techstack.png\" alt=\"Web Tech Stack\"></a></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>I tried to give a comprehensive overview of all important and fashionable technologies concerning web- and cross-platform development, so I created kind of a mind map outlining relevant technologies in terms of languages, frameworks, libraries, webservices and tools. I consider them kind of must-haves for JavaScript fullstack web-developers. You might have already heard of most of them and probably you have even worked with the majority.</p>\n<p>Feel free to give your opinion on this collection and maybe add something you‚Äôre missing (in the comments after having logged in, via mail or via <a href=\"http://telegram.me/n1try\" target=\"_blank\" rel=\"noopener\">telegram.me/n1try</a>).</p>\n<p><a href=\"/imgages/webdev_techstack_large.png\"><img src=\"/imgages/webdev_techstack.png\" alt=\"Web Tech Stack\"></a></p>\n"},{"title":"Webdevlist.net - The Developer's Resource Collection","date":"2016-09-21T21:02:25.000Z","_content":"\n![](/images/webdevlist.jpg)\n\nI just put up a new project of mine, which is called Webdevlist.\n\n### So what is Webdevlist?\n\nI‚Äôm pretty sure most developers know it all too well: you browse Twitter, Reddit or Stackoverflow on a late Sunday evening and come across some cool programming framework or webservice. You think like _‚ÄúWow that‚Äôs cool! Could be helpful some time. I need to remember it.‚Äù_ Then you put the link anywhere in a messy text file or the like and never visit it again. Webdevlist‚Äôs aim is to become a comprehensive, community-maintained collection of useful resources concerning software development. Such could include frameworks and libraries for any language, devtools, software applications, apps, webservices like PaaS, SaaS or IaaS and also learning resources like guides and tutorials. Everytime you find a cool tool on the web, just post it to the list. Everytime you‚Äôre looking for a tool to help you with your development problems come visit the list. Easy enough.\n\n### Tech facts\n\nWebdevlist‚Äôs frontend is built with [Angular2](https://angular.io/ \"Angular2\"), which just has had its first final release. The backend makes use of [LoopbackJS](http://loopback.io/ \"LoopbackJS\"), which is a mighty framework to build REST APIs with a minimum of boilerplate code. Additionally the site is powered by HTTP/2.0 to get some more speed.\n\n### Is it finished, yet?\n\nNo, it isn‚Äôt. Actually, it probably never will be. I‚Äôm continuously going to add new technology to Webdevlist‚Äôs stack and change and refactor things. Currently I‚Äôm considering to switch to [GraphQL](http://graphql.org/learn/ \"GraphQL\"). Right now, Webdevlist is in some kind of beta state, meaning that it still might have bugs.\n\nI‚Äôd really appreciate to get feedback on this project!\n\n[>> Webdevlist.net](https://webdevlist.net)  \n[>> Webdevlist on GitHub](https://github.com/muety/webdevlist.net) ","source":"_posts/webdevlistnet-the-developers-resource-collection.md","raw":"---\ntitle: Webdevlist.net - The Developer's Resource Collection\ndate: 2016-09-21 23:02:25\ntags:\n---\n\n![](/images/webdevlist.jpg)\n\nI just put up a new project of mine, which is called Webdevlist.\n\n### So what is Webdevlist?\n\nI‚Äôm pretty sure most developers know it all too well: you browse Twitter, Reddit or Stackoverflow on a late Sunday evening and come across some cool programming framework or webservice. You think like _‚ÄúWow that‚Äôs cool! Could be helpful some time. I need to remember it.‚Äù_ Then you put the link anywhere in a messy text file or the like and never visit it again. Webdevlist‚Äôs aim is to become a comprehensive, community-maintained collection of useful resources concerning software development. Such could include frameworks and libraries for any language, devtools, software applications, apps, webservices like PaaS, SaaS or IaaS and also learning resources like guides and tutorials. Everytime you find a cool tool on the web, just post it to the list. Everytime you‚Äôre looking for a tool to help you with your development problems come visit the list. Easy enough.\n\n### Tech facts\n\nWebdevlist‚Äôs frontend is built with [Angular2](https://angular.io/ \"Angular2\"), which just has had its first final release. The backend makes use of [LoopbackJS](http://loopback.io/ \"LoopbackJS\"), which is a mighty framework to build REST APIs with a minimum of boilerplate code. Additionally the site is powered by HTTP/2.0 to get some more speed.\n\n### Is it finished, yet?\n\nNo, it isn‚Äôt. Actually, it probably never will be. I‚Äôm continuously going to add new technology to Webdevlist‚Äôs stack and change and refactor things. Currently I‚Äôm considering to switch to [GraphQL](http://graphql.org/learn/ \"GraphQL\"). Right now, Webdevlist is in some kind of beta state, meaning that it still might have bugs.\n\nI‚Äôd really appreciate to get feedback on this project!\n\n[>> Webdevlist.net](https://webdevlist.net)  \n[>> Webdevlist on GitHub](https://github.com/muety/webdevlist.net) ","slug":"webdevlistnet-the-developers-resource-collection","published":1,"updated":"2020-06-06T15:14:33.460Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhja001340mqympb5mgx","content":"<p><img src=\"/images/webdevlist.jpg\" alt></p>\n<p>I just put up a new project of mine, which is called Webdevlist.</p>\n<h3 id=\"So-what-is-Webdevlist\">So what is Webdevlist?</h3>\n<p>I‚Äôm pretty sure most developers know it all too well: you browse Twitter, Reddit or Stackoverflow on a late Sunday evening and come across some cool programming framework or webservice. You think like <em>‚ÄúWow that‚Äôs cool! Could be helpful some time. I need to remember it.‚Äù</em> Then you put the link anywhere in a messy text file or the like and never visit it again. Webdevlist‚Äôs aim is to become a comprehensive, community-maintained collection of useful resources concerning software development. Such could include frameworks and libraries for any language, devtools, software applications, apps, webservices like PaaS, SaaS or IaaS and also learning resources like guides and tutorials. Everytime you find a cool tool on the web, just post it to the list. Everytime you‚Äôre looking for a tool to help you with your development problems come visit the list. Easy enough.</p>\n<h3 id=\"Tech-facts\">Tech facts</h3>\n<p>Webdevlist‚Äôs frontend is built with <a href=\"https://angular.io/\" title=\"Angular2\" target=\"_blank\" rel=\"noopener\">Angular2</a>, which just has had its first final release. The backend makes use of <a href=\"http://loopback.io/\" title=\"LoopbackJS\" target=\"_blank\" rel=\"noopener\">LoopbackJS</a>, which is a mighty framework to build REST APIs with a minimum of boilerplate code. Additionally the site is powered by HTTP/2.0 to get some more speed.</p>\n<h3 id=\"Is-it-finished-yet\">Is it finished, yet?</h3>\n<p>No, it isn‚Äôt. Actually, it probably never will be. I‚Äôm continuously going to add new technology to Webdevlist‚Äôs stack and change and refactor things. Currently I‚Äôm considering to switch to <a href=\"http://graphql.org/learn/\" title=\"GraphQL\" target=\"_blank\" rel=\"noopener\">GraphQL</a>. Right now, Webdevlist is in some kind of beta state, meaning that it still might have bugs.</p>\n<p>I‚Äôd really appreciate to get feedback on this project!</p>\n<p><a href=\"https://webdevlist.net\" target=\"_blank\" rel=\"noopener\">&gt;&gt; Webdevlist.net</a><br>\n<a href=\"https://github.com/muety/webdevlist.net\" target=\"_blank\" rel=\"noopener\">&gt;&gt; Webdevlist on GitHub</a></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"/images/webdevlist.jpg\" alt></p>\n<p>I just put up a new project of mine, which is called Webdevlist.</p>\n<h3 id=\"So-what-is-Webdevlist\">So what is Webdevlist?</h3>\n<p>I‚Äôm pretty sure most developers know it all too well: you browse Twitter, Reddit or Stackoverflow on a late Sunday evening and come across some cool programming framework or webservice. You think like <em>‚ÄúWow that‚Äôs cool! Could be helpful some time. I need to remember it.‚Äù</em> Then you put the link anywhere in a messy text file or the like and never visit it again. Webdevlist‚Äôs aim is to become a comprehensive, community-maintained collection of useful resources concerning software development. Such could include frameworks and libraries for any language, devtools, software applications, apps, webservices like PaaS, SaaS or IaaS and also learning resources like guides and tutorials. Everytime you find a cool tool on the web, just post it to the list. Everytime you‚Äôre looking for a tool to help you with your development problems come visit the list. Easy enough.</p>\n<h3 id=\"Tech-facts\">Tech facts</h3>\n<p>Webdevlist‚Äôs frontend is built with <a href=\"https://angular.io/\" title=\"Angular2\" target=\"_blank\" rel=\"noopener\">Angular2</a>, which just has had its first final release. The backend makes use of <a href=\"http://loopback.io/\" title=\"LoopbackJS\" target=\"_blank\" rel=\"noopener\">LoopbackJS</a>, which is a mighty framework to build REST APIs with a minimum of boilerplate code. Additionally the site is powered by HTTP/2.0 to get some more speed.</p>\n<h3 id=\"Is-it-finished-yet\">Is it finished, yet?</h3>\n<p>No, it isn‚Äôt. Actually, it probably never will be. I‚Äôm continuously going to add new technology to Webdevlist‚Äôs stack and change and refactor things. Currently I‚Äôm considering to switch to <a href=\"http://graphql.org/learn/\" title=\"GraphQL\" target=\"_blank\" rel=\"noopener\">GraphQL</a>. Right now, Webdevlist is in some kind of beta state, meaning that it still might have bugs.</p>\n<p>I‚Äôd really appreciate to get feedback on this project!</p>\n<p><a href=\"https://webdevlist.net\" target=\"_blank\" rel=\"noopener\">&gt;&gt; Webdevlist.net</a><br>\n<a href=\"https://github.com/muety/webdevlist.net\" target=\"_blank\" rel=\"noopener\">&gt;&gt; Webdevlist on GitHub</a></p>\n"},{"title":"What I like about developing apps with Flutter","date":"2019-03-11T07:45:20.000Z","_content":"\nAfter hearing about [Flutter](https://flutter.dev) by [Matt Carroll](https://twitter.com/@flttry) and [Abraham Williams](https://twitter.com/abraham) at the [SFHTML5 Meetup](https://www.meetup.com/de-DE/sfhtml5/events/256523273/) hosted by Google in San Francisco a few weeks ago, I decided to give it a try. I developed a small [bookmark manager](https://github.com/muety/anchr-android) app for Android while attempting to learn Flutter.\n\n# What is Flutter?\nFlutter is an open-source framework for developing cross-platform mobile apps. Or in their own words it ...\n> [...] allows you to build beautiful native apps on iOS and Android from a single codebase. \n\n\"Cross-platform\" means that you develop an app that can run on multiple platforms, in this case, Android, iOS and (in the future) also [Fuchsia](https://en.wikipedia.org/wiki/Google_Fuchsia). Traditionally, you would have to write the same app multiple times, e.g. once in Java for Android and a second time in Swift for iOS. This is a pain for developers, obviously, and Flutter tries to overcome it. \n\nThere are already several approaches out there, trying to solve cross-platform mobile development. They include **hybrid** app frameworks like [Ionic](https://ionicframework.com/) and [Cordova](https://cordova.apache.org/) as well as **native cross-platform** frameworks like [React Native](https://facebook.github.io/react-native/), [NativeScript](https://www.nativescript.org/) and [Xamarin](https://visualstudio.microsoft.com/xamarin/).\n\nBy the way, there is is a very interesting comparison of [Ionic vs. React Native](https://www.codementor.io/fmcorz/react-native-vs-ionic-du1087rsw). \n\nWhile hybrid frameworks basically render a website to the device's screen and native frameworks translate TypeScript (or C#) code into native iOS / Android components, Flutter [\"works more like a game engine\"](https://buildflutter.com/how-flutter-works/). It has an extremely efficient engine that layouts widgets and renders them to a canvas. Therefore it usually a lot faster and less laggy than hybrid frameworks. \n\nUnlike most other cross-platform frameworks, where you usually program in JavaScript (or C# in the case of Xamarin), Flutter relies on [Dart](https://dartlang.org), created by Google. Consequently, in order to learn Flutter, you would usually not only have to learn the framework itself, but also a new programming language. This may seem like a high entry barrier, but it actually is not.\n\n# Pros\n## What I **DO** like about Dart\n1. **It looks familiar.** Dart really just feels like Java and JavaScript having a baby. It does not follow any exotic paradigms. Instead, if you know Java and JavaScript, learning Dart is quite easy since there are only very few new concepts and syntaxes. \nFor instance, it is **object-oriented** and has a proper inheritance system, similar to Java. In addition, it has a **static typing system**, including generics, but with **optional type declarations**, similar to Scala.\nJavaScript people will find well-known concepts as well, such as **futures, generators and `async / await`**.\nSimilar to Scala's _traits_, Dart even supports a kind of **multiple class inheritance** using _mixins_, which is quite handy once you get used to it. Having a Java background another cool new thing in Dart is **factory constructors**.\n2. **It feels consistent.** What I did not like about TypeScript is that sometimes it just feels a bit hacky, e.g. when third-party typings for a certain library are buggy or even missing. In my opinion, this originates in TypeScript's approach of trying to extend an existing programming language with whole new functionality. While TypeScripts is still a great language, Dart just feels more consistent in its whole. It was designed completely from scratch with all the above concepts in mind and since it is quite young, it still feels clean and minimalistic. However, since it is not that widely used, yet, the ecosystem (including tooling, libraries, ...) is way smaller compared to JavaScript or native Android / iOS.\n3. **It is multi-purpose.** Dart can not only be used for mobile development but is quite generic. You can use it to write your backend and there are transpilers to use it in the browser as well. Since frameworks like Angular start to have Dart support (see [AngularDart](https://webdev.dartlang.org/angular/)), I feel like it could [gain popularity](https://medium.com/@mswehli/why-dart-is-the-language-to-learn-of-2018-e5fa12adb6c1) in the next years. \n4. **It has named parameters üòÉ.** They exist in many programming languages, including Python and Scala and I really got used to them. In my opinion, code gets a lot cleaner with named parameters. \n\n## What I **DO** like about Flutter\n1. **Easy layouting.** Dart follows the paradigm that _\"everything is a widget\"_. So instead of defining layouts with XML (like in Android) or with HTML + CSS (like in the web), everything is done programmatically. The way you build components is by recursively nesting widgets into other widgets. Such might include UI components like a `TextView` and `FloatingActionButton` or more abstract things like a `GestureDetector` or `Padding`. The structure is always the same and that makes UI composition easy to me.\n2. **Tons of built-in components.** Flutter comes with a giant [catalog of widgets](https://flutter.dev/docs/development/ui/widgets) for layouting, styling, animations and UI components (all of which perfectly follow Material design). So far I could find everything I needed in the widget catalog without having to use any poorly-maintained third-party libs.\n3. **It feels so real.** Often times, hybrid apps still do not completely feel like a real app, even if they are using Material design libraries etc. Maybe the navigation drawer's sliding animation is too rough, a page transition is slightly laggy or the text style just does not look quite right. At least this is what I experienced with Ionic. With Flutter, however, I could not tell the difference compared to a truly native app. Everything is super fast, smooth and pixel-perfect. \n4. **It is future-proof.** Google is pushing Flutter really hard as they keep posting blog articles and developer videos about it. I feel like they really want people to adopt it, which might relate to their development on Fuchsia. No matter whether or not they are planning to [replace Android with Fuchsia](https://www.reddit.com/r/androiddev/comments/6aga8e/in_your_opinion_will_google_fuchsia_replace/) some day, Flutter is definitely a good thing to know.\n5. **Good tooling.** Although the ecosystem around Flutter is not that big, yet, the built-in tooling is great. Flutter comes with an intuitive **CLI** and is well integrated with my favorite editors IntelliJ (**Android Studio**) and **Visual Studio Code**. \n6. **Open-Source.** I love open-source, so I am glad that Flutter is completely open to the community and available on [GitHub](https://github.com/flutter/flutter). \n\n# Cons\n\n## What I **DO NOT** like about Dart\n1. **No functional APIs.** I do not expect a functional programming language like Scala, but since I really got used to Java 8's stream API, I am a bit sad that Dart has almost no functional APIs. In my opinion, code gets a lot cleaner with such.\n\n## What I **DO NOT** like about Flutter\n1. **Is it truly cross-platform?** This is rather a question than something I strictly do not like. So far, I only developed an Android app with Flutter and I wonder whether it is actually possible to use the exact same code for iOS. Sometimes you will still need to access native APIs, e.g. when attempting to [receive a sharing intent](https://muetsch.io/how-to-receive-sharing-intents-in-flutter.html), or have varying design elements. So although I am not totally sure about this, I would assume that for large projects you would still need to write two separate Flutter apps for Android and iOS, but have the ability to reuse large parts of the code in form of a shared library. \n2. **Apps are large * .** Maybe this will be improved in the future, but right now Flutter apps are pretty large. My [simple bookmark manager](https://github.com/muety/anchr-android) is 46 MB in size when installed.\n\n# Where to get started?\nIf you want to start learning Flutter, I would recommend the following. \n1. Take the [Tour of Dart](https://www.dartlang.org/guides/language/language-tour) and get familiar with Dart's syntax and concepts.\n2. Walk through [A month of Flutter](https://bendyworks.com/blog/a-month-of-flutter) to start building a real-world Flutter app step-by-step.\n\nHave fun and happy coding! ü§ì\n\n***** As it turned out, this is actually not a valid point of criticism, since I had generated the app in debug mode. When building a release, the total size is only 6.4 MB.","source":"_posts/what-i-like-about-developing-apps-with-flutter.md","raw":"---\ntitle: What I like about developing apps with Flutter\ndate: 2019-03-11 08:45:20\ntags:\n---\n\nAfter hearing about [Flutter](https://flutter.dev) by [Matt Carroll](https://twitter.com/@flttry) and [Abraham Williams](https://twitter.com/abraham) at the [SFHTML5 Meetup](https://www.meetup.com/de-DE/sfhtml5/events/256523273/) hosted by Google in San Francisco a few weeks ago, I decided to give it a try. I developed a small [bookmark manager](https://github.com/muety/anchr-android) app for Android while attempting to learn Flutter.\n\n# What is Flutter?\nFlutter is an open-source framework for developing cross-platform mobile apps. Or in their own words it ...\n> [...] allows you to build beautiful native apps on iOS and Android from a single codebase. \n\n\"Cross-platform\" means that you develop an app that can run on multiple platforms, in this case, Android, iOS and (in the future) also [Fuchsia](https://en.wikipedia.org/wiki/Google_Fuchsia). Traditionally, you would have to write the same app multiple times, e.g. once in Java for Android and a second time in Swift for iOS. This is a pain for developers, obviously, and Flutter tries to overcome it. \n\nThere are already several approaches out there, trying to solve cross-platform mobile development. They include **hybrid** app frameworks like [Ionic](https://ionicframework.com/) and [Cordova](https://cordova.apache.org/) as well as **native cross-platform** frameworks like [React Native](https://facebook.github.io/react-native/), [NativeScript](https://www.nativescript.org/) and [Xamarin](https://visualstudio.microsoft.com/xamarin/).\n\nBy the way, there is is a very interesting comparison of [Ionic vs. React Native](https://www.codementor.io/fmcorz/react-native-vs-ionic-du1087rsw). \n\nWhile hybrid frameworks basically render a website to the device's screen and native frameworks translate TypeScript (or C#) code into native iOS / Android components, Flutter [\"works more like a game engine\"](https://buildflutter.com/how-flutter-works/). It has an extremely efficient engine that layouts widgets and renders them to a canvas. Therefore it usually a lot faster and less laggy than hybrid frameworks. \n\nUnlike most other cross-platform frameworks, where you usually program in JavaScript (or C# in the case of Xamarin), Flutter relies on [Dart](https://dartlang.org), created by Google. Consequently, in order to learn Flutter, you would usually not only have to learn the framework itself, but also a new programming language. This may seem like a high entry barrier, but it actually is not.\n\n# Pros\n## What I **DO** like about Dart\n1. **It looks familiar.** Dart really just feels like Java and JavaScript having a baby. It does not follow any exotic paradigms. Instead, if you know Java and JavaScript, learning Dart is quite easy since there are only very few new concepts and syntaxes. \nFor instance, it is **object-oriented** and has a proper inheritance system, similar to Java. In addition, it has a **static typing system**, including generics, but with **optional type declarations**, similar to Scala.\nJavaScript people will find well-known concepts as well, such as **futures, generators and `async / await`**.\nSimilar to Scala's _traits_, Dart even supports a kind of **multiple class inheritance** using _mixins_, which is quite handy once you get used to it. Having a Java background another cool new thing in Dart is **factory constructors**.\n2. **It feels consistent.** What I did not like about TypeScript is that sometimes it just feels a bit hacky, e.g. when third-party typings for a certain library are buggy or even missing. In my opinion, this originates in TypeScript's approach of trying to extend an existing programming language with whole new functionality. While TypeScripts is still a great language, Dart just feels more consistent in its whole. It was designed completely from scratch with all the above concepts in mind and since it is quite young, it still feels clean and minimalistic. However, since it is not that widely used, yet, the ecosystem (including tooling, libraries, ...) is way smaller compared to JavaScript or native Android / iOS.\n3. **It is multi-purpose.** Dart can not only be used for mobile development but is quite generic. You can use it to write your backend and there are transpilers to use it in the browser as well. Since frameworks like Angular start to have Dart support (see [AngularDart](https://webdev.dartlang.org/angular/)), I feel like it could [gain popularity](https://medium.com/@mswehli/why-dart-is-the-language-to-learn-of-2018-e5fa12adb6c1) in the next years. \n4. **It has named parameters üòÉ.** They exist in many programming languages, including Python and Scala and I really got used to them. In my opinion, code gets a lot cleaner with named parameters. \n\n## What I **DO** like about Flutter\n1. **Easy layouting.** Dart follows the paradigm that _\"everything is a widget\"_. So instead of defining layouts with XML (like in Android) or with HTML + CSS (like in the web), everything is done programmatically. The way you build components is by recursively nesting widgets into other widgets. Such might include UI components like a `TextView` and `FloatingActionButton` or more abstract things like a `GestureDetector` or `Padding`. The structure is always the same and that makes UI composition easy to me.\n2. **Tons of built-in components.** Flutter comes with a giant [catalog of widgets](https://flutter.dev/docs/development/ui/widgets) for layouting, styling, animations and UI components (all of which perfectly follow Material design). So far I could find everything I needed in the widget catalog without having to use any poorly-maintained third-party libs.\n3. **It feels so real.** Often times, hybrid apps still do not completely feel like a real app, even if they are using Material design libraries etc. Maybe the navigation drawer's sliding animation is too rough, a page transition is slightly laggy or the text style just does not look quite right. At least this is what I experienced with Ionic. With Flutter, however, I could not tell the difference compared to a truly native app. Everything is super fast, smooth and pixel-perfect. \n4. **It is future-proof.** Google is pushing Flutter really hard as they keep posting blog articles and developer videos about it. I feel like they really want people to adopt it, which might relate to their development on Fuchsia. No matter whether or not they are planning to [replace Android with Fuchsia](https://www.reddit.com/r/androiddev/comments/6aga8e/in_your_opinion_will_google_fuchsia_replace/) some day, Flutter is definitely a good thing to know.\n5. **Good tooling.** Although the ecosystem around Flutter is not that big, yet, the built-in tooling is great. Flutter comes with an intuitive **CLI** and is well integrated with my favorite editors IntelliJ (**Android Studio**) and **Visual Studio Code**. \n6. **Open-Source.** I love open-source, so I am glad that Flutter is completely open to the community and available on [GitHub](https://github.com/flutter/flutter). \n\n# Cons\n\n## What I **DO NOT** like about Dart\n1. **No functional APIs.** I do not expect a functional programming language like Scala, but since I really got used to Java 8's stream API, I am a bit sad that Dart has almost no functional APIs. In my opinion, code gets a lot cleaner with such.\n\n## What I **DO NOT** like about Flutter\n1. **Is it truly cross-platform?** This is rather a question than something I strictly do not like. So far, I only developed an Android app with Flutter and I wonder whether it is actually possible to use the exact same code for iOS. Sometimes you will still need to access native APIs, e.g. when attempting to [receive a sharing intent](https://muetsch.io/how-to-receive-sharing-intents-in-flutter.html), or have varying design elements. So although I am not totally sure about this, I would assume that for large projects you would still need to write two separate Flutter apps for Android and iOS, but have the ability to reuse large parts of the code in form of a shared library. \n2. **Apps are large * .** Maybe this will be improved in the future, but right now Flutter apps are pretty large. My [simple bookmark manager](https://github.com/muety/anchr-android) is 46 MB in size when installed.\n\n# Where to get started?\nIf you want to start learning Flutter, I would recommend the following. \n1. Take the [Tour of Dart](https://www.dartlang.org/guides/language/language-tour) and get familiar with Dart's syntax and concepts.\n2. Walk through [A month of Flutter](https://bendyworks.com/blog/a-month-of-flutter) to start building a real-world Flutter app step-by-step.\n\nHave fun and happy coding! ü§ì\n\n***** As it turned out, this is actually not a valid point of criticism, since I had generated the app in debug mode. When building a release, the total size is only 6.4 MB.","slug":"what-i-like-about-developing-apps-with-flutter","published":1,"updated":"2020-06-06T15:14:33.460Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhjc001440mqfq6brisx","content":"<p>After hearing about <a href=\"https://flutter.dev\" target=\"_blank\" rel=\"noopener\">Flutter</a> by <a href=\"https://twitter.com/@flttry\" target=\"_blank\" rel=\"noopener\">Matt Carroll</a> and <a href=\"https://twitter.com/abraham\" target=\"_blank\" rel=\"noopener\">Abraham Williams</a> at the <a href=\"https://www.meetup.com/de-DE/sfhtml5/events/256523273/\" target=\"_blank\" rel=\"noopener\">SFHTML5 Meetup</a> hosted by Google in San Francisco a few weeks ago, I decided to give it a try. I developed a small <a href=\"https://github.com/muety/anchr-android\" target=\"_blank\" rel=\"noopener\">bookmark manager</a> app for Android while attempting to learn Flutter.</p>\n<h1>What is Flutter?</h1>\n<p>Flutter is an open-source framework for developing cross-platform mobile apps. Or in their own words it ‚Ä¶</p>\n<blockquote>\n<p>[‚Ä¶] allows you to build beautiful native apps on iOS and Android from a single codebase.</p>\n</blockquote>\n<p>‚ÄúCross-platform‚Äù means that you develop an app that can run on multiple platforms, in this case, Android, iOS and (in the future) also <a href=\"https://en.wikipedia.org/wiki/Google_Fuchsia\" target=\"_blank\" rel=\"noopener\">Fuchsia</a>. Traditionally, you would have to write the same app multiple times, e.g. once in Java for Android and a second time in Swift for iOS. This is a pain for developers, obviously, and Flutter tries to overcome it.</p>\n<p>There are already several approaches out there, trying to solve cross-platform mobile development. They include <strong>hybrid</strong> app frameworks like <a href=\"https://ionicframework.com/\" target=\"_blank\" rel=\"noopener\">Ionic</a> and <a href=\"https://cordova.apache.org/\" target=\"_blank\" rel=\"noopener\">Cordova</a> as well as <strong>native cross-platform</strong> frameworks like <a href=\"https://facebook.github.io/react-native/\" target=\"_blank\" rel=\"noopener\">React Native</a>, <a href=\"https://www.nativescript.org/\" target=\"_blank\" rel=\"noopener\">NativeScript</a> and <a href=\"https://visualstudio.microsoft.com/xamarin/\" target=\"_blank\" rel=\"noopener\">Xamarin</a>.</p>\n<p>By the way, there is is a very interesting comparison of <a href=\"https://www.codementor.io/fmcorz/react-native-vs-ionic-du1087rsw\" target=\"_blank\" rel=\"noopener\">Ionic vs. React Native</a>.</p>\n<p>While hybrid frameworks basically render a website to the device‚Äôs screen and native frameworks translate TypeScript (or C#) code into native iOS / Android components, Flutter <a href=\"https://buildflutter.com/how-flutter-works/\" target=\"_blank\" rel=\"noopener\">‚Äúworks more like a game engine‚Äù</a>. It has an extremely efficient engine that layouts widgets and renders them to a canvas. Therefore it usually a lot faster and less laggy than hybrid frameworks.</p>\n<p>Unlike most other cross-platform frameworks, where you usually program in JavaScript (or C# in the case of Xamarin), Flutter relies on <a href=\"https://dartlang.org\" target=\"_blank\" rel=\"noopener\">Dart</a>, created by Google. Consequently, in order to learn Flutter, you would usually not only have to learn the framework itself, but also a new programming language. This may seem like a high entry barrier, but it actually is not.</p>\n<h1>Pros</h1>\n<h2 id=\"What-I-DO-like-about-Dart\">What I <strong>DO</strong> like about Dart</h2>\n<ol>\n<li><strong>It looks familiar.</strong> Dart really just feels like Java and JavaScript having a baby. It does not follow any exotic paradigms. Instead, if you know Java and JavaScript, learning Dart is quite easy since there are only very few new concepts and syntaxes.<br>\nFor instance, it is <strong>object-oriented</strong> and has a proper inheritance system, similar to Java. In addition, it has a <strong>static typing system</strong>, including generics, but with <strong>optional type declarations</strong>, similar to Scala.<br>\nJavaScript people will find well-known concepts as well, such as <strong>futures, generators and <code>async / await</code></strong>.<br>\nSimilar to Scala‚Äôs <em>traits</em>, Dart even supports a kind of <strong>multiple class inheritance</strong> using <em>mixins</em>, which is quite handy once you get used to it. Having a Java background another cool new thing in Dart is <strong>factory constructors</strong>.</li>\n<li><strong>It feels consistent.</strong> What I did not like about TypeScript is that sometimes it just feels a bit hacky, e.g. when third-party typings for a certain library are buggy or even missing. In my opinion, this originates in TypeScript‚Äôs approach of trying to extend an existing programming language with whole new functionality. While TypeScripts is still a great language, Dart just feels more consistent in its whole. It was designed completely from scratch with all the above concepts in mind and since it is quite young, it still feels clean and minimalistic. However, since it is not that widely used, yet, the ecosystem (including tooling, libraries, ‚Ä¶) is way smaller compared to JavaScript or native Android / iOS.</li>\n<li><strong>It is multi-purpose.</strong> Dart can not only be used for mobile development but is quite generic. You can use it to write your backend and there are transpilers to use it in the browser as well. Since frameworks like Angular start to have Dart support (see <a href=\"https://webdev.dartlang.org/angular/\" target=\"_blank\" rel=\"noopener\">AngularDart</a>), I feel like it could <a href=\"https://medium.com/@mswehli/why-dart-is-the-language-to-learn-of-2018-e5fa12adb6c1\" target=\"_blank\" rel=\"noopener\">gain popularity</a> in the next years.</li>\n<li><strong>It has named parameters üòÉ.</strong> They exist in many programming languages, including Python and Scala and I really got used to them. In my opinion, code gets a lot cleaner with named parameters.</li>\n</ol>\n<h2 id=\"What-I-DO-like-about-Flutter\">What I <strong>DO</strong> like about Flutter</h2>\n<ol>\n<li><strong>Easy layouting.</strong> Dart follows the paradigm that <em>‚Äúeverything is a widget‚Äù</em>. So instead of defining layouts with XML (like in Android) or with HTML + CSS (like in the web), everything is done programmatically. The way you build components is by recursively nesting widgets into other widgets. Such might include UI components like a <code>TextView</code> and <code>FloatingActionButton</code> or more abstract things like a <code>GestureDetector</code> or <code>Padding</code>. The structure is always the same and that makes UI composition easy to me.</li>\n<li><strong>Tons of built-in components.</strong> Flutter comes with a giant <a href=\"https://flutter.dev/docs/development/ui/widgets\" target=\"_blank\" rel=\"noopener\">catalog of widgets</a> for layouting, styling, animations and UI components (all of which perfectly follow Material design). So far I could find everything I needed in the widget catalog without having to use any poorly-maintained third-party libs.</li>\n<li><strong>It feels so real.</strong> Often times, hybrid apps still do not completely feel like a real app, even if they are using Material design libraries etc. Maybe the navigation drawer‚Äôs sliding animation is too rough, a page transition is slightly laggy or the text style just does not look quite right. At least this is what I experienced with Ionic. With Flutter, however, I could not tell the difference compared to a truly native app. Everything is super fast, smooth and pixel-perfect.</li>\n<li><strong>It is future-proof.</strong> Google is pushing Flutter really hard as they keep posting blog articles and developer videos about it. I feel like they really want people to adopt it, which might relate to their development on Fuchsia. No matter whether or not they are planning to <a href=\"https://www.reddit.com/r/androiddev/comments/6aga8e/in_your_opinion_will_google_fuchsia_replace/\" target=\"_blank\" rel=\"noopener\">replace Android with Fuchsia</a> some day, Flutter is definitely a good thing to know.</li>\n<li><strong>Good tooling.</strong> Although the ecosystem around Flutter is not that big, yet, the built-in tooling is great. Flutter comes with an intuitive <strong>CLI</strong> and is well integrated with my favorite editors IntelliJ (<strong>Android Studio</strong>) and <strong>Visual Studio Code</strong>.</li>\n<li><strong>Open-Source.</strong> I love open-source, so I am glad that Flutter is completely open to the community and available on <a href=\"https://github.com/flutter/flutter\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</li>\n</ol>\n<h1>Cons</h1>\n<h2 id=\"What-I-DO-NOT-like-about-Dart\">What I <strong>DO NOT</strong> like about Dart</h2>\n<ol>\n<li><strong>No functional APIs.</strong> I do not expect a functional programming language like Scala, but since I really got used to Java 8‚Äôs stream API, I am a bit sad that Dart has almost no functional APIs. In my opinion, code gets a lot cleaner with such.</li>\n</ol>\n<h2 id=\"What-I-DO-NOT-like-about-Flutter\">What I <strong>DO NOT</strong> like about Flutter</h2>\n<ol>\n<li><strong>Is it truly cross-platform?</strong> This is rather a question than something I strictly do not like. So far, I only developed an Android app with Flutter and I wonder whether it is actually possible to use the exact same code for iOS. Sometimes you will still need to access native APIs, e.g. when attempting to <a href=\"https://muetsch.io/how-to-receive-sharing-intents-in-flutter.html\">receive a sharing intent</a>, or have varying design elements. So although I am not totally sure about this, I would assume that for large projects you would still need to write two separate Flutter apps for Android and iOS, but have the ability to reuse large parts of the code in form of a shared library.</li>\n<li><strong>Apps are large * .</strong> Maybe this will be improved in the future, but right now Flutter apps are pretty large. My <a href=\"https://github.com/muety/anchr-android\" target=\"_blank\" rel=\"noopener\">simple bookmark manager</a> is 46 MB in size when installed.</li>\n</ol>\n<h1>Where to get started?</h1>\n<p>If you want to start learning Flutter, I would recommend the following.</p>\n<ol>\n<li>Take the <a href=\"https://www.dartlang.org/guides/language/language-tour\" target=\"_blank\" rel=\"noopener\">Tour of Dart</a> and get familiar with Dart‚Äôs syntax and concepts.</li>\n<li>Walk through <a href=\"https://bendyworks.com/blog/a-month-of-flutter\" target=\"_blank\" rel=\"noopener\">A month of Flutter</a> to start building a real-world Flutter app step-by-step.</li>\n</ol>\n<p>Have fun and happy coding! ü§ì</p>\n<p>***** As it turned out, this is actually not a valid point of criticism, since I had generated the app in debug mode. When building a release, the total size is only 6.4 MB.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>After hearing about <a href=\"https://flutter.dev\" target=\"_blank\" rel=\"noopener\">Flutter</a> by <a href=\"https://twitter.com/@flttry\" target=\"_blank\" rel=\"noopener\">Matt Carroll</a> and <a href=\"https://twitter.com/abraham\" target=\"_blank\" rel=\"noopener\">Abraham Williams</a> at the <a href=\"https://www.meetup.com/de-DE/sfhtml5/events/256523273/\" target=\"_blank\" rel=\"noopener\">SFHTML5 Meetup</a> hosted by Google in San Francisco a few weeks ago, I decided to give it a try. I developed a small <a href=\"https://github.com/muety/anchr-android\" target=\"_blank\" rel=\"noopener\">bookmark manager</a> app for Android while attempting to learn Flutter.</p>\n<h1>What is Flutter?</h1>\n<p>Flutter is an open-source framework for developing cross-platform mobile apps. Or in their own words it ‚Ä¶</p>\n<blockquote>\n<p>[‚Ä¶] allows you to build beautiful native apps on iOS and Android from a single codebase.</p>\n</blockquote>\n<p>‚ÄúCross-platform‚Äù means that you develop an app that can run on multiple platforms, in this case, Android, iOS and (in the future) also <a href=\"https://en.wikipedia.org/wiki/Google_Fuchsia\" target=\"_blank\" rel=\"noopener\">Fuchsia</a>. Traditionally, you would have to write the same app multiple times, e.g. once in Java for Android and a second time in Swift for iOS. This is a pain for developers, obviously, and Flutter tries to overcome it.</p>\n<p>There are already several approaches out there, trying to solve cross-platform mobile development. They include <strong>hybrid</strong> app frameworks like <a href=\"https://ionicframework.com/\" target=\"_blank\" rel=\"noopener\">Ionic</a> and <a href=\"https://cordova.apache.org/\" target=\"_blank\" rel=\"noopener\">Cordova</a> as well as <strong>native cross-platform</strong> frameworks like <a href=\"https://facebook.github.io/react-native/\" target=\"_blank\" rel=\"noopener\">React Native</a>, <a href=\"https://www.nativescript.org/\" target=\"_blank\" rel=\"noopener\">NativeScript</a> and <a href=\"https://visualstudio.microsoft.com/xamarin/\" target=\"_blank\" rel=\"noopener\">Xamarin</a>.</p>\n<p>By the way, there is is a very interesting comparison of <a href=\"https://www.codementor.io/fmcorz/react-native-vs-ionic-du1087rsw\" target=\"_blank\" rel=\"noopener\">Ionic vs. React Native</a>.</p>\n<p>While hybrid frameworks basically render a website to the device‚Äôs screen and native frameworks translate TypeScript (or C#) code into native iOS / Android components, Flutter <a href=\"https://buildflutter.com/how-flutter-works/\" target=\"_blank\" rel=\"noopener\">‚Äúworks more like a game engine‚Äù</a>. It has an extremely efficient engine that layouts widgets and renders them to a canvas. Therefore it usually a lot faster and less laggy than hybrid frameworks.</p>\n<p>Unlike most other cross-platform frameworks, where you usually program in JavaScript (or C# in the case of Xamarin), Flutter relies on <a href=\"https://dartlang.org\" target=\"_blank\" rel=\"noopener\">Dart</a>, created by Google. Consequently, in order to learn Flutter, you would usually not only have to learn the framework itself, but also a new programming language. This may seem like a high entry barrier, but it actually is not.</p>\n<h1>Pros</h1>\n<h2 id=\"What-I-DO-like-about-Dart\">What I <strong>DO</strong> like about Dart</h2>\n<ol>\n<li><strong>It looks familiar.</strong> Dart really just feels like Java and JavaScript having a baby. It does not follow any exotic paradigms. Instead, if you know Java and JavaScript, learning Dart is quite easy since there are only very few new concepts and syntaxes.<br>\nFor instance, it is <strong>object-oriented</strong> and has a proper inheritance system, similar to Java. In addition, it has a <strong>static typing system</strong>, including generics, but with <strong>optional type declarations</strong>, similar to Scala.<br>\nJavaScript people will find well-known concepts as well, such as <strong>futures, generators and <code>async / await</code></strong>.<br>\nSimilar to Scala‚Äôs <em>traits</em>, Dart even supports a kind of <strong>multiple class inheritance</strong> using <em>mixins</em>, which is quite handy once you get used to it. Having a Java background another cool new thing in Dart is <strong>factory constructors</strong>.</li>\n<li><strong>It feels consistent.</strong> What I did not like about TypeScript is that sometimes it just feels a bit hacky, e.g. when third-party typings for a certain library are buggy or even missing. In my opinion, this originates in TypeScript‚Äôs approach of trying to extend an existing programming language with whole new functionality. While TypeScripts is still a great language, Dart just feels more consistent in its whole. It was designed completely from scratch with all the above concepts in mind and since it is quite young, it still feels clean and minimalistic. However, since it is not that widely used, yet, the ecosystem (including tooling, libraries, ‚Ä¶) is way smaller compared to JavaScript or native Android / iOS.</li>\n<li><strong>It is multi-purpose.</strong> Dart can not only be used for mobile development but is quite generic. You can use it to write your backend and there are transpilers to use it in the browser as well. Since frameworks like Angular start to have Dart support (see <a href=\"https://webdev.dartlang.org/angular/\" target=\"_blank\" rel=\"noopener\">AngularDart</a>), I feel like it could <a href=\"https://medium.com/@mswehli/why-dart-is-the-language-to-learn-of-2018-e5fa12adb6c1\" target=\"_blank\" rel=\"noopener\">gain popularity</a> in the next years.</li>\n<li><strong>It has named parameters üòÉ.</strong> They exist in many programming languages, including Python and Scala and I really got used to them. In my opinion, code gets a lot cleaner with named parameters.</li>\n</ol>\n<h2 id=\"What-I-DO-like-about-Flutter\">What I <strong>DO</strong> like about Flutter</h2>\n<ol>\n<li><strong>Easy layouting.</strong> Dart follows the paradigm that <em>‚Äúeverything is a widget‚Äù</em>. So instead of defining layouts with XML (like in Android) or with HTML + CSS (like in the web), everything is done programmatically. The way you build components is by recursively nesting widgets into other widgets. Such might include UI components like a <code>TextView</code> and <code>FloatingActionButton</code> or more abstract things like a <code>GestureDetector</code> or <code>Padding</code>. The structure is always the same and that makes UI composition easy to me.</li>\n<li><strong>Tons of built-in components.</strong> Flutter comes with a giant <a href=\"https://flutter.dev/docs/development/ui/widgets\" target=\"_blank\" rel=\"noopener\">catalog of widgets</a> for layouting, styling, animations and UI components (all of which perfectly follow Material design). So far I could find everything I needed in the widget catalog without having to use any poorly-maintained third-party libs.</li>\n<li><strong>It feels so real.</strong> Often times, hybrid apps still do not completely feel like a real app, even if they are using Material design libraries etc. Maybe the navigation drawer‚Äôs sliding animation is too rough, a page transition is slightly laggy or the text style just does not look quite right. At least this is what I experienced with Ionic. With Flutter, however, I could not tell the difference compared to a truly native app. Everything is super fast, smooth and pixel-perfect.</li>\n<li><strong>It is future-proof.</strong> Google is pushing Flutter really hard as they keep posting blog articles and developer videos about it. I feel like they really want people to adopt it, which might relate to their development on Fuchsia. No matter whether or not they are planning to <a href=\"https://www.reddit.com/r/androiddev/comments/6aga8e/in_your_opinion_will_google_fuchsia_replace/\" target=\"_blank\" rel=\"noopener\">replace Android with Fuchsia</a> some day, Flutter is definitely a good thing to know.</li>\n<li><strong>Good tooling.</strong> Although the ecosystem around Flutter is not that big, yet, the built-in tooling is great. Flutter comes with an intuitive <strong>CLI</strong> and is well integrated with my favorite editors IntelliJ (<strong>Android Studio</strong>) and <strong>Visual Studio Code</strong>.</li>\n<li><strong>Open-Source.</strong> I love open-source, so I am glad that Flutter is completely open to the community and available on <a href=\"https://github.com/flutter/flutter\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</li>\n</ol>\n<h1>Cons</h1>\n<h2 id=\"What-I-DO-NOT-like-about-Dart\">What I <strong>DO NOT</strong> like about Dart</h2>\n<ol>\n<li><strong>No functional APIs.</strong> I do not expect a functional programming language like Scala, but since I really got used to Java 8‚Äôs stream API, I am a bit sad that Dart has almost no functional APIs. In my opinion, code gets a lot cleaner with such.</li>\n</ol>\n<h2 id=\"What-I-DO-NOT-like-about-Flutter\">What I <strong>DO NOT</strong> like about Flutter</h2>\n<ol>\n<li><strong>Is it truly cross-platform?</strong> This is rather a question than something I strictly do not like. So far, I only developed an Android app with Flutter and I wonder whether it is actually possible to use the exact same code for iOS. Sometimes you will still need to access native APIs, e.g. when attempting to <a href=\"https://muetsch.io/how-to-receive-sharing-intents-in-flutter.html\">receive a sharing intent</a>, or have varying design elements. So although I am not totally sure about this, I would assume that for large projects you would still need to write two separate Flutter apps for Android and iOS, but have the ability to reuse large parts of the code in form of a shared library.</li>\n<li><strong>Apps are large * .</strong> Maybe this will be improved in the future, but right now Flutter apps are pretty large. My <a href=\"https://github.com/muety/anchr-android\" target=\"_blank\" rel=\"noopener\">simple bookmark manager</a> is 46 MB in size when installed.</li>\n</ol>\n<h1>Where to get started?</h1>\n<p>If you want to start learning Flutter, I would recommend the following.</p>\n<ol>\n<li>Take the <a href=\"https://www.dartlang.org/guides/language/language-tour\" target=\"_blank\" rel=\"noopener\">Tour of Dart</a> and get familiar with Dart‚Äôs syntax and concepts.</li>\n<li>Walk through <a href=\"https://bendyworks.com/blog/a-month-of-flutter\" target=\"_blank\" rel=\"noopener\">A month of Flutter</a> to start building a real-world Flutter app step-by-step.</li>\n</ol>\n<p>Have fun and happy coding! ü§ì</p>\n<p>***** As it turned out, this is actually not a valid point of criticism, since I had generated the app in debug mode. When building a release, the total size is only 6.4 MB.</p>\n"},{"title":"Why RAID 10 is better than RAID 01","date":"2015-11-19T21:43:31.000Z","_content":"\nSince it took me a while to completely understand why a RAID 1+0 configuration should be better than a RAID 0+1 one in terms of failure tolerance I want to put my insights down.  \nFirst you should have a basic understanding of what the first two RAID levels are and what it means to nest them. Very basically at level 0 data gets striped, meaning a datum gets split up into two or more blocks that get stored on a different hard drive each. Goal is to improve read and write performance linearly to the number of disks used, because the fragments don‚Äôt have to be read sequentially anymore but in parallel. Level 1 is about mirroring a datum on two disks with the goal to improve security. Of course, both levels can be combined ‚Äì you could either mirror striped data or stripe mirrored data which finally gives both: security and performance. In each case at least four disks are needed, while the half of the disks usually is a mirror. So if you took six disks you would do 3-striping. With eight disks you would do 4-striping and so on. Technically you could have more than one mirror (like doing 2-striping and having a 3-mirror or even more) but it‚Äôs very unusual.  \nThe following diagrams shall illustrate these two ways and are useful for further explanations. In both cases we have a RAID with six disks.\n\n![raid01](/images/raid01.png)  \n\n*Disks 4, 5 and 6 are the mirrors of 1, 2 and 3.*\n\n![raid10](/images/raid10.png)  \n\n*Disk 2 mirrors 1, 4 mirrors 3 and 6 mirrors 5.*\n\nWe assert that RAID 10 is better in terms of fault tolerance because a total failure (= loss of data) is less likely. In other words if some drives crash in a RAID 01 configuration the chance that those are the right drives for suffering a data loss is higher.  \nFirst of all, both configurations can easily survive the crash of one drive. No matter which drive (see figures above) crashes, we have a second one with the exact same data on it in any case. Potentially both configurations can handle the simultaneous crash of two or even more drives (up to N/2), if they‚Äôre the right ones, but in the worst case, the second crash could end up in a total failure. What you need to make clear before understanding how RAID 01 is worse than RAID 10 is that a RAID 0 (sub)system immediately gets unusable if one of its disks goes down. This is apparent: In the upper figure (figure 1 from now on) data is divided up into three strips in both RAID 0 subsystem. So if one of their disks fails (assume a crash of Disc1), the subsystem is broken since the first two parts of a date won‚Äôt make sense without the third. You would still be in posession of all data, nothing is lost yet, but nevertheless the left RAID 0 subsystem is down. If a second drive fails this should only be 2 or 3 (since the left system in inacessible anyway) to keep the entire system up. Disc 4, 5 or 6 failing would cause a total crash. So the chance of the second failing disk resulting in a total crash is 3/5\\. Now take a look at figure 2\\. The crash of one disk in a RAID 1 (sub)system won‚Äôt make this subsystem go down because the RAID controller will seamlessly switch to the mirror drive which has exactly identical data. In figure 2 all RAID 0 disks (which actually are stanalone RAID 1 systems again) need to keep running for the entire system to stay up. So theoretically there wouldn‚Äôt be a problem with disks 1, 3 and 5 (or 2, 4 and 6) could crashing simultaneously. After one disk having failed (assume a crash of Disc1 again) the second one failing could be 3, 4, 5 or 6 ‚Äì all but NOT 2\\. The probability of a total crash is 1/5 (namely Disc2 of the remaining five) now and therefore lower than 3/5 with RAID 01\\. Hope you got it‚Ä¶","source":"_posts/why-raid-10-is-better-than-raid-01.md","raw":"---\ntitle: Why RAID 10 is better than RAID 01\ndate: 2015-11-19 22:43:31\ntags:\n---\n\nSince it took me a while to completely understand why a RAID 1+0 configuration should be better than a RAID 0+1 one in terms of failure tolerance I want to put my insights down.  \nFirst you should have a basic understanding of what the first two RAID levels are and what it means to nest them. Very basically at level 0 data gets striped, meaning a datum gets split up into two or more blocks that get stored on a different hard drive each. Goal is to improve read and write performance linearly to the number of disks used, because the fragments don‚Äôt have to be read sequentially anymore but in parallel. Level 1 is about mirroring a datum on two disks with the goal to improve security. Of course, both levels can be combined ‚Äì you could either mirror striped data or stripe mirrored data which finally gives both: security and performance. In each case at least four disks are needed, while the half of the disks usually is a mirror. So if you took six disks you would do 3-striping. With eight disks you would do 4-striping and so on. Technically you could have more than one mirror (like doing 2-striping and having a 3-mirror or even more) but it‚Äôs very unusual.  \nThe following diagrams shall illustrate these two ways and are useful for further explanations. In both cases we have a RAID with six disks.\n\n![raid01](/images/raid01.png)  \n\n*Disks 4, 5 and 6 are the mirrors of 1, 2 and 3.*\n\n![raid10](/images/raid10.png)  \n\n*Disk 2 mirrors 1, 4 mirrors 3 and 6 mirrors 5.*\n\nWe assert that RAID 10 is better in terms of fault tolerance because a total failure (= loss of data) is less likely. In other words if some drives crash in a RAID 01 configuration the chance that those are the right drives for suffering a data loss is higher.  \nFirst of all, both configurations can easily survive the crash of one drive. No matter which drive (see figures above) crashes, we have a second one with the exact same data on it in any case. Potentially both configurations can handle the simultaneous crash of two or even more drives (up to N/2), if they‚Äôre the right ones, but in the worst case, the second crash could end up in a total failure. What you need to make clear before understanding how RAID 01 is worse than RAID 10 is that a RAID 0 (sub)system immediately gets unusable if one of its disks goes down. This is apparent: In the upper figure (figure 1 from now on) data is divided up into three strips in both RAID 0 subsystem. So if one of their disks fails (assume a crash of Disc1), the subsystem is broken since the first two parts of a date won‚Äôt make sense without the third. You would still be in posession of all data, nothing is lost yet, but nevertheless the left RAID 0 subsystem is down. If a second drive fails this should only be 2 or 3 (since the left system in inacessible anyway) to keep the entire system up. Disc 4, 5 or 6 failing would cause a total crash. So the chance of the second failing disk resulting in a total crash is 3/5\\. Now take a look at figure 2\\. The crash of one disk in a RAID 1 (sub)system won‚Äôt make this subsystem go down because the RAID controller will seamlessly switch to the mirror drive which has exactly identical data. In figure 2 all RAID 0 disks (which actually are stanalone RAID 1 systems again) need to keep running for the entire system to stay up. So theoretically there wouldn‚Äôt be a problem with disks 1, 3 and 5 (or 2, 4 and 6) could crashing simultaneously. After one disk having failed (assume a crash of Disc1 again) the second one failing could be 3, 4, 5 or 6 ‚Äì all but NOT 2\\. The probability of a total crash is 1/5 (namely Disc2 of the remaining five) now and therefore lower than 3/5 with RAID 01\\. Hope you got it‚Ä¶","slug":"why-raid-10-is-better-than-raid-01","published":1,"updated":"2020-06-06T15:14:33.460Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb3slhjd001540mqmalxsmxd","content":"<p>Since it took me a while to completely understand why a RAID 1+0 configuration should be better than a RAID 0+1 one in terms of failure tolerance I want to put my insights down.<br>\nFirst you should have a basic understanding of what the first two RAID levels are and what it means to nest them. Very basically at level 0 data gets striped, meaning a datum gets split up into two or more blocks that get stored on a different hard drive each. Goal is to improve read and write performance linearly to the number of disks used, because the fragments don‚Äôt have to be read sequentially anymore but in parallel. Level 1 is about mirroring a datum on two disks with the goal to improve security. Of course, both levels can be combined ‚Äì you could either mirror striped data or stripe mirrored data which finally gives both: security and performance. In each case at least four disks are needed, while the half of the disks usually is a mirror. So if you took six disks you would do 3-striping. With eight disks you would do 4-striping and so on. Technically you could have more than one mirror (like doing 2-striping and having a 3-mirror or even more) but it‚Äôs very unusual.<br>\nThe following diagrams shall illustrate these two ways and are useful for further explanations. In both cases we have a RAID with six disks.</p>\n<p><img src=\"/images/raid01.png\" alt=\"raid01\"></p>\n<p><em>Disks 4, 5 and 6 are the mirrors of 1, 2 and 3.</em></p>\n<p><img src=\"/images/raid10.png\" alt=\"raid10\"></p>\n<p><em>Disk 2 mirrors 1, 4 mirrors 3 and 6 mirrors 5.</em></p>\n<p>We assert that RAID 10 is better in terms of fault tolerance because a total failure (= loss of data) is less likely. In other words if some drives crash in a RAID 01 configuration the chance that those are the right drives for suffering a data loss is higher.<br>\nFirst of all, both configurations can easily survive the crash of one drive. No matter which drive (see figures above) crashes, we have a second one with the exact same data on it in any case. Potentially both configurations can handle the simultaneous crash of two or even more drives (up to N/2), if they‚Äôre the right ones, but in the worst case, the second crash could end up in a total failure. What you need to make clear before understanding how RAID 01 is worse than RAID 10 is that a RAID 0 (sub)system immediately gets unusable if one of its disks goes down. This is apparent: In the upper figure (figure 1 from now on) data is divided up into three strips in both RAID 0 subsystem. So if one of their disks fails (assume a crash of Disc1), the subsystem is broken since the first two parts of a date won‚Äôt make sense without the third. You would still be in posession of all data, nothing is lost yet, but nevertheless the left RAID 0 subsystem is down. If a second drive fails this should only be 2 or 3 (since the left system in inacessible anyway) to keep the entire system up. Disc 4, 5 or 6 failing would cause a total crash. So the chance of the second failing disk resulting in a total crash is 3/5. Now take a look at figure 2. The crash of one disk in a RAID 1 (sub)system won‚Äôt make this subsystem go down because the RAID controller will seamlessly switch to the mirror drive which has exactly identical data. In figure 2 all RAID 0 disks (which actually are stanalone RAID 1 systems again) need to keep running for the entire system to stay up. So theoretically there wouldn‚Äôt be a problem with disks 1, 3 and 5 (or 2, 4 and 6) could crashing simultaneously. After one disk having failed (assume a crash of Disc1 again) the second one failing could be 3, 4, 5 or 6 ‚Äì all but NOT 2. The probability of a total crash is 1/5 (namely Disc2 of the remaining five) now and therefore lower than 3/5 with RAID 01. Hope you got it‚Ä¶</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Since it took me a while to completely understand why a RAID 1+0 configuration should be better than a RAID 0+1 one in terms of failure tolerance I want to put my insights down.<br>\nFirst you should have a basic understanding of what the first two RAID levels are and what it means to nest them. Very basically at level 0 data gets striped, meaning a datum gets split up into two or more blocks that get stored on a different hard drive each. Goal is to improve read and write performance linearly to the number of disks used, because the fragments don‚Äôt have to be read sequentially anymore but in parallel. Level 1 is about mirroring a datum on two disks with the goal to improve security. Of course, both levels can be combined ‚Äì you could either mirror striped data or stripe mirrored data which finally gives both: security and performance. In each case at least four disks are needed, while the half of the disks usually is a mirror. So if you took six disks you would do 3-striping. With eight disks you would do 4-striping and so on. Technically you could have more than one mirror (like doing 2-striping and having a 3-mirror or even more) but it‚Äôs very unusual.<br>\nThe following diagrams shall illustrate these two ways and are useful for further explanations. In both cases we have a RAID with six disks.</p>\n<p><img src=\"/images/raid01.png\" alt=\"raid01\"></p>\n<p><em>Disks 4, 5 and 6 are the mirrors of 1, 2 and 3.</em></p>\n<p><img src=\"/images/raid10.png\" alt=\"raid10\"></p>\n<p><em>Disk 2 mirrors 1, 4 mirrors 3 and 6 mirrors 5.</em></p>\n<p>We assert that RAID 10 is better in terms of fault tolerance because a total failure (= loss of data) is less likely. In other words if some drives crash in a RAID 01 configuration the chance that those are the right drives for suffering a data loss is higher.<br>\nFirst of all, both configurations can easily survive the crash of one drive. No matter which drive (see figures above) crashes, we have a second one with the exact same data on it in any case. Potentially both configurations can handle the simultaneous crash of two or even more drives (up to N/2), if they‚Äôre the right ones, but in the worst case, the second crash could end up in a total failure. What you need to make clear before understanding how RAID 01 is worse than RAID 10 is that a RAID 0 (sub)system immediately gets unusable if one of its disks goes down. This is apparent: In the upper figure (figure 1 from now on) data is divided up into three strips in both RAID 0 subsystem. So if one of their disks fails (assume a crash of Disc1), the subsystem is broken since the first two parts of a date won‚Äôt make sense without the third. You would still be in posession of all data, nothing is lost yet, but nevertheless the left RAID 0 subsystem is down. If a second drive fails this should only be 2 or 3 (since the left system in inacessible anyway) to keep the entire system up. Disc 4, 5 or 6 failing would cause a total crash. So the chance of the second failing disk resulting in a total crash is 3/5. Now take a look at figure 2. The crash of one disk in a RAID 1 (sub)system won‚Äôt make this subsystem go down because the RAID controller will seamlessly switch to the mirror drive which has exactly identical data. In figure 2 all RAID 0 disks (which actually are stanalone RAID 1 systems again) need to keep running for the entire system to stay up. So theoretically there wouldn‚Äôt be a problem with disks 1, 3 and 5 (or 2, 4 and 6) could crashing simultaneously. After one disk having failed (assume a crash of Disc1 again) the second one failing could be 3, 4, 5 or 6 ‚Äì all but NOT 2. The probability of a total crash is 1/5 (namely Disc2 of the remaining five) now and therefore lower than 3/5 with RAID 01. Hope you got it‚Ä¶</p>\n"},{"title":"Modern, reactive web APIs with GraphQL, Go and Server-Sent Events ‚Äì Part 2","date":"2020-06-06T16:34:02.000Z","_content":"\nIn the [previous](https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.html) part, concepts and benefits of GraphQL and [GraphQL Subscriptions](https://graphql.org/blog/subscriptions-in-graphql-and-relay/) were presented and proposed as a modern, highly flexible alternative for designing web APIs.\n\nIn this post, we are going to look at actual code. For demonstration purposes, a small single-page application (SPA) is built as an example. It can serve as a cleanly structured starting point for new apps based on the proposed tech stack.\n\n\\> **Code**: [muety/go-graphql-sse-example](https://github.com/muety/go-graphql-sse-example)\n\n# What to build?\nThe demo web app built in the context of this article comprised of a client-side frontend, built with [VueJS](https://vuejs.org) and a server-side component built with Go. The two components interact with each other through a GraphQL API, which additionally offers the option to subscribe to data updates using [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events).\n\nWith this demo app, we aim to mimic the functionality of a very basic, minimalist food ordering system. Customers can choose from a list of food products and place their orders. They are shown a waiting number an estimated processing time for their orders and are notified once the order is ready to be picked up. One the other side there is the kiosk operator, who views a live dashboard of currently pending orders, which appear just as they are being placed. \n\n![](images/graphql_screenshots1.png)\n\n## Technology Stack\nIn summary, the following technologies are used:\n* [Go](https://golang.org) as the primary backend-side language \n* [GraphQL](https://graphql.org/) as a \"protocol\" for defining web interfaces\n* [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events) as a simple protocol for live updates, used as an implementation of [GraphQL Subscriptions](https://graphql.org/blog/subscriptions-in-graphql-and-relay/) here\n* [MongoDB](https://www.mongodb.com/) as a flexible document database for storage\n* [VueJS](https://vuejs.org/) as a frontend framework to build single-page web applications\n\nIn addition, [graphql-request](https://www.npmjs.com/package/graphql-request) is used as a little helper library on the frontend to issue GraphQL queries more easily. It constitutes a light-weight wrapper around the plain [Fetch API](https://developer.mozilla.org/de/docs/Web/API/Fetch_API).\n\nOn the backend side, GraphQL development is facilitated by the excellent [graphql-go](github.com/graph-gophers/graphql-go) package, which already provides well-defined guidelines to get started (thanks to the authors!).\n\n# Data Model\nWith GraphQL, you need to specify a [schema](https://graphql.org/learn/schema/) using a GrapQL-specific syntax. It includes all entities, which your API should be able to deal with and is essentially a set of type definitions, split among one or more `.graphql` files.\n\nEvery GraphQL app consists of root types, defining all supported queries, mutations, and subscriptions as well as entity types. \n\nFor our app, the root schema looks like this ([schema/schema.graphql](https://github.com/muety/go-graphql-sse-example/blob/master/schema/schema.graphql)):\n\n```graphql\nschema {\n    query: Query\n    mutation: Mutation\n    subscription: Subscription\n}\n\ntype Query {\n    product(id: ID!): Product\n    products(): [Product]\n    order(id: ID!): Order\n    orders(status: String): [Order]\n}\n\ntype Mutation {\n    createOrder(order: OrderInput!): Order\n    updateOrder(order: OrderUpdateInput!): Order\n}\n\ntype Subscription {\n    orderCreated(): Order\n    orderChanged(id: ID!): Order\n}\n```\n\nThe type definition for the `Product` type referenced in the root schema is given as ([schema/type/product.graphql](https://github.com/muety/go-graphql-sse-example/blob/master/schema/type/product.graphql)):\n\n```graphql\ntype Product {\n    id: ID!\n    name: String\n    description: String\n    price: Float\n}\n```\n\nThe entire schema can be found in the GitHub [repository](https://github.com/muety/go-graphql-sse-example/tree/master/schema/type).\n\nAfter having defined your schema, in the case of [graphql-go](github.com/graph-gophers/graphql-go), it gets transformed into Go code, so it can be compiled into the final Go executable. It can then be loaded on application startup (see [server.go](https://github.com/muety/go-graphql-sse-example/blob/master/server.go#L35)) and served via an HTTP endpoint. Pretty straightforward!\n\n```go\ngraphqlSchema := graphql.MustParseSchema(schema.GetRootSchema(), &resolver.Resolver{})\nhttp.Handle(\"/api/query\", middleware.AddContext(ctx, &middleware.GraphQL{Schema: graphqlSchema}))\n```\n\n# Resolvers\nAfter having defined and loaded the schema, so-called [resolvers](https://graphql.org/learn/execution/#root-fields-resolvers) need to be defined. The concept of resolvers is common among most GraphQL server libraries for various different programming languages. A resolver is responsible for providing the data for every field of an entity, e.g. for the `ID`, `name`, `description`, and `price` fields. Such fields can be literals, as it is the case with all fields of the `Product` type, but also other nested entities, like in the `products` field of the following `Order` type:\n\n```graphql\ntype Order {\n    id: ID!\n    ...\n    products: [Product]\n}\n```\n\nFor literals, the resolver simply fills in the actual string, number, or boolean. For complex types, it delegates their resolution to their respective resolvers recursively. For instance, to resolve `products`, the `orderResolver` will ask a `productResolver` to do its respective duty. \n\nWhen working with `go-graphql`, a simple resolver looks like this ([resolvers/product.go](https://github.com/muety/go-graphql-sse-example/blob/master/resolver/product.go)):\n\n```go\ntype productResolver struct {\n\tp *model.Product\n}\n\nfunc (r *productResolver) Id() graphql.ID {\n\treturn graphql.ID(r.p.Id)\n}\n\nfunc (r *productResolver) Name() *string {\n\treturn &r.p.Name\n}\n\nfunc (r *productResolver) Description() *string {\n\treturn &r.p.Description\n}\n\nfunc (r *productResolver) Price() *float64 {\n\treturn &r.p.Price\n}\n\n```\n\nVery simple. It is instantiated with a reference to a `Product` struct, which was previously loaded from the database and simply maps attributes of the raw data model to the respective GraphQL fields. Please note that it doesn't have to be that trivial. For instance, your MongoDB schema might define `firstName` and `lastName` fields, while your GraphQL type only has `name`. In that case, the resolver would have to do some basic concatenation.\n\nThings get a little more complex when dealing with non-literal fields, like the `Order's` `products` above. In our database schema, an order only holds a list of product IDs as `items`. However, the GraphQL schema declares to return actual product objects. To do so, the respective resolver method first fetches the products for every ID from the database and then passes it on to `productResolver`s (see [resolvers/order.go](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/resolver/order.go#L53)):\n\n```go\n...\nfunc (r *orderResolver) Products(ctx context.Context) (*[]*productResolver, error) {\n\tl := make([]*productResolver, len(r.o.Items))\n\n\tproducts, err := ctx.Value(service.KeyProductService).(*service.ProductService).GetBatchMap(r.o.Items)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i, id := range r.o.Items {\n\t\tl[i] = &productResolver{p: products[id]}\n\t}\n\n\treturn &l, nil\n}\n...\n```\n\nResolvers are very modular, coherent in themselves and can be composed together to build up an entire API. Note that even the very entrypoint of the GraphQL API is just a \"root\" resolver. You can find all resolvers, including those for mutations and subscriptions as well, in the [repo](https://github.com/muety/go-graphql-sse-example/tree/master/resolver).\n\n# Subscriptions\nProbably the most interesting part here is subscriptions, as they provide a nice mechanism to make web applications reactive to backend-side data updates. As mentioned in the previous article, the GraphQL specification does not dictate how to technically implement subscriptions. Therefore, we decided to use Server-Sent Events (SSE) as a server-to-client communication channel. Technically, SSEs are simply a long-running HTTP request, which data is written to in form of a text stream and therefore very light-weight and easy to use. \n\nOn the backend side, we introduce a light-weight [event bus](github.com/leandro-lugaresi/hub) to our [services](https://github.com/muety/go-graphql-sse-example/tree/master/service). For every data change, i.e. updates, creations or deletions, an event is published to the bus (see [services/order.go](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/service/order.go#L66)):\n\n```go\nfunc (s *OrderService) Create(order *model.Order) (*model.Order, error) {\n\t...\n\torder.Id = res.InsertedID.(primitive.ObjectID).Hex()\n\ts.Hub.Publish(eventhub.Message{\n\t\tName:   KeyOrderCreated, // order.create\n\t\tFields: eventhub.Fields{\"id\": order.Id},\n    })\n    ...\n}\n```\n\nInside the resolver responsible for `orderCreated` queries, a subscription to the event bus is made once the user requests that subscription.\n\n```go\nfunc (r *Resolver) OrderCreated(ctx context.Context) (chan *orderResolver, error) {\n\tc := make(chan *orderResolver)\n\tgo subscribeOrder(service.KeyOrderCreated, ctx, c)\n\treturn c, nil\n}\n\nfunc subscribeOrder(key string, ctx context.Context, c chan *orderResolver) {\n\tsrv := ctx.Value(service.KeyOrderService).(*service.OrderService)\n\tsub := srv.Hub.NonBlockingSubscribe(10, key)\n\n\tdefer func() {\n\t\tsrv.Hub.Unsubscribe(sub)\n\t\tclose(c)\n\t}()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase m := <-sub.Receiver:\n\t\t\tif u, err := srv.Get(m.Fields[\"id\"].(string)); err != nil {\n\t\t\t\tlog.Println(err)\n\t\t\t} else {\n\t\t\t\tc <- &orderResolver{u}\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\nWhen a new order is inserted via the respective service, the above method is triggered as a consequence of being subscribed to `order.create` events. It reads the orer's ID from the event, uses the qualified service to fetch it from the database and passes it on to an `orderResolver` to translate it into the schema-conformal format. \n\nThe HTTP handler, which dispatched the user's GraphQL query to the above resolver, in turn, has subscribed to the result Go channel and writes every incoming `Order` instance to always-open HTTP stream (see [middleware/graphql.go](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/middleware/graphql.go#L59)):\n\n```go\n...\nc, err := h.Schema.Subscribe(ctx, params.Query, params.OperationName, params.Variables)\n...\nfor r := range c {\n        response := r.(*graphql.Response)\n        responseJSON, err := json.Marshal(response)\n        ...\n        fmt.Fprintf(w, \"data: %s\\n\\n\", responseJSON)\n        flusher.Flush()\n        ...\n}\n...\n```\n\n# Running Queries\n## Using GraphiQL\nDuring development, queries against a GraphQL API can be issued using the interactive GraphiQL browser, as demonstrated here.\n\n![](images/graphql_screencast2.gif)\n\n# Programatically\nOn the client-side of our application, GraphQL queries are run to consume the API. After all, any client, that is able to speak HTTP, can also consume a GraphQL API, as GraphQL requests are really just `POST` requests with a certain query in the body. \n\nFor instance, the [Vuex](https://vuex.vuejs.org/) store action responsible for loading a list of products in the frontend is this (see [store/products.js](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/web/src/store/products.js#L30)):\n\n```javascript\nasync fetchProducts({commit}) {\n    const q = `{\n        products() {\n        id\n        name\n        description\n        price\n        }\n    }`\n\n    const data = await Vue.$api.graphql.request(q)\n    commit('addProducts', data.products.map(Product.new))\n}\n```\n\nFor subscriptions, we rely on a slightly [modified version](https://github.com/muety/go-graphql-sse-example/blob/master/web/src/vendor/sse.js) of [sse.js](https://github.com/mpetazzoni/sse.js). It acts as a minimal wrapper around the browser's standard [EventSource](https://developer.mozilla.org/en-US/docs/Web/API/EventSource) and allows us to run SSE requests as `POST` (instead of `GET`) queries and pass some data in the body. We do so to fit well with GraphQL, where queries are always `POST` requests with a respective query body. \n\nSubscribing to new orders in the frontend is mostly done like this:\n\n```javascript\nasync subscribeOrderCreated({commit}) {\n    const q = `subscription {\n        orderCreated() {\n            id\n            queueId\n            createdAt\n            updatedAt\n            status\n            eta\n            totalSum\n            products {\n                id\n                name\n            }\n        }\n    }`\n\n    const source = Vue.$api.sse.request(q, null)\n    source.addEventListener('message', e => {\n        const payload = JSON.parse(e.data)\n        ...\n        commit('addOrder', new Order(payload.data.orderCreated))\n    })\n    ...\n    source.stream()\n}\n```\n\n# Demo\nAnd here's a live demo in action!\n\n![](images/graphql_screencast1.gif)\n\n# Outlook\nWhile the current demo implementation serves as a ‚Äì in our opinion ‚Äì clean and well-structured starting point for building GraphQL-based web apps, its current state has one major drawback. It lacks authentication and authorization. Usually, you want to control which user can query and modify which data. Therefore, another blog post will follow in the future, which explains how to authorize GraphQL query endpoints.\n\nApart from that, we consider the present technology stack a promising choice for new web applications with the potential to facilitate clean, well-organized code and to make development easier, and more flexible. ","source":"_posts/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.md","raw":"---\ntitle: 'Modern, reactive web APIs with GraphQL, Go and Server-Sent Events ‚Äì Part 2'\ndate: 2020-06-06 18:34:02\ntags:\n---\n\nIn the [previous](https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.html) part, concepts and benefits of GraphQL and [GraphQL Subscriptions](https://graphql.org/blog/subscriptions-in-graphql-and-relay/) were presented and proposed as a modern, highly flexible alternative for designing web APIs.\n\nIn this post, we are going to look at actual code. For demonstration purposes, a small single-page application (SPA) is built as an example. It can serve as a cleanly structured starting point for new apps based on the proposed tech stack.\n\n\\> **Code**: [muety/go-graphql-sse-example](https://github.com/muety/go-graphql-sse-example)\n\n# What to build?\nThe demo web app built in the context of this article comprised of a client-side frontend, built with [VueJS](https://vuejs.org) and a server-side component built with Go. The two components interact with each other through a GraphQL API, which additionally offers the option to subscribe to data updates using [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events).\n\nWith this demo app, we aim to mimic the functionality of a very basic, minimalist food ordering system. Customers can choose from a list of food products and place their orders. They are shown a waiting number an estimated processing time for their orders and are notified once the order is ready to be picked up. One the other side there is the kiosk operator, who views a live dashboard of currently pending orders, which appear just as they are being placed. \n\n![](images/graphql_screenshots1.png)\n\n## Technology Stack\nIn summary, the following technologies are used:\n* [Go](https://golang.org) as the primary backend-side language \n* [GraphQL](https://graphql.org/) as a \"protocol\" for defining web interfaces\n* [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events) as a simple protocol for live updates, used as an implementation of [GraphQL Subscriptions](https://graphql.org/blog/subscriptions-in-graphql-and-relay/) here\n* [MongoDB](https://www.mongodb.com/) as a flexible document database for storage\n* [VueJS](https://vuejs.org/) as a frontend framework to build single-page web applications\n\nIn addition, [graphql-request](https://www.npmjs.com/package/graphql-request) is used as a little helper library on the frontend to issue GraphQL queries more easily. It constitutes a light-weight wrapper around the plain [Fetch API](https://developer.mozilla.org/de/docs/Web/API/Fetch_API).\n\nOn the backend side, GraphQL development is facilitated by the excellent [graphql-go](github.com/graph-gophers/graphql-go) package, which already provides well-defined guidelines to get started (thanks to the authors!).\n\n# Data Model\nWith GraphQL, you need to specify a [schema](https://graphql.org/learn/schema/) using a GrapQL-specific syntax. It includes all entities, which your API should be able to deal with and is essentially a set of type definitions, split among one or more `.graphql` files.\n\nEvery GraphQL app consists of root types, defining all supported queries, mutations, and subscriptions as well as entity types. \n\nFor our app, the root schema looks like this ([schema/schema.graphql](https://github.com/muety/go-graphql-sse-example/blob/master/schema/schema.graphql)):\n\n```graphql\nschema {\n    query: Query\n    mutation: Mutation\n    subscription: Subscription\n}\n\ntype Query {\n    product(id: ID!): Product\n    products(): [Product]\n    order(id: ID!): Order\n    orders(status: String): [Order]\n}\n\ntype Mutation {\n    createOrder(order: OrderInput!): Order\n    updateOrder(order: OrderUpdateInput!): Order\n}\n\ntype Subscription {\n    orderCreated(): Order\n    orderChanged(id: ID!): Order\n}\n```\n\nThe type definition for the `Product` type referenced in the root schema is given as ([schema/type/product.graphql](https://github.com/muety/go-graphql-sse-example/blob/master/schema/type/product.graphql)):\n\n```graphql\ntype Product {\n    id: ID!\n    name: String\n    description: String\n    price: Float\n}\n```\n\nThe entire schema can be found in the GitHub [repository](https://github.com/muety/go-graphql-sse-example/tree/master/schema/type).\n\nAfter having defined your schema, in the case of [graphql-go](github.com/graph-gophers/graphql-go), it gets transformed into Go code, so it can be compiled into the final Go executable. It can then be loaded on application startup (see [server.go](https://github.com/muety/go-graphql-sse-example/blob/master/server.go#L35)) and served via an HTTP endpoint. Pretty straightforward!\n\n```go\ngraphqlSchema := graphql.MustParseSchema(schema.GetRootSchema(), &resolver.Resolver{})\nhttp.Handle(\"/api/query\", middleware.AddContext(ctx, &middleware.GraphQL{Schema: graphqlSchema}))\n```\n\n# Resolvers\nAfter having defined and loaded the schema, so-called [resolvers](https://graphql.org/learn/execution/#root-fields-resolvers) need to be defined. The concept of resolvers is common among most GraphQL server libraries for various different programming languages. A resolver is responsible for providing the data for every field of an entity, e.g. for the `ID`, `name`, `description`, and `price` fields. Such fields can be literals, as it is the case with all fields of the `Product` type, but also other nested entities, like in the `products` field of the following `Order` type:\n\n```graphql\ntype Order {\n    id: ID!\n    ...\n    products: [Product]\n}\n```\n\nFor literals, the resolver simply fills in the actual string, number, or boolean. For complex types, it delegates their resolution to their respective resolvers recursively. For instance, to resolve `products`, the `orderResolver` will ask a `productResolver` to do its respective duty. \n\nWhen working with `go-graphql`, a simple resolver looks like this ([resolvers/product.go](https://github.com/muety/go-graphql-sse-example/blob/master/resolver/product.go)):\n\n```go\ntype productResolver struct {\n\tp *model.Product\n}\n\nfunc (r *productResolver) Id() graphql.ID {\n\treturn graphql.ID(r.p.Id)\n}\n\nfunc (r *productResolver) Name() *string {\n\treturn &r.p.Name\n}\n\nfunc (r *productResolver) Description() *string {\n\treturn &r.p.Description\n}\n\nfunc (r *productResolver) Price() *float64 {\n\treturn &r.p.Price\n}\n\n```\n\nVery simple. It is instantiated with a reference to a `Product` struct, which was previously loaded from the database and simply maps attributes of the raw data model to the respective GraphQL fields. Please note that it doesn't have to be that trivial. For instance, your MongoDB schema might define `firstName` and `lastName` fields, while your GraphQL type only has `name`. In that case, the resolver would have to do some basic concatenation.\n\nThings get a little more complex when dealing with non-literal fields, like the `Order's` `products` above. In our database schema, an order only holds a list of product IDs as `items`. However, the GraphQL schema declares to return actual product objects. To do so, the respective resolver method first fetches the products for every ID from the database and then passes it on to `productResolver`s (see [resolvers/order.go](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/resolver/order.go#L53)):\n\n```go\n...\nfunc (r *orderResolver) Products(ctx context.Context) (*[]*productResolver, error) {\n\tl := make([]*productResolver, len(r.o.Items))\n\n\tproducts, err := ctx.Value(service.KeyProductService).(*service.ProductService).GetBatchMap(r.o.Items)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i, id := range r.o.Items {\n\t\tl[i] = &productResolver{p: products[id]}\n\t}\n\n\treturn &l, nil\n}\n...\n```\n\nResolvers are very modular, coherent in themselves and can be composed together to build up an entire API. Note that even the very entrypoint of the GraphQL API is just a \"root\" resolver. You can find all resolvers, including those for mutations and subscriptions as well, in the [repo](https://github.com/muety/go-graphql-sse-example/tree/master/resolver).\n\n# Subscriptions\nProbably the most interesting part here is subscriptions, as they provide a nice mechanism to make web applications reactive to backend-side data updates. As mentioned in the previous article, the GraphQL specification does not dictate how to technically implement subscriptions. Therefore, we decided to use Server-Sent Events (SSE) as a server-to-client communication channel. Technically, SSEs are simply a long-running HTTP request, which data is written to in form of a text stream and therefore very light-weight and easy to use. \n\nOn the backend side, we introduce a light-weight [event bus](github.com/leandro-lugaresi/hub) to our [services](https://github.com/muety/go-graphql-sse-example/tree/master/service). For every data change, i.e. updates, creations or deletions, an event is published to the bus (see [services/order.go](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/service/order.go#L66)):\n\n```go\nfunc (s *OrderService) Create(order *model.Order) (*model.Order, error) {\n\t...\n\torder.Id = res.InsertedID.(primitive.ObjectID).Hex()\n\ts.Hub.Publish(eventhub.Message{\n\t\tName:   KeyOrderCreated, // order.create\n\t\tFields: eventhub.Fields{\"id\": order.Id},\n    })\n    ...\n}\n```\n\nInside the resolver responsible for `orderCreated` queries, a subscription to the event bus is made once the user requests that subscription.\n\n```go\nfunc (r *Resolver) OrderCreated(ctx context.Context) (chan *orderResolver, error) {\n\tc := make(chan *orderResolver)\n\tgo subscribeOrder(service.KeyOrderCreated, ctx, c)\n\treturn c, nil\n}\n\nfunc subscribeOrder(key string, ctx context.Context, c chan *orderResolver) {\n\tsrv := ctx.Value(service.KeyOrderService).(*service.OrderService)\n\tsub := srv.Hub.NonBlockingSubscribe(10, key)\n\n\tdefer func() {\n\t\tsrv.Hub.Unsubscribe(sub)\n\t\tclose(c)\n\t}()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase m := <-sub.Receiver:\n\t\t\tif u, err := srv.Get(m.Fields[\"id\"].(string)); err != nil {\n\t\t\t\tlog.Println(err)\n\t\t\t} else {\n\t\t\t\tc <- &orderResolver{u}\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\nWhen a new order is inserted via the respective service, the above method is triggered as a consequence of being subscribed to `order.create` events. It reads the orer's ID from the event, uses the qualified service to fetch it from the database and passes it on to an `orderResolver` to translate it into the schema-conformal format. \n\nThe HTTP handler, which dispatched the user's GraphQL query to the above resolver, in turn, has subscribed to the result Go channel and writes every incoming `Order` instance to always-open HTTP stream (see [middleware/graphql.go](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/middleware/graphql.go#L59)):\n\n```go\n...\nc, err := h.Schema.Subscribe(ctx, params.Query, params.OperationName, params.Variables)\n...\nfor r := range c {\n        response := r.(*graphql.Response)\n        responseJSON, err := json.Marshal(response)\n        ...\n        fmt.Fprintf(w, \"data: %s\\n\\n\", responseJSON)\n        flusher.Flush()\n        ...\n}\n...\n```\n\n# Running Queries\n## Using GraphiQL\nDuring development, queries against a GraphQL API can be issued using the interactive GraphiQL browser, as demonstrated here.\n\n![](images/graphql_screencast2.gif)\n\n# Programatically\nOn the client-side of our application, GraphQL queries are run to consume the API. After all, any client, that is able to speak HTTP, can also consume a GraphQL API, as GraphQL requests are really just `POST` requests with a certain query in the body. \n\nFor instance, the [Vuex](https://vuex.vuejs.org/) store action responsible for loading a list of products in the frontend is this (see [store/products.js](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/web/src/store/products.js#L30)):\n\n```javascript\nasync fetchProducts({commit}) {\n    const q = `{\n        products() {\n        id\n        name\n        description\n        price\n        }\n    }`\n\n    const data = await Vue.$api.graphql.request(q)\n    commit('addProducts', data.products.map(Product.new))\n}\n```\n\nFor subscriptions, we rely on a slightly [modified version](https://github.com/muety/go-graphql-sse-example/blob/master/web/src/vendor/sse.js) of [sse.js](https://github.com/mpetazzoni/sse.js). It acts as a minimal wrapper around the browser's standard [EventSource](https://developer.mozilla.org/en-US/docs/Web/API/EventSource) and allows us to run SSE requests as `POST` (instead of `GET`) queries and pass some data in the body. We do so to fit well with GraphQL, where queries are always `POST` requests with a respective query body. \n\nSubscribing to new orders in the frontend is mostly done like this:\n\n```javascript\nasync subscribeOrderCreated({commit}) {\n    const q = `subscription {\n        orderCreated() {\n            id\n            queueId\n            createdAt\n            updatedAt\n            status\n            eta\n            totalSum\n            products {\n                id\n                name\n            }\n        }\n    }`\n\n    const source = Vue.$api.sse.request(q, null)\n    source.addEventListener('message', e => {\n        const payload = JSON.parse(e.data)\n        ...\n        commit('addOrder', new Order(payload.data.orderCreated))\n    })\n    ...\n    source.stream()\n}\n```\n\n# Demo\nAnd here's a live demo in action!\n\n![](images/graphql_screencast1.gif)\n\n# Outlook\nWhile the current demo implementation serves as a ‚Äì in our opinion ‚Äì clean and well-structured starting point for building GraphQL-based web apps, its current state has one major drawback. It lacks authentication and authorization. Usually, you want to control which user can query and modify which data. Therefore, another blog post will follow in the future, which explains how to authorize GraphQL query endpoints.\n\nApart from that, we consider the present technology stack a promising choice for new web applications with the potential to facilitate clean, well-organized code and to make development easier, and more flexible. ","slug":"modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2","published":1,"updated":"2020-06-08T06:40:35.998Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb67j1lj0000txmqm9czrnzs","content":"<p>In the <a href=\"https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.html\">previous</a> part, concepts and benefits of GraphQL and <a href=\"https://graphql.org/blog/subscriptions-in-graphql-and-relay/\" target=\"_blank\" rel=\"noopener\">GraphQL Subscriptions</a> were presented and proposed as a modern, highly flexible alternative for designing web APIs.</p>\n<p>In this post, we are going to look at actual code. For demonstration purposes, a small single-page application (SPA) is built as an example. It can serve as a cleanly structured starting point for new apps based on the proposed tech stack.</p>\n<p>&gt; <strong>Code</strong>: <a href=\"https://github.com/muety/go-graphql-sse-example\" target=\"_blank\" rel=\"noopener\">muety/go-graphql-sse-example</a></p>\n<h1>What to build?</h1>\n<p>The demo web app built in the context of this article comprised of a client-side frontend, built with <a href=\"https://vuejs.org\" target=\"_blank\" rel=\"noopener\">VueJS</a> and a server-side component built with Go. The two components interact with each other through a GraphQL API, which additionally offers the option to subscribe to data updates using <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\" target=\"_blank\" rel=\"noopener\">Server-Sent Events</a>.</p>\n<p>With this demo app, we aim to mimic the functionality of a very basic, minimalist food ordering system. Customers can choose from a list of food products and place their orders. They are shown a waiting number an estimated processing time for their orders and are notified once the order is ready to be picked up. One the other side there is the kiosk operator, who views a live dashboard of currently pending orders, which appear just as they are being placed.</p>\n<p><img src=\"images/graphql_screenshots1.png\" alt></p>\n<h2 id=\"Technology-Stack\">Technology Stack</h2>\n<p>In summary, the following technologies are used:</p>\n<ul>\n<li><a href=\"https://golang.org\" target=\"_blank\" rel=\"noopener\">Go</a> as the primary backend-side language</li>\n<li><a href=\"https://graphql.org/\" target=\"_blank\" rel=\"noopener\">GraphQL</a> as a ‚Äúprotocol‚Äù for defining web interfaces</li>\n<li><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\" target=\"_blank\" rel=\"noopener\">Server-Sent Events</a> as a simple protocol for live updates, used as an implementation of <a href=\"https://graphql.org/blog/subscriptions-in-graphql-and-relay/\" target=\"_blank\" rel=\"noopener\">GraphQL Subscriptions</a> here</li>\n<li><a href=\"https://www.mongodb.com/\" target=\"_blank\" rel=\"noopener\">MongoDB</a> as a flexible document database for storage</li>\n<li><a href=\"https://vuejs.org/\" target=\"_blank\" rel=\"noopener\">VueJS</a> as a frontend framework to build single-page web applications</li>\n</ul>\n<p>In addition, <a href=\"https://www.npmjs.com/package/graphql-request\" target=\"_blank\" rel=\"noopener\">graphql-request</a> is used as a little helper library on the frontend to issue GraphQL queries more easily. It constitutes a light-weight wrapper around the plain <a href=\"https://developer.mozilla.org/de/docs/Web/API/Fetch_API\" target=\"_blank\" rel=\"noopener\">Fetch API</a>.</p>\n<p>On the backend side, GraphQL development is facilitated by the excellent <a href=\"github.com/graph-gophers/graphql-go\">graphql-go</a> package, which already provides well-defined guidelines to get started (thanks to the authors!).</p>\n<h1>Data Model</h1>\n<p>With GraphQL, you need to specify a <a href=\"https://graphql.org/learn/schema/\" target=\"_blank\" rel=\"noopener\">schema</a> using a GrapQL-specific syntax. It includes all entities, which your API should be able to deal with and is essentially a set of type definitions, split among one or more <code>.graphql</code> files.</p>\n<p>Every GraphQL app consists of root types, defining all supported queries, mutations, and subscriptions as well as entity types.</p>\n<p>For our app, the root schema looks like this (<a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/schema/schema.graphql\" target=\"_blank\" rel=\"noopener\">schema/schema.graphql</a>):</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">schema &#123;</span><br><span class=\"line\">    query: Query</span><br><span class=\"line\">    mutation: Mutation</span><br><span class=\"line\">    subscription: Subscription</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type Query &#123;</span><br><span class=\"line\">    product(id: ID!): Product</span><br><span class=\"line\">    products(): [Product]</span><br><span class=\"line\">    order(id: ID!): Order</span><br><span class=\"line\">    orders(status: String): [Order]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type Mutation &#123;</span><br><span class=\"line\">    createOrder(order: OrderInput!): Order</span><br><span class=\"line\">    updateOrder(order: OrderUpdateInput!): Order</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type Subscription &#123;</span><br><span class=\"line\">    orderCreated(): Order</span><br><span class=\"line\">    orderChanged(id: ID!): Order</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>The type definition for the <code>Product</code> type referenced in the root schema is given as (<a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/schema/type/product.graphql\" target=\"_blank\" rel=\"noopener\">schema/type/product.graphql</a>):</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type Product &#123;</span><br><span class=\"line\">    id: ID!</span><br><span class=\"line\">    name: String</span><br><span class=\"line\">    description: String</span><br><span class=\"line\">    price: Float</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>The entire schema can be found in the GitHub <a href=\"https://github.com/muety/go-graphql-sse-example/tree/master/schema/type\" target=\"_blank\" rel=\"noopener\">repository</a>.</p>\n<p>After having defined your schema, in the case of <a href=\"github.com/graph-gophers/graphql-go\">graphql-go</a>, it gets transformed into Go code, so it can be compiled into the final Go executable. It can then be loaded on application startup (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/server.go#L35\" target=\"_blank\" rel=\"noopener\">server.go</a>) and served via an HTTP endpoint. Pretty straightforward!</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">graphqlSchema := graphql.MustParseSchema(schema.GetRootSchema(), &amp;resolver.Resolver&#123;&#125;)</span><br><span class=\"line\">http.Handle(<span class=\"string\">\"/api/query\"</span>, middleware.AddContext(ctx, &amp;middleware.GraphQL&#123;Schema: graphqlSchema&#125;))</span><br></pre></td></tr></table></figure>\n<h1>Resolvers</h1>\n<p>After having defined and loaded the schema, so-called <a href=\"https://graphql.org/learn/execution/#root-fields-resolvers\" target=\"_blank\" rel=\"noopener\">resolvers</a> need to be defined. The concept of resolvers is common among most GraphQL server libraries for various different programming languages. A resolver is responsible for providing the data for every field of an entity, e.g. for the <code>ID</code>, <code>name</code>, <code>description</code>, and <code>price</code> fields. Such fields can be literals, as it is the case with all fields of the <code>Product</code> type, but also other nested entities, like in the <code>products</code> field of the following <code>Order</code> type:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type Order &#123;</span><br><span class=\"line\">    id: ID!</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    products: [Product]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>For literals, the resolver simply fills in the actual string, number, or boolean. For complex types, it delegates their resolution to their respective resolvers recursively. For instance, to resolve <code>products</code>, the <code>orderResolver</code> will ask a <code>productResolver</code> to do its respective duty.</p>\n<p>When working with <code>go-graphql</code>, a simple resolver looks like this (<a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/resolver/product.go\" target=\"_blank\" rel=\"noopener\">resolvers/product.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> productResolver <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">\tp *model.Product</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Id</span><span class=\"params\">()</span> <span class=\"title\">graphql</span>.<span class=\"title\">ID</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> graphql.ID(r.p.Id)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Name</span><span class=\"params\">()</span> *<span class=\"title\">string</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;r.p.Name</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Description</span><span class=\"params\">()</span> *<span class=\"title\">string</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;r.p.Description</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Price</span><span class=\"params\">()</span> *<span class=\"title\">float64</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;r.p.Price</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Very simple. It is instantiated with a reference to a <code>Product</code> struct, which was previously loaded from the database and simply maps attributes of the raw data model to the respective GraphQL fields. Please note that it doesn‚Äôt have to be that trivial. For instance, your MongoDB schema might define <code>firstName</code> and <code>lastName</code> fields, while your GraphQL type only has <code>name</code>. In that case, the resolver would have to do some basic concatenation.</p>\n<p>Things get a little more complex when dealing with non-literal fields, like the <code>Order's</code> <code>products</code> above. In our database schema, an order only holds a list of product IDs as <code>items</code>. However, the GraphQL schema declares to return actual product objects. To do so, the respective resolver method first fetches the products for every ID from the database and then passes it on to <code>productResolver</code>s (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/resolver/order.go#L53\" target=\"_blank\" rel=\"noopener\">resolvers/order.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *orderResolver)</span> <span class=\"title\">Products</span><span class=\"params\">(ctx context.Context)</span> <span class=\"params\">(*[]*productResolver, error)</span></span> &#123;</span><br><span class=\"line\">\tl := <span class=\"built_in\">make</span>([]*productResolver, <span class=\"built_in\">len</span>(r.o.Items))</span><br><span class=\"line\"></span><br><span class=\"line\">\tproducts, err := ctx.Value(service.KeyProductService).(*service.ProductService).GetBatchMap(r.o.Items)</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> err != <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"literal\">nil</span>, err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i, id := <span class=\"keyword\">range</span> r.o.Items &#123;</span><br><span class=\"line\">\t\tl[i] = &amp;productResolver&#123;p: products[id]&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;l, <span class=\"literal\">nil</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>Resolvers are very modular, coherent in themselves and can be composed together to build up an entire API. Note that even the very entrypoint of the GraphQL API is just a ‚Äúroot‚Äù resolver. You can find all resolvers, including those for mutations and subscriptions as well, in the <a href=\"https://github.com/muety/go-graphql-sse-example/tree/master/resolver\" target=\"_blank\" rel=\"noopener\">repo</a>.</p>\n<h1>Subscriptions</h1>\n<p>Probably the most interesting part here is subscriptions, as they provide a nice mechanism to make web applications reactive to backend-side data updates. As mentioned in the previous article, the GraphQL specification does not dictate how to technically implement subscriptions. Therefore, we decided to use Server-Sent Events (SSE) as a server-to-client communication channel. Technically, SSEs are simply a long-running HTTP request, which data is written to in form of a text stream and therefore very light-weight and easy to use.</p>\n<p>On the backend side, we introduce a light-weight <a href=\"github.com/leandro-lugaresi/hub\">event bus</a> to our <a href=\"https://github.com/muety/go-graphql-sse-example/tree/master/service\" target=\"_blank\" rel=\"noopener\">services</a>. For every data change, i.e. updates, creations or deletions, an event is published to the bus (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/service/order.go#L66\" target=\"_blank\" rel=\"noopener\">services/order.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(s *OrderService)</span> <span class=\"title\">Create</span><span class=\"params\">(order *model.Order)</span> <span class=\"params\">(*model.Order, error)</span></span> &#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\torder.Id = res.InsertedID.(primitive.ObjectID).Hex()</span><br><span class=\"line\">\ts.Hub.Publish(eventhub.Message&#123;</span><br><span class=\"line\">\t\tName:   KeyOrderCreated, <span class=\"comment\">// order.create</span></span><br><span class=\"line\">\t\tFields: eventhub.Fields&#123;<span class=\"string\">\"id\"</span>: order.Id&#125;,</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Inside the resolver responsible for <code>orderCreated</code> queries, a subscription to the event bus is made once the user requests that subscription.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *Resolver)</span> <span class=\"title\">OrderCreated</span><span class=\"params\">(ctx context.Context)</span> <span class=\"params\">(<span class=\"keyword\">chan</span> *orderResolver, error)</span></span> &#123;</span><br><span class=\"line\">\tc := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> *orderResolver)</span><br><span class=\"line\">\t<span class=\"keyword\">go</span> subscribeOrder(service.KeyOrderCreated, ctx, c)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> c, <span class=\"literal\">nil</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">subscribeOrder</span><span class=\"params\">(key <span class=\"keyword\">string</span>, ctx context.Context, c <span class=\"keyword\">chan</span> *orderResolver)</span></span> &#123;</span><br><span class=\"line\">\tsrv := ctx.Value(service.KeyOrderService).(*service.OrderService)</span><br><span class=\"line\">\tsub := srv.Hub.NonBlockingSubscribe(<span class=\"number\">10</span>, key)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">defer</span> <span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t\tsrv.Hub.Unsubscribe(sub)</span><br><span class=\"line\">\t\t<span class=\"built_in\">close</span>(c)</span><br><span class=\"line\">\t&#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">select</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> &lt;-ctx.Done():</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> m := &lt;-sub.Receiver:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> u, err := srv.Get(m.Fields[<span class=\"string\">\"id\"</span>].(<span class=\"keyword\">string</span>)); err != <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">\t\t\t\tlog.Println(err)</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\tc &lt;- &amp;orderResolver&#123;u&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>When a new order is inserted via the respective service, the above method is triggered as a consequence of being subscribed to <code>order.create</code> events. It reads the orer‚Äôs ID from the event, uses the qualified service to fetch it from the database and passes it on to an <code>orderResolver</code> to translate it into the schema-conformal format.</p>\n<p>The HTTP handler, which dispatched the user‚Äôs GraphQL query to the above resolver, in turn, has subscribed to the result Go channel and writes every incoming <code>Order</code> instance to always-open HTTP stream (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/middleware/graphql.go#L59\" target=\"_blank\" rel=\"noopener\">middleware/graphql.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">c, err := h.Schema.Subscribe(ctx, params.Query, params.OperationName, params.Variables)</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"keyword\">for</span> r := <span class=\"keyword\">range</span> c &#123;</span><br><span class=\"line\">        response := r.(*graphql.Response)</span><br><span class=\"line\">        responseJSON, err := json.Marshal(response)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        fmt.Fprintf(w, <span class=\"string\">\"data: %s\\n\\n\"</span>, responseJSON)</span><br><span class=\"line\">        flusher.Flush()</span><br><span class=\"line\">        ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<h1>Running Queries</h1>\n<h2 id=\"Using-GraphiQL\">Using GraphiQL</h2>\n<p>During development, queries against a GraphQL API can be issued using the interactive GraphiQL browser, as demonstrated here.</p>\n<p><img src=\"images/graphql_screencast2.gif\" alt></p>\n<h1>Programatically</h1>\n<p>On the client-side of our application, GraphQL queries are run to consume the API. After all, any client, that is able to speak HTTP, can also consume a GraphQL API, as GraphQL requests are really just <code>POST</code> requests with a certain query in the body.</p>\n<p>For instance, the <a href=\"https://vuex.vuejs.org/\" target=\"_blank\" rel=\"noopener\">Vuex</a> store action responsible for loading a list of products in the frontend is this (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/web/src/store/products.js#L30\" target=\"_blank\" rel=\"noopener\">store/products.js</a>):</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">async</span> fetchProducts(&#123;commit&#125;) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> q = <span class=\"string\">`&#123;</span></span><br><span class=\"line\"><span class=\"string\">        products() &#123;</span></span><br><span class=\"line\"><span class=\"string\">        id</span></span><br><span class=\"line\"><span class=\"string\">        name</span></span><br><span class=\"line\"><span class=\"string\">        description</span></span><br><span class=\"line\"><span class=\"string\">        price</span></span><br><span class=\"line\"><span class=\"string\">        &#125;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;`</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">const</span> data = <span class=\"keyword\">await</span> Vue.$api.graphql.request(q)</span><br><span class=\"line\">    commit(<span class=\"string\">'addProducts'</span>, data.products.map(Product.new))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>For subscriptions, we rely on a slightly <a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/web/src/vendor/sse.js\" target=\"_blank\" rel=\"noopener\">modified version</a> of <a href=\"https://github.com/mpetazzoni/sse.js\" target=\"_blank\" rel=\"noopener\">sse.js</a>. It acts as a minimal wrapper around the browser‚Äôs standard <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/EventSource\" target=\"_blank\" rel=\"noopener\">EventSource</a> and allows us to run SSE requests as <code>POST</code> (instead of <code>GET</code>) queries and pass some data in the body. We do so to fit well with GraphQL, where queries are always <code>POST</code> requests with a respective query body.</p>\n<p>Subscribing to new orders in the frontend is mostly done like this:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">async</span> subscribeOrderCreated(&#123;commit&#125;) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> q = <span class=\"string\">`subscription &#123;</span></span><br><span class=\"line\"><span class=\"string\">        orderCreated() &#123;</span></span><br><span class=\"line\"><span class=\"string\">            id</span></span><br><span class=\"line\"><span class=\"string\">            queueId</span></span><br><span class=\"line\"><span class=\"string\">            createdAt</span></span><br><span class=\"line\"><span class=\"string\">            updatedAt</span></span><br><span class=\"line\"><span class=\"string\">            status</span></span><br><span class=\"line\"><span class=\"string\">            eta</span></span><br><span class=\"line\"><span class=\"string\">            totalSum</span></span><br><span class=\"line\"><span class=\"string\">            products &#123;</span></span><br><span class=\"line\"><span class=\"string\">                id</span></span><br><span class=\"line\"><span class=\"string\">                name</span></span><br><span class=\"line\"><span class=\"string\">            &#125;</span></span><br><span class=\"line\"><span class=\"string\">        &#125;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;`</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">const</span> source = Vue.$api.sse.request(q, <span class=\"literal\">null</span>)</span><br><span class=\"line\">    source.addEventListener(<span class=\"string\">'message'</span>, e =&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">const</span> payload = <span class=\"built_in\">JSON</span>.parse(e.data)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        commit(<span class=\"string\">'addOrder'</span>, <span class=\"keyword\">new</span> Order(payload.data.orderCreated))</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    source.stream()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1>Demo</h1>\n<p>And here‚Äôs a live demo in action!</p>\n<p><img src=\"images/graphql_screencast1.gif\" alt></p>\n<h1>Outlook</h1>\n<p>While the current demo implementation serves as a ‚Äì in our opinion ‚Äì clean and well-structured starting point for building GraphQL-based web apps, its current state has one major drawback. It lacks authentication and authorization. Usually, you want to control which user can query and modify which data. Therefore, another blog post will follow in the future, which explains how to authorize GraphQL query endpoints.</p>\n<p>Apart from that, we consider the present technology stack a promising choice for new web applications with the potential to facilitate clean, well-organized code and to make development easier, and more flexible.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":"üêô KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":"Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for caf√©s and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":"Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":"Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":"QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":"MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":"telegram-middleman-bot","url":"https://github.com/muety/telegram-middleman-bot","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":"Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":"Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>In the <a href=\"https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.html\">previous</a> part, concepts and benefits of GraphQL and <a href=\"https://graphql.org/blog/subscriptions-in-graphql-and-relay/\" target=\"_blank\" rel=\"noopener\">GraphQL Subscriptions</a> were presented and proposed as a modern, highly flexible alternative for designing web APIs.</p>\n<p>In this post, we are going to look at actual code. For demonstration purposes, a small single-page application (SPA) is built as an example. It can serve as a cleanly structured starting point for new apps based on the proposed tech stack.</p>\n<p>&gt; <strong>Code</strong>: <a href=\"https://github.com/muety/go-graphql-sse-example\" target=\"_blank\" rel=\"noopener\">muety/go-graphql-sse-example</a></p>\n<h1>What to build?</h1>\n<p>The demo web app built in the context of this article comprised of a client-side frontend, built with <a href=\"https://vuejs.org\" target=\"_blank\" rel=\"noopener\">VueJS</a> and a server-side component built with Go. The two components interact with each other through a GraphQL API, which additionally offers the option to subscribe to data updates using <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\" target=\"_blank\" rel=\"noopener\">Server-Sent Events</a>.</p>\n<p>With this demo app, we aim to mimic the functionality of a very basic, minimalist food ordering system. Customers can choose from a list of food products and place their orders. They are shown a waiting number an estimated processing time for their orders and are notified once the order is ready to be picked up. One the other side there is the kiosk operator, who views a live dashboard of currently pending orders, which appear just as they are being placed.</p>\n<p><img src=\"images/graphql_screenshots1.png\" alt></p>\n<h2 id=\"Technology-Stack\">Technology Stack</h2>\n<p>In summary, the following technologies are used:</p>\n<ul>\n<li><a href=\"https://golang.org\" target=\"_blank\" rel=\"noopener\">Go</a> as the primary backend-side language</li>\n<li><a href=\"https://graphql.org/\" target=\"_blank\" rel=\"noopener\">GraphQL</a> as a ‚Äúprotocol‚Äù for defining web interfaces</li>\n<li><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\" target=\"_blank\" rel=\"noopener\">Server-Sent Events</a> as a simple protocol for live updates, used as an implementation of <a href=\"https://graphql.org/blog/subscriptions-in-graphql-and-relay/\" target=\"_blank\" rel=\"noopener\">GraphQL Subscriptions</a> here</li>\n<li><a href=\"https://www.mongodb.com/\" target=\"_blank\" rel=\"noopener\">MongoDB</a> as a flexible document database for storage</li>\n<li><a href=\"https://vuejs.org/\" target=\"_blank\" rel=\"noopener\">VueJS</a> as a frontend framework to build single-page web applications</li>\n</ul>\n<p>In addition, <a href=\"https://www.npmjs.com/package/graphql-request\" target=\"_blank\" rel=\"noopener\">graphql-request</a> is used as a little helper library on the frontend to issue GraphQL queries more easily. It constitutes a light-weight wrapper around the plain <a href=\"https://developer.mozilla.org/de/docs/Web/API/Fetch_API\" target=\"_blank\" rel=\"noopener\">Fetch API</a>.</p>\n<p>On the backend side, GraphQL development is facilitated by the excellent <a href=\"github.com/graph-gophers/graphql-go\">graphql-go</a> package, which already provides well-defined guidelines to get started (thanks to the authors!).</p>\n<h1>Data Model</h1>\n<p>With GraphQL, you need to specify a <a href=\"https://graphql.org/learn/schema/\" target=\"_blank\" rel=\"noopener\">schema</a> using a GrapQL-specific syntax. It includes all entities, which your API should be able to deal with and is essentially a set of type definitions, split among one or more <code>.graphql</code> files.</p>\n<p>Every GraphQL app consists of root types, defining all supported queries, mutations, and subscriptions as well as entity types.</p>\n<p>For our app, the root schema looks like this (<a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/schema/schema.graphql\" target=\"_blank\" rel=\"noopener\">schema/schema.graphql</a>):</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">schema &#123;</span><br><span class=\"line\">    query: Query</span><br><span class=\"line\">    mutation: Mutation</span><br><span class=\"line\">    subscription: Subscription</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type Query &#123;</span><br><span class=\"line\">    product(id: ID!): Product</span><br><span class=\"line\">    products(): [Product]</span><br><span class=\"line\">    order(id: ID!): Order</span><br><span class=\"line\">    orders(status: String): [Order]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type Mutation &#123;</span><br><span class=\"line\">    createOrder(order: OrderInput!): Order</span><br><span class=\"line\">    updateOrder(order: OrderUpdateInput!): Order</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type Subscription &#123;</span><br><span class=\"line\">    orderCreated(): Order</span><br><span class=\"line\">    orderChanged(id: ID!): Order</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>The type definition for the <code>Product</code> type referenced in the root schema is given as (<a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/schema/type/product.graphql\" target=\"_blank\" rel=\"noopener\">schema/type/product.graphql</a>):</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type Product &#123;</span><br><span class=\"line\">    id: ID!</span><br><span class=\"line\">    name: String</span><br><span class=\"line\">    description: String</span><br><span class=\"line\">    price: Float</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>The entire schema can be found in the GitHub <a href=\"https://github.com/muety/go-graphql-sse-example/tree/master/schema/type\" target=\"_blank\" rel=\"noopener\">repository</a>.</p>\n<p>After having defined your schema, in the case of <a href=\"github.com/graph-gophers/graphql-go\">graphql-go</a>, it gets transformed into Go code, so it can be compiled into the final Go executable. It can then be loaded on application startup (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/server.go#L35\" target=\"_blank\" rel=\"noopener\">server.go</a>) and served via an HTTP endpoint. Pretty straightforward!</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">graphqlSchema := graphql.MustParseSchema(schema.GetRootSchema(), &amp;resolver.Resolver&#123;&#125;)</span><br><span class=\"line\">http.Handle(<span class=\"string\">\"/api/query\"</span>, middleware.AddContext(ctx, &amp;middleware.GraphQL&#123;Schema: graphqlSchema&#125;))</span><br></pre></td></tr></table></figure>\n<h1>Resolvers</h1>\n<p>After having defined and loaded the schema, so-called <a href=\"https://graphql.org/learn/execution/#root-fields-resolvers\" target=\"_blank\" rel=\"noopener\">resolvers</a> need to be defined. The concept of resolvers is common among most GraphQL server libraries for various different programming languages. A resolver is responsible for providing the data for every field of an entity, e.g. for the <code>ID</code>, <code>name</code>, <code>description</code>, and <code>price</code> fields. Such fields can be literals, as it is the case with all fields of the <code>Product</code> type, but also other nested entities, like in the <code>products</code> field of the following <code>Order</code> type:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type Order &#123;</span><br><span class=\"line\">    id: ID!</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    products: [Product]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>For literals, the resolver simply fills in the actual string, number, or boolean. For complex types, it delegates their resolution to their respective resolvers recursively. For instance, to resolve <code>products</code>, the <code>orderResolver</code> will ask a <code>productResolver</code> to do its respective duty.</p>\n<p>When working with <code>go-graphql</code>, a simple resolver looks like this (<a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/resolver/product.go\" target=\"_blank\" rel=\"noopener\">resolvers/product.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> productResolver <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">\tp *model.Product</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Id</span><span class=\"params\">()</span> <span class=\"title\">graphql</span>.<span class=\"title\">ID</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> graphql.ID(r.p.Id)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Name</span><span class=\"params\">()</span> *<span class=\"title\">string</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;r.p.Name</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Description</span><span class=\"params\">()</span> *<span class=\"title\">string</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;r.p.Description</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Price</span><span class=\"params\">()</span> *<span class=\"title\">float64</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;r.p.Price</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Very simple. It is instantiated with a reference to a <code>Product</code> struct, which was previously loaded from the database and simply maps attributes of the raw data model to the respective GraphQL fields. Please note that it doesn‚Äôt have to be that trivial. For instance, your MongoDB schema might define <code>firstName</code> and <code>lastName</code> fields, while your GraphQL type only has <code>name</code>. In that case, the resolver would have to do some basic concatenation.</p>\n<p>Things get a little more complex when dealing with non-literal fields, like the <code>Order's</code> <code>products</code> above. In our database schema, an order only holds a list of product IDs as <code>items</code>. However, the GraphQL schema declares to return actual product objects. To do so, the respective resolver method first fetches the products for every ID from the database and then passes it on to <code>productResolver</code>s (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/resolver/order.go#L53\" target=\"_blank\" rel=\"noopener\">resolvers/order.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *orderResolver)</span> <span class=\"title\">Products</span><span class=\"params\">(ctx context.Context)</span> <span class=\"params\">(*[]*productResolver, error)</span></span> &#123;</span><br><span class=\"line\">\tl := <span class=\"built_in\">make</span>([]*productResolver, <span class=\"built_in\">len</span>(r.o.Items))</span><br><span class=\"line\"></span><br><span class=\"line\">\tproducts, err := ctx.Value(service.KeyProductService).(*service.ProductService).GetBatchMap(r.o.Items)</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> err != <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"literal\">nil</span>, err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i, id := <span class=\"keyword\">range</span> r.o.Items &#123;</span><br><span class=\"line\">\t\tl[i] = &amp;productResolver&#123;p: products[id]&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;l, <span class=\"literal\">nil</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>Resolvers are very modular, coherent in themselves and can be composed together to build up an entire API. Note that even the very entrypoint of the GraphQL API is just a ‚Äúroot‚Äù resolver. You can find all resolvers, including those for mutations and subscriptions as well, in the <a href=\"https://github.com/muety/go-graphql-sse-example/tree/master/resolver\" target=\"_blank\" rel=\"noopener\">repo</a>.</p>\n<h1>Subscriptions</h1>\n<p>Probably the most interesting part here is subscriptions, as they provide a nice mechanism to make web applications reactive to backend-side data updates. As mentioned in the previous article, the GraphQL specification does not dictate how to technically implement subscriptions. Therefore, we decided to use Server-Sent Events (SSE) as a server-to-client communication channel. Technically, SSEs are simply a long-running HTTP request, which data is written to in form of a text stream and therefore very light-weight and easy to use.</p>\n<p>On the backend side, we introduce a light-weight <a href=\"github.com/leandro-lugaresi/hub\">event bus</a> to our <a href=\"https://github.com/muety/go-graphql-sse-example/tree/master/service\" target=\"_blank\" rel=\"noopener\">services</a>. For every data change, i.e. updates, creations or deletions, an event is published to the bus (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/service/order.go#L66\" target=\"_blank\" rel=\"noopener\">services/order.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(s *OrderService)</span> <span class=\"title\">Create</span><span class=\"params\">(order *model.Order)</span> <span class=\"params\">(*model.Order, error)</span></span> &#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\torder.Id = res.InsertedID.(primitive.ObjectID).Hex()</span><br><span class=\"line\">\ts.Hub.Publish(eventhub.Message&#123;</span><br><span class=\"line\">\t\tName:   KeyOrderCreated, <span class=\"comment\">// order.create</span></span><br><span class=\"line\">\t\tFields: eventhub.Fields&#123;<span class=\"string\">\"id\"</span>: order.Id&#125;,</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Inside the resolver responsible for <code>orderCreated</code> queries, a subscription to the event bus is made once the user requests that subscription.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *Resolver)</span> <span class=\"title\">OrderCreated</span><span class=\"params\">(ctx context.Context)</span> <span class=\"params\">(<span class=\"keyword\">chan</span> *orderResolver, error)</span></span> &#123;</span><br><span class=\"line\">\tc := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> *orderResolver)</span><br><span class=\"line\">\t<span class=\"keyword\">go</span> subscribeOrder(service.KeyOrderCreated, ctx, c)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> c, <span class=\"literal\">nil</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">subscribeOrder</span><span class=\"params\">(key <span class=\"keyword\">string</span>, ctx context.Context, c <span class=\"keyword\">chan</span> *orderResolver)</span></span> &#123;</span><br><span class=\"line\">\tsrv := ctx.Value(service.KeyOrderService).(*service.OrderService)</span><br><span class=\"line\">\tsub := srv.Hub.NonBlockingSubscribe(<span class=\"number\">10</span>, key)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">defer</span> <span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t\tsrv.Hub.Unsubscribe(sub)</span><br><span class=\"line\">\t\t<span class=\"built_in\">close</span>(c)</span><br><span class=\"line\">\t&#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">select</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> &lt;-ctx.Done():</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> m := &lt;-sub.Receiver:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> u, err := srv.Get(m.Fields[<span class=\"string\">\"id\"</span>].(<span class=\"keyword\">string</span>)); err != <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">\t\t\t\tlog.Println(err)</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\tc &lt;- &amp;orderResolver&#123;u&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>When a new order is inserted via the respective service, the above method is triggered as a consequence of being subscribed to <code>order.create</code> events. It reads the orer‚Äôs ID from the event, uses the qualified service to fetch it from the database and passes it on to an <code>orderResolver</code> to translate it into the schema-conformal format.</p>\n<p>The HTTP handler, which dispatched the user‚Äôs GraphQL query to the above resolver, in turn, has subscribed to the result Go channel and writes every incoming <code>Order</code> instance to always-open HTTP stream (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/middleware/graphql.go#L59\" target=\"_blank\" rel=\"noopener\">middleware/graphql.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">c, err := h.Schema.Subscribe(ctx, params.Query, params.OperationName, params.Variables)</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"keyword\">for</span> r := <span class=\"keyword\">range</span> c &#123;</span><br><span class=\"line\">        response := r.(*graphql.Response)</span><br><span class=\"line\">        responseJSON, err := json.Marshal(response)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        fmt.Fprintf(w, <span class=\"string\">\"data: %s\\n\\n\"</span>, responseJSON)</span><br><span class=\"line\">        flusher.Flush()</span><br><span class=\"line\">        ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<h1>Running Queries</h1>\n<h2 id=\"Using-GraphiQL\">Using GraphiQL</h2>\n<p>During development, queries against a GraphQL API can be issued using the interactive GraphiQL browser, as demonstrated here.</p>\n<p><img src=\"images/graphql_screencast2.gif\" alt></p>\n<h1>Programatically</h1>\n<p>On the client-side of our application, GraphQL queries are run to consume the API. After all, any client, that is able to speak HTTP, can also consume a GraphQL API, as GraphQL requests are really just <code>POST</code> requests with a certain query in the body.</p>\n<p>For instance, the <a href=\"https://vuex.vuejs.org/\" target=\"_blank\" rel=\"noopener\">Vuex</a> store action responsible for loading a list of products in the frontend is this (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/web/src/store/products.js#L30\" target=\"_blank\" rel=\"noopener\">store/products.js</a>):</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">async</span> fetchProducts(&#123;commit&#125;) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> q = <span class=\"string\">`&#123;</span></span><br><span class=\"line\"><span class=\"string\">        products() &#123;</span></span><br><span class=\"line\"><span class=\"string\">        id</span></span><br><span class=\"line\"><span class=\"string\">        name</span></span><br><span class=\"line\"><span class=\"string\">        description</span></span><br><span class=\"line\"><span class=\"string\">        price</span></span><br><span class=\"line\"><span class=\"string\">        &#125;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;`</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">const</span> data = <span class=\"keyword\">await</span> Vue.$api.graphql.request(q)</span><br><span class=\"line\">    commit(<span class=\"string\">'addProducts'</span>, data.products.map(Product.new))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>For subscriptions, we rely on a slightly <a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/web/src/vendor/sse.js\" target=\"_blank\" rel=\"noopener\">modified version</a> of <a href=\"https://github.com/mpetazzoni/sse.js\" target=\"_blank\" rel=\"noopener\">sse.js</a>. It acts as a minimal wrapper around the browser‚Äôs standard <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/EventSource\" target=\"_blank\" rel=\"noopener\">EventSource</a> and allows us to run SSE requests as <code>POST</code> (instead of <code>GET</code>) queries and pass some data in the body. We do so to fit well with GraphQL, where queries are always <code>POST</code> requests with a respective query body.</p>\n<p>Subscribing to new orders in the frontend is mostly done like this:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">async</span> subscribeOrderCreated(&#123;commit&#125;) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> q = <span class=\"string\">`subscription &#123;</span></span><br><span class=\"line\"><span class=\"string\">        orderCreated() &#123;</span></span><br><span class=\"line\"><span class=\"string\">            id</span></span><br><span class=\"line\"><span class=\"string\">            queueId</span></span><br><span class=\"line\"><span class=\"string\">            createdAt</span></span><br><span class=\"line\"><span class=\"string\">            updatedAt</span></span><br><span class=\"line\"><span class=\"string\">            status</span></span><br><span class=\"line\"><span class=\"string\">            eta</span></span><br><span class=\"line\"><span class=\"string\">            totalSum</span></span><br><span class=\"line\"><span class=\"string\">            products &#123;</span></span><br><span class=\"line\"><span class=\"string\">                id</span></span><br><span class=\"line\"><span class=\"string\">                name</span></span><br><span class=\"line\"><span class=\"string\">            &#125;</span></span><br><span class=\"line\"><span class=\"string\">        &#125;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;`</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">const</span> source = Vue.$api.sse.request(q, <span class=\"literal\">null</span>)</span><br><span class=\"line\">    source.addEventListener(<span class=\"string\">'message'</span>, e =&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">const</span> payload = <span class=\"built_in\">JSON</span>.parse(e.data)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        commit(<span class=\"string\">'addOrder'</span>, <span class=\"keyword\">new</span> Order(payload.data.orderCreated))</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    source.stream()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h1>Demo</h1>\n<p>And here‚Äôs a live demo in action!</p>\n<p><img src=\"images/graphql_screencast1.gif\" alt></p>\n<h1>Outlook</h1>\n<p>While the current demo implementation serves as a ‚Äì in our opinion ‚Äì clean and well-structured starting point for building GraphQL-based web apps, its current state has one major drawback. It lacks authentication and authorization. Usually, you want to control which user can query and modify which data. Therefore, another blog post will follow in the future, which explains how to authorize GraphQL query endpoints.</p>\n<p>Apart from that, we consider the present technology stack a promising choice for new web applications with the potential to facilitate clean, well-organized code and to make development easier, and more flexible.</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}