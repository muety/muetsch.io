{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"source/favicon.ico","path":"favicon.ico","modified":0,"renderable":0},{"_id":"source/images/Webserver_memory_graph.jpg","path":"images/Webserver_memory_graph.jpg","modified":0,"renderable":0},{"_id":"source/images/academic_faces1.png","path":"images/academic_faces1.png","modified":0,"renderable":0},{"_id":"source/images/academic_faces2.png","path":"images/academic_faces2.png","modified":0,"renderable":0},{"_id":"source/images/academic_faces3.png","path":"images/academic_faces3.png","modified":0,"renderable":0},{"_id":"source/images/anchr_1.jpg","path":"images/anchr_1.jpg","modified":0,"renderable":0},{"_id":"source/images/anchr_2.jpg","path":"images/anchr_2.jpg","modified":0,"renderable":0},{"_id":"source/images/angular2_logo.png","path":"images/angular2_logo.png","modified":0,"renderable":0},{"_id":"source/images/apple-touch-icon.png","path":"images/apple-touch-icon.png","modified":0,"renderable":0},{"_id":"source/images/benchmarks.svg","path":"images/benchmarks.svg","modified":0,"renderable":0},{"_id":"source/images/benchmarks2.svg","path":"images/benchmarks2.svg","modified":0,"renderable":0},{"_id":"source/images/cartpole1.jpg","path":"images/cartpole1.jpg","modified":0,"renderable":0},{"_id":"source/images/cartpole2.png","path":"images/cartpole2.png","modified":0,"renderable":0},{"_id":"source/images/cartpole3.png","path":"images/cartpole3.png","modified":0,"renderable":0},{"_id":"source/images/cartpole4.png","path":"images/cartpole4.png","modified":0,"renderable":0},{"_id":"source/images/cert.png","path":"images/cert.png","modified":0,"renderable":0},{"_id":"source/images/crawlbuddy2.png","path":"images/crawlbuddy2.png","modified":0,"renderable":0},{"_id":"source/images/dns1.png","path":"images/dns1.png","modified":0,"renderable":0},{"_id":"source/images/dns2.png","path":"images/dns2.png","modified":0,"renderable":0},{"_id":"source/images/do.png","path":"images/do.png","modified":0,"renderable":0},{"_id":"source/images/doodlerbot_icon.png","path":"images/doodlerbot_icon.png","modified":0,"renderable":0},{"_id":"source/images/dqn1.png","path":"images/dqn1.png","modified":0,"renderable":0},{"_id":"source/images/dqn2.png","path":"images/dqn2.png","modified":0,"renderable":0},{"_id":"source/images/dqn3.png","path":"images/dqn3.png","modified":0,"renderable":0},{"_id":"source/images/dqn4.png","path":"images/dqn4.png","modified":0,"renderable":0},{"_id":"source/images/expensebot_icon.png","path":"images/expensebot_icon.png","modified":0,"renderable":0},{"_id":"source/images/gh_eer.png","path":"images/gh_eer.png","modified":0,"renderable":0},{"_id":"source/images/gh_ka_world.png","path":"images/gh_ka_world.png","modified":0,"renderable":0},{"_id":"source/images/gh_location_langs.png","path":"images/gh_location_langs.png","modified":0,"renderable":0},{"_id":"source/images/gh_locations.png","path":"images/gh_locations.png","modified":0,"renderable":0},{"_id":"source/images/gh_popular_lang.png","path":"images/gh_popular_lang.png","modified":0,"renderable":0},{"_id":"source/images/graphql_cover.png","path":"images/graphql_cover.png","modified":0,"renderable":0},{"_id":"source/images/graphql_github.png","path":"images/graphql_github.png","modified":0,"renderable":0},{"_id":"source/images/graphql_screencast1.gif","path":"images/graphql_screencast1.gif","modified":0,"renderable":0},{"_id":"source/images/graphql_screencast2.gif","path":"images/graphql_screencast2.gif","modified":0,"renderable":0},{"_id":"source/images/graphql_screenshots1.png","path":"images/graphql_screenshots1.png","modified":0,"renderable":0},{"_id":"source/images/halite_game.png","path":"images/halite_game.png","modified":0,"renderable":0},{"_id":"source/images/halite_langs.png","path":"images/halite_langs.png","modified":0,"renderable":0},{"_id":"source/images/k9_logo.png","path":"images/k9_logo.png","modified":0,"renderable":0},{"_id":"source/images/lock.jpg","path":"images/lock.jpg","modified":0,"renderable":0},{"_id":"source/images/mborg_logo.jpg","path":"images/mborg_logo.jpg","modified":0,"renderable":0},{"_id":"source/images/middleman.png","path":"images/middleman.png","modified":0,"renderable":0},{"_id":"source/images/middleman2.png","path":"images/middleman2.png","modified":0,"renderable":0},{"_id":"source/images/mqtt_bench_2.png","path":"images/mqtt_bench_2.png","modified":0,"renderable":0},{"_id":"source/images/nextcloud_migration.png","path":"images/nextcloud_migration.png","modified":0,"renderable":0},{"_id":"source/images/nloss_2.png","path":"images/nloss_2.png","modified":0,"renderable":0},{"_id":"source/images/nloss_3.png","path":"images/nloss_3.png","modified":0,"renderable":0},{"_id":"source/images/okc_logo.png","path":"images/okc_logo.png","modified":0,"renderable":0},{"_id":"source/images/push_screenshot1.png","path":"images/push_screenshot1.png","modified":0,"renderable":0},{"_id":"source/images/push_screenshot2.png","path":"images/push_screenshot2.png","modified":0,"renderable":0},{"_id":"source/images/qn1.png","path":"images/qn1.png","modified":0,"renderable":0},{"_id":"source/images/qn2.png","path":"images/qn2.png","modified":0,"renderable":0},{"_id":"source/images/qn3.png","path":"images/qn3.png","modified":0,"renderable":0},{"_id":"source/images/qn4.png","path":"images/qn4.png","modified":0,"renderable":0},{"_id":"source/images/qn_feature.png","path":"images/qn_feature.png","modified":0,"renderable":0},{"_id":"source/images/qn_icon.png","path":"images/qn_icon.png","modified":0,"renderable":0},{"_id":"source/images/quadkeys.jpg","path":"images/quadkeys.jpg","modified":0,"renderable":0},{"_id":"source/images/raid01.png","path":"images/raid01.png","modified":0,"renderable":0},{"_id":"source/images/raid10.png","path":"images/raid10.png","modified":0,"renderable":0},{"_id":"source/images/scorecard.jpg","path":"images/scorecard.jpg","modified":0,"renderable":0},{"_id":"source/images/statista.png","path":"images/statista.png","modified":0,"renderable":0},{"_id":"source/images/svhn_cropped_images.png","path":"images/svhn_cropped_images.png","modified":0,"renderable":0},{"_id":"source/images/svhn_cropping.png","path":"images/svhn_cropping.png","modified":0,"renderable":0},{"_id":"source/images/svhn_labelimg.png","path":"images/svhn_labelimg.png","modified":0,"renderable":0},{"_id":"source/images/svhn_labels.png","path":"images/svhn_labels.png","modified":0,"renderable":0},{"_id":"source/images/svhn_steps.png","path":"images/svhn_steps.png","modified":0,"renderable":0},{"_id":"source/images/talkycars2.png","path":"images/talkycars2.png","modified":0,"renderable":0},{"_id":"source/images/talkycars3.png","path":"images/talkycars3.png","modified":0,"renderable":0},{"_id":"source/images/talkycars4.png","path":"images/talkycars4.png","modified":0,"renderable":0},{"_id":"source/images/talkycars5.png","path":"images/talkycars5.png","modified":0,"renderable":0},{"_id":"source/images/talkycars6.png","path":"images/talkycars6.png","modified":0,"renderable":0},{"_id":"source/images/talkycars7.png","path":"images/talkycars7.png","modified":0,"renderable":0},{"_id":"source/images/talkycars8.png","path":"images/talkycars8.png","modified":0,"renderable":0},{"_id":"source/images/talkycars9.png","path":"images/talkycars9.png","modified":0,"renderable":0},{"_id":"source/images/tb_logo.png","path":"images/tb_logo.png","modified":0,"renderable":0},{"_id":"source/images/tello1.jpg","path":"images/tello1.jpg","modified":0,"renderable":0},{"_id":"source/images/tello2.png","path":"images/tello2.png","modified":0,"renderable":0},{"_id":"source/images/thesis_mockup.png","path":"images/thesis_mockup.png","modified":0,"renderable":0},{"_id":"source/images/thesis_stack.png","path":"images/thesis_stack.png","modified":0,"renderable":0},{"_id":"source/images/trivia.jpg","path":"images/trivia.jpg","modified":0,"renderable":0},{"_id":"source/images/unhosted.jpg","path":"images/unhosted.jpg","modified":0,"renderable":0},{"_id":"source/images/webdev_techstack.png","path":"images/webdev_techstack.png","modified":0,"renderable":0},{"_id":"source/images/webdev_techstack_large.png","path":"images/webdev_techstack_large.png","modified":0,"renderable":0},{"_id":"source/images/webdevlist.jpg","path":"images/webdevlist.jpg","modified":0,"renderable":0},{"_id":"source/images/webserver_performance.png","path":"images/webserver_performance.png","modified":0,"renderable":0},{"_id":"source/images/webservers.png","path":"images/webservers.png","modified":0,"renderable":0},{"_id":"themes/cactus-dark/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/images/favicon-192x192.png","path":"images/favicon-192x192.png","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/images/favicon.ico","path":"images/favicon.ico","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/jquery/jquery.min.js","path":"lib/jquery/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/justified-gallery/jquery.justifiedGallery.min.js","path":"lib/justified-gallery/jquery.justifiedGallery.min.js","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/justified-gallery/justifiedGallery.min.css","path":"lib/justified-gallery/justifiedGallery.min.css","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/styles.css","path":"lib/meslo-LG/styles.css","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-Bold.ttf","path":"lib/meslo-LG/fonts/MesloLGL-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-BoldItalic.ttf","path":"lib/meslo-LG/fonts/MesloLGL-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-Italic.ttf","path":"lib/meslo-LG/fonts/MesloLGL-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-Regular.ttf","path":"lib/meslo-LG/fonts/MesloLGL-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-Bold.ttf","path":"lib/meslo-LG/fonts/MesloLGM-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-BoldItalic.ttf","path":"lib/meslo-LG/fonts/MesloLGM-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-Italic.ttf","path":"lib/meslo-LG/fonts/MesloLGM-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-Regular.ttf","path":"lib/meslo-LG/fonts/MesloLGM-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-Bold.ttf","path":"lib/meslo-LG/fonts/MesloLGS-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-BoldItalic.ttf","path":"lib/meslo-LG/fonts/MesloLGS-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-Italic.ttf","path":"lib/meslo-LG/fonts/MesloLGS-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-Regular.ttf","path":"lib/meslo-LG/fonts/MesloLGS-Regular.ttf","modified":0,"renderable":1},{"_id":"source/images/nodered-flow.png","path":"images/nodered-flow.png","modified":0,"renderable":0},{"_id":"source/images/nodered-flow2.png","path":"images/nodered-flow2.png","modified":0,"renderable":0},{"_id":"source/images/world_poverty.png","path":"images/world_poverty.png","modified":0,"renderable":0},{"_id":"source/images/betterplace.png","path":"images/betterplace.png","modified":0,"renderable":0},{"_id":"source/images/gh_sponsors.png","path":"images/gh_sponsors.png","modified":0,"renderable":0},{"_id":"source/images/foss_sponsoring.png","path":"images/foss_sponsoring.png","modified":0,"renderable":0},{"_id":"source/images/java_cert1.png","path":"images/java_cert1.png","modified":0,"renderable":0},{"_id":"source/images/java_cert2.png","path":"images/java_cert2.png","modified":0,"renderable":0},{"_id":"source/images/java_cert3.png","path":"images/java_cert3.png","modified":0,"renderable":0},{"_id":"source/images/java_cert4.png","path":"images/java_cert4.png","modified":0,"renderable":0},{"_id":"source/images/java_cert5.png","path":"images/java_cert5.png","modified":0,"renderable":0},{"_id":"source/images/java_cert6.png","path":"images/java_cert6.png","modified":0,"renderable":0},{"_id":"source/images/java_cert7.png","path":"images/java_cert7.png","modified":0,"renderable":0}],"Cache":[{"_id":"source/_data/projects.json","hash":"fa33cf8312f4e7418a0a82f0bd908def38d46e0b","modified":1604345792042},{"_id":"source/_data/badges.json","hash":"a08c9f1779f04c488b8f318666df1965c59fa154","modified":1609778895697},{"_id":"source/_posts/anchr-io-image-uploads-bookmarks-and-shortlink-service.md","hash":"aa07470b08317dc91ccaf921a925f2c39182a8af","modified":1604088340280},{"_id":"source/_posts/basic-benchmarks-of-5-different-mqtt-brokers.md","hash":"c191347bbb81d86d75f25ee11b39f89136231226","modified":1604088340280},{"_id":"source/_posts/building-a-cloud-native-web-scraper-using-8-different-aws-services.md","hash":"5e192fa87ce383a9426d96671d02d7ab39079f4a","modified":1604088340280},{"_id":"source/_posts/caddy-a-modern-web-server-vs-nginx.md","hash":"03ea9f5fc3883673b079d478e84431c9f4d285d0","modified":1604088340281},{"_id":"source/_posts/cartpole-with-a-deep-q-network.md","hash":"56094af6645f56246e8fde7820b188e040d4cb9f","modified":1604088340281},{"_id":"source/_posts/cartpole-with-qlearning-first-experiences-with-openai-gym.md","hash":"81fdfb37f958ff2564c9b5bd57925f19a8f0732a","modified":1604088340281},{"_id":"source/_posts/design-of-a-linked-dataenabled-microservice-platform-for-the-industrial-internet-of-things.md","hash":"2b1791629d2f2fffa3bb787c28fbc9b0b53329c9","modified":1604088340281},{"_id":"source/_posts/detecting-academics-major-from-facial-images.md","hash":"feece3dfe4bab6b099c751a317e734fbc1bf17a1","modified":1604088340282},{"_id":"source/_posts/digitalocean-my-preferred-cloud-hosting-provider.md","hash":"5b56a32dbd669dad34422c15b34d83680734d75b","modified":1604088340282},{"_id":"source/_posts/exploratory-analysis-on-github-data.md","hash":"8bf9af1e6c06e711f340e1f9c22d09742c6c83c1","modified":1604088340282},{"_id":"source/_posts/flying-a-dji-tello-drone-with-go.md","hash":"326d83975801abf8ea0d3fb2ebbec0329ded66e4","modified":1604088340282},{"_id":"source/_posts/halite-a-rule-based-ai-bot.md","hash":"a9de4bdae1360111a55491ade9e6503d0be23124","modified":1604088340283},{"_id":"source/_posts/how-to-enable-dns-over-tls-on-ubuntu-using-coredns.md","hash":"44820ba5aeb5264935b8a0026a5ccba89ff76b32","modified":1604088340283},{"_id":"source/_posts/how-to-load-svg-into-imageview-by-url-in-android.md","hash":"d1fef41e625b21066a61704358b14055c7dff96b","modified":1604088340291},{"_id":"source/_posts/how-to-load-yago-into-apache-jena-fuseki.md","hash":"47a6356bc8cfce54d352a58373a76ac38f3009ea","modified":1604088340286},{"_id":"source/_posts/how-to-make-telegram-bots.md","hash":"f8301172c8718c9f1dc2536f7f32779483b6dfeb","modified":1604088340287},{"_id":"source/_posts/how-to-receive-sharing-intents-in-flutter.md","hash":"f9726102fe6df4f3cd68c3056197b329ab492a0c","modified":1604088340290},{"_id":"source/_posts/http-performance-java-jersey-vs-go-vs-nodejs.md","hash":"1275fd0526b96f4945ba0b321e12c0bc7bd14183","modified":1604088340287},{"_id":"source/_posts/http20-server-push-proxy.md","hash":"2ea167e35e1dff883f35b11755442895ee5d37d2","modified":1604088340283},{"_id":"source/_posts/innovation-in-germany-not.md","hash":"27bc1e2acb0843065443492bd18dce9a107a0ee4","modified":1604088340283},{"_id":"source/_posts/instant-messenger-security-encryption-overview.md","hash":"9edcc044fa42c87b0911512f59d7f22ba73a7d02","modified":1604088340283},{"_id":"source/_posts/learning-angular2-what-is-new.md","hash":"e0c70f68b237a4d4ef8f8fdbb78a7f299235d9b5","modified":1604088340284},{"_id":"source/_posts/linkeddata-trivia-game.md","hash":"22077c9bdc8f6f5693b6933178fa33a6ccdd4673","modified":1604088340284},{"_id":"source/_posts/linux-cache-information-bash-script.md","hash":"9b5526414d08dc4102119d7c31996c9961288fbb","modified":1604088340287},{"_id":"source/_posts/migrate-maildir-to-new-server-using-imapsync.md","hash":"15ae57c2734c5e3514532806234212184f340166","modified":1604088340287},{"_id":"source/_posts/migrating-nextcloud-from-sqlite-to-mysql-with-docker.md","hash":"d1ef03affb6a9b9dbd4983ddc8b6afd35f8cbb13","modified":1604088340291},{"_id":"source/_posts/ml-telegram-chat-message-classification.md","hash":"4a5b3f5230185168fd13a2520a95956a0a716157","modified":1604088340288},{"_id":"source/_posts/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.md","hash":"91b24a5f8ed5a3aa38337ee55331818608d6d9d8","modified":1604088340284},{"_id":"source/_posts/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.md","hash":"8fa2f777291484be52658b5878fdef8413b854d3","modified":1604088340284},{"_id":"source/_posts/my-teck-stack-if-i-had-to-build-an-app-today.md","hash":"173cd62d9eccceda4387127935c58d132aff6774","modified":1604088340288},{"_id":"source/_posts/overgrive-not-starting-on-ubuntu-18-04.md","hash":"d93db4eaa9f0d98fa99438cb21e919be3b3fbb8b","modified":1604088340291},{"_id":"source/_posts/pgp-encrypted-personal-e-mail-inbox.md","hash":"6b53e2734e79a95e2d3c87154fbbbea47ca257b0","modified":1604088340286},{"_id":"source/_posts/quiznerd-my-experiences-with-the-android-developer-nanodegree.md","hash":"f7b081ed2d0cc4b2d24e06ca7da31e63be5be643","modified":1604088340284},{"_id":"source/_posts/talkycars-a-distributed-software-platform-for-cooperative-perception.md","hash":"1b8548d0113012477da31646ef64010ec9635555","modified":1604088340285},{"_id":"source/_posts/telegram-bot-example-code-in-nodejs.md","hash":"239d417e5bb97d5dc2997ef95b103cd9949472b4","modified":1604088340289},{"_id":"source/_posts/telegram-expensebot-doodlerbot.md","hash":"8299aae697ea39bb79d72d796377bb2b9a3a0558","modified":1604088340285},{"_id":"source/_posts/telegram-middleman-bot-push-notifications-as-easy-as-post.md","hash":"f0e8caad0a1645819b1a3d9833e3680b1cbbdfea","modified":1604088340285},{"_id":"source/_posts/transfer-learning-for-multi-digit-recognition-using-tensorflow-object-detection-and-svhn-classifier.md","hash":"27c85ca69c1a245f85fa4d9492c836b870d58174","modified":1604088340285},{"_id":"source/_posts/unhostedorg-applications-with-remotestorageio-and-webfingernet.md","hash":"f857c973605d117f80374918b4756081dccad729","modified":1604088340286},{"_id":"source/_posts/web-development-technology-stack.md","hash":"3196d6ac39cf01f233d0919f4d4405e768dfa565","modified":1604088340289},{"_id":"source/_posts/webdevlistnet-the-developers-resource-collection.md","hash":"8858bf2d2930d64083df236cbab091e9970c1dc4","modified":1604088340286},{"_id":"source/_posts/what-i-like-about-developing-apps-with-flutter.md","hash":"e966434a8a7e403a7cddd2931e4a60a8a8be3cbb","modified":1604088340290},{"_id":"source/_posts/why-raid-10-is-better-than-raid-01.md","hash":"aaefbd6d3af02609622cdbc12c497862366370b1","modified":1604088340286},{"_id":"source/articles/index.md","hash":"2265dc41dd7b9083d5ba7a2a4a78ae6109ef27b3","modified":1604088340233},{"_id":"source/about/index.md","hash":"1999418a217f4d0486aed734855ba88695674f00","modified":1604346058278},{"_id":"source/imprint/index.md","hash":"2c2cd06c84a2548e6d1883d62dbbe16e4ff61e8f","modified":1604088340277},{"_id":"source/images/Webserver_memory_graph.jpg","hash":"aeaf52174560a2d3a7ff32fda21a26b96c907cc9","modified":1604088340239},{"_id":"source/images/academic_faces3.png","hash":"34b476eb62b5b5e7f2a14e13dbd91df0336ed53c","modified":1604088340241},{"_id":"source/images/anchr_1.jpg","hash":"0d50b24329bdcddea234f0c3ade3f2846daae148","modified":1604088340242},{"_id":"source/images/anchr_2.jpg","hash":"d07dfedb2fc549983f63bf7251f3d011b96188aa","modified":1604088340242},{"_id":"source/images/angular2_logo.png","hash":"189713e0c0de88477c6726fc59b4cd1cfb16b05e","modified":1604088340242},{"_id":"source/images/apple-touch-icon.png","hash":"07059d8fe92f685176d96045edb70c01deb8c4d0","modified":1604088340242},{"_id":"source/images/benchmarks.svg","hash":"e9fbfdf69d7ac1c9890d541fb07d69f844fbb99a","modified":1604088340244},{"_id":"source/images/benchmarks2.svg","hash":"c4bd56f99f3a8810fd50a3caf7a99213b407a0d5","modified":1604088340244},{"_id":"source/images/cartpole1.jpg","hash":"3e237ad7b3bb78ada0d507c8f644a791797b194e","modified":1604088340244},{"_id":"source/images/cartpole2.png","hash":"bb292c589bd25b8b59c75989377163232d29b1c8","modified":1604088340244},{"_id":"source/images/cartpole3.png","hash":"03883366ab1f16b607653ef239b99ede1ac56efc","modified":1604088340245},{"_id":"source/images/cartpole4.png","hash":"d92a7e37c8e88986a51e9a7e6b0124093b5a7425","modified":1604088340245},{"_id":"source/images/cert.png","hash":"e4d28e0c8650b56a92b7ae739f51a92e6e3b83f6","modified":1604088340245},{"_id":"source/images/crawlbuddy2.png","hash":"1325a3db38972d5ec796edeb3658c16110ce7703","modified":1604088340246},{"_id":"source/images/doodlerbot_icon.png","hash":"7e548feeabd180f1e5af390365ce88fe70fe619e","modified":1604088340248},{"_id":"source/images/dqn1.png","hash":"500531da445ad95337e7d60b0834ce15ed235b41","modified":1604088340248},{"_id":"source/images/dqn2.png","hash":"07d2f6e25d0328c8986d173efab87bc59e63f944","modified":1604088340249},{"_id":"source/images/dqn3.png","hash":"664f10963a00e2d66fe7dde7e78848d26dbf78cd","modified":1604088340249},{"_id":"source/images/dqn4.png","hash":"e564dd5d0e8258145f93d65eb1a600a9d5489826","modified":1604088340249},{"_id":"source/images/expensebot_icon.png","hash":"479cb80aa0620db49e0a353ac7f550f7cc9f205d","modified":1604088340249},{"_id":"source/images/gh_ka_world.png","hash":"a0255eeaed9a38c0e836366cf95a3cee3f0f78ab","modified":1604088340250},{"_id":"source/images/gh_location_langs.png","hash":"4009dd4fdd7ef840a05e72015bd27e33304e9f70","modified":1604088340251},{"_id":"source/images/gh_locations.png","hash":"facdac2dd039f0a0ed316bcd565c7fa00b8671b6","modified":1604088340251},{"_id":"source/images/gh_popular_lang.png","hash":"b01bc1aeb03d98939236a1a248e0a49ecbeeeb8b","modified":1604088340251},{"_id":"source/images/graphql_cover.png","hash":"c8fe5419bd25058d2895012590a3acd8f776cfd6","modified":1604088340251},{"_id":"source/images/halite_langs.png","hash":"68561eee10ca68ffafaadcbe07056dc039d3be23","modified":1604088340254},{"_id":"source/images/k9_logo.png","hash":"f5466432594bb1f5ddd85ab68a2afd8dbb485897","modified":1604088340243},{"_id":"source/images/mborg_logo.jpg","hash":"d076e242d80a7ceaa59953f886470ba09ef44ebf","modified":1604088340239},{"_id":"source/images/middleman.png","hash":"f09ddfd0c8c0973d16567a1122981e6b14db615b","modified":1604088340255},{"_id":"source/images/middleman2.png","hash":"456e72dd3824a95589cc264627b81596eb589f96","modified":1604088340255},{"_id":"source/images/mqtt_bench_2.png","hash":"6c06e0cf8b8fa8f1c8f515109f1f2421125797fc","modified":1604088340255},{"_id":"source/images/nextcloud_migration.png","hash":"5b409ba4fe94b8d44fd82ac35a55454edd7878c0","modified":1604088340239},{"_id":"source/images/nloss_2.png","hash":"7e69de590e0d696ce0f1f536761f334c8e23fb7f","modified":1604088340256},{"_id":"source/images/nloss_3.png","hash":"59451eafeba92955cf576b495cf4c2f0e91b7d26","modified":1604088340256},{"_id":"source/images/okc_logo.png","hash":"b302af9ab45815922da6f091ff69bd8b0c0ccfd4","modified":1604088340243},{"_id":"source/images/push_screenshot1.png","hash":"91d5fa3e4c5a3a3d0f369a8d90055d029e8ef28e","modified":1604088340256},{"_id":"source/images/push_screenshot2.png","hash":"144ca3caed3e5b9285f7f8edeba41917266ed529","modified":1604088340256},{"_id":"source/images/qn1.png","hash":"17a5644a157e5ea4bc14e59803e864e2fc16cc14","modified":1604088340257},{"_id":"source/images/qn2.png","hash":"54be91ee889ba922dfc8c45c2a74c443dd78d71f","modified":1604088340257},{"_id":"source/images/qn3.png","hash":"a739db3163e768538b63016ccef5aa2b350e7379","modified":1604088340257},{"_id":"source/images/qn4.png","hash":"78c55641b41fc2adb3741d296bc3dad6ecea52b3","modified":1604088340258},{"_id":"source/images/qn_feature.png","hash":"cfb596148c2709567f2ab2677a8a057f27a34be8","modified":1604088340258},{"_id":"source/images/qn_icon.png","hash":"ed89d828e187737ab741237cb46fa15420601199","modified":1604088340258},{"_id":"source/images/quadkeys.jpg","hash":"8fa393c45198a6135475ac8f6079eaf3d7bc89e2","modified":1604088340259},{"_id":"source/images/raid01.png","hash":"15da9f10e5a4d3f452287daf4420da8582d3ad31","modified":1604088340259},{"_id":"source/images/raid10.png","hash":"ae8cb17a2ae2eba7d238a8d56136f60823172242","modified":1604088340259},{"_id":"source/images/statista.png","hash":"4385242cac93067b8ca8dd02c01a1778bf1c6102","modified":1604088340260},{"_id":"source/images/svhn_cropping.png","hash":"018a3065fce6d4e2ff3ff5af3e057d5026e14aee","modified":1604088340261},{"_id":"source/images/svhn_labels.png","hash":"0607b91a6fe26c164c2b179c99111b7a6bf7c57c","modified":1604088340264},{"_id":"source/images/svhn_steps.png","hash":"5c98dfb047be7e5ee72f5eca4163709af2c5491e","modified":1604088340265},{"_id":"source/images/talkycars3.png","hash":"f15645d9cc2fe5add254ea60a5719cb65cfcfd71","modified":1604088340266},{"_id":"source/images/talkycars6.png","hash":"4319a647f51126c23572c02ff2a5c222f7d5c729","modified":1604088340270},{"_id":"source/images/talkycars7.png","hash":"51e8d0404af9962e4fadc366db7b5faf77d82354","modified":1604088340270},{"_id":"source/images/talkycars8.png","hash":"6a1a3f1c0f4a6ad144bc7a291f2cd9a17db58d70","modified":1604088340271},{"_id":"source/images/tb_logo.png","hash":"0e3df9518c2bd3d41b4ed053df7acade6428c166","modified":1604088340236},{"_id":"source/images/trivia.jpg","hash":"8e7715fa940d79f43fd1e2a9ff8c2149cf0b11a8","modified":1604088340274},{"_id":"source/images/unhosted.jpg","hash":"6e0bde1ad7bfb3ba8d9b634f6518299e2d9d1f1b","modified":1604088340274},{"_id":"source/images/webdev_techstack.png","hash":"e3f4b8cdac4233ebcb5d7e20ee42703c9ffad6ba","modified":1604088340275},{"_id":"source/images/webdevlist.jpg","hash":"cfa9e038e301cdcd577c7833fc8b220f6c0d6a98","modified":1604088340276},{"_id":"source/images/webserver_performance.png","hash":"737f74276f3832a342464110566fdab49593de94","modified":1604088340276},{"_id":"source/favicon.ico","hash":"20ed69c9944365966aa23c9c8c7f9c4a4211dfde","modified":1604088340232},{"_id":"source/images/dns1.png","hash":"433d04ba49041ffb476a9078ee718b406c9da509","modified":1604088340246},{"_id":"source/images/do.png","hash":"dd43ce0ffde6885ad251e6e4edff64b36ddecd02","modified":1604088340248},{"_id":"source/images/gh_eer.png","hash":"b665077e6cc582c5f80dc60050de443c5162fef9","modified":1604088340250},{"_id":"source/images/graphql_github.png","hash":"09adc322fa961c5987b96f9490b0f828e9fe8499","modified":1604088340252},{"_id":"source/images/talkycars5.png","hash":"90a03e1a3662025d340800e5e768fb994bc180e4","modified":1604088340270},{"_id":"source/images/tello1.jpg","hash":"de1755a8066e6d7708664d42c4d0cc272dca7b7a","modified":1604088340272},{"_id":"source/images/thesis_mockup.png","hash":"a5ec4a8949edd6a168c866c675ab6c4e2bc00d33","modified":1604088340273},{"_id":"source/images/thesis_stack.png","hash":"5a410fcb932c552dbc0096f59fda594be334e9cd","modified":1604088340274},{"_id":"source/images/webdev_techstack_large.png","hash":"76169ecb9523c245c6588156fcff24c851b5c43b","modified":1604088340275},{"_id":"source/images/webservers.png","hash":"c1550f9bcb7350c9ebef5090484960877d8dda55","modified":1604088340276},{"_id":"source/images/academic_faces2.png","hash":"2b720cc693e62ea98de454ce2ed44e63cece1848","modified":1604088340241},{"_id":"source/images/graphql_screenshots1.png","hash":"5cd775bd60413f33042928effb6a8d456a513be0","modified":1604088340253},{"_id":"source/images/lock.jpg","hash":"ab512ec0b728dd40e796425b410189b1e732bee9","modified":1604088340244},{"_id":"source/images/scorecard.jpg","hash":"e4b56c4f825d42890e6c735c17d6a29a80a7f075","modified":1604088340260},{"_id":"source/images/svhn_cropped_images.png","hash":"c952784933038f0569e0b3dba353433481afc00d","modified":1604088340261},{"_id":"source/images/talkycars2.png","hash":"e606e884e5042002f2fd7416fb29f86627c640df","modified":1604088340265},{"_id":"source/images/tello2.png","hash":"74c9b351d3cb971d31c5a91a9f6633b3c2a0312a","modified":1604088340273},{"_id":"source/images/dns2.png","hash":"f6d051761163a98aaf7be38b302eff9a1ab84493","modified":1604088340247},{"_id":"source/images/talkycars4.png","hash":"2f17e26d3fafcfa9d9469b1984199ac0678bcdbc","modified":1604088340267},{"_id":"source/images/academic_faces1.png","hash":"3f28ba07d6a0ed06102da625d36d3c7adf8968ea","modified":1604088340240},{"_id":"source/images/halite_game.png","hash":"d67ea5a35af5f4cf91808c78eb156e259fbf50bf","modified":1604088340254},{"_id":"source/images/talkycars9.png","hash":"cfa583e39080d27fefa7d3430b695dca1eb2a097","modified":1604088340272},{"_id":"source/images/graphql_screencast1.gif","hash":"63805d525e34d18f85a528eaa44374a0d529b583","modified":1604088340238},{"_id":"themes/cactus-dark/LICENSE","hash":"4d5f5f360a18c68f0fd1897bdb1eb1210c2893e3","modified":1604088340310},{"_id":"themes/cactus-dark/README.md","hash":"f38b2f4771eeccc0ae0959ac3e3c485a9d159d4a","modified":1604088340311},{"_id":"themes/cactus-dark/_config.yml","hash":"832e859f70db2af75ae4d1f9b3c1f8a4fe9cc260","modified":1613721707880},{"_id":"themes/cactus-dark/layout/archive.ejs","hash":"ab9798bf534485a4fed4d3089011421858afdd26","modified":1604088340303},{"_id":"themes/cactus-dark/layout/index.ejs","hash":"2954d576adf4bfd9b08163cdc30f37bde9fee2a3","modified":1604088340303},{"_id":"themes/cactus-dark/layout/layout.ejs","hash":"8484532ad7c4da22f46fc1394bb2fd9ded34be1f","modified":1604088340304},{"_id":"themes/cactus-dark/layout/page.ejs","hash":"f889c261f5a1f4db7f80c1f9263f1d13e3d8bd1a","modified":1609778689874},{"_id":"themes/cactus-dark/layout/post.ejs","hash":"262eb3a8604659ba73dd0b9ddb55d1a64656eaf1","modified":1609780477468},{"_id":"themes/cactus-dark/scripts/meta.js","hash":"fa6055a39851c9953d033e70c1614547b94dce60","modified":1604088340311},{"_id":"themes/cactus-dark/scripts/thumbnail.js","hash":"df8829fd8c3119650037eba5ec11bdce06acff9d","modified":1604088340311},{"_id":"themes/cactus-dark/layout/_partial/comments.ejs","hash":"853a4500da515ef3facc51a055886eaf8efd080d","modified":1604088340305},{"_id":"themes/cactus-dark/layout/_partial/footer.ejs","hash":"7f6b3f126a58e6734b658ab57bc6b41822bc9342","modified":1604088340306},{"_id":"themes/cactus-dark/layout/_partial/head.ejs","hash":"7f4c7a4efe22f0735d7bfdd2045283bde447b6b7","modified":1604088340307},{"_id":"themes/cactus-dark/layout/_partial/header.ejs","hash":"889fe54bbfd1fb3357e8c0614d57a437a72f782a","modified":1604088340307},{"_id":"themes/cactus-dark/layout/_partial/pagination.ejs","hash":"ca660c59aec56daa4a7b41715b97434d4a24c37e","modified":1604088340307},{"_id":"themes/cactus-dark/layout/_partial/scripts.ejs","hash":"ffdf85e347233b6dc3b12296cd3d25cd1d0bd8e6","modified":1604088340310},{"_id":"themes/cactus-dark/layout/_partial/styles.ejs","hash":"e62b799d8ac369d1f1b36bd2649ecc34aec3384c","modified":1604088340310},{"_id":"themes/cactus-dark/source/css/_extend.styl","hash":"faca25132d55e8989d1c1d638e55d1e97de3c561","modified":1604088340314},{"_id":"themes/cactus-dark/source/css/_mixins.styl","hash":"c921ceb620deedddd38c9cec28190995e8764bab","modified":1604088340323},{"_id":"themes/cactus-dark/source/css/_util.styl","hash":"219187b864e44380c0fd91870ae7fa9d3e3321cd","modified":1604088340325},{"_id":"themes/cactus-dark/source/css/_variables.styl","hash":"80345f77f0e601669047cbb3c44491720c3b5c13","modified":1604088340326},{"_id":"themes/cactus-dark/source/css/style.styl","hash":"9a989e414ab2fa12f39791f2ea07c22aec00c670","modified":1604088340313},{"_id":"themes/cactus-dark/source/images/favicon-192x192.png","hash":"07059d8fe92f685176d96045edb70c01deb8c4d0","modified":1604088340326},{"_id":"themes/cactus-dark/source/images/favicon.ico","hash":"164bc240105d72d826efc048442d85dcf90d2cce","modified":1604088340327},{"_id":"themes/cactus-dark/source/js/main.js","hash":"2703a7cb4fc7056d13215b9fde675da426b9cdc4","modified":1604088340327},{"_id":"themes/cactus-dark/layout/_partial/post/actions_desktop.ejs","hash":"2319dea76f205c27dd59c994921f66350df8027a","modified":1604088340308},{"_id":"themes/cactus-dark/layout/_partial/post/actions_mobile.ejs","hash":"e7638a83e5aaa4bf5b24440ca76fec8eb563bed7","modified":1604088340308},{"_id":"themes/cactus-dark/layout/_partial/post/date.ejs","hash":"0fcb431aff779812395031a5d88a6a6f4df1a920","modified":1609778857511},{"_id":"themes/cactus-dark/layout/_partial/post/gallery.ejs","hash":"3b3d068def14a500b7ac65270eab913d6d67d670","modified":1609780529575},{"_id":"themes/cactus-dark/layout/_partial/post/share.ejs","hash":"25a3406f97e976ec13239f0d3f32f9e512511f50","modified":1604088340309},{"_id":"themes/cactus-dark/layout/_partial/post/tag.ejs","hash":"bfab03ef986d35ccad583f2d2b575db4a8d2789e","modified":1604088340309},{"_id":"themes/cactus-dark/layout/_partial/post/title.ejs","hash":"27e4b0e93fc276571aaf61f71cc742beec6879a4","modified":1609779861590},{"_id":"themes/cactus-dark/source/css/_highlight/agate.styl","hash":"601eb70448a16b918df132f6fc41e891ae053653","modified":1604088340316},{"_id":"themes/cactus-dark/source/css/_highlight/androidstudio.styl","hash":"65d09f1b0e81c6a182f549fd3de51e59823c97ae","modified":1604088340316},{"_id":"themes/cactus-dark/source/css/_highlight/arta.styl","hash":"1a5accc115f41d1b669ed708ac6a29abac876599","modified":1604088340317},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-cave-dark.styl","hash":"bc647b2c1d971d7cc947aa1ed66e9fd115261921","modified":1604088340317},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-dune-dark.styl","hash":"df50a85a4b14c7ca6e825d665594b91229d0e460","modified":1604088340317},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-estuary-dark.styl","hash":"d84382bc8298f96730757391d3e761b7e640f406","modified":1604088340317},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-forest-dark.styl","hash":"57c154c6045a038dc7df0a25927853e10bf48c4a","modified":1604088340317},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-heath-dark.styl","hash":"b0cf13b2233e7bc38342032d2d7296591a4c2bcf","modified":1604088340318},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-lakeside-dark.styl","hash":"bb0a8c4ad0dd8e3e7de7122ddf268fc42aa94acb","modified":1604088340318},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-plateau-dark.styl","hash":"09c64f1a7052aec9070c36c0431df25216afaea1","modified":1604088340318},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-savanna-dark.styl","hash":"a16c919a1ccf2f845488078fb341381bec46b1f3","modified":1604088340318},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-seaside-dark.styl","hash":"ce233a101daea7124cbfcd34add43ccfe2e1e1c7","modified":1604088340318},{"_id":"themes/cactus-dark/source/css/_highlight/atelier-sulphurpool-dark.styl","hash":"414b0cfc142f70afe359c16450b651e28bf7325a","modified":1604088340318},{"_id":"themes/cactus-dark/source/css/_highlight/codepen-embed.styl","hash":"f4dcc84d8e39f9831a5efe80e51923fc3054feb0","modified":1604088340319},{"_id":"themes/cactus-dark/source/css/_highlight/dark.styl","hash":"71ce56d311cc2f3a605f6e2c495ccd7236878404","modified":1604088340319},{"_id":"themes/cactus-dark/source/css/_highlight/darkula.styl","hash":"ad0d5728d21645039c9f199e7a56814170ed3bab","modified":1604088340316},{"_id":"themes/cactus-dark/source/css/_highlight/far.styl","hash":"d9928010ffe71e80b97a5afcba1a4975efdd7372","modified":1604088340319},{"_id":"themes/cactus-dark/source/css/_highlight/hopscotch.styl","hash":"b374c6550b89b4751aedc8fbc3cf98d95bd70ead","modified":1604088340319},{"_id":"themes/cactus-dark/source/css/_highlight/hybrid.styl","hash":"ea8d7ddc258b073308746385f5cb85aabb8bfb83","modified":1604088340320},{"_id":"themes/cactus-dark/source/css/_highlight/ir-black.styl","hash":"693078bbd72a2091ed30f506cc55949600b717af","modified":1604088340320},{"_id":"themes/cactus-dark/source/css/_highlight/kimbie.styl","hash":"45dbb168f22d739d0109745d2decd66b5f94e786","modified":1604088340320},{"_id":"themes/cactus-dark/source/css/_highlight/monokai-sublime.styl","hash":"25aa2fc1dbe38593e7c7ebe525438a39574d9935","modified":1604088340320},{"_id":"themes/cactus-dark/source/css/_highlight/monokai.styl","hash":"5a4fe9f957fd7a368c21b62a818403db4270452f","modified":1604088340320},{"_id":"themes/cactus-dark/source/css/_highlight/obsidian.styl","hash":"55572bbcfee1de6c31ac54681bb00336f5ae826d","modified":1604088340320},{"_id":"themes/cactus-dark/source/css/_highlight/paraiso.styl","hash":"f1537bd868579fa018ecdbfd2eb922dcf3ba2cac","modified":1604088340321},{"_id":"themes/cactus-dark/source/css/_highlight/pojoaque.styl","hash":"77dae9dc41945359d17fe84dbd317f1b40b2ee33","modified":1604088340321},{"_id":"themes/cactus-dark/source/css/_highlight/railscasts.styl","hash":"acd620f8bb7ff0e3fe5f9a22b4433ceef93a05e6","modified":1604088340321},{"_id":"themes/cactus-dark/source/css/_highlight/rainbow.styl","hash":"ce73b858fc0aba0e57ef9fb136c083082746bc1d","modified":1604088340321},{"_id":"themes/cactus-dark/source/css/_highlight/solarized-dark.styl","hash":"702b9299a48c90124e3ac1d45f1591042f2beccc","modified":1604088340321},{"_id":"themes/cactus-dark/source/css/_highlight/sunburst.styl","hash":"a0b5b5129547a23865d400cfa562ea0ac1ee3958","modified":1604088340322},{"_id":"themes/cactus-dark/source/css/_highlight/tomorrow-night-blue.styl","hash":"8b3087d4422be6eb800935a22eb11e035341c4ba","modified":1604088340322},{"_id":"themes/cactus-dark/source/css/_highlight/tomorrow-night-bright.styl","hash":"0ac6af6ecb446b5b60d6226748e4a6532db34f57","modified":1604088340322},{"_id":"themes/cactus-dark/source/css/_highlight/tomorrow-night-eighties.styl","hash":"fa57b3bb7857a160fc856dbe319b31e30cc5d771","modified":1604088340322},{"_id":"themes/cactus-dark/source/css/_highlight/tomorrow-night.styl","hash":"19b3080d4b066b40d50d7e7f297472482b5801fd","modified":1604088340322},{"_id":"themes/cactus-dark/source/css/_highlight/zenburn.styl","hash":"fc5ec840435dad80964d04519d3f882ddc03746a","modified":1604088340323},{"_id":"themes/cactus-dark/source/css/_partial/archive.styl","hash":"18fa7f84a9783c5fb56c9f450ea93bd88408e682","modified":1604088340323},{"_id":"themes/cactus-dark/source/css/_partial/article.styl","hash":"3dbf627d9f27ebf0b10cdc4d28341e35786b3cf5","modified":1604088340324},{"_id":"themes/cactus-dark/source/css/_partial/comments.styl","hash":"11fb41241a13971d23fc3f7e6d60315c7f248396","modified":1604088340324},{"_id":"themes/cactus-dark/source/css/_partial/footer.styl","hash":"344f6877733a488f7f07f87fbaa518295948766f","modified":1604088340324},{"_id":"themes/cactus-dark/source/css/_partial/header.styl","hash":"86676f767cfacd9203477de5ed1545bd51b0169c","modified":1604088340324},{"_id":"themes/cactus-dark/source/css/_partial/index.styl","hash":"4734fb46f11a0b5425f17c9db4441bb9711d8b9e","modified":1604088340324},{"_id":"themes/cactus-dark/source/css/_partial/pagination.styl","hash":"03a1b81d60dae3dd55963b7e74a6fee83470e6bb","modified":1604088340325},{"_id":"themes/cactus-dark/source/lib/justified-gallery/jquery.justifiedGallery.min.js","hash":"b2683e7a872bc109b1756a65188a37cef7d0bd5c","modified":1604088340336},{"_id":"themes/cactus-dark/source/lib/justified-gallery/justifiedGallery.min.css","hash":"13fbcba5e97aa88b748d94d3efc4718475279907","modified":1604088340336},{"_id":"themes/cactus-dark/source/lib/meslo-LG/styles.css","hash":"eb88d0b9f1bbef99070e9627e2c96d892036bf7e","modified":1604088340358},{"_id":"themes/cactus-dark/source/css/_partial/post/actions_desktop.styl","hash":"a9f9b6382d313f9ef9ff9f53bd0db11e5b36edf4","modified":1604088340325},{"_id":"themes/cactus-dark/source/css/_partial/post/actions_mobile.styl","hash":"e6a802d7ee1023c5fc5fac18bb0ba3dc03ef2ac8","modified":1604088340325},{"_id":"themes/cactus-dark/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1604088340329},{"_id":"themes/cactus-dark/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1604088340330},{"_id":"themes/cactus-dark/source/lib/jquery/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1604088340335},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1604088340334},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1604088340334},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1604088340335},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1604088340331},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1604088340333},{"_id":"source/images/graphql_screencast2.gif","hash":"a840bbef10d6871ad47b113d49550c3308aa5f4e","modified":1604088340269},{"_id":"source/images/svhn_labelimg.png","hash":"ac92c45effcfc6fb5b8671165b6365ae4ce3126e","modified":1604088340264},{"_id":"themes/cactus-dark/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1604088340333},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-Italic.ttf","hash":"96c97a0a098ca40802f948ae56fa37aa6683d034","modified":1604088340342},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-Italic.ttf","hash":"68700db02debd4b922304134da83b829cbfddfc9","modified":1604088340349},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-Italic.ttf","hash":"7f7cdbdcc26279c04046632e22d872f111bc9399","modified":1604088340356},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-Bold.ttf","hash":"bfa1ed9a263ed78462f06d322de13bd5bd0906b2","modified":1604088340339},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-BoldItalic.ttf","hash":"a9a431fc7a6c3a67c98021d4035c12a07a4f1070","modified":1604088340341},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGL-Regular.ttf","hash":"2b912dd13f052f645ee19951604610bb350d50af","modified":1604088340344},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-Bold.ttf","hash":"a8a8df3393bccc365335fc5eb0a62a6b7ccd32b9","modified":1604088340346},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-BoldItalic.ttf","hash":"65ddb11e75ee93909e845ab912a36717c48f1c94","modified":1604088340348},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGM-Regular.ttf","hash":"5e220152adefe905b2197f873d7cee99eca50e91","modified":1604088340351},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-Bold.ttf","hash":"df202ce09cbdc70bc16b81983a13ef0f94e46f10","modified":1604088340353},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-BoldItalic.ttf","hash":"d895a1bd25e36c58b7f463ebe14de09f186d5ab4","modified":1604088340354},{"_id":"themes/cactus-dark/source/lib/meslo-LG/fonts/MesloLGS-Regular.ttf","hash":"56fa0e33a390b704afc56af93a31576ccdbbdd9e","modified":1604088340358},{"_id":"public/articles/index.html","hash":"d7576dea5be4e59c7b40dbedaf729f98f3b41488","modified":1609780595646},{"_id":"public/about/index.html","hash":"87a162c0b70031306cdb8988f2037378c50c5594","modified":1613721723106},{"_id":"public/imprint/index.html","hash":"bb10b4cfd4ecda5269aec32fd45bfb5c19577121","modified":1613721723106},{"_id":"public/migrating-nextcloud-from-sqlite-to-mysql-with-docker.html","hash":"f47ffdd90d636e08bdbbcc9a5b046fdb10cead94","modified":1613721723106},{"_id":"public/pgp-encrypted-personal-e-mail-inbox.html","hash":"fd86314900edd555821fa4bd6041746a9c6c72fc","modified":1613721723106},{"_id":"public/talkycars-a-distributed-software-platform-for-cooperative-perception.html","hash":"5a7b90fd124017047105b4f48ec2302468ef092f","modified":1613721723106},{"_id":"public/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.html","hash":"be39f2b9f317a6a262328f3951406216c41909b8","modified":1613721723106},{"_id":"public/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.html","hash":"338e2e0e0b3a43ab5d9c8c8b2b2582b1af757080","modified":1613721723106},{"_id":"public/how-to-enable-dns-over-tls-on-ubuntu-using-coredns.html","hash":"2fe9acdc4eaf8a041513d46b48c812ec18e31420","modified":1613721723106},{"_id":"public/flying-a-dji-tello-drone-with-go.html","hash":"33fafc3acda1f53a88ae156f7ae42e8c7297808d","modified":1613721723106},{"_id":"public/transfer-learning-for-multi-digit-recognition-using-tensorflow-object-detection-and-svhn-classifier.html","hash":"969ff366a75930e9925e5e5928a126f3001ae4f3","modified":1613721723106},{"_id":"public/basic-benchmarks-of-5-different-mqtt-brokers.html","hash":"cc5eecadc97eb1f0abda3cd9077b15a8aef8bd32","modified":1613721723106},{"_id":"public/exploratory-analysis-on-github-data.html","hash":"4080f5426c94351ccf03e13125ca5c74d9f37b47","modified":1613721723106},{"_id":"public/what-i-like-about-developing-apps-with-flutter.html","hash":"380510417d184505e9cdcbcfcf0685043d904faa","modified":1613721723106},{"_id":"public/how-to-receive-sharing-intents-in-flutter.html","hash":"480f2155c50a8dfc3498beeb39bc32d905506b4d","modified":1613721723106},{"_id":"public/quiznerd-my-experiences-with-the-android-developer-nanodegree.html","hash":"39eca017dc319557cd0028f4fcca827837308483","modified":1613721723106},{"_id":"public/detecting-academics-major-from-facial-images.html","hash":"a401e9079589d55b9a655cc536e57a99de0f6f2d","modified":1613721723106},{"_id":"public/building-a-cloud-native-web-scraper-using-8-different-aws-services.html","hash":"49c7856dcda88442ebe2464d7b4a92cb86d3c82b","modified":1613721723106},{"_id":"public/how-to-load-svg-into-imageview-by-url-in-android.html","hash":"fa7bcac09f20d95ecb781d0810c4beafb63152ae","modified":1613721723106},{"_id":"public/overgrive-not-starting-on-ubuntu-18-04.html","hash":"cb5c2a787ccc83ca89cd35367e93987363575d9b","modified":1613721723106},{"_id":"public/halite-a-rule-based-ai-bot.html","hash":"9629a6439960603eaae6dee67a0f64fa8c935146","modified":1613721723106},{"_id":"public/cartpole-with-a-deep-q-network.html","hash":"5f3c070ac3156f0104f3736cb6dfe5feb31bf493","modified":1613721723106},{"_id":"public/cartpole-with-qlearning-first-experiences-with-openai-gym.html","hash":"bbe9e6563c76f688b0e84d0ee1afb4961907f78c","modified":1613721723106},{"_id":"public/telegram-middleman-bot-push-notifications-as-easy-as-post.html","hash":"b644647eb33abb2a873e49edcd86ea75264f65df","modified":1613721723106},{"_id":"public/ml-telegram-chat-message-classification.html","hash":"ef83090e86899184c6d96e3f6af4c9244cf1ea2a","modified":1613721723106},{"_id":"public/linkeddata-trivia-game.html","hash":"ac9046345c3d215575492232cb398caf932e0767","modified":1613721723106},{"_id":"public/caddy-a-modern-web-server-vs-nginx.html","hash":"c09f6b42adeab166313516e79e0fe29ff656a49b","modified":1613721723106},{"_id":"public/http-performance-java-jersey-vs-go-vs-nodejs.html","hash":"7a1cd2c735f977280f7c099eb8c8fada610192cd","modified":1613721723106},{"_id":"public/http20-server-push-proxy.html","hash":"37abdc530aa4d1b278a104542b67be006e13d287","modified":1613721723106},{"_id":"public/my-teck-stack-if-i-had-to-build-an-app-today.html","hash":"eff4b0e3378a17fbb93c9a5ecdac7099f8ea652b","modified":1613721723106},{"_id":"public/how-to-load-yago-into-apache-jena-fuseki.html","hash":"77867ca2d19291d7be9a953556b3b5575b7fe12f","modified":1613721723106},{"_id":"public/design-of-a-linked-dataenabled-microservice-platform-for-the-industrial-internet-of-things.html","hash":"700775b13ff11d5c354201e0488bdb4aa4499433","modified":1613721723106},{"_id":"public/webdevlistnet-the-developers-resource-collection.html","hash":"b81c1cdec26b002e32656cafdd2173feec48f271","modified":1613721723106},{"_id":"public/migrate-maildir-to-new-server-using-imapsync.html","hash":"40211feef0e942fcfc865190dce95ce6ae841368","modified":1613721723106},{"_id":"public/innovation-in-germany-not.html","hash":"43e001ebf9c8a79b6ebe04e57e73ce6d6f64e899","modified":1613721723106},{"_id":"public/telegram-expensebot-doodlerbot.html","hash":"582265372235f1a63c8f4867446bc192ebeb8d1c","modified":1613721723106},{"_id":"public/unhostedorg-applications-with-remotestorageio-and-webfingernet.html","hash":"4d6786057c9aaa9d1f84118adb29c83a4d68c8c3","modified":1613721723106},{"_id":"public/digitalocean-my-preferred-cloud-hosting-provider.html","hash":"7a706ad8dc3a1486735d73bad8554e3a6a666490","modified":1613721723106},{"_id":"public/web-development-technology-stack.html","hash":"1d35f1962dedac86d403d1235af6beed509ee76c","modified":1613721723106},{"_id":"public/learning-angular2-what-is-new.html","hash":"cc11775150eecd9df820d05cb0b64046add1b521","modified":1613721723106},{"_id":"public/instant-messenger-security-encryption-overview.html","hash":"d8741a36ee21324ae6a900555057c2b82a03b63f","modified":1613721723106},{"_id":"public/anchr-io-image-uploads-bookmarks-and-shortlink-service.html","hash":"a6abbc06d1af5f6cecc1d4a485db0c5ecfde8739","modified":1613721723106},{"_id":"public/telegram-bot-example-code-in-nodejs.html","hash":"86388a78f922421de839679d0f2509ff62d33710","modified":1613721723106},{"_id":"public/why-raid-10-is-better-than-raid-01.html","hash":"f42791b458a54b0e9a122cfb9df2a4b60de4a66f","modified":1613721723106},{"_id":"public/how-to-make-telegram-bots.html","hash":"080c06b23b25d90d394f8a8afaaec62120523bda","modified":1613721723106},{"_id":"public/linux-cache-information-bash-script.html","hash":"ab21dd7d43aa7307f062e22cea0043cde8efbf2e","modified":1613721723106},{"_id":"public/archives/index.html","hash":"94f8da7e7738ab5f268bfef5adb7f2aaf01da48b","modified":1616165198310},{"_id":"public/archives/page/2/index.html","hash":"59e7ca327f7e585fea450e3a5d501583e48df7ee","modified":1616165198310},{"_id":"public/archives/page/3/index.html","hash":"456ba99413eb974e0ecffd26abf2e72a8b09b35b","modified":1616165198310},{"_id":"public/archives/2015/index.html","hash":"9f6500d4bac0262f71eb6bffe55fc10143707273","modified":1613721723106},{"_id":"public/archives/2015/02/index.html","hash":"5781e0e457ac75f0ca6bec9de209ad89752f63bd","modified":1613721723106},{"_id":"public/archives/2015/06/index.html","hash":"e41e6796c7352cfb4a2541a19eeb686d28f8aed8","modified":1613721723106},{"_id":"public/archives/2015/11/index.html","hash":"cf75b61aeecc4b33ae0063c436e9b914e70f0aac","modified":1613721723106},{"_id":"public/archives/2015/12/index.html","hash":"a3d49fca5bf95fd4678be36854620494f35241a8","modified":1613721723106},{"_id":"public/archives/2016/index.html","hash":"f68f2c76edc35d4c9b3af2a4fcead27758e82d65","modified":1613721723106},{"_id":"public/archives/2016/02/index.html","hash":"f2145a7164fd02a88b0e65c553bb151d9cbed214","modified":1613721723106},{"_id":"public/archives/2016/03/index.html","hash":"3bd1a4f31ea511ec5c31bf53b539fd39e2d14e5b","modified":1613721723106},{"_id":"public/archives/2016/04/index.html","hash":"43984867f361875ae0c668824ee1da8a088088bf","modified":1613721723106},{"_id":"public/archives/2016/05/index.html","hash":"f910b9a618c382b22ec6bdb1ef8fe20d9a640ae4","modified":1613721723106},{"_id":"public/archives/2016/07/index.html","hash":"39b448db4de037397d5e17c0fb250f6b161fd2a3","modified":1613721723106},{"_id":"public/archives/2016/09/index.html","hash":"54e480f251dc375575f4ed725fc40f15ea3ebda5","modified":1613721723106},{"_id":"public/archives/2016/10/index.html","hash":"c7a9bcabb663bf4ae525adacc03803fe1cc2e836","modified":1613721723106},{"_id":"public/archives/2016/11/index.html","hash":"686040f3c544c553d93f682c58d0c26c26320ba7","modified":1613721723106},{"_id":"public/archives/2017/index.html","hash":"899b2818c6f1857cf7911c689366350b949cc886","modified":1613721723106},{"_id":"public/archives/2017/01/index.html","hash":"7e6ad830bb2a087c4c69a272c471d02b15bf7bad","modified":1613721723106},{"_id":"public/archives/2017/02/index.html","hash":"ab1c4feb7dca51e682d7a4d35474bdf76697e7ab","modified":1613721723106},{"_id":"public/archives/2017/07/index.html","hash":"3b16f98a208c5c5613be27e5cf9f20d632e912eb","modified":1613721723106},{"_id":"public/archives/2017/08/index.html","hash":"e6093e9b5647e56b3fe8674be12c14fba308f8f9","modified":1613721723106},{"_id":"public/archives/2017/09/index.html","hash":"b89a4b4de694d91c01e9fa1d60dfa85770b79783","modified":1613721723106},{"_id":"public/archives/2018/index.html","hash":"197dbc67acf0440e8011c31813bacc3e10196d99","modified":1613721723106},{"_id":"public/archives/2018/01/index.html","hash":"cb634a674bcd8949d09726a9ec22e59a96c75eec","modified":1613721723106},{"_id":"public/archives/2018/06/index.html","hash":"6f18b46081f772cc1d149db66afd71f989dbb9ef","modified":1613721723106},{"_id":"public/archives/2018/07/index.html","hash":"e96f280c16827567c034f02ece5b3e93646c63c4","modified":1613721723106},{"_id":"public/archives/2018/12/index.html","hash":"da17b4bf807e848f48c9c98a231064a9db506c7e","modified":1613721723106},{"_id":"public/archives/2019/index.html","hash":"c4844aaadfe33fce4f46665f0a92008f00860afe","modified":1613721723106},{"_id":"public/archives/2019/01/index.html","hash":"f0111fd9cbe16953cb786d650974eedb3b7125bb","modified":1613721723106},{"_id":"public/archives/2019/03/index.html","hash":"00fa90f2caaf9779a736e45561d0d16516f715f8","modified":1613721723106},{"_id":"public/archives/2019/04/index.html","hash":"34498ff01e640dc90cd4c79af56a525fbb80eae9","modified":1613721723106},{"_id":"public/archives/2019/07/index.html","hash":"6799b6ac606d2632cb7252c619f67f83ff0021c8","modified":1613721723106},{"_id":"public/archives/2019/09/index.html","hash":"80f8877273fc26b9306ca82ed0f7341bdbc78e4f","modified":1613721723106},{"_id":"public/archives/2019/10/index.html","hash":"db3e511346f2c053e08705999889778c28ac3d13","modified":1613721723106},{"_id":"public/archives/2020/index.html","hash":"a103d64d87c54db2f9bd1505aaf997e67e54fde9","modified":1613721723106},{"_id":"public/archives/2020/04/index.html","hash":"f88ece7da9b9a0a6d133d3fe07f3911d865ca822","modified":1613721723106},{"_id":"public/archives/2020/06/index.html","hash":"f75e1075d2483b5a3462ea39e3b845e73e05e3d9","modified":1613721723106},{"_id":"public/archives/2020/09/index.html","hash":"87fceedca7fe0caafc23b1277e170cc659f9a9c3","modified":1613721723106},{"_id":"public/archives/2020/10/index.html","hash":"34e49f78e23e0d74f5723bf570154749c3c033af","modified":1613721723106},{"_id":"public/index.html","hash":"1c9c0c226b19e088f4ce6a87b4fbd1d0b62a9bed","modified":1616165198310},{"_id":"public/page/2/index.html","hash":"d343868f0d05bd8afee2be1663151d4e5f9964f8","modified":1616165198310},{"_id":"public/page/3/index.html","hash":"5cf36d4d859e8f93bd2b43ffce5b72afac13bdfd","modified":1616165198310},{"_id":"public/images/Webserver_memory_graph.jpg","hash":"aeaf52174560a2d3a7ff32fda21a26b96c907cc9","modified":1604346375854},{"_id":"public/images/angular2_logo.png","hash":"189713e0c0de88477c6726fc59b4cd1cfb16b05e","modified":1604346375854},{"_id":"public/images/anchr_2.jpg","hash":"d07dfedb2fc549983f63bf7251f3d011b96188aa","modified":1604346375854},{"_id":"public/images/anchr_1.jpg","hash":"0d50b24329bdcddea234f0c3ade3f2846daae148","modified":1604346375854},{"_id":"public/images/academic_faces3.png","hash":"34b476eb62b5b5e7f2a14e13dbd91df0336ed53c","modified":1604346375854},{"_id":"public/images/apple-touch-icon.png","hash":"07059d8fe92f685176d96045edb70c01deb8c4d0","modified":1604346375854},{"_id":"public/images/benchmarks2.svg","hash":"c4bd56f99f3a8810fd50a3caf7a99213b407a0d5","modified":1604346375854},{"_id":"public/images/benchmarks.svg","hash":"e9fbfdf69d7ac1c9890d541fb07d69f844fbb99a","modified":1604346375854},{"_id":"public/images/cartpole1.jpg","hash":"3e237ad7b3bb78ada0d507c8f644a791797b194e","modified":1604346375854},{"_id":"public/images/cartpole2.png","hash":"bb292c589bd25b8b59c75989377163232d29b1c8","modified":1604346375854},{"_id":"public/images/cartpole3.png","hash":"03883366ab1f16b607653ef239b99ede1ac56efc","modified":1604346375854},{"_id":"public/images/cartpole4.png","hash":"d92a7e37c8e88986a51e9a7e6b0124093b5a7425","modified":1604346375854},{"_id":"public/images/cert.png","hash":"e4d28e0c8650b56a92b7ae739f51a92e6e3b83f6","modified":1604346375854},{"_id":"public/images/crawlbuddy2.png","hash":"1325a3db38972d5ec796edeb3658c16110ce7703","modified":1604346375854},{"_id":"public/images/doodlerbot_icon.png","hash":"7e548feeabd180f1e5af390365ce88fe70fe619e","modified":1604346375854},{"_id":"public/images/dqn1.png","hash":"500531da445ad95337e7d60b0834ce15ed235b41","modified":1604346375854},{"_id":"public/images/dqn2.png","hash":"07d2f6e25d0328c8986d173efab87bc59e63f944","modified":1604346375854},{"_id":"public/images/dqn3.png","hash":"664f10963a00e2d66fe7dde7e78848d26dbf78cd","modified":1604346375854},{"_id":"public/images/dqn4.png","hash":"e564dd5d0e8258145f93d65eb1a600a9d5489826","modified":1604346375854},{"_id":"public/images/expensebot_icon.png","hash":"479cb80aa0620db49e0a353ac7f550f7cc9f205d","modified":1604346375854},{"_id":"public/images/gh_ka_world.png","hash":"a0255eeaed9a38c0e836366cf95a3cee3f0f78ab","modified":1604346375854},{"_id":"public/images/gh_location_langs.png","hash":"4009dd4fdd7ef840a05e72015bd27e33304e9f70","modified":1604346375854},{"_id":"public/images/gh_locations.png","hash":"facdac2dd039f0a0ed316bcd565c7fa00b8671b6","modified":1604346375854},{"_id":"public/images/gh_popular_lang.png","hash":"b01bc1aeb03d98939236a1a248e0a49ecbeeeb8b","modified":1604346375854},{"_id":"public/images/graphql_cover.png","hash":"c8fe5419bd25058d2895012590a3acd8f776cfd6","modified":1604346375854},{"_id":"public/images/halite_langs.png","hash":"68561eee10ca68ffafaadcbe07056dc039d3be23","modified":1604346375854},{"_id":"public/images/k9_logo.png","hash":"f5466432594bb1f5ddd85ab68a2afd8dbb485897","modified":1604346375854},{"_id":"public/images/mborg_logo.jpg","hash":"d076e242d80a7ceaa59953f886470ba09ef44ebf","modified":1604346375854},{"_id":"public/images/middleman.png","hash":"f09ddfd0c8c0973d16567a1122981e6b14db615b","modified":1604346375854},{"_id":"public/images/middleman2.png","hash":"456e72dd3824a95589cc264627b81596eb589f96","modified":1604346375854},{"_id":"public/images/mqtt_bench_2.png","hash":"6c06e0cf8b8fa8f1c8f515109f1f2421125797fc","modified":1604346375854},{"_id":"public/images/nextcloud_migration.png","hash":"5b409ba4fe94b8d44fd82ac35a55454edd7878c0","modified":1604346375854},{"_id":"public/images/nloss_3.png","hash":"59451eafeba92955cf576b495cf4c2f0e91b7d26","modified":1604346375854},{"_id":"public/images/nloss_2.png","hash":"7e69de590e0d696ce0f1f536761f334c8e23fb7f","modified":1604346375854},{"_id":"public/images/okc_logo.png","hash":"b302af9ab45815922da6f091ff69bd8b0c0ccfd4","modified":1604346375854},{"_id":"public/images/push_screenshot1.png","hash":"91d5fa3e4c5a3a3d0f369a8d90055d029e8ef28e","modified":1604346375854},{"_id":"public/images/push_screenshot2.png","hash":"144ca3caed3e5b9285f7f8edeba41917266ed529","modified":1604346375854},{"_id":"public/images/qn1.png","hash":"17a5644a157e5ea4bc14e59803e864e2fc16cc14","modified":1604346375854},{"_id":"public/images/qn2.png","hash":"54be91ee889ba922dfc8c45c2a74c443dd78d71f","modified":1604346375854},{"_id":"public/images/qn3.png","hash":"a739db3163e768538b63016ccef5aa2b350e7379","modified":1604346375854},{"_id":"public/images/qn4.png","hash":"78c55641b41fc2adb3741d296bc3dad6ecea52b3","modified":1604346375854},{"_id":"public/images/qn_feature.png","hash":"cfb596148c2709567f2ab2677a8a057f27a34be8","modified":1604346375854},{"_id":"public/images/qn_icon.png","hash":"ed89d828e187737ab741237cb46fa15420601199","modified":1604346375854},{"_id":"public/images/quadkeys.jpg","hash":"8fa393c45198a6135475ac8f6079eaf3d7bc89e2","modified":1604346375854},{"_id":"public/images/raid01.png","hash":"15da9f10e5a4d3f452287daf4420da8582d3ad31","modified":1604346375854},{"_id":"public/images/raid10.png","hash":"ae8cb17a2ae2eba7d238a8d56136f60823172242","modified":1604346375854},{"_id":"public/images/statista.png","hash":"4385242cac93067b8ca8dd02c01a1778bf1c6102","modified":1604346375854},{"_id":"public/images/svhn_cropping.png","hash":"018a3065fce6d4e2ff3ff5af3e057d5026e14aee","modified":1604346375854},{"_id":"public/images/svhn_steps.png","hash":"5c98dfb047be7e5ee72f5eca4163709af2c5491e","modified":1604346375854},{"_id":"public/images/svhn_labels.png","hash":"0607b91a6fe26c164c2b179c99111b7a6bf7c57c","modified":1604346375854},{"_id":"public/images/talkycars3.png","hash":"f15645d9cc2fe5add254ea60a5719cb65cfcfd71","modified":1604346375854},{"_id":"public/images/talkycars6.png","hash":"4319a647f51126c23572c02ff2a5c222f7d5c729","modified":1604346375854},{"_id":"public/images/talkycars8.png","hash":"6a1a3f1c0f4a6ad144bc7a291f2cd9a17db58d70","modified":1604346375854},{"_id":"public/images/talkycars7.png","hash":"51e8d0404af9962e4fadc366db7b5faf77d82354","modified":1604346375854},{"_id":"public/images/tb_logo.png","hash":"0e3df9518c2bd3d41b4ed053df7acade6428c166","modified":1604346375854},{"_id":"public/images/trivia.jpg","hash":"8e7715fa940d79f43fd1e2a9ff8c2149cf0b11a8","modified":1604346375854},{"_id":"public/images/unhosted.jpg","hash":"6e0bde1ad7bfb3ba8d9b634f6518299e2d9d1f1b","modified":1604346375854},{"_id":"public/images/webdev_techstack.png","hash":"e3f4b8cdac4233ebcb5d7e20ee42703c9ffad6ba","modified":1604346375854},{"_id":"public/images/webdevlist.jpg","hash":"cfa9e038e301cdcd577c7833fc8b220f6c0d6a98","modified":1604346375854},{"_id":"public/images/webserver_performance.png","hash":"737f74276f3832a342464110566fdab49593de94","modified":1604346375854},{"_id":"public/images/favicon-192x192.png","hash":"07059d8fe92f685176d96045edb70c01deb8c4d0","modified":1604346375854},{"_id":"public/images/favicon.ico","hash":"164bc240105d72d826efc048442d85dcf90d2cce","modified":1604346375854},{"_id":"public/favicon.ico","hash":"20ed69c9944365966aa23c9c8c7f9c4a4211dfde","modified":1604346375854},{"_id":"public/images/dns1.png","hash":"433d04ba49041ffb476a9078ee718b406c9da509","modified":1604346375854},{"_id":"public/images/do.png","hash":"dd43ce0ffde6885ad251e6e4edff64b36ddecd02","modified":1604346375854},{"_id":"public/images/gh_eer.png","hash":"b665077e6cc582c5f80dc60050de443c5162fef9","modified":1604346375854},{"_id":"public/images/graphql_github.png","hash":"09adc322fa961c5987b96f9490b0f828e9fe8499","modified":1604346375854},{"_id":"public/images/talkycars5.png","hash":"90a03e1a3662025d340800e5e768fb994bc180e4","modified":1604346375854},{"_id":"public/images/tello1.jpg","hash":"de1755a8066e6d7708664d42c4d0cc272dca7b7a","modified":1604346375854},{"_id":"public/images/thesis_mockup.png","hash":"a5ec4a8949edd6a168c866c675ab6c4e2bc00d33","modified":1604346375854},{"_id":"public/images/thesis_stack.png","hash":"5a410fcb932c552dbc0096f59fda594be334e9cd","modified":1604346375854},{"_id":"public/images/webdev_techstack_large.png","hash":"76169ecb9523c245c6588156fcff24c851b5c43b","modified":1604346375854},{"_id":"public/images/webservers.png","hash":"c1550f9bcb7350c9ebef5090484960877d8dda55","modified":1604346375854},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1604346375854},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1604346375854},{"_id":"public/images/academic_faces2.png","hash":"2b720cc693e62ea98de454ce2ed44e63cece1848","modified":1604346375854},{"_id":"public/images/graphql_screenshots1.png","hash":"5cd775bd60413f33042928effb6a8d456a513be0","modified":1604346375854},{"_id":"public/images/lock.jpg","hash":"ab512ec0b728dd40e796425b410189b1e732bee9","modified":1604346375854},{"_id":"public/images/scorecard.jpg","hash":"e4b56c4f825d42890e6c735c17d6a29a80a7f075","modified":1604346375854},{"_id":"public/images/svhn_cropped_images.png","hash":"c952784933038f0569e0b3dba353433481afc00d","modified":1604346375854},{"_id":"public/images/talkycars2.png","hash":"e606e884e5042002f2fd7416fb29f86627c640df","modified":1604346375854},{"_id":"public/images/tello2.png","hash":"74c9b351d3cb971d31c5a91a9f6633b3c2a0312a","modified":1604346375854},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1604346375854},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1604346375854},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1604346375854},{"_id":"public/css/style.css","hash":"221260b019ecdb03a88188b911edf13138906179","modified":1604346375854},{"_id":"public/js/main.js","hash":"2703a7cb4fc7056d13215b9fde675da426b9cdc4","modified":1604346375854},{"_id":"public/lib/justified-gallery/justifiedGallery.min.css","hash":"13fbcba5e97aa88b748d94d3efc4718475279907","modified":1604346375854},{"_id":"public/lib/meslo-LG/styles.css","hash":"eb88d0b9f1bbef99070e9627e2c96d892036bf7e","modified":1604346375854},{"_id":"public/images/dns2.png","hash":"f6d051761163a98aaf7be38b302eff9a1ab84493","modified":1604346375854},{"_id":"public/images/talkycars4.png","hash":"2f17e26d3fafcfa9d9469b1984199ac0678bcdbc","modified":1604346375854},{"_id":"public/lib/justified-gallery/jquery.justifiedGallery.min.js","hash":"b2683e7a872bc109b1756a65188a37cef7d0bd5c","modified":1604346375854},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1604346375854},{"_id":"public/images/academic_faces1.png","hash":"3f28ba07d6a0ed06102da625d36d3c7adf8968ea","modified":1604346375854},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1604346375854},{"_id":"public/images/halite_game.png","hash":"d67ea5a35af5f4cf91808c78eb156e259fbf50bf","modified":1604346375854},{"_id":"public/images/talkycars9.png","hash":"cfa583e39080d27fefa7d3430b695dca1eb2a097","modified":1604346375854},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1604346375854},{"_id":"public/lib/jquery/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1604346375854},{"_id":"public/images/graphql_screencast1.gif","hash":"63805d525e34d18f85a528eaa44374a0d529b583","modified":1604346375854},{"_id":"public/lib/meslo-LG/fonts/MesloLGL-Italic.ttf","hash":"96c97a0a098ca40802f948ae56fa37aa6683d034","modified":1604346375854},{"_id":"public/lib/meslo-LG/fonts/MesloLGM-Italic.ttf","hash":"68700db02debd4b922304134da83b829cbfddfc9","modified":1604346375854},{"_id":"public/lib/meslo-LG/fonts/MesloLGS-Italic.ttf","hash":"7f7cdbdcc26279c04046632e22d872f111bc9399","modified":1604346375854},{"_id":"public/lib/meslo-LG/fonts/MesloLGL-Bold.ttf","hash":"bfa1ed9a263ed78462f06d322de13bd5bd0906b2","modified":1604346375854},{"_id":"public/lib/meslo-LG/fonts/MesloLGL-BoldItalic.ttf","hash":"a9a431fc7a6c3a67c98021d4035c12a07a4f1070","modified":1604346375854},{"_id":"public/lib/meslo-LG/fonts/MesloLGL-Regular.ttf","hash":"2b912dd13f052f645ee19951604610bb350d50af","modified":1604346375854},{"_id":"public/lib/meslo-LG/fonts/MesloLGM-Bold.ttf","hash":"a8a8df3393bccc365335fc5eb0a62a6b7ccd32b9","modified":1604346375854},{"_id":"public/lib/meslo-LG/fonts/MesloLGM-BoldItalic.ttf","hash":"65ddb11e75ee93909e845ab912a36717c48f1c94","modified":1604346375854},{"_id":"public/lib/meslo-LG/fonts/MesloLGM-Regular.ttf","hash":"5e220152adefe905b2197f873d7cee99eca50e91","modified":1604346375854},{"_id":"public/lib/meslo-LG/fonts/MesloLGS-Bold.ttf","hash":"df202ce09cbdc70bc16b81983a13ef0f94e46f10","modified":1604346375854},{"_id":"public/lib/meslo-LG/fonts/MesloLGS-BoldItalic.ttf","hash":"d895a1bd25e36c58b7f463ebe14de09f186d5ab4","modified":1604346375854},{"_id":"public/lib/meslo-LG/fonts/MesloLGS-Regular.ttf","hash":"56fa0e33a390b704afc56af93a31576ccdbbdd9e","modified":1604346375854},{"_id":"public/images/graphql_screencast2.gif","hash":"a840bbef10d6871ad47b113d49550c3308aa5f4e","modified":1604346375854},{"_id":"public/images/svhn_labelimg.png","hash":"ac92c45effcfc6fb5b8671165b6365ae4ce3126e","modified":1604346375854},{"_id":"source/_posts/mastering-software-versioning-in-javascript-projects.md","hash":"2d294e395a0c0baf15b63d9025f75f30c0f7f4e5","modified":1604767949982},{"_id":"public/mastering-software-versioning-in-javascript-projects.html","hash":"c029bbe34dcf430e14045134c2f02f1732cd1c08","modified":1613721723106},{"_id":"public/archives/2020/11/index.html","hash":"98688f55316fcc708faa0365654e76c1657e8d0b","modified":1613721723106},{"_id":"source/_posts/watching-stock-prices-with-node-red-and-webhook2telegram.md","hash":"7de729cd3b629b2b5ffaf404bf31452d2ed121f4","modified":1606770279835},{"_id":"source/images/nodered-flow2.png","hash":"e845f1d26ec0d8690560ac412b7e13b4e0c8c10d","modified":1606768563538},{"_id":"source/images/nodered-flow.png","hash":"95ba424738f4ce197890ed4bb5b8ac4ec81b9bd8","modified":1606767871319},{"_id":"public/watching-stock-prices-with-node-red-and-webhook2telegram.html","hash":"9cf9f7e77a23e3e311e180610ed9be81dd71856d","modified":1613721723106},{"_id":"public/images/nodered-flow2.png","hash":"e845f1d26ec0d8690560ac412b7e13b4e0c8c10d","modified":1606768753239},{"_id":"public/images/nodered-flow.png","hash":"95ba424738f4ce197890ed4bb5b8ac4ec81b9bd8","modified":1606768753239},{"_id":"public/rss2.xml","hash":"560075f989a639371f132c8434ea9874cf0f05ef","modified":1616165198310},{"_id":"source/_posts/sponsoring-open-source-and-donating-for-a-good-cause.md","hash":"5385639c217b0b1b9f640233eaa9bb086498559b","modified":1607780679361},{"_id":"source/_posts/donating-for-a-good-cause.md","hash":"a4783d0b966514e2b86b37ee578971a62e022f59","modified":1607803734206},{"_id":"source/images/world_poverty.png","hash":"cba5e9c6100a34414b3a589426299e109dc9b5ee","modified":1607780843407},{"_id":"source/images/betterplace.png","hash":"5d3ff688519e30ddf7afa3325e428ec0800b4dc3","modified":1607783355791},{"_id":"public/donating-for-a-good-cause.html","hash":"4f70f62ca419fc703216e310f55e4bf685f40dee","modified":1613721723106},{"_id":"public/archives/page/4/index.html","hash":"20fb0442718fa5ff08d35a6187342b06aa92567f","modified":1616165198310},{"_id":"public/archives/2020/12/index.html","hash":"940b0cadcbd935febe2f3e90b166f7c427316180","modified":1613721723106},{"_id":"public/page/4/index.html","hash":"cb835e51613ffff52c8e88860ca47efe4aae9129","modified":1616165198310},{"_id":"public/images/world_poverty.png","hash":"cba5e9c6100a34414b3a589426299e109dc9b5ee","modified":1607797040915},{"_id":"public/images/betterplace.png","hash":"5d3ff688519e30ddf7afa3325e428ec0800b4dc3","modified":1607797040915},{"_id":"source/_posts/consider-sponsoring-open-source.md","hash":"e5c8239ce77f8ec74616086640bb6034c05a3508","modified":1609780554796},{"_id":"source/images/gh_sponsors.png","hash":"26e5275231fb92c45c99abe4122aeeaafe85bc4a","modified":1607802123321},{"_id":"source/images/foss_sponsoring.png","hash":"d6e01968bf6d568901d9a7d070860d8f4e92f55c","modified":1607804298921},{"_id":"public/consider-sponsoring-open-source.html","hash":"f732d17385d19af4c2109bb8b3333808ab68b90e","modified":1616165198310},{"_id":"public/images/gh_sponsors.png","hash":"26e5275231fb92c45c99abe4122aeeaafe85bc4a","modified":1607804050965},{"_id":"public/images/foss_sponsoring.png","hash":"d6e01968bf6d568901d9a7d070860d8f4e92f55c","modified":1607804311739},{"_id":"source/_posts/my-experiences-with-the-oracle-java-11-developer-certification.md","hash":"0560970d7ec7d639d2f0adda93e1cc295986839c","modified":1616165093203},{"_id":"source/images/java_cert4.png","hash":"915a15eecfb6f505fa3d55de650da52160051d02","modified":1616157302547},{"_id":"source/images/java_cert6.png","hash":"8f4184951598d1f37a4869180af4e86a7fd470a6","modified":1616157319093},{"_id":"source/images/java_cert7.png","hash":"84f0d4eca5a8ab58e883bb5b3e123bb6da03bd0a","modified":1616158529430},{"_id":"source/images/java_cert3.png","hash":"bdcab69f991500e31484e4b4c32988138b83f7c8","modified":1616157144464},{"_id":"source/images/java_cert1.png","hash":"d82cbdcbcadfb02711d74f3cf0a021f033145ee7","modified":1616155354315},{"_id":"source/images/java_cert5.png","hash":"1e243af1a6673564c68c18b3cbee03de2cf8bd3e","modified":1616157310944},{"_id":"source/images/java_cert2.png","hash":"f15a7bd453095b03b0461d808a62ea834e769715","modified":1616155505931},{"_id":"public/my-experiences-with-the-oracle-java-11-developer-certification.html","hash":"fdd323c4cd7914c9f1443590bec3bea7c20b738f","modified":1616165198310},{"_id":"public/archives/2021/index.html","hash":"1a5789a7b8930d667834b98d895f42d36cc46396","modified":1616165198310},{"_id":"public/archives/2021/03/index.html","hash":"909565ba6b286246cb86d75caad9a9adf5936b42","modified":1616165198310},{"_id":"public/images/java_cert4.png","hash":"915a15eecfb6f505fa3d55de650da52160051d02","modified":1616165198310},{"_id":"public/images/java_cert6.png","hash":"8f4184951598d1f37a4869180af4e86a7fd470a6","modified":1616165198310},{"_id":"public/images/java_cert7.png","hash":"84f0d4eca5a8ab58e883bb5b3e123bb6da03bd0a","modified":1616165198310},{"_id":"public/images/java_cert3.png","hash":"bdcab69f991500e31484e4b4c32988138b83f7c8","modified":1616165198310},{"_id":"public/images/java_cert1.png","hash":"d82cbdcbcadfb02711d74f3cf0a021f033145ee7","modified":1616165198310},{"_id":"public/images/java_cert5.png","hash":"1e243af1a6673564c68c18b3cbee03de2cf8bd3e","modified":1616165198310},{"_id":"public/images/java_cert2.png","hash":"f15a7bd453095b03b0461d808a62ea834e769715","modified":1616165198310}],"Category":[],"Data":[{"_id":"badges","data":["https://badges.fw-web.space/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://badges.fw-web.space/badge/IPv6-enabled-2BBC8A?style=flat-square"]},{"_id":"projects","data":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}],"Page":[{"title":"about","date":"2017-05-03T20:21:16.000Z","_content":"\n# About\n\n## Me\n\nHey, welcome and thank you for visiting my webpage.\n\nMy name is Ferdinand Mtsch, I am 26 years old, living in Karlsruhe, Germany and currently working as a software engineer at [Frachtwerk GmbH](https://frachtwerk.de).\n\nEralier this year I finished my Master's thesis at [ITIV](https://itiv.kit.edu) on the subjects of _Cooperative Perception and Cellular Vehicle-2-X Communication for Autonomous Driving_ as part of my studies on [information engineering and management](https://informationswirtschaft.org) at the [Karlsruhe Institute of Technology](https://kit.edu). Prior to this, I got my Bachelor's degree in 2016 with a final thesis about _Microservices and Semantic Web_ at [TECO](https://teco.edu).\n\nMy interests are  among others  software development, especially in a web- and mobile context, as well as data science and machine leatning. I consider myself very open-minded and progressive and I am continuously interested in learning new technology.\n\nBesides my studies I used to work as a student employee at [Inovex](https://inovex.de) in Karlsruhe and as a self-employed Freelancer developer. In addition, I did a rewarding internship at the [Volkswagen Electronic Research Lab (ERL)](https://vwiecc.com/) in Belmont, California as part of the software platforms team.\nI participated in realizing software projects for small and mid-sized businesses in the context of Industry 4.0, the Internet of Things, big-scale data analytics, web platforms and more. \n\n## My topics\nFull-Stack Web, Mobile, DevOps, Data Science, Machine Learning\n\n## My Languages\nJava (), JavaScript (), Go (), Python (), Scala (), R (), Dart ()\n\n## My Social Networks\nIf you are interested in some of my projects, take a look at the [Projects](/#projects) section or review my profile on [GitHub](https://github.com/muety). Please also have a look at my [LinkedIn](https://www.linkedin.com/in/ferdinand-m%C3%BCtsch/) profile.","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-05-03 22:21:16\n---\n\n# About\n\n## Me\n\nHey, welcome and thank you for visiting my webpage.\n\nMy name is Ferdinand Mtsch, I am 26 years old, living in Karlsruhe, Germany and currently working as a software engineer at [Frachtwerk GmbH](https://frachtwerk.de).\n\nEralier this year I finished my Master's thesis at [ITIV](https://itiv.kit.edu) on the subjects of _Cooperative Perception and Cellular Vehicle-2-X Communication for Autonomous Driving_ as part of my studies on [information engineering and management](https://informationswirtschaft.org) at the [Karlsruhe Institute of Technology](https://kit.edu). Prior to this, I got my Bachelor's degree in 2016 with a final thesis about _Microservices and Semantic Web_ at [TECO](https://teco.edu).\n\nMy interests are  among others  software development, especially in a web- and mobile context, as well as data science and machine leatning. I consider myself very open-minded and progressive and I am continuously interested in learning new technology.\n\nBesides my studies I used to work as a student employee at [Inovex](https://inovex.de) in Karlsruhe and as a self-employed Freelancer developer. In addition, I did a rewarding internship at the [Volkswagen Electronic Research Lab (ERL)](https://vwiecc.com/) in Belmont, California as part of the software platforms team.\nI participated in realizing software projects for small and mid-sized businesses in the context of Industry 4.0, the Internet of Things, big-scale data analytics, web platforms and more. \n\n## My topics\nFull-Stack Web, Mobile, DevOps, Data Science, Machine Learning\n\n## My Languages\nJava (), JavaScript (), Go (), Python (), Scala (), R (), Dart ()\n\n## My Social Networks\nIf you are interested in some of my projects, take a look at the [Projects](/#projects) section or review my profile on [GitHub](https://github.com/muety). Please also have a look at my [LinkedIn](https://www.linkedin.com/in/ferdinand-m%C3%BCtsch/) profile.","updated":"2020-11-02T19:40:58.278Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckh0yeqld0002o2e09qmna9pa","content":"<h1 id=\"About\"><a href=\"#About\" class=\"headerlink\" title=\"About\"></a>About</h1><h2 id=\"Me\"><a href=\"#Me\" class=\"headerlink\" title=\"Me\"></a>Me</h2><p>Hey, welcome and thank you for visiting my webpage.</p>\n<p>My name is Ferdinand Mtsch, I am 26 years old, living in Karlsruhe, Germany and currently working as a software engineer at <a href=\"https://frachtwerk.de/\">Frachtwerk GmbH</a>.</p>\n<p>Eralier this year I finished my Masters thesis at <a href=\"https://itiv.kit.edu/\">ITIV</a> on the subjects of <em>Cooperative Perception and Cellular Vehicle-2-X Communication for Autonomous Driving</em> as part of my studies on <a href=\"https://informationswirtschaft.org/\">information engineering and management</a> at the <a href=\"https://kit.edu/\">Karlsruhe Institute of Technology</a>. Prior to this, I got my Bachelors degree in 2016 with a final thesis about <em>Microservices and Semantic Web</em> at <a href=\"https://teco.edu/\">TECO</a>.</p>\n<p>My interests are  among others  software development, especially in a web- and mobile context, as well as data science and machine leatning. I consider myself very open-minded and progressive and I am continuously interested in learning new technology.</p>\n<p>Besides my studies I used to work as a student employee at <a href=\"https://inovex.de/\">Inovex</a> in Karlsruhe and as a self-employed Freelancer developer. In addition, I did a rewarding internship at the <a href=\"https://vwiecc.com/\">Volkswagen Electronic Research Lab (ERL)</a> in Belmont, California as part of the software platforms team.<br>I participated in realizing software projects for small and mid-sized businesses in the context of Industry 4.0, the Internet of Things, big-scale data analytics, web platforms and more. </p>\n<h2 id=\"My-topics\"><a href=\"#My-topics\" class=\"headerlink\" title=\"My topics\"></a>My topics</h2><p>Full-Stack Web, Mobile, DevOps, Data Science, Machine Learning</p>\n<h2 id=\"My-Languages\"><a href=\"#My-Languages\" class=\"headerlink\" title=\"My Languages\"></a>My Languages</h2><p>Java (), JavaScript (), Go (), Python (), Scala (), R (), Dart ()</p>\n<h2 id=\"My-Social-Networks\"><a href=\"#My-Social-Networks\" class=\"headerlink\" title=\"My Social Networks\"></a>My Social Networks</h2><p>If you are interested in some of my projects, take a look at the <a href=\"/#projects\">Projects</a> section or review my profile on <a href=\"https://github.com/muety\">GitHub</a>. Please also have a look at my <a href=\"https://www.linkedin.com/in/ferdinand-m%C3%BCtsch/\">LinkedIn</a> profile.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1 id=\"About\"><a href=\"#About\" class=\"headerlink\" title=\"About\"></a>About</h1><h2 id=\"Me\"><a href=\"#Me\" class=\"headerlink\" title=\"Me\"></a>Me</h2><p>Hey, welcome and thank you for visiting my webpage.</p>\n<p>My name is Ferdinand Mtsch, I am 26 years old, living in Karlsruhe, Germany and currently working as a software engineer at <a href=\"https://frachtwerk.de/\">Frachtwerk GmbH</a>.</p>\n<p>Eralier this year I finished my Masters thesis at <a href=\"https://itiv.kit.edu/\">ITIV</a> on the subjects of <em>Cooperative Perception and Cellular Vehicle-2-X Communication for Autonomous Driving</em> as part of my studies on <a href=\"https://informationswirtschaft.org/\">information engineering and management</a> at the <a href=\"https://kit.edu/\">Karlsruhe Institute of Technology</a>. Prior to this, I got my Bachelors degree in 2016 with a final thesis about <em>Microservices and Semantic Web</em> at <a href=\"https://teco.edu/\">TECO</a>.</p>\n<p>My interests are  among others  software development, especially in a web- and mobile context, as well as data science and machine leatning. I consider myself very open-minded and progressive and I am continuously interested in learning new technology.</p>\n<p>Besides my studies I used to work as a student employee at <a href=\"https://inovex.de/\">Inovex</a> in Karlsruhe and as a self-employed Freelancer developer. In addition, I did a rewarding internship at the <a href=\"https://vwiecc.com/\">Volkswagen Electronic Research Lab (ERL)</a> in Belmont, California as part of the software platforms team.<br>I participated in realizing software projects for small and mid-sized businesses in the context of Industry 4.0, the Internet of Things, big-scale data analytics, web platforms and more. </p>\n<h2 id=\"My-topics\"><a href=\"#My-topics\" class=\"headerlink\" title=\"My topics\"></a>My topics</h2><p>Full-Stack Web, Mobile, DevOps, Data Science, Machine Learning</p>\n<h2 id=\"My-Languages\"><a href=\"#My-Languages\" class=\"headerlink\" title=\"My Languages\"></a>My Languages</h2><p>Java (), JavaScript (), Go (), Python (), Scala (), R (), Dart ()</p>\n<h2 id=\"My-Social-Networks\"><a href=\"#My-Social-Networks\" class=\"headerlink\" title=\"My Social Networks\"></a>My Social Networks</h2><p>If you are interested in some of my projects, take a look at the <a href=\"/#projects\">Projects</a> section or review my profile on <a href=\"https://github.com/muety\">GitHub</a>. Please also have a look at my <a href=\"https://www.linkedin.com/in/ferdinand-m%C3%BCtsch/\">LinkedIn</a> profile.</p>\n"},{"title":"imprint","date":"2019-01-02T07:46:32.000Z","_content":"\n# Imprint\n\n### Legal Disclosure & Privacy Statement\nInformation in accordance with section 5 German TMG\n\n```\nFerdinand Mtsch\nVorholzstrae 11 \n76137 Karlsruhe\nGermany\n```\n\n### Contact\n```\nTelephone: 017645641974\nE-Mail: ferdinand@muetsch.io\nWeb: www.muetsch.io\n```\n\n### Disclaimer\n#### Accountability for content\nThe contents of our pages have been created with the utmost care. However, we cannot guarantee the contents' accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this context, please note that we are accordingly not obliged to monitor merely the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per  8 to 10 of the Telemedia Act (TMG).\n\n#### Accountability for links\nResponsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately. \n\n#### Copyright\nOur web pages and their contents are subject to German copyright law. Unless expressly permitted by law ( 44a et seq. of the copyright law), every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are allowed only for private use, so must not serve either directly or indirectly for earnings. Unauthorized utilization of copyrighted works is punishable ( 106 of the copyright law).\n\n#### General\nYour personal data (e.g. title, name, house address, e-mail address, phone number, bank details, credit card number) are processed by us only in accordance with the provisions of German data privacy laws. The following provisions describe the type, scope and purpose of collecting, processing and utilizing personal data. This data privacy policy applies only to our web pages. If links on our pages route you to other pages, please inquire there about how your data are handled in such cases.\n\n#### Inventory data\n(1) Your personal data, insofar as these are necessary for this contractual relationship (inventory data) in terms of its establishment, organization of content and modifications, are used exclusively for fulfilling the contract. For goods to be delivered, for instance, your name and address must be relayed to the supplier of the goods. \n(2) Without your explicit consent or a legal basis, your personal data are not passed on to third parties outside the scope of fulfilling this contract. After completion of the contract, your data are blocked against further use. After expiry of deadlines as per tax-related and commercial regulations, these data are deleted unless you have expressly consented to their further use.\n\n#### Information about cookies\n(1) To optimize our web presence, we use cookies. These are small text files stored in your computer's main memory. These cookies are deleted after you close the browser. Other cookies remain on your computer (long-term cookies) and permit its recognition on your next visit. This allows us to improve your access to our site.\n(2) You can prevent storage of cookies by choosing a \"disable cookies\" option in your browser settings. But this can limit the functionality of our Internet offers as a result.\n\n#### Disclosure\nAccording to the Federal Data Protection Act, you have a right to free-of-charge information about your stored data, and possibly entitlement to correction, blocking or deletion of such data. Inquiries can be directed to the following e-mail addresses: ( [ferdinand@muetsch.io](mailto:ferdinand@muetsch.io) )\n\nSource: [twiggs translations](http://www.twigg.de/)\n\n### German Privacy Policy (Datenschutzerklrung)\nDiese Datenschutzerklrung klrt Sie ber die Art, den Umfang und Zweck der Verarbeitung von personenbezogenen Daten (nachfolgend kurz Daten) innerhalb unseres Onlineangebotes und der mit ihm verbundenen Webseiten, Funktionen und Inhalte sowie externen Onlineprsenzen, wie z.B. unser Social Media Profile auf. (nachfolgend gemeinsam bezeichnet als Onlineangebot). Im Hinblick auf die verwendeten Begrifflichkeiten, wie z.B. Verarbeitung oder Verantwortlicher verweisen wir auf die Definitionen im Art. 4 der Datenschutzgrundverordnung (DSGVO).\n\n#### Arten der verarbeiteten Daten:\n- Bestandsdaten (z.B., Namen, Adressen).\n- Kontaktdaten (z.B., E-Mail, Telefonnummern).\n- Inhaltsdaten (z.B., Texteingaben, Fotografien, Videos).\n- Nutzungsdaten (z.B., besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten).\n- Meta-/Kommunikationsdaten (z.B., Gerte-Informationen, IP-Adressen).\n\n#### Kategorien betroffener Personen\nBesucher und Nutzer des Onlineangebotes (Nachfolgend bezeichnen wir die betroffenen Personen zusammenfassend auch als Nutzer).\n\n#### Zweck der Verarbeitung\n- Zurverfgungstellung des Onlineangebotes, seiner Funktionen und Inhalte.\n- Beantwortung von Kontaktanfragen und Kommunikation mit Nutzern.\n- Sicherheitsmanahmen.\n- Reichweitenmessung/Marketing\n\n#### Verwendete Begrifflichkeiten \nPersonenbezogene Daten sind alle Informationen, die sich auf eine identifizierte oder identifizierbare natrliche Person (im Folgenden betroffene Person) beziehen; als identifizierbar wird eine natrliche Person angesehen, die direkt oder indirekt, insbesondere mittels Zuordnung zu einer Kennung wie einem Namen, zu einer Kennnummer, zu Standortdaten, zu einer Online-Kennung (z.B. Cookie) oder zu einem oder mehreren besonderen Merkmalen identifiziert werden kann, die Ausdruck der physischen, physiologischen, genetischen, psychischen, wirtschaftlichen, kulturellen oder sozialen Identitt dieser natrlichen Person sind.\n\nVerarbeitung ist jeder mit oder ohne Hilfe automatisierter Verfahren ausgefhrten Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten. Der Begriff reicht weit und umfasst praktisch jeden Umgang mit Daten.\n\nAls Verantwortlicher wird die natrliche oder juristische Person, Behrde, Einrichtung oder andere Stelle, die allein oder gemeinsam mit anderen ber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten entscheidet, bezeichnet.\n\n#### Magebliche Rechtsgrundlagen\nNach Magabe des Art. 13 DSGVO teilen wir Ihnen die Rechtsgrundlagen unserer Datenverarbeitungen mit. Sofern die Rechtsgrundlage in der Datenschutzerklrung nicht genannt wird, gilt Folgendes: Die Rechtsgrundlage fr die Einholung von Einwilligungen ist Art. 6 Abs. 1 lit. a und Art. 7 DSGVO, die Rechtsgrundlage fr die Verarbeitung zur Erfllung unserer Leistungen und Durchfhrung vertraglicher Manahmen sowie Beantwortung von Anfragen ist Art. 6 Abs. 1 lit. b DSGVO, die Rechtsgrundlage fr die Verarbeitung zur Erfllung unserer rechtlichen Verpflichtungen ist Art. 6 Abs. 1 lit. c DSGVO, und die Rechtsgrundlage fr die Verarbeitung zur Wahrung unserer berechtigten Interessen ist Art. 6 Abs. 1 lit. f DSGVO. Fr den Fall, dass lebenswichtige Interessen der betroffenen Person oder einer anderen natrlichen Person eine Verarbeitung personenbezogener Daten erforderlich machen, dient Art. 6 Abs. 1 lit. d DSGVO als Rechtsgrundlage.\n\n#### Zusammenarbeit mit Auftragsverarbeitern und Dritten\nSofern wir im Rahmen unserer Verarbeitung Daten gegenber anderen Personen und Unternehmen (Auftragsverarbeitern oder Dritten) offenbaren, sie an diese bermitteln oder ihnen sonst Zugriff auf die Daten gewhren, erfolgt dies nur auf Grundlage einer gesetzlichen Erlaubnis (z.B. wenn eine bermittlung der Daten an Dritte, wie an Zahlungsdienstleister, gem. Art. 6 Abs. 1 lit. b DSGVO zur Vertragserfllung erforderlich ist), Sie eingewilligt haben, eine rechtliche Verpflichtung dies vorsieht oder auf Grundlage unserer berechtigten Interessen (z.B. beim Einsatz von Beauftragten, Webhostern, etc.).\n\nSofern wir Dritte mit der Verarbeitung von Daten auf Grundlage eines sog. Auftragsverarbeitungsvertrages beauftragen, geschieht dies auf Grundlage des Art. 28 DSGVO.\n\n#### bermittlungen in Drittlnder\nSofern wir Daten in einem Drittland (d.h. auerhalb der Europischen Union (EU) oder des Europischen Wirtschaftsraums (EWR)) verarbeiten oder dies im Rahmen der Inanspruchnahme von Diensten Dritter oder Offenlegung, bzw. bermittlung von Daten an Dritte geschieht, erfolgt dies nur, wenn es zur Erfllung unserer (vor)vertraglichen Pflichten, auf Grundlage Ihrer Einwilligung, aufgrund einer rechtlichen Verpflichtung oder auf Grundlage unserer berechtigten Interessen geschieht. Vorbehaltlich gesetzlicher oder vertraglicher Erlaubnisse, verarbeiten oder lassen wir die Daten in einem Drittland nur beim Vorliegen der besonderen Voraussetzungen der Art. 44 ff. DSGVO verarbeiten. D.h. die Verarbeitung erfolgt z.B. auf Grundlage besonderer Garantien, wie der offiziell anerkannten Feststellung eines der EU entsprechenden Datenschutzniveaus (z.B. fr die USA durch das Privacy Shield) oder Beachtung offiziell anerkannter spezieller vertraglicher Verpflichtungen (so genannte Standardvertragsklauseln).\n\n#### Rechte der betroffenen Personen\nSie haben das Recht, eine Besttigung darber zu verlangen, ob betreffende Daten verarbeitet werden und auf Auskunft ber diese Daten sowie auf weitere Informationen und Kopie der Daten entsprechend Art. 15 DSGVO.\n\nSie haben entsprechend. Art. 16 DSGVO das Recht, die Vervollstndigung der Sie betreffenden Daten oder die Berichtigung der Sie betreffenden unrichtigen Daten zu verlangen.\n\nSie haben nach Magabe des Art. 17 DSGVO das Recht zu verlangen, dass betreffende Daten unverzglich gelscht werden, bzw. alternativ nach Magabe des Art. 18 DSGVO eine Einschrnkung der Verarbeitung der Daten zu verlangen.\n\nSie haben das Recht zu verlangen, dass die Sie betreffenden Daten, die Sie uns bereitgestellt haben nach Magabe des Art. 20 DSGVO zu erhalten und deren bermittlung an andere Verantwortliche zu fordern.\n\nSie haben ferner gem. Art. 77 DSGVO das Recht, eine Beschwerde bei der zustndigen Aufsichtsbehrde einzureichen.\n\n#### Widerrufsrecht\nSie haben das Recht, erteilte Einwilligungen gem. Art. 7 Abs. 3 DSGVO mit Wirkung fr die Zukunft zu widerrufen\n\n#### Widerspruchsrecht\nSie knnen der knftigen Verarbeitung der Sie betreffenden Daten nach Magabe des Art. 21 DSGVO jederzeit widersprechen. Der Widerspruch kann insbesondere gegen die Verarbeitung fr Zwecke der Direktwerbung erfolgen.\n\n#### Cookies und Widerspruchsrecht bei Direktwerbung\nAls Cookies werden kleine Dateien bezeichnet, die auf Rechnern der Nutzer gespeichert werden. Innerhalb der Cookies knnen unterschiedliche Angaben gespeichert werden. Ein Cookie dient primr dazu, die Angaben zu einem Nutzer (bzw. dem Gert auf dem das Cookie gespeichert ist) whrend oder auch nach seinem Besuch innerhalb eines Onlineangebotes zu speichern. Als temporre Cookies, bzw. Session-Cookies oder transiente Cookies, werden Cookies bezeichnet, die gelscht werden, nachdem ein Nutzer ein Onlineangebot verlsst und seinen Browser schliet. In einem solchen Cookie kann z.B. der Inhalt eines Warenkorbs in einem Onlineshop oder ein Login-Staus gespeichert werden. Als permanent oder persistent werden Cookies bezeichnet, die auch nach dem Schlieen des Browsers gespeichert bleiben. So kann z.B. der Login-Status gespeichert werden, wenn die Nutzer diese nach mehreren Tagen aufsuchen. Ebenso knnen in einem solchen Cookie die Interessen der Nutzer gespeichert werden, die fr Reichweitenmessung oder Marketingzwecke verwendet werden. Als Third-Party-Cookie werden Cookies bezeichnet, die von anderen Anbietern als dem Verantwortlichen, der das Onlineangebot betreibt, angeboten werden (andernfalls, wenn es nur dessen Cookies sind spricht man von First-Party Cookies).\n\nWir knnen temporre und permanente Cookies einsetzen und klren hierber im Rahmen unserer Datenschutzerklrung auf.\n\nFalls die Nutzer nicht mchten, dass Cookies auf ihrem Rechner gespeichert werden, werden sie gebeten die entsprechende Option in den Systemeinstellungen ihres Browsers zu deaktivieren. Gespeicherte Cookies knnen in den Systemeinstellungen des Browsers gelscht werden. Der Ausschluss von Cookies kann zu Funktionseinschrnkungen dieses Onlineangebotes fhren.\n\nEin genereller Widerspruch gegen den Einsatz der zu Zwecken des Onlinemarketing eingesetzten Cookies kann bei einer Vielzahl der Dienste, vor allem im Fall des Trackings, ber die US-amerikanische Seite http://www.aboutads.info/choices/ oder die EU-Seite http://www.youronlinechoices.com/ erklrt werden. Des Weiteren kann die Speicherung von Cookies mittels deren Abschaltung in den Einstellungen des Browsers erreicht werden. Bitte beachten Sie, dass dann gegebenenfalls nicht alle Funktionen dieses Onlineangebotes genutzt werden knnen.\n\n#### Lschung von Daten\nDie von uns verarbeiteten Daten werden nach Magabe der Art. 17 und 18 DSGVO gelscht oder in ihrer Verarbeitung eingeschrnkt. Sofern nicht im Rahmen dieser Datenschutzerklrung ausdrcklich angegeben, werden die bei uns gespeicherten Daten gelscht, sobald sie fr ihre Zweckbestimmung nicht mehr erforderlich sind und der Lschung keine gesetzlichen Aufbewahrungspflichten entgegenstehen. Sofern die Daten nicht gelscht werden, weil sie fr andere und gesetzlich zulssige Zwecke erforderlich sind, wird deren Verarbeitung eingeschrnkt. D.h. die Daten werden gesperrt und nicht fr andere Zwecke verarbeitet. Das gilt z.B. fr Daten, die aus handels- oder steuerrechtlichen Grnden aufbewahrt werden mssen.\n\nNach gesetzlichen Vorgaben in Deutschland erfolgt die Aufbewahrung insbesondere fr 6 Jahre gem  257 Abs. 1 HGB (Handelsbcher, Inventare, Erffnungsbilanzen, Jahresabschlsse, Handelsbriefe, Buchungsbelege, etc.) sowie fr 10 Jahre gem  147 Abs. 1 AO (Bcher, Aufzeichnungen, Lageberichte, Buchungsbelege, Handels- und Geschftsbriefe, Fr Besteuerung relevante Unterlagen, etc.).\n\nNach gesetzlichen Vorgaben in sterreich erfolgt die Aufbewahrung insbesondere fr 7 J gem  132 Abs. 1 BAO (Buchhaltungsunterlagen, Belege/Rechnungen, Konten, Belege, Geschftspapiere, Aufstellung der Einnahmen und Ausgaben, etc.), fr 22 Jahre im Zusammenhang mit Grundstcken und fr 10 Jahre bei Unterlagen im Zusammenhang mit elektronisch erbrachten Leistungen, Telekommunikations-, Rundfunk- und Fernsehleistungen, die an Nichtunternehmer in EU-Mitgliedstaaten erbracht werden und fr die der Mini-One-Stop-Shop (MOSS) in Anspruch genommen wird.\n\n#### Hosting\nDie von uns in Anspruch genommenen Hosting-Leistungen dienen der Zurverfgungstellung der folgenden Leistungen: Infrastruktur- und Plattformdienstleistungen, Rechenkapazitt, Speicherplatz und Datenbankdienste, Sicherheitsleistungen sowie technische Wartungsleistungen, die wir zum Zwecke des Betriebs dieses Onlineangebotes einsetzen.\n\nHierbei verarbeiten wir, bzw. unser Hostinganbieter Bestandsdaten, Kontaktdaten, Inhaltsdaten, Vertragsdaten, Nutzungsdaten, Meta- und Kommunikationsdaten von Kunden, Interessenten und Besuchern dieses Onlineangebotes auf Grundlage unserer berechtigten Interessen an einer effizienten und sicheren Zurverfgungstellung dieses Onlineangebotes gem. Art. 6 Abs. 1 lit. f DSGVO i.V.m. Art. 28 DSGVO (Abschluss Auftragsverarbeitungsvertrag).\n\n#### Erhebung von Zugriffsdaten und Logfiles\nWir, bzw. unser Hostinganbieter, erhebt auf Grundlage unserer berechtigten Interessen im Sinne des Art. 6 Abs. 1 lit. f. DSGVO Daten ber jeden Zugriff auf den Server, auf dem sich dieser Dienst befindet (sogenannte Serverlogfiles). Zu den Zugriffsdaten gehren Name der abgerufenen Webseite, Datei, Datum und Uhrzeit des Abrufs, bertragene Datenmenge, Meldung ber erfolgreichen Abruf, Browsertyp nebst Version, das Betriebssystem des Nutzers, Referrer URL (die zuvor besuchte Seite), IP-Adresse und der anfragende Provider.\n\nLogfile-Informationen werden aus Sicherheitsgrnden (z.B. zur Aufklrung von Missbrauchs- oder Betrugshandlungen) fr die Dauer von maximal 7 Tagen gespeichert und danach gelscht. Daten, deren weitere Aufbewahrung zu Beweiszwecken erforderlich ist, sind bis zur endgltigen Klrung des jeweiligen Vorfalls von der Lschung ausgenommen.\n\n#### Google Universal Analytics\nWir setzen auf Grundlage unserer berechtigten Interessen (d.h. Interesse an der Analyse, Optimierung und wirtschaftlichem Betrieb unseres Onlineangebotes im Sinne des Art. 6 Abs. 1 lit. f. DSGVO) Google Analytics, einen Webanalysedienst der Google LLC (Google) ein. Google verwendet Cookies. Die durch das Cookie erzeugten Informationen ber Benutzung des Onlineangebotes durch die Nutzer werden in der Regel an einen Server von Google in den USA bertragen und dort gespeichert.\n\nGoogle ist unter dem Privacy-Shield-Abkommen zertifiziert und bietet hierdurch eine Garantie, das europische Datenschutzrecht einzuhalten (https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active).\n\nGoogle wird diese Informationen in unserem Auftrag benutzen, um die Nutzung unseres Onlineangebotes durch die Nutzer auszuwerten, um Reports ber die Aktivitten innerhalb dieses Onlineangebotes zusammenzustellen und um weitere, mit der Nutzung dieses Onlineangebotes und der Internetnutzung verbundene Dienstleistungen, uns gegenber zu erbringen. Dabei knnen aus den verarbeiteten Daten pseudonyme Nutzungsprofile der Nutzer erstellt werden.\n\nWir setzen Google Analytics in der Ausgestaltung als Universal-Analytics ein. Universal Analytics bezeichnet ein Verfahren von Google Analytics, bei dem die Nutzeranalyse auf Grundlage einer pseudonymen Nutzer-ID erfolgt und damit ein pseudonymes Profil des Nutzers mit Informationen aus der Nutzung verschiedener Gerten erstellt wird (sog. Cross-Device-Tracking).\n\nWir setzen Google Analytics nur mit aktivierter IP-Anonymisierung ein. Das bedeutet, die IP-Adresse der Nutzer wird von Google innerhalb von Mitgliedstaaten der Europischen Union oder in anderen Vertragsstaaten des Abkommens ber den Europischen Wirtschaftsraum gekrzt. Nur in Ausnahmefllen wird die volle IP-Adresse an einen Server von Google in den USA bertragen und dort gekrzt.\n\nDie von dem Browser des Nutzers bermittelte IP-Adresse wird nicht mit anderen Daten von Google zusammengefhrt. Die Nutzer knnen die Speicherung der Cookies durch eine entsprechende Einstellung ihrer Browser-Software verhindern; die Nutzer knnen darber hinaus die Erfassung der durch das Cookie erzeugten und auf ihre Nutzung des Onlineangebotes bezogenen Daten an Google sowie die Verarbeitung dieser Daten durch Google verhindern, indem sie das unter folgendem Link verfgbare Browser-Plugin herunterladen und installieren: http://tools.google.com/dlpage/gaoptout?hl=de.\n\nWeitere Informationen zur Datennutzung durch Google, Einstellungs- und Widerspruchsmglichkeiten erfahren Sie auf den Webseiten von Google: https://www.google.com/intl/de/policies/privacy/partners (Datennutzung durch Google bei Ihrer Nutzung von Websites oder Apps unserer Partner), http://www.google.com/policies/technologies/ads (Datennutzung zu Werbezwecken), http://www.google.de/settings/ads (Informationen verwalten, die Google verwendet, um Ihnen Werbung einzublenden).\n\n#### Youtube\nWir binden die Videos der Plattform YouTube des Anbieters Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA, ein. Datenschutzerklrung: https://www.google.com/policies/privacy/, Opt-Out: https://adssettings.google.com/authenticated.\n\n[Erstellt mit Datenschutz-Generator.de von RA Dr. Thomas Schwenke](https://datenschutz-generator.de/)","source":"imprint/index.md","raw":"---\ntitle: imprint\ndate: 2019-01-02 08:46:32\n---\n\n# Imprint\n\n### Legal Disclosure & Privacy Statement\nInformation in accordance with section 5 German TMG\n\n```\nFerdinand Mtsch\nVorholzstrae 11 \n76137 Karlsruhe\nGermany\n```\n\n### Contact\n```\nTelephone: 017645641974\nE-Mail: ferdinand@muetsch.io\nWeb: www.muetsch.io\n```\n\n### Disclaimer\n#### Accountability for content\nThe contents of our pages have been created with the utmost care. However, we cannot guarantee the contents' accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this context, please note that we are accordingly not obliged to monitor merely the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per  8 to 10 of the Telemedia Act (TMG).\n\n#### Accountability for links\nResponsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately. \n\n#### Copyright\nOur web pages and their contents are subject to German copyright law. Unless expressly permitted by law ( 44a et seq. of the copyright law), every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are allowed only for private use, so must not serve either directly or indirectly for earnings. Unauthorized utilization of copyrighted works is punishable ( 106 of the copyright law).\n\n#### General\nYour personal data (e.g. title, name, house address, e-mail address, phone number, bank details, credit card number) are processed by us only in accordance with the provisions of German data privacy laws. The following provisions describe the type, scope and purpose of collecting, processing and utilizing personal data. This data privacy policy applies only to our web pages. If links on our pages route you to other pages, please inquire there about how your data are handled in such cases.\n\n#### Inventory data\n(1) Your personal data, insofar as these are necessary for this contractual relationship (inventory data) in terms of its establishment, organization of content and modifications, are used exclusively for fulfilling the contract. For goods to be delivered, for instance, your name and address must be relayed to the supplier of the goods. \n(2) Without your explicit consent or a legal basis, your personal data are not passed on to third parties outside the scope of fulfilling this contract. After completion of the contract, your data are blocked against further use. After expiry of deadlines as per tax-related and commercial regulations, these data are deleted unless you have expressly consented to their further use.\n\n#### Information about cookies\n(1) To optimize our web presence, we use cookies. These are small text files stored in your computer's main memory. These cookies are deleted after you close the browser. Other cookies remain on your computer (long-term cookies) and permit its recognition on your next visit. This allows us to improve your access to our site.\n(2) You can prevent storage of cookies by choosing a \"disable cookies\" option in your browser settings. But this can limit the functionality of our Internet offers as a result.\n\n#### Disclosure\nAccording to the Federal Data Protection Act, you have a right to free-of-charge information about your stored data, and possibly entitlement to correction, blocking or deletion of such data. Inquiries can be directed to the following e-mail addresses: ( [ferdinand@muetsch.io](mailto:ferdinand@muetsch.io) )\n\nSource: [twiggs translations](http://www.twigg.de/)\n\n### German Privacy Policy (Datenschutzerklrung)\nDiese Datenschutzerklrung klrt Sie ber die Art, den Umfang und Zweck der Verarbeitung von personenbezogenen Daten (nachfolgend kurz Daten) innerhalb unseres Onlineangebotes und der mit ihm verbundenen Webseiten, Funktionen und Inhalte sowie externen Onlineprsenzen, wie z.B. unser Social Media Profile auf. (nachfolgend gemeinsam bezeichnet als Onlineangebot). Im Hinblick auf die verwendeten Begrifflichkeiten, wie z.B. Verarbeitung oder Verantwortlicher verweisen wir auf die Definitionen im Art. 4 der Datenschutzgrundverordnung (DSGVO).\n\n#### Arten der verarbeiteten Daten:\n- Bestandsdaten (z.B., Namen, Adressen).\n- Kontaktdaten (z.B., E-Mail, Telefonnummern).\n- Inhaltsdaten (z.B., Texteingaben, Fotografien, Videos).\n- Nutzungsdaten (z.B., besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten).\n- Meta-/Kommunikationsdaten (z.B., Gerte-Informationen, IP-Adressen).\n\n#### Kategorien betroffener Personen\nBesucher und Nutzer des Onlineangebotes (Nachfolgend bezeichnen wir die betroffenen Personen zusammenfassend auch als Nutzer).\n\n#### Zweck der Verarbeitung\n- Zurverfgungstellung des Onlineangebotes, seiner Funktionen und Inhalte.\n- Beantwortung von Kontaktanfragen und Kommunikation mit Nutzern.\n- Sicherheitsmanahmen.\n- Reichweitenmessung/Marketing\n\n#### Verwendete Begrifflichkeiten \nPersonenbezogene Daten sind alle Informationen, die sich auf eine identifizierte oder identifizierbare natrliche Person (im Folgenden betroffene Person) beziehen; als identifizierbar wird eine natrliche Person angesehen, die direkt oder indirekt, insbesondere mittels Zuordnung zu einer Kennung wie einem Namen, zu einer Kennnummer, zu Standortdaten, zu einer Online-Kennung (z.B. Cookie) oder zu einem oder mehreren besonderen Merkmalen identifiziert werden kann, die Ausdruck der physischen, physiologischen, genetischen, psychischen, wirtschaftlichen, kulturellen oder sozialen Identitt dieser natrlichen Person sind.\n\nVerarbeitung ist jeder mit oder ohne Hilfe automatisierter Verfahren ausgefhrten Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten. Der Begriff reicht weit und umfasst praktisch jeden Umgang mit Daten.\n\nAls Verantwortlicher wird die natrliche oder juristische Person, Behrde, Einrichtung oder andere Stelle, die allein oder gemeinsam mit anderen ber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten entscheidet, bezeichnet.\n\n#### Magebliche Rechtsgrundlagen\nNach Magabe des Art. 13 DSGVO teilen wir Ihnen die Rechtsgrundlagen unserer Datenverarbeitungen mit. Sofern die Rechtsgrundlage in der Datenschutzerklrung nicht genannt wird, gilt Folgendes: Die Rechtsgrundlage fr die Einholung von Einwilligungen ist Art. 6 Abs. 1 lit. a und Art. 7 DSGVO, die Rechtsgrundlage fr die Verarbeitung zur Erfllung unserer Leistungen und Durchfhrung vertraglicher Manahmen sowie Beantwortung von Anfragen ist Art. 6 Abs. 1 lit. b DSGVO, die Rechtsgrundlage fr die Verarbeitung zur Erfllung unserer rechtlichen Verpflichtungen ist Art. 6 Abs. 1 lit. c DSGVO, und die Rechtsgrundlage fr die Verarbeitung zur Wahrung unserer berechtigten Interessen ist Art. 6 Abs. 1 lit. f DSGVO. Fr den Fall, dass lebenswichtige Interessen der betroffenen Person oder einer anderen natrlichen Person eine Verarbeitung personenbezogener Daten erforderlich machen, dient Art. 6 Abs. 1 lit. d DSGVO als Rechtsgrundlage.\n\n#### Zusammenarbeit mit Auftragsverarbeitern und Dritten\nSofern wir im Rahmen unserer Verarbeitung Daten gegenber anderen Personen und Unternehmen (Auftragsverarbeitern oder Dritten) offenbaren, sie an diese bermitteln oder ihnen sonst Zugriff auf die Daten gewhren, erfolgt dies nur auf Grundlage einer gesetzlichen Erlaubnis (z.B. wenn eine bermittlung der Daten an Dritte, wie an Zahlungsdienstleister, gem. Art. 6 Abs. 1 lit. b DSGVO zur Vertragserfllung erforderlich ist), Sie eingewilligt haben, eine rechtliche Verpflichtung dies vorsieht oder auf Grundlage unserer berechtigten Interessen (z.B. beim Einsatz von Beauftragten, Webhostern, etc.).\n\nSofern wir Dritte mit der Verarbeitung von Daten auf Grundlage eines sog. Auftragsverarbeitungsvertrages beauftragen, geschieht dies auf Grundlage des Art. 28 DSGVO.\n\n#### bermittlungen in Drittlnder\nSofern wir Daten in einem Drittland (d.h. auerhalb der Europischen Union (EU) oder des Europischen Wirtschaftsraums (EWR)) verarbeiten oder dies im Rahmen der Inanspruchnahme von Diensten Dritter oder Offenlegung, bzw. bermittlung von Daten an Dritte geschieht, erfolgt dies nur, wenn es zur Erfllung unserer (vor)vertraglichen Pflichten, auf Grundlage Ihrer Einwilligung, aufgrund einer rechtlichen Verpflichtung oder auf Grundlage unserer berechtigten Interessen geschieht. Vorbehaltlich gesetzlicher oder vertraglicher Erlaubnisse, verarbeiten oder lassen wir die Daten in einem Drittland nur beim Vorliegen der besonderen Voraussetzungen der Art. 44 ff. DSGVO verarbeiten. D.h. die Verarbeitung erfolgt z.B. auf Grundlage besonderer Garantien, wie der offiziell anerkannten Feststellung eines der EU entsprechenden Datenschutzniveaus (z.B. fr die USA durch das Privacy Shield) oder Beachtung offiziell anerkannter spezieller vertraglicher Verpflichtungen (so genannte Standardvertragsklauseln).\n\n#### Rechte der betroffenen Personen\nSie haben das Recht, eine Besttigung darber zu verlangen, ob betreffende Daten verarbeitet werden und auf Auskunft ber diese Daten sowie auf weitere Informationen und Kopie der Daten entsprechend Art. 15 DSGVO.\n\nSie haben entsprechend. Art. 16 DSGVO das Recht, die Vervollstndigung der Sie betreffenden Daten oder die Berichtigung der Sie betreffenden unrichtigen Daten zu verlangen.\n\nSie haben nach Magabe des Art. 17 DSGVO das Recht zu verlangen, dass betreffende Daten unverzglich gelscht werden, bzw. alternativ nach Magabe des Art. 18 DSGVO eine Einschrnkung der Verarbeitung der Daten zu verlangen.\n\nSie haben das Recht zu verlangen, dass die Sie betreffenden Daten, die Sie uns bereitgestellt haben nach Magabe des Art. 20 DSGVO zu erhalten und deren bermittlung an andere Verantwortliche zu fordern.\n\nSie haben ferner gem. Art. 77 DSGVO das Recht, eine Beschwerde bei der zustndigen Aufsichtsbehrde einzureichen.\n\n#### Widerrufsrecht\nSie haben das Recht, erteilte Einwilligungen gem. Art. 7 Abs. 3 DSGVO mit Wirkung fr die Zukunft zu widerrufen\n\n#### Widerspruchsrecht\nSie knnen der knftigen Verarbeitung der Sie betreffenden Daten nach Magabe des Art. 21 DSGVO jederzeit widersprechen. Der Widerspruch kann insbesondere gegen die Verarbeitung fr Zwecke der Direktwerbung erfolgen.\n\n#### Cookies und Widerspruchsrecht bei Direktwerbung\nAls Cookies werden kleine Dateien bezeichnet, die auf Rechnern der Nutzer gespeichert werden. Innerhalb der Cookies knnen unterschiedliche Angaben gespeichert werden. Ein Cookie dient primr dazu, die Angaben zu einem Nutzer (bzw. dem Gert auf dem das Cookie gespeichert ist) whrend oder auch nach seinem Besuch innerhalb eines Onlineangebotes zu speichern. Als temporre Cookies, bzw. Session-Cookies oder transiente Cookies, werden Cookies bezeichnet, die gelscht werden, nachdem ein Nutzer ein Onlineangebot verlsst und seinen Browser schliet. In einem solchen Cookie kann z.B. der Inhalt eines Warenkorbs in einem Onlineshop oder ein Login-Staus gespeichert werden. Als permanent oder persistent werden Cookies bezeichnet, die auch nach dem Schlieen des Browsers gespeichert bleiben. So kann z.B. der Login-Status gespeichert werden, wenn die Nutzer diese nach mehreren Tagen aufsuchen. Ebenso knnen in einem solchen Cookie die Interessen der Nutzer gespeichert werden, die fr Reichweitenmessung oder Marketingzwecke verwendet werden. Als Third-Party-Cookie werden Cookies bezeichnet, die von anderen Anbietern als dem Verantwortlichen, der das Onlineangebot betreibt, angeboten werden (andernfalls, wenn es nur dessen Cookies sind spricht man von First-Party Cookies).\n\nWir knnen temporre und permanente Cookies einsetzen und klren hierber im Rahmen unserer Datenschutzerklrung auf.\n\nFalls die Nutzer nicht mchten, dass Cookies auf ihrem Rechner gespeichert werden, werden sie gebeten die entsprechende Option in den Systemeinstellungen ihres Browsers zu deaktivieren. Gespeicherte Cookies knnen in den Systemeinstellungen des Browsers gelscht werden. Der Ausschluss von Cookies kann zu Funktionseinschrnkungen dieses Onlineangebotes fhren.\n\nEin genereller Widerspruch gegen den Einsatz der zu Zwecken des Onlinemarketing eingesetzten Cookies kann bei einer Vielzahl der Dienste, vor allem im Fall des Trackings, ber die US-amerikanische Seite http://www.aboutads.info/choices/ oder die EU-Seite http://www.youronlinechoices.com/ erklrt werden. Des Weiteren kann die Speicherung von Cookies mittels deren Abschaltung in den Einstellungen des Browsers erreicht werden. Bitte beachten Sie, dass dann gegebenenfalls nicht alle Funktionen dieses Onlineangebotes genutzt werden knnen.\n\n#### Lschung von Daten\nDie von uns verarbeiteten Daten werden nach Magabe der Art. 17 und 18 DSGVO gelscht oder in ihrer Verarbeitung eingeschrnkt. Sofern nicht im Rahmen dieser Datenschutzerklrung ausdrcklich angegeben, werden die bei uns gespeicherten Daten gelscht, sobald sie fr ihre Zweckbestimmung nicht mehr erforderlich sind und der Lschung keine gesetzlichen Aufbewahrungspflichten entgegenstehen. Sofern die Daten nicht gelscht werden, weil sie fr andere und gesetzlich zulssige Zwecke erforderlich sind, wird deren Verarbeitung eingeschrnkt. D.h. die Daten werden gesperrt und nicht fr andere Zwecke verarbeitet. Das gilt z.B. fr Daten, die aus handels- oder steuerrechtlichen Grnden aufbewahrt werden mssen.\n\nNach gesetzlichen Vorgaben in Deutschland erfolgt die Aufbewahrung insbesondere fr 6 Jahre gem  257 Abs. 1 HGB (Handelsbcher, Inventare, Erffnungsbilanzen, Jahresabschlsse, Handelsbriefe, Buchungsbelege, etc.) sowie fr 10 Jahre gem  147 Abs. 1 AO (Bcher, Aufzeichnungen, Lageberichte, Buchungsbelege, Handels- und Geschftsbriefe, Fr Besteuerung relevante Unterlagen, etc.).\n\nNach gesetzlichen Vorgaben in sterreich erfolgt die Aufbewahrung insbesondere fr 7 J gem  132 Abs. 1 BAO (Buchhaltungsunterlagen, Belege/Rechnungen, Konten, Belege, Geschftspapiere, Aufstellung der Einnahmen und Ausgaben, etc.), fr 22 Jahre im Zusammenhang mit Grundstcken und fr 10 Jahre bei Unterlagen im Zusammenhang mit elektronisch erbrachten Leistungen, Telekommunikations-, Rundfunk- und Fernsehleistungen, die an Nichtunternehmer in EU-Mitgliedstaaten erbracht werden und fr die der Mini-One-Stop-Shop (MOSS) in Anspruch genommen wird.\n\n#### Hosting\nDie von uns in Anspruch genommenen Hosting-Leistungen dienen der Zurverfgungstellung der folgenden Leistungen: Infrastruktur- und Plattformdienstleistungen, Rechenkapazitt, Speicherplatz und Datenbankdienste, Sicherheitsleistungen sowie technische Wartungsleistungen, die wir zum Zwecke des Betriebs dieses Onlineangebotes einsetzen.\n\nHierbei verarbeiten wir, bzw. unser Hostinganbieter Bestandsdaten, Kontaktdaten, Inhaltsdaten, Vertragsdaten, Nutzungsdaten, Meta- und Kommunikationsdaten von Kunden, Interessenten und Besuchern dieses Onlineangebotes auf Grundlage unserer berechtigten Interessen an einer effizienten und sicheren Zurverfgungstellung dieses Onlineangebotes gem. Art. 6 Abs. 1 lit. f DSGVO i.V.m. Art. 28 DSGVO (Abschluss Auftragsverarbeitungsvertrag).\n\n#### Erhebung von Zugriffsdaten und Logfiles\nWir, bzw. unser Hostinganbieter, erhebt auf Grundlage unserer berechtigten Interessen im Sinne des Art. 6 Abs. 1 lit. f. DSGVO Daten ber jeden Zugriff auf den Server, auf dem sich dieser Dienst befindet (sogenannte Serverlogfiles). Zu den Zugriffsdaten gehren Name der abgerufenen Webseite, Datei, Datum und Uhrzeit des Abrufs, bertragene Datenmenge, Meldung ber erfolgreichen Abruf, Browsertyp nebst Version, das Betriebssystem des Nutzers, Referrer URL (die zuvor besuchte Seite), IP-Adresse und der anfragende Provider.\n\nLogfile-Informationen werden aus Sicherheitsgrnden (z.B. zur Aufklrung von Missbrauchs- oder Betrugshandlungen) fr die Dauer von maximal 7 Tagen gespeichert und danach gelscht. Daten, deren weitere Aufbewahrung zu Beweiszwecken erforderlich ist, sind bis zur endgltigen Klrung des jeweiligen Vorfalls von der Lschung ausgenommen.\n\n#### Google Universal Analytics\nWir setzen auf Grundlage unserer berechtigten Interessen (d.h. Interesse an der Analyse, Optimierung und wirtschaftlichem Betrieb unseres Onlineangebotes im Sinne des Art. 6 Abs. 1 lit. f. DSGVO) Google Analytics, einen Webanalysedienst der Google LLC (Google) ein. Google verwendet Cookies. Die durch das Cookie erzeugten Informationen ber Benutzung des Onlineangebotes durch die Nutzer werden in der Regel an einen Server von Google in den USA bertragen und dort gespeichert.\n\nGoogle ist unter dem Privacy-Shield-Abkommen zertifiziert und bietet hierdurch eine Garantie, das europische Datenschutzrecht einzuhalten (https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&status=Active).\n\nGoogle wird diese Informationen in unserem Auftrag benutzen, um die Nutzung unseres Onlineangebotes durch die Nutzer auszuwerten, um Reports ber die Aktivitten innerhalb dieses Onlineangebotes zusammenzustellen und um weitere, mit der Nutzung dieses Onlineangebotes und der Internetnutzung verbundene Dienstleistungen, uns gegenber zu erbringen. Dabei knnen aus den verarbeiteten Daten pseudonyme Nutzungsprofile der Nutzer erstellt werden.\n\nWir setzen Google Analytics in der Ausgestaltung als Universal-Analytics ein. Universal Analytics bezeichnet ein Verfahren von Google Analytics, bei dem die Nutzeranalyse auf Grundlage einer pseudonymen Nutzer-ID erfolgt und damit ein pseudonymes Profil des Nutzers mit Informationen aus der Nutzung verschiedener Gerten erstellt wird (sog. Cross-Device-Tracking).\n\nWir setzen Google Analytics nur mit aktivierter IP-Anonymisierung ein. Das bedeutet, die IP-Adresse der Nutzer wird von Google innerhalb von Mitgliedstaaten der Europischen Union oder in anderen Vertragsstaaten des Abkommens ber den Europischen Wirtschaftsraum gekrzt. Nur in Ausnahmefllen wird die volle IP-Adresse an einen Server von Google in den USA bertragen und dort gekrzt.\n\nDie von dem Browser des Nutzers bermittelte IP-Adresse wird nicht mit anderen Daten von Google zusammengefhrt. Die Nutzer knnen die Speicherung der Cookies durch eine entsprechende Einstellung ihrer Browser-Software verhindern; die Nutzer knnen darber hinaus die Erfassung der durch das Cookie erzeugten und auf ihre Nutzung des Onlineangebotes bezogenen Daten an Google sowie die Verarbeitung dieser Daten durch Google verhindern, indem sie das unter folgendem Link verfgbare Browser-Plugin herunterladen und installieren: http://tools.google.com/dlpage/gaoptout?hl=de.\n\nWeitere Informationen zur Datennutzung durch Google, Einstellungs- und Widerspruchsmglichkeiten erfahren Sie auf den Webseiten von Google: https://www.google.com/intl/de/policies/privacy/partners (Datennutzung durch Google bei Ihrer Nutzung von Websites oder Apps unserer Partner), http://www.google.com/policies/technologies/ads (Datennutzung zu Werbezwecken), http://www.google.de/settings/ads (Informationen verwalten, die Google verwendet, um Ihnen Werbung einzublenden).\n\n#### Youtube\nWir binden die Videos der Plattform YouTube des Anbieters Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA, ein. Datenschutzerklrung: https://www.google.com/policies/privacy/, Opt-Out: https://adssettings.google.com/authenticated.\n\n[Erstellt mit Datenschutz-Generator.de von RA Dr. Thomas Schwenke](https://datenschutz-generator.de/)","updated":"2020-10-30T20:05:40.277Z","path":"imprint/index.html","comments":1,"layout":"page","_id":"ckh0yeqmp0018o2e0df8j125f","content":"<h1 id=\"Imprint\"><a href=\"#Imprint\" class=\"headerlink\" title=\"Imprint\"></a>Imprint</h1><h3 id=\"Legal-Disclosure-amp-Privacy-Statement\"><a href=\"#Legal-Disclosure-amp-Privacy-Statement\" class=\"headerlink\" title=\"Legal Disclosure &amp; Privacy Statement\"></a>Legal Disclosure &amp; Privacy Statement</h3><p>Information in accordance with section 5 German TMG</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Ferdinand Mtsch</span><br><span class=\"line\">Vorholzstrae 11 </span><br><span class=\"line\">76137 Karlsruhe</span><br><span class=\"line\">Germany</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Telephone: 017645641974</span><br><span class=\"line\">E-Mail: ferdinand@muetsch.io</span><br><span class=\"line\">Web: www.muetsch.io</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Disclaimer\"><a href=\"#Disclaimer\" class=\"headerlink\" title=\"Disclaimer\"></a>Disclaimer</h3><h4 id=\"Accountability-for-content\"><a href=\"#Accountability-for-content\" class=\"headerlink\" title=\"Accountability for content\"></a>Accountability for content</h4><p>The contents of our pages have been created with the utmost care. However, we cannot guarantee the contents accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this context, please note that we are accordingly not obliged to monitor merely the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per  8 to 10 of the Telemedia Act (TMG).</p>\n<h4 id=\"Accountability-for-links\"><a href=\"#Accountability-for-links\" class=\"headerlink\" title=\"Accountability for links\"></a>Accountability for links</h4><p>Responsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately. </p>\n<h4 id=\"Copyright\"><a href=\"#Copyright\" class=\"headerlink\" title=\"Copyright\"></a>Copyright</h4><p>Our web pages and their contents are subject to German copyright law. Unless expressly permitted by law ( 44a et seq. of the copyright law), every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are allowed only for private use, so must not serve either directly or indirectly for earnings. Unauthorized utilization of copyrighted works is punishable ( 106 of the copyright law).</p>\n<h4 id=\"General\"><a href=\"#General\" class=\"headerlink\" title=\"General\"></a>General</h4><p>Your personal data (e.g. title, name, house address, e-mail address, phone number, bank details, credit card number) are processed by us only in accordance with the provisions of German data privacy laws. The following provisions describe the type, scope and purpose of collecting, processing and utilizing personal data. This data privacy policy applies only to our web pages. If links on our pages route you to other pages, please inquire there about how your data are handled in such cases.</p>\n<h4 id=\"Inventory-data\"><a href=\"#Inventory-data\" class=\"headerlink\" title=\"Inventory data\"></a>Inventory data</h4><p>(1) Your personal data, insofar as these are necessary for this contractual relationship (inventory data) in terms of its establishment, organization of content and modifications, are used exclusively for fulfilling the contract. For goods to be delivered, for instance, your name and address must be relayed to the supplier of the goods.<br>(2) Without your explicit consent or a legal basis, your personal data are not passed on to third parties outside the scope of fulfilling this contract. After completion of the contract, your data are blocked against further use. After expiry of deadlines as per tax-related and commercial regulations, these data are deleted unless you have expressly consented to their further use.</p>\n<h4 id=\"Information-about-cookies\"><a href=\"#Information-about-cookies\" class=\"headerlink\" title=\"Information about cookies\"></a>Information about cookies</h4><p>(1) To optimize our web presence, we use cookies. These are small text files stored in your computers main memory. These cookies are deleted after you close the browser. Other cookies remain on your computer (long-term cookies) and permit its recognition on your next visit. This allows us to improve your access to our site.<br>(2) You can prevent storage of cookies by choosing a disable cookies option in your browser settings. But this can limit the functionality of our Internet offers as a result.</p>\n<h4 id=\"Disclosure\"><a href=\"#Disclosure\" class=\"headerlink\" title=\"Disclosure\"></a>Disclosure</h4><p>According to the Federal Data Protection Act, you have a right to free-of-charge information about your stored data, and possibly entitlement to correction, blocking or deletion of such data. Inquiries can be directed to the following e-mail addresses: ( <a href=\"mailto:ferdinand@muetsch.io\">ferdinand@muetsch.io</a> )</p>\n<p>Source: <a href=\"http://www.twigg.de/\">twiggs translations</a></p>\n<h3 id=\"German-Privacy-Policy-Datenschutzerklarung\"><a href=\"#German-Privacy-Policy-Datenschutzerklarung\" class=\"headerlink\" title=\"German Privacy Policy (Datenschutzerklrung)\"></a>German Privacy Policy (Datenschutzerklrung)</h3><p>Diese Datenschutzerklrung klrt Sie ber die Art, den Umfang und Zweck der Verarbeitung von personenbezogenen Daten (nachfolgend kurz Daten) innerhalb unseres Onlineangebotes und der mit ihm verbundenen Webseiten, Funktionen und Inhalte sowie externen Onlineprsenzen, wie z.B. unser Social Media Profile auf. (nachfolgend gemeinsam bezeichnet als Onlineangebot). Im Hinblick auf die verwendeten Begrifflichkeiten, wie z.B. Verarbeitung oder Verantwortlicher verweisen wir auf die Definitionen im Art. 4 der Datenschutzgrundverordnung (DSGVO).</p>\n<h4 id=\"Arten-der-verarbeiteten-Daten\"><a href=\"#Arten-der-verarbeiteten-Daten\" class=\"headerlink\" title=\"Arten der verarbeiteten Daten:\"></a>Arten der verarbeiteten Daten:</h4><ul>\n<li>Bestandsdaten (z.B., Namen, Adressen).</li>\n<li>Kontaktdaten (z.B., E-Mail, Telefonnummern).</li>\n<li>Inhaltsdaten (z.B., Texteingaben, Fotografien, Videos).</li>\n<li>Nutzungsdaten (z.B., besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten).</li>\n<li>Meta-/Kommunikationsdaten (z.B., Gerte-Informationen, IP-Adressen).</li>\n</ul>\n<h4 id=\"Kategorien-betroffener-Personen\"><a href=\"#Kategorien-betroffener-Personen\" class=\"headerlink\" title=\"Kategorien betroffener Personen\"></a>Kategorien betroffener Personen</h4><p>Besucher und Nutzer des Onlineangebotes (Nachfolgend bezeichnen wir die betroffenen Personen zusammenfassend auch als Nutzer).</p>\n<h4 id=\"Zweck-der-Verarbeitung\"><a href=\"#Zweck-der-Verarbeitung\" class=\"headerlink\" title=\"Zweck der Verarbeitung\"></a>Zweck der Verarbeitung</h4><ul>\n<li>Zurverfgungstellung des Onlineangebotes, seiner Funktionen und Inhalte.</li>\n<li>Beantwortung von Kontaktanfragen und Kommunikation mit Nutzern.</li>\n<li>Sicherheitsmanahmen.</li>\n<li>Reichweitenmessung/Marketing</li>\n</ul>\n<h4 id=\"Verwendete-Begrifflichkeiten\"><a href=\"#Verwendete-Begrifflichkeiten\" class=\"headerlink\" title=\"Verwendete Begrifflichkeiten\"></a>Verwendete Begrifflichkeiten</h4><p>Personenbezogene Daten sind alle Informationen, die sich auf eine identifizierte oder identifizierbare natrliche Person (im Folgenden betroffene Person) beziehen; als identifizierbar wird eine natrliche Person angesehen, die direkt oder indirekt, insbesondere mittels Zuordnung zu einer Kennung wie einem Namen, zu einer Kennnummer, zu Standortdaten, zu einer Online-Kennung (z.B. Cookie) oder zu einem oder mehreren besonderen Merkmalen identifiziert werden kann, die Ausdruck der physischen, physiologischen, genetischen, psychischen, wirtschaftlichen, kulturellen oder sozialen Identitt dieser natrlichen Person sind.</p>\n<p>Verarbeitung ist jeder mit oder ohne Hilfe automatisierter Verfahren ausgefhrten Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten. Der Begriff reicht weit und umfasst praktisch jeden Umgang mit Daten.</p>\n<p>Als Verantwortlicher wird die natrliche oder juristische Person, Behrde, Einrichtung oder andere Stelle, die allein oder gemeinsam mit anderen ber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten entscheidet, bezeichnet.</p>\n<h4 id=\"Masgebliche-Rechtsgrundlagen\"><a href=\"#Masgebliche-Rechtsgrundlagen\" class=\"headerlink\" title=\"Magebliche Rechtsgrundlagen\"></a>Magebliche Rechtsgrundlagen</h4><p>Nach Magabe des Art. 13 DSGVO teilen wir Ihnen die Rechtsgrundlagen unserer Datenverarbeitungen mit. Sofern die Rechtsgrundlage in der Datenschutzerklrung nicht genannt wird, gilt Folgendes: Die Rechtsgrundlage fr die Einholung von Einwilligungen ist Art. 6 Abs. 1 lit. a und Art. 7 DSGVO, die Rechtsgrundlage fr die Verarbeitung zur Erfllung unserer Leistungen und Durchfhrung vertraglicher Manahmen sowie Beantwortung von Anfragen ist Art. 6 Abs. 1 lit. b DSGVO, die Rechtsgrundlage fr die Verarbeitung zur Erfllung unserer rechtlichen Verpflichtungen ist Art. 6 Abs. 1 lit. c DSGVO, und die Rechtsgrundlage fr die Verarbeitung zur Wahrung unserer berechtigten Interessen ist Art. 6 Abs. 1 lit. f DSGVO. Fr den Fall, dass lebenswichtige Interessen der betroffenen Person oder einer anderen natrlichen Person eine Verarbeitung personenbezogener Daten erforderlich machen, dient Art. 6 Abs. 1 lit. d DSGVO als Rechtsgrundlage.</p>\n<h4 id=\"Zusammenarbeit-mit-Auftragsverarbeitern-und-Dritten\"><a href=\"#Zusammenarbeit-mit-Auftragsverarbeitern-und-Dritten\" class=\"headerlink\" title=\"Zusammenarbeit mit Auftragsverarbeitern und Dritten\"></a>Zusammenarbeit mit Auftragsverarbeitern und Dritten</h4><p>Sofern wir im Rahmen unserer Verarbeitung Daten gegenber anderen Personen und Unternehmen (Auftragsverarbeitern oder Dritten) offenbaren, sie an diese bermitteln oder ihnen sonst Zugriff auf die Daten gewhren, erfolgt dies nur auf Grundlage einer gesetzlichen Erlaubnis (z.B. wenn eine bermittlung der Daten an Dritte, wie an Zahlungsdienstleister, gem. Art. 6 Abs. 1 lit. b DSGVO zur Vertragserfllung erforderlich ist), Sie eingewilligt haben, eine rechtliche Verpflichtung dies vorsieht oder auf Grundlage unserer berechtigten Interessen (z.B. beim Einsatz von Beauftragten, Webhostern, etc.).</p>\n<p>Sofern wir Dritte mit der Verarbeitung von Daten auf Grundlage eines sog. Auftragsverarbeitungsvertrages beauftragen, geschieht dies auf Grundlage des Art. 28 DSGVO.</p>\n<h4 id=\"Ubermittlungen-in-Drittlander\"><a href=\"#Ubermittlungen-in-Drittlander\" class=\"headerlink\" title=\"bermittlungen in Drittlnder\"></a>bermittlungen in Drittlnder</h4><p>Sofern wir Daten in einem Drittland (d.h. auerhalb der Europischen Union (EU) oder des Europischen Wirtschaftsraums (EWR)) verarbeiten oder dies im Rahmen der Inanspruchnahme von Diensten Dritter oder Offenlegung, bzw. bermittlung von Daten an Dritte geschieht, erfolgt dies nur, wenn es zur Erfllung unserer (vor)vertraglichen Pflichten, auf Grundlage Ihrer Einwilligung, aufgrund einer rechtlichen Verpflichtung oder auf Grundlage unserer berechtigten Interessen geschieht. Vorbehaltlich gesetzlicher oder vertraglicher Erlaubnisse, verarbeiten oder lassen wir die Daten in einem Drittland nur beim Vorliegen der besonderen Voraussetzungen der Art. 44 ff. DSGVO verarbeiten. D.h. die Verarbeitung erfolgt z.B. auf Grundlage besonderer Garantien, wie der offiziell anerkannten Feststellung eines der EU entsprechenden Datenschutzniveaus (z.B. fr die USA durch das Privacy Shield) oder Beachtung offiziell anerkannter spezieller vertraglicher Verpflichtungen (so genannte Standardvertragsklauseln).</p>\n<h4 id=\"Rechte-der-betroffenen-Personen\"><a href=\"#Rechte-der-betroffenen-Personen\" class=\"headerlink\" title=\"Rechte der betroffenen Personen\"></a>Rechte der betroffenen Personen</h4><p>Sie haben das Recht, eine Besttigung darber zu verlangen, ob betreffende Daten verarbeitet werden und auf Auskunft ber diese Daten sowie auf weitere Informationen und Kopie der Daten entsprechend Art. 15 DSGVO.</p>\n<p>Sie haben entsprechend. Art. 16 DSGVO das Recht, die Vervollstndigung der Sie betreffenden Daten oder die Berichtigung der Sie betreffenden unrichtigen Daten zu verlangen.</p>\n<p>Sie haben nach Magabe des Art. 17 DSGVO das Recht zu verlangen, dass betreffende Daten unverzglich gelscht werden, bzw. alternativ nach Magabe des Art. 18 DSGVO eine Einschrnkung der Verarbeitung der Daten zu verlangen.</p>\n<p>Sie haben das Recht zu verlangen, dass die Sie betreffenden Daten, die Sie uns bereitgestellt haben nach Magabe des Art. 20 DSGVO zu erhalten und deren bermittlung an andere Verantwortliche zu fordern.</p>\n<p>Sie haben ferner gem. Art. 77 DSGVO das Recht, eine Beschwerde bei der zustndigen Aufsichtsbehrde einzureichen.</p>\n<h4 id=\"Widerrufsrecht\"><a href=\"#Widerrufsrecht\" class=\"headerlink\" title=\"Widerrufsrecht\"></a>Widerrufsrecht</h4><p>Sie haben das Recht, erteilte Einwilligungen gem. Art. 7 Abs. 3 DSGVO mit Wirkung fr die Zukunft zu widerrufen</p>\n<h4 id=\"Widerspruchsrecht\"><a href=\"#Widerspruchsrecht\" class=\"headerlink\" title=\"Widerspruchsrecht\"></a>Widerspruchsrecht</h4><p>Sie knnen der knftigen Verarbeitung der Sie betreffenden Daten nach Magabe des Art. 21 DSGVO jederzeit widersprechen. Der Widerspruch kann insbesondere gegen die Verarbeitung fr Zwecke der Direktwerbung erfolgen.</p>\n<h4 id=\"Cookies-und-Widerspruchsrecht-bei-Direktwerbung\"><a href=\"#Cookies-und-Widerspruchsrecht-bei-Direktwerbung\" class=\"headerlink\" title=\"Cookies und Widerspruchsrecht bei Direktwerbung\"></a>Cookies und Widerspruchsrecht bei Direktwerbung</h4><p>Als Cookies werden kleine Dateien bezeichnet, die auf Rechnern der Nutzer gespeichert werden. Innerhalb der Cookies knnen unterschiedliche Angaben gespeichert werden. Ein Cookie dient primr dazu, die Angaben zu einem Nutzer (bzw. dem Gert auf dem das Cookie gespeichert ist) whrend oder auch nach seinem Besuch innerhalb eines Onlineangebotes zu speichern. Als temporre Cookies, bzw. Session-Cookies oder transiente Cookies, werden Cookies bezeichnet, die gelscht werden, nachdem ein Nutzer ein Onlineangebot verlsst und seinen Browser schliet. In einem solchen Cookie kann z.B. der Inhalt eines Warenkorbs in einem Onlineshop oder ein Login-Staus gespeichert werden. Als permanent oder persistent werden Cookies bezeichnet, die auch nach dem Schlieen des Browsers gespeichert bleiben. So kann z.B. der Login-Status gespeichert werden, wenn die Nutzer diese nach mehreren Tagen aufsuchen. Ebenso knnen in einem solchen Cookie die Interessen der Nutzer gespeichert werden, die fr Reichweitenmessung oder Marketingzwecke verwendet werden. Als Third-Party-Cookie werden Cookies bezeichnet, die von anderen Anbietern als dem Verantwortlichen, der das Onlineangebot betreibt, angeboten werden (andernfalls, wenn es nur dessen Cookies sind spricht man von First-Party Cookies).</p>\n<p>Wir knnen temporre und permanente Cookies einsetzen und klren hierber im Rahmen unserer Datenschutzerklrung auf.</p>\n<p>Falls die Nutzer nicht mchten, dass Cookies auf ihrem Rechner gespeichert werden, werden sie gebeten die entsprechende Option in den Systemeinstellungen ihres Browsers zu deaktivieren. Gespeicherte Cookies knnen in den Systemeinstellungen des Browsers gelscht werden. Der Ausschluss von Cookies kann zu Funktionseinschrnkungen dieses Onlineangebotes fhren.</p>\n<p>Ein genereller Widerspruch gegen den Einsatz der zu Zwecken des Onlinemarketing eingesetzten Cookies kann bei einer Vielzahl der Dienste, vor allem im Fall des Trackings, ber die US-amerikanische Seite <a href=\"http://www.aboutads.info/choices/\">http://www.aboutads.info/choices/</a> oder die EU-Seite <a href=\"http://www.youronlinechoices.com/\">http://www.youronlinechoices.com/</a> erklrt werden. Des Weiteren kann die Speicherung von Cookies mittels deren Abschaltung in den Einstellungen des Browsers erreicht werden. Bitte beachten Sie, dass dann gegebenenfalls nicht alle Funktionen dieses Onlineangebotes genutzt werden knnen.</p>\n<h4 id=\"Loschung-von-Daten\"><a href=\"#Loschung-von-Daten\" class=\"headerlink\" title=\"Lschung von Daten\"></a>Lschung von Daten</h4><p>Die von uns verarbeiteten Daten werden nach Magabe der Art. 17 und 18 DSGVO gelscht oder in ihrer Verarbeitung eingeschrnkt. Sofern nicht im Rahmen dieser Datenschutzerklrung ausdrcklich angegeben, werden die bei uns gespeicherten Daten gelscht, sobald sie fr ihre Zweckbestimmung nicht mehr erforderlich sind und der Lschung keine gesetzlichen Aufbewahrungspflichten entgegenstehen. Sofern die Daten nicht gelscht werden, weil sie fr andere und gesetzlich zulssige Zwecke erforderlich sind, wird deren Verarbeitung eingeschrnkt. D.h. die Daten werden gesperrt und nicht fr andere Zwecke verarbeitet. Das gilt z.B. fr Daten, die aus handels- oder steuerrechtlichen Grnden aufbewahrt werden mssen.</p>\n<p>Nach gesetzlichen Vorgaben in Deutschland erfolgt die Aufbewahrung insbesondere fr 6 Jahre gem  257 Abs. 1 HGB (Handelsbcher, Inventare, Erffnungsbilanzen, Jahresabschlsse, Handelsbriefe, Buchungsbelege, etc.) sowie fr 10 Jahre gem  147 Abs. 1 AO (Bcher, Aufzeichnungen, Lageberichte, Buchungsbelege, Handels- und Geschftsbriefe, Fr Besteuerung relevante Unterlagen, etc.).</p>\n<p>Nach gesetzlichen Vorgaben in sterreich erfolgt die Aufbewahrung insbesondere fr 7 J gem  132 Abs. 1 BAO (Buchhaltungsunterlagen, Belege/Rechnungen, Konten, Belege, Geschftspapiere, Aufstellung der Einnahmen und Ausgaben, etc.), fr 22 Jahre im Zusammenhang mit Grundstcken und fr 10 Jahre bei Unterlagen im Zusammenhang mit elektronisch erbrachten Leistungen, Telekommunikations-, Rundfunk- und Fernsehleistungen, die an Nichtunternehmer in EU-Mitgliedstaaten erbracht werden und fr die der Mini-One-Stop-Shop (MOSS) in Anspruch genommen wird.</p>\n<h4 id=\"Hosting\"><a href=\"#Hosting\" class=\"headerlink\" title=\"Hosting\"></a>Hosting</h4><p>Die von uns in Anspruch genommenen Hosting-Leistungen dienen der Zurverfgungstellung der folgenden Leistungen: Infrastruktur- und Plattformdienstleistungen, Rechenkapazitt, Speicherplatz und Datenbankdienste, Sicherheitsleistungen sowie technische Wartungsleistungen, die wir zum Zwecke des Betriebs dieses Onlineangebotes einsetzen.</p>\n<p>Hierbei verarbeiten wir, bzw. unser Hostinganbieter Bestandsdaten, Kontaktdaten, Inhaltsdaten, Vertragsdaten, Nutzungsdaten, Meta- und Kommunikationsdaten von Kunden, Interessenten und Besuchern dieses Onlineangebotes auf Grundlage unserer berechtigten Interessen an einer effizienten und sicheren Zurverfgungstellung dieses Onlineangebotes gem. Art. 6 Abs. 1 lit. f DSGVO i.V.m. Art. 28 DSGVO (Abschluss Auftragsverarbeitungsvertrag).</p>\n<h4 id=\"Erhebung-von-Zugriffsdaten-und-Logfiles\"><a href=\"#Erhebung-von-Zugriffsdaten-und-Logfiles\" class=\"headerlink\" title=\"Erhebung von Zugriffsdaten und Logfiles\"></a>Erhebung von Zugriffsdaten und Logfiles</h4><p>Wir, bzw. unser Hostinganbieter, erhebt auf Grundlage unserer berechtigten Interessen im Sinne des Art. 6 Abs. 1 lit. f. DSGVO Daten ber jeden Zugriff auf den Server, auf dem sich dieser Dienst befindet (sogenannte Serverlogfiles). Zu den Zugriffsdaten gehren Name der abgerufenen Webseite, Datei, Datum und Uhrzeit des Abrufs, bertragene Datenmenge, Meldung ber erfolgreichen Abruf, Browsertyp nebst Version, das Betriebssystem des Nutzers, Referrer URL (die zuvor besuchte Seite), IP-Adresse und der anfragende Provider.</p>\n<p>Logfile-Informationen werden aus Sicherheitsgrnden (z.B. zur Aufklrung von Missbrauchs- oder Betrugshandlungen) fr die Dauer von maximal 7 Tagen gespeichert und danach gelscht. Daten, deren weitere Aufbewahrung zu Beweiszwecken erforderlich ist, sind bis zur endgltigen Klrung des jeweiligen Vorfalls von der Lschung ausgenommen.</p>\n<h4 id=\"Google-Universal-Analytics\"><a href=\"#Google-Universal-Analytics\" class=\"headerlink\" title=\"Google Universal Analytics\"></a>Google Universal Analytics</h4><p>Wir setzen auf Grundlage unserer berechtigten Interessen (d.h. Interesse an der Analyse, Optimierung und wirtschaftlichem Betrieb unseres Onlineangebotes im Sinne des Art. 6 Abs. 1 lit. f. DSGVO) Google Analytics, einen Webanalysedienst der Google LLC (Google) ein. Google verwendet Cookies. Die durch das Cookie erzeugten Informationen ber Benutzung des Onlineangebotes durch die Nutzer werden in der Regel an einen Server von Google in den USA bertragen und dort gespeichert.</p>\n<p>Google ist unter dem Privacy-Shield-Abkommen zertifiziert und bietet hierdurch eine Garantie, das europische Datenschutzrecht einzuhalten (<a href=\"https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&amp;status=Active\">https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&amp;status=Active</a>).</p>\n<p>Google wird diese Informationen in unserem Auftrag benutzen, um die Nutzung unseres Onlineangebotes durch die Nutzer auszuwerten, um Reports ber die Aktivitten innerhalb dieses Onlineangebotes zusammenzustellen und um weitere, mit der Nutzung dieses Onlineangebotes und der Internetnutzung verbundene Dienstleistungen, uns gegenber zu erbringen. Dabei knnen aus den verarbeiteten Daten pseudonyme Nutzungsprofile der Nutzer erstellt werden.</p>\n<p>Wir setzen Google Analytics in der Ausgestaltung als Universal-Analytics ein. Universal Analytics bezeichnet ein Verfahren von Google Analytics, bei dem die Nutzeranalyse auf Grundlage einer pseudonymen Nutzer-ID erfolgt und damit ein pseudonymes Profil des Nutzers mit Informationen aus der Nutzung verschiedener Gerten erstellt wird (sog. Cross-Device-Tracking).</p>\n<p>Wir setzen Google Analytics nur mit aktivierter IP-Anonymisierung ein. Das bedeutet, die IP-Adresse der Nutzer wird von Google innerhalb von Mitgliedstaaten der Europischen Union oder in anderen Vertragsstaaten des Abkommens ber den Europischen Wirtschaftsraum gekrzt. Nur in Ausnahmefllen wird die volle IP-Adresse an einen Server von Google in den USA bertragen und dort gekrzt.</p>\n<p>Die von dem Browser des Nutzers bermittelte IP-Adresse wird nicht mit anderen Daten von Google zusammengefhrt. Die Nutzer knnen die Speicherung der Cookies durch eine entsprechende Einstellung ihrer Browser-Software verhindern; die Nutzer knnen darber hinaus die Erfassung der durch das Cookie erzeugten und auf ihre Nutzung des Onlineangebotes bezogenen Daten an Google sowie die Verarbeitung dieser Daten durch Google verhindern, indem sie das unter folgendem Link verfgbare Browser-Plugin herunterladen und installieren: <a href=\"http://tools.google.com/dlpage/gaoptout?hl=de\">http://tools.google.com/dlpage/gaoptout?hl=de</a>.</p>\n<p>Weitere Informationen zur Datennutzung durch Google, Einstellungs- und Widerspruchsmglichkeiten erfahren Sie auf den Webseiten von Google: <a href=\"https://www.google.com/intl/de/policies/privacy/partners\">https://www.google.com/intl/de/policies/privacy/partners</a> (Datennutzung durch Google bei Ihrer Nutzung von Websites oder Apps unserer Partner), <a href=\"http://www.google.com/policies/technologies/ads\">http://www.google.com/policies/technologies/ads</a> (Datennutzung zu Werbezwecken), <a href=\"http://www.google.de/settings/ads\">http://www.google.de/settings/ads</a> (Informationen verwalten, die Google verwendet, um Ihnen Werbung einzublenden).</p>\n<h4 id=\"Youtube\"><a href=\"#Youtube\" class=\"headerlink\" title=\"Youtube\"></a>Youtube</h4><p>Wir binden die Videos der Plattform YouTube des Anbieters Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA, ein. Datenschutzerklrung: <a href=\"https://www.google.com/policies/privacy/\">https://www.google.com/policies/privacy/</a>, Opt-Out: <a href=\"https://adssettings.google.com/authenticated\">https://adssettings.google.com/authenticated</a>.</p>\n<p><a href=\"https://datenschutz-generator.de/\">Erstellt mit Datenschutz-Generator.de von RA Dr. Thomas Schwenke</a></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1 id=\"Imprint\"><a href=\"#Imprint\" class=\"headerlink\" title=\"Imprint\"></a>Imprint</h1><h3 id=\"Legal-Disclosure-amp-Privacy-Statement\"><a href=\"#Legal-Disclosure-amp-Privacy-Statement\" class=\"headerlink\" title=\"Legal Disclosure &amp; Privacy Statement\"></a>Legal Disclosure &amp; Privacy Statement</h3><p>Information in accordance with section 5 German TMG</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Ferdinand Mtsch</span><br><span class=\"line\">Vorholzstrae 11 </span><br><span class=\"line\">76137 Karlsruhe</span><br><span class=\"line\">Germany</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Telephone: 017645641974</span><br><span class=\"line\">E-Mail: ferdinand@muetsch.io</span><br><span class=\"line\">Web: www.muetsch.io</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Disclaimer\"><a href=\"#Disclaimer\" class=\"headerlink\" title=\"Disclaimer\"></a>Disclaimer</h3><h4 id=\"Accountability-for-content\"><a href=\"#Accountability-for-content\" class=\"headerlink\" title=\"Accountability for content\"></a>Accountability for content</h4><p>The contents of our pages have been created with the utmost care. However, we cannot guarantee the contents accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this context, please note that we are accordingly not obliged to monitor merely the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per  8 to 10 of the Telemedia Act (TMG).</p>\n<h4 id=\"Accountability-for-links\"><a href=\"#Accountability-for-links\" class=\"headerlink\" title=\"Accountability for links\"></a>Accountability for links</h4><p>Responsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately. </p>\n<h4 id=\"Copyright\"><a href=\"#Copyright\" class=\"headerlink\" title=\"Copyright\"></a>Copyright</h4><p>Our web pages and their contents are subject to German copyright law. Unless expressly permitted by law ( 44a et seq. of the copyright law), every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are allowed only for private use, so must not serve either directly or indirectly for earnings. Unauthorized utilization of copyrighted works is punishable ( 106 of the copyright law).</p>\n<h4 id=\"General\"><a href=\"#General\" class=\"headerlink\" title=\"General\"></a>General</h4><p>Your personal data (e.g. title, name, house address, e-mail address, phone number, bank details, credit card number) are processed by us only in accordance with the provisions of German data privacy laws. The following provisions describe the type, scope and purpose of collecting, processing and utilizing personal data. This data privacy policy applies only to our web pages. If links on our pages route you to other pages, please inquire there about how your data are handled in such cases.</p>\n<h4 id=\"Inventory-data\"><a href=\"#Inventory-data\" class=\"headerlink\" title=\"Inventory data\"></a>Inventory data</h4><p>(1) Your personal data, insofar as these are necessary for this contractual relationship (inventory data) in terms of its establishment, organization of content and modifications, are used exclusively for fulfilling the contract. For goods to be delivered, for instance, your name and address must be relayed to the supplier of the goods.<br>(2) Without your explicit consent or a legal basis, your personal data are not passed on to third parties outside the scope of fulfilling this contract. After completion of the contract, your data are blocked against further use. After expiry of deadlines as per tax-related and commercial regulations, these data are deleted unless you have expressly consented to their further use.</p>\n<h4 id=\"Information-about-cookies\"><a href=\"#Information-about-cookies\" class=\"headerlink\" title=\"Information about cookies\"></a>Information about cookies</h4><p>(1) To optimize our web presence, we use cookies. These are small text files stored in your computers main memory. These cookies are deleted after you close the browser. Other cookies remain on your computer (long-term cookies) and permit its recognition on your next visit. This allows us to improve your access to our site.<br>(2) You can prevent storage of cookies by choosing a disable cookies option in your browser settings. But this can limit the functionality of our Internet offers as a result.</p>\n<h4 id=\"Disclosure\"><a href=\"#Disclosure\" class=\"headerlink\" title=\"Disclosure\"></a>Disclosure</h4><p>According to the Federal Data Protection Act, you have a right to free-of-charge information about your stored data, and possibly entitlement to correction, blocking or deletion of such data. Inquiries can be directed to the following e-mail addresses: ( <a href=\"mailto:ferdinand@muetsch.io\">ferdinand@muetsch.io</a> )</p>\n<p>Source: <a href=\"http://www.twigg.de/\">twiggs translations</a></p>\n<h3 id=\"German-Privacy-Policy-Datenschutzerklarung\"><a href=\"#German-Privacy-Policy-Datenschutzerklarung\" class=\"headerlink\" title=\"German Privacy Policy (Datenschutzerklrung)\"></a>German Privacy Policy (Datenschutzerklrung)</h3><p>Diese Datenschutzerklrung klrt Sie ber die Art, den Umfang und Zweck der Verarbeitung von personenbezogenen Daten (nachfolgend kurz Daten) innerhalb unseres Onlineangebotes und der mit ihm verbundenen Webseiten, Funktionen und Inhalte sowie externen Onlineprsenzen, wie z.B. unser Social Media Profile auf. (nachfolgend gemeinsam bezeichnet als Onlineangebot). Im Hinblick auf die verwendeten Begrifflichkeiten, wie z.B. Verarbeitung oder Verantwortlicher verweisen wir auf die Definitionen im Art. 4 der Datenschutzgrundverordnung (DSGVO).</p>\n<h4 id=\"Arten-der-verarbeiteten-Daten\"><a href=\"#Arten-der-verarbeiteten-Daten\" class=\"headerlink\" title=\"Arten der verarbeiteten Daten:\"></a>Arten der verarbeiteten Daten:</h4><ul>\n<li>Bestandsdaten (z.B., Namen, Adressen).</li>\n<li>Kontaktdaten (z.B., E-Mail, Telefonnummern).</li>\n<li>Inhaltsdaten (z.B., Texteingaben, Fotografien, Videos).</li>\n<li>Nutzungsdaten (z.B., besuchte Webseiten, Interesse an Inhalten, Zugriffszeiten).</li>\n<li>Meta-/Kommunikationsdaten (z.B., Gerte-Informationen, IP-Adressen).</li>\n</ul>\n<h4 id=\"Kategorien-betroffener-Personen\"><a href=\"#Kategorien-betroffener-Personen\" class=\"headerlink\" title=\"Kategorien betroffener Personen\"></a>Kategorien betroffener Personen</h4><p>Besucher und Nutzer des Onlineangebotes (Nachfolgend bezeichnen wir die betroffenen Personen zusammenfassend auch als Nutzer).</p>\n<h4 id=\"Zweck-der-Verarbeitung\"><a href=\"#Zweck-der-Verarbeitung\" class=\"headerlink\" title=\"Zweck der Verarbeitung\"></a>Zweck der Verarbeitung</h4><ul>\n<li>Zurverfgungstellung des Onlineangebotes, seiner Funktionen und Inhalte.</li>\n<li>Beantwortung von Kontaktanfragen und Kommunikation mit Nutzern.</li>\n<li>Sicherheitsmanahmen.</li>\n<li>Reichweitenmessung/Marketing</li>\n</ul>\n<h4 id=\"Verwendete-Begrifflichkeiten\"><a href=\"#Verwendete-Begrifflichkeiten\" class=\"headerlink\" title=\"Verwendete Begrifflichkeiten\"></a>Verwendete Begrifflichkeiten</h4><p>Personenbezogene Daten sind alle Informationen, die sich auf eine identifizierte oder identifizierbare natrliche Person (im Folgenden betroffene Person) beziehen; als identifizierbar wird eine natrliche Person angesehen, die direkt oder indirekt, insbesondere mittels Zuordnung zu einer Kennung wie einem Namen, zu einer Kennnummer, zu Standortdaten, zu einer Online-Kennung (z.B. Cookie) oder zu einem oder mehreren besonderen Merkmalen identifiziert werden kann, die Ausdruck der physischen, physiologischen, genetischen, psychischen, wirtschaftlichen, kulturellen oder sozialen Identitt dieser natrlichen Person sind.</p>\n<p>Verarbeitung ist jeder mit oder ohne Hilfe automatisierter Verfahren ausgefhrten Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten. Der Begriff reicht weit und umfasst praktisch jeden Umgang mit Daten.</p>\n<p>Als Verantwortlicher wird die natrliche oder juristische Person, Behrde, Einrichtung oder andere Stelle, die allein oder gemeinsam mit anderen ber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten entscheidet, bezeichnet.</p>\n<h4 id=\"Masgebliche-Rechtsgrundlagen\"><a href=\"#Masgebliche-Rechtsgrundlagen\" class=\"headerlink\" title=\"Magebliche Rechtsgrundlagen\"></a>Magebliche Rechtsgrundlagen</h4><p>Nach Magabe des Art. 13 DSGVO teilen wir Ihnen die Rechtsgrundlagen unserer Datenverarbeitungen mit. Sofern die Rechtsgrundlage in der Datenschutzerklrung nicht genannt wird, gilt Folgendes: Die Rechtsgrundlage fr die Einholung von Einwilligungen ist Art. 6 Abs. 1 lit. a und Art. 7 DSGVO, die Rechtsgrundlage fr die Verarbeitung zur Erfllung unserer Leistungen und Durchfhrung vertraglicher Manahmen sowie Beantwortung von Anfragen ist Art. 6 Abs. 1 lit. b DSGVO, die Rechtsgrundlage fr die Verarbeitung zur Erfllung unserer rechtlichen Verpflichtungen ist Art. 6 Abs. 1 lit. c DSGVO, und die Rechtsgrundlage fr die Verarbeitung zur Wahrung unserer berechtigten Interessen ist Art. 6 Abs. 1 lit. f DSGVO. Fr den Fall, dass lebenswichtige Interessen der betroffenen Person oder einer anderen natrlichen Person eine Verarbeitung personenbezogener Daten erforderlich machen, dient Art. 6 Abs. 1 lit. d DSGVO als Rechtsgrundlage.</p>\n<h4 id=\"Zusammenarbeit-mit-Auftragsverarbeitern-und-Dritten\"><a href=\"#Zusammenarbeit-mit-Auftragsverarbeitern-und-Dritten\" class=\"headerlink\" title=\"Zusammenarbeit mit Auftragsverarbeitern und Dritten\"></a>Zusammenarbeit mit Auftragsverarbeitern und Dritten</h4><p>Sofern wir im Rahmen unserer Verarbeitung Daten gegenber anderen Personen und Unternehmen (Auftragsverarbeitern oder Dritten) offenbaren, sie an diese bermitteln oder ihnen sonst Zugriff auf die Daten gewhren, erfolgt dies nur auf Grundlage einer gesetzlichen Erlaubnis (z.B. wenn eine bermittlung der Daten an Dritte, wie an Zahlungsdienstleister, gem. Art. 6 Abs. 1 lit. b DSGVO zur Vertragserfllung erforderlich ist), Sie eingewilligt haben, eine rechtliche Verpflichtung dies vorsieht oder auf Grundlage unserer berechtigten Interessen (z.B. beim Einsatz von Beauftragten, Webhostern, etc.).</p>\n<p>Sofern wir Dritte mit der Verarbeitung von Daten auf Grundlage eines sog. Auftragsverarbeitungsvertrages beauftragen, geschieht dies auf Grundlage des Art. 28 DSGVO.</p>\n<h4 id=\"Ubermittlungen-in-Drittlander\"><a href=\"#Ubermittlungen-in-Drittlander\" class=\"headerlink\" title=\"bermittlungen in Drittlnder\"></a>bermittlungen in Drittlnder</h4><p>Sofern wir Daten in einem Drittland (d.h. auerhalb der Europischen Union (EU) oder des Europischen Wirtschaftsraums (EWR)) verarbeiten oder dies im Rahmen der Inanspruchnahme von Diensten Dritter oder Offenlegung, bzw. bermittlung von Daten an Dritte geschieht, erfolgt dies nur, wenn es zur Erfllung unserer (vor)vertraglichen Pflichten, auf Grundlage Ihrer Einwilligung, aufgrund einer rechtlichen Verpflichtung oder auf Grundlage unserer berechtigten Interessen geschieht. Vorbehaltlich gesetzlicher oder vertraglicher Erlaubnisse, verarbeiten oder lassen wir die Daten in einem Drittland nur beim Vorliegen der besonderen Voraussetzungen der Art. 44 ff. DSGVO verarbeiten. D.h. die Verarbeitung erfolgt z.B. auf Grundlage besonderer Garantien, wie der offiziell anerkannten Feststellung eines der EU entsprechenden Datenschutzniveaus (z.B. fr die USA durch das Privacy Shield) oder Beachtung offiziell anerkannter spezieller vertraglicher Verpflichtungen (so genannte Standardvertragsklauseln).</p>\n<h4 id=\"Rechte-der-betroffenen-Personen\"><a href=\"#Rechte-der-betroffenen-Personen\" class=\"headerlink\" title=\"Rechte der betroffenen Personen\"></a>Rechte der betroffenen Personen</h4><p>Sie haben das Recht, eine Besttigung darber zu verlangen, ob betreffende Daten verarbeitet werden und auf Auskunft ber diese Daten sowie auf weitere Informationen und Kopie der Daten entsprechend Art. 15 DSGVO.</p>\n<p>Sie haben entsprechend. Art. 16 DSGVO das Recht, die Vervollstndigung der Sie betreffenden Daten oder die Berichtigung der Sie betreffenden unrichtigen Daten zu verlangen.</p>\n<p>Sie haben nach Magabe des Art. 17 DSGVO das Recht zu verlangen, dass betreffende Daten unverzglich gelscht werden, bzw. alternativ nach Magabe des Art. 18 DSGVO eine Einschrnkung der Verarbeitung der Daten zu verlangen.</p>\n<p>Sie haben das Recht zu verlangen, dass die Sie betreffenden Daten, die Sie uns bereitgestellt haben nach Magabe des Art. 20 DSGVO zu erhalten und deren bermittlung an andere Verantwortliche zu fordern.</p>\n<p>Sie haben ferner gem. Art. 77 DSGVO das Recht, eine Beschwerde bei der zustndigen Aufsichtsbehrde einzureichen.</p>\n<h4 id=\"Widerrufsrecht\"><a href=\"#Widerrufsrecht\" class=\"headerlink\" title=\"Widerrufsrecht\"></a>Widerrufsrecht</h4><p>Sie haben das Recht, erteilte Einwilligungen gem. Art. 7 Abs. 3 DSGVO mit Wirkung fr die Zukunft zu widerrufen</p>\n<h4 id=\"Widerspruchsrecht\"><a href=\"#Widerspruchsrecht\" class=\"headerlink\" title=\"Widerspruchsrecht\"></a>Widerspruchsrecht</h4><p>Sie knnen der knftigen Verarbeitung der Sie betreffenden Daten nach Magabe des Art. 21 DSGVO jederzeit widersprechen. Der Widerspruch kann insbesondere gegen die Verarbeitung fr Zwecke der Direktwerbung erfolgen.</p>\n<h4 id=\"Cookies-und-Widerspruchsrecht-bei-Direktwerbung\"><a href=\"#Cookies-und-Widerspruchsrecht-bei-Direktwerbung\" class=\"headerlink\" title=\"Cookies und Widerspruchsrecht bei Direktwerbung\"></a>Cookies und Widerspruchsrecht bei Direktwerbung</h4><p>Als Cookies werden kleine Dateien bezeichnet, die auf Rechnern der Nutzer gespeichert werden. Innerhalb der Cookies knnen unterschiedliche Angaben gespeichert werden. Ein Cookie dient primr dazu, die Angaben zu einem Nutzer (bzw. dem Gert auf dem das Cookie gespeichert ist) whrend oder auch nach seinem Besuch innerhalb eines Onlineangebotes zu speichern. Als temporre Cookies, bzw. Session-Cookies oder transiente Cookies, werden Cookies bezeichnet, die gelscht werden, nachdem ein Nutzer ein Onlineangebot verlsst und seinen Browser schliet. In einem solchen Cookie kann z.B. der Inhalt eines Warenkorbs in einem Onlineshop oder ein Login-Staus gespeichert werden. Als permanent oder persistent werden Cookies bezeichnet, die auch nach dem Schlieen des Browsers gespeichert bleiben. So kann z.B. der Login-Status gespeichert werden, wenn die Nutzer diese nach mehreren Tagen aufsuchen. Ebenso knnen in einem solchen Cookie die Interessen der Nutzer gespeichert werden, die fr Reichweitenmessung oder Marketingzwecke verwendet werden. Als Third-Party-Cookie werden Cookies bezeichnet, die von anderen Anbietern als dem Verantwortlichen, der das Onlineangebot betreibt, angeboten werden (andernfalls, wenn es nur dessen Cookies sind spricht man von First-Party Cookies).</p>\n<p>Wir knnen temporre und permanente Cookies einsetzen und klren hierber im Rahmen unserer Datenschutzerklrung auf.</p>\n<p>Falls die Nutzer nicht mchten, dass Cookies auf ihrem Rechner gespeichert werden, werden sie gebeten die entsprechende Option in den Systemeinstellungen ihres Browsers zu deaktivieren. Gespeicherte Cookies knnen in den Systemeinstellungen des Browsers gelscht werden. Der Ausschluss von Cookies kann zu Funktionseinschrnkungen dieses Onlineangebotes fhren.</p>\n<p>Ein genereller Widerspruch gegen den Einsatz der zu Zwecken des Onlinemarketing eingesetzten Cookies kann bei einer Vielzahl der Dienste, vor allem im Fall des Trackings, ber die US-amerikanische Seite <a href=\"http://www.aboutads.info/choices/\">http://www.aboutads.info/choices/</a> oder die EU-Seite <a href=\"http://www.youronlinechoices.com/\">http://www.youronlinechoices.com/</a> erklrt werden. Des Weiteren kann die Speicherung von Cookies mittels deren Abschaltung in den Einstellungen des Browsers erreicht werden. Bitte beachten Sie, dass dann gegebenenfalls nicht alle Funktionen dieses Onlineangebotes genutzt werden knnen.</p>\n<h4 id=\"Loschung-von-Daten\"><a href=\"#Loschung-von-Daten\" class=\"headerlink\" title=\"Lschung von Daten\"></a>Lschung von Daten</h4><p>Die von uns verarbeiteten Daten werden nach Magabe der Art. 17 und 18 DSGVO gelscht oder in ihrer Verarbeitung eingeschrnkt. Sofern nicht im Rahmen dieser Datenschutzerklrung ausdrcklich angegeben, werden die bei uns gespeicherten Daten gelscht, sobald sie fr ihre Zweckbestimmung nicht mehr erforderlich sind und der Lschung keine gesetzlichen Aufbewahrungspflichten entgegenstehen. Sofern die Daten nicht gelscht werden, weil sie fr andere und gesetzlich zulssige Zwecke erforderlich sind, wird deren Verarbeitung eingeschrnkt. D.h. die Daten werden gesperrt und nicht fr andere Zwecke verarbeitet. Das gilt z.B. fr Daten, die aus handels- oder steuerrechtlichen Grnden aufbewahrt werden mssen.</p>\n<p>Nach gesetzlichen Vorgaben in Deutschland erfolgt die Aufbewahrung insbesondere fr 6 Jahre gem  257 Abs. 1 HGB (Handelsbcher, Inventare, Erffnungsbilanzen, Jahresabschlsse, Handelsbriefe, Buchungsbelege, etc.) sowie fr 10 Jahre gem  147 Abs. 1 AO (Bcher, Aufzeichnungen, Lageberichte, Buchungsbelege, Handels- und Geschftsbriefe, Fr Besteuerung relevante Unterlagen, etc.).</p>\n<p>Nach gesetzlichen Vorgaben in sterreich erfolgt die Aufbewahrung insbesondere fr 7 J gem  132 Abs. 1 BAO (Buchhaltungsunterlagen, Belege/Rechnungen, Konten, Belege, Geschftspapiere, Aufstellung der Einnahmen und Ausgaben, etc.), fr 22 Jahre im Zusammenhang mit Grundstcken und fr 10 Jahre bei Unterlagen im Zusammenhang mit elektronisch erbrachten Leistungen, Telekommunikations-, Rundfunk- und Fernsehleistungen, die an Nichtunternehmer in EU-Mitgliedstaaten erbracht werden und fr die der Mini-One-Stop-Shop (MOSS) in Anspruch genommen wird.</p>\n<h4 id=\"Hosting\"><a href=\"#Hosting\" class=\"headerlink\" title=\"Hosting\"></a>Hosting</h4><p>Die von uns in Anspruch genommenen Hosting-Leistungen dienen der Zurverfgungstellung der folgenden Leistungen: Infrastruktur- und Plattformdienstleistungen, Rechenkapazitt, Speicherplatz und Datenbankdienste, Sicherheitsleistungen sowie technische Wartungsleistungen, die wir zum Zwecke des Betriebs dieses Onlineangebotes einsetzen.</p>\n<p>Hierbei verarbeiten wir, bzw. unser Hostinganbieter Bestandsdaten, Kontaktdaten, Inhaltsdaten, Vertragsdaten, Nutzungsdaten, Meta- und Kommunikationsdaten von Kunden, Interessenten und Besuchern dieses Onlineangebotes auf Grundlage unserer berechtigten Interessen an einer effizienten und sicheren Zurverfgungstellung dieses Onlineangebotes gem. Art. 6 Abs. 1 lit. f DSGVO i.V.m. Art. 28 DSGVO (Abschluss Auftragsverarbeitungsvertrag).</p>\n<h4 id=\"Erhebung-von-Zugriffsdaten-und-Logfiles\"><a href=\"#Erhebung-von-Zugriffsdaten-und-Logfiles\" class=\"headerlink\" title=\"Erhebung von Zugriffsdaten und Logfiles\"></a>Erhebung von Zugriffsdaten und Logfiles</h4><p>Wir, bzw. unser Hostinganbieter, erhebt auf Grundlage unserer berechtigten Interessen im Sinne des Art. 6 Abs. 1 lit. f. DSGVO Daten ber jeden Zugriff auf den Server, auf dem sich dieser Dienst befindet (sogenannte Serverlogfiles). Zu den Zugriffsdaten gehren Name der abgerufenen Webseite, Datei, Datum und Uhrzeit des Abrufs, bertragene Datenmenge, Meldung ber erfolgreichen Abruf, Browsertyp nebst Version, das Betriebssystem des Nutzers, Referrer URL (die zuvor besuchte Seite), IP-Adresse und der anfragende Provider.</p>\n<p>Logfile-Informationen werden aus Sicherheitsgrnden (z.B. zur Aufklrung von Missbrauchs- oder Betrugshandlungen) fr die Dauer von maximal 7 Tagen gespeichert und danach gelscht. Daten, deren weitere Aufbewahrung zu Beweiszwecken erforderlich ist, sind bis zur endgltigen Klrung des jeweiligen Vorfalls von der Lschung ausgenommen.</p>\n<h4 id=\"Google-Universal-Analytics\"><a href=\"#Google-Universal-Analytics\" class=\"headerlink\" title=\"Google Universal Analytics\"></a>Google Universal Analytics</h4><p>Wir setzen auf Grundlage unserer berechtigten Interessen (d.h. Interesse an der Analyse, Optimierung und wirtschaftlichem Betrieb unseres Onlineangebotes im Sinne des Art. 6 Abs. 1 lit. f. DSGVO) Google Analytics, einen Webanalysedienst der Google LLC (Google) ein. Google verwendet Cookies. Die durch das Cookie erzeugten Informationen ber Benutzung des Onlineangebotes durch die Nutzer werden in der Regel an einen Server von Google in den USA bertragen und dort gespeichert.</p>\n<p>Google ist unter dem Privacy-Shield-Abkommen zertifiziert und bietet hierdurch eine Garantie, das europische Datenschutzrecht einzuhalten (<a href=\"https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&amp;status=Active\">https://www.privacyshield.gov/participant?id=a2zt000000001L5AAI&amp;status=Active</a>).</p>\n<p>Google wird diese Informationen in unserem Auftrag benutzen, um die Nutzung unseres Onlineangebotes durch die Nutzer auszuwerten, um Reports ber die Aktivitten innerhalb dieses Onlineangebotes zusammenzustellen und um weitere, mit der Nutzung dieses Onlineangebotes und der Internetnutzung verbundene Dienstleistungen, uns gegenber zu erbringen. Dabei knnen aus den verarbeiteten Daten pseudonyme Nutzungsprofile der Nutzer erstellt werden.</p>\n<p>Wir setzen Google Analytics in der Ausgestaltung als Universal-Analytics ein. Universal Analytics bezeichnet ein Verfahren von Google Analytics, bei dem die Nutzeranalyse auf Grundlage einer pseudonymen Nutzer-ID erfolgt und damit ein pseudonymes Profil des Nutzers mit Informationen aus der Nutzung verschiedener Gerten erstellt wird (sog. Cross-Device-Tracking).</p>\n<p>Wir setzen Google Analytics nur mit aktivierter IP-Anonymisierung ein. Das bedeutet, die IP-Adresse der Nutzer wird von Google innerhalb von Mitgliedstaaten der Europischen Union oder in anderen Vertragsstaaten des Abkommens ber den Europischen Wirtschaftsraum gekrzt. Nur in Ausnahmefllen wird die volle IP-Adresse an einen Server von Google in den USA bertragen und dort gekrzt.</p>\n<p>Die von dem Browser des Nutzers bermittelte IP-Adresse wird nicht mit anderen Daten von Google zusammengefhrt. Die Nutzer knnen die Speicherung der Cookies durch eine entsprechende Einstellung ihrer Browser-Software verhindern; die Nutzer knnen darber hinaus die Erfassung der durch das Cookie erzeugten und auf ihre Nutzung des Onlineangebotes bezogenen Daten an Google sowie die Verarbeitung dieser Daten durch Google verhindern, indem sie das unter folgendem Link verfgbare Browser-Plugin herunterladen und installieren: <a href=\"http://tools.google.com/dlpage/gaoptout?hl=de\">http://tools.google.com/dlpage/gaoptout?hl=de</a>.</p>\n<p>Weitere Informationen zur Datennutzung durch Google, Einstellungs- und Widerspruchsmglichkeiten erfahren Sie auf den Webseiten von Google: <a href=\"https://www.google.com/intl/de/policies/privacy/partners\">https://www.google.com/intl/de/policies/privacy/partners</a> (Datennutzung durch Google bei Ihrer Nutzung von Websites oder Apps unserer Partner), <a href=\"http://www.google.com/policies/technologies/ads\">http://www.google.com/policies/technologies/ads</a> (Datennutzung zu Werbezwecken), <a href=\"http://www.google.de/settings/ads\">http://www.google.de/settings/ads</a> (Informationen verwalten, die Google verwendet, um Ihnen Werbung einzublenden).</p>\n<h4 id=\"Youtube\"><a href=\"#Youtube\" class=\"headerlink\" title=\"Youtube\"></a>Youtube</h4><p>Wir binden die Videos der Plattform YouTube des Anbieters Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA, ein. Datenschutzerklrung: <a href=\"https://www.google.com/policies/privacy/\">https://www.google.com/policies/privacy/</a>, Opt-Out: <a href=\"https://adssettings.google.com/authenticated\">https://adssettings.google.com/authenticated</a>.</p>\n<p><a href=\"https://datenschutz-generator.de/\">Erstellt mit Datenschutz-Generator.de von RA Dr. Thomas Schwenke</a></p>\n"}],"Post":[{"title":"Anchr.io  Image uploads, bookmarks and shortlink service","date":"2015-12-01T21:47:35.000Z","_content":"\nI want to present my latest project called [Anchr.io](https://anchr.io). It claims to be a useful little helper or toolbox or the like for common tasks on the internet. The idea arised when someday I considered it useful to have a collection of web links or bookmarks  like those you have in Chrome or Firefox  accessible from everywhere without needing to synchronize your browser profile. Just like if youre anywhere on an other PC, find a useful article on the internet and want to save it quickly for later at home. This is what Anchrs **collections** feature does. It saves links  with an optional description for easier search and separated into seperate categories / collections.\n\n![Anchr collections](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/anchr_2.jpg)\n\nThe second feature is to **upload images**. You can easily upload one or more photos from your computer oder mobile device and send them to friends or include them into forum posts or the like. Special with Anchrs image hosting is that users are given the opportunity to client-sided **encrypt images** with a password. As a result no one without the password will ever see their photos content.\n\n![Anchr images](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/anchr_1.jpg)\n\nThe last feature are **shortlinks**  actually not any different from those you know from [goo.gl](http://goo.gl) or [bit.ly](http://bit.ly). Theyre useful if you have a very long web link including many query parameters, access tokens, session ids, special characters and the like and want to share them. Often special characters break the linking or your chat application has a maximum length for hyperlinks. Or you just want to keep clarity in your document or emails. In this case it can be very helpful to make the links as short as any possible  to be precise of a length of 22 bytes with Anchr.\n\nAnchrs focus is on ease and quickness of use  short loading times, flat menu hierarchies, etc.\n\nAt the end just a few words about the technical aspect in addition. Anchr.io is separated clearly into backend and frontend or server- and client-application. Both are kept as modular as possible, trying to follow the MVC pattern. Interface between front- and backend is a REST API. The server is based in Node.js with a bunch of very cool new frameworks while the client is an Angular application. Concluding the development was both fun and such a good practice to me and the result will definitely make some of my daily processes a small little easier. I hope you give the app a try and leave me some feedback to dev(at)anchr.io.","source":"_posts/anchr-io-image-uploads-bookmarks-and-shortlink-service.md","raw":"---\ntitle: 'Anchr.io  Image uploads, bookmarks and shortlink service'\ndate: 2015-12-01 22:47:35\ntags:\n---\n\nI want to present my latest project called [Anchr.io](https://anchr.io). It claims to be a useful little helper or toolbox or the like for common tasks on the internet. The idea arised when someday I considered it useful to have a collection of web links or bookmarks  like those you have in Chrome or Firefox  accessible from everywhere without needing to synchronize your browser profile. Just like if youre anywhere on an other PC, find a useful article on the internet and want to save it quickly for later at home. This is what Anchrs **collections** feature does. It saves links  with an optional description for easier search and separated into seperate categories / collections.\n\n![Anchr collections](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/anchr_2.jpg)\n\nThe second feature is to **upload images**. You can easily upload one or more photos from your computer oder mobile device and send them to friends or include them into forum posts or the like. Special with Anchrs image hosting is that users are given the opportunity to client-sided **encrypt images** with a password. As a result no one without the password will ever see their photos content.\n\n![Anchr images](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/anchr_1.jpg)\n\nThe last feature are **shortlinks**  actually not any different from those you know from [goo.gl](http://goo.gl) or [bit.ly](http://bit.ly). Theyre useful if you have a very long web link including many query parameters, access tokens, session ids, special characters and the like and want to share them. Often special characters break the linking or your chat application has a maximum length for hyperlinks. Or you just want to keep clarity in your document or emails. In this case it can be very helpful to make the links as short as any possible  to be precise of a length of 22 bytes with Anchr.\n\nAnchrs focus is on ease and quickness of use  short loading times, flat menu hierarchies, etc.\n\nAt the end just a few words about the technical aspect in addition. Anchr.io is separated clearly into backend and frontend or server- and client-application. Both are kept as modular as possible, trying to follow the MVC pattern. Interface between front- and backend is a REST API. The server is based in Node.js with a bunch of very cool new frameworks while the client is an Angular application. Concluding the development was both fun and such a good practice to me and the result will definitely make some of my daily processes a small little easier. I hope you give the app a try and leave me some feedback to dev(at)anchr.io.","slug":"anchr-io-image-uploads-bookmarks-and-shortlink-service","published":1,"updated":"2020-10-30T20:05:40.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlb0001o2e0bqqabb0n","content":"<p>I want to present my latest project called <a href=\"https://anchr.io/\">Anchr.io</a>. It claims to be a useful little helper or toolbox or the like for common tasks on the internet. The idea arised when someday I considered it useful to have a collection of web links or bookmarks  like those you have in Chrome or Firefox  accessible from everywhere without needing to synchronize your browser profile. Just like if youre anywhere on an other PC, find a useful article on the internet and want to save it quickly for later at home. This is what Anchrs <strong>collections</strong> feature does. It saves links  with an optional description for easier search and separated into seperate categories / collections.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/anchr_2.jpg\" alt=\"Anchr collections\"></p>\n<p>The second feature is to <strong>upload images</strong>. You can easily upload one or more photos from your computer oder mobile device and send them to friends or include them into forum posts or the like. Special with Anchrs image hosting is that users are given the opportunity to client-sided <strong>encrypt images</strong> with a password. As a result no one without the password will ever see their photos content.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/anchr_1.jpg\" alt=\"Anchr images\"></p>\n<p>The last feature are <strong>shortlinks</strong>  actually not any different from those you know from <a href=\"http://goo.gl/\">goo.gl</a> or <a href=\"http://bit.ly/\">bit.ly</a>. Theyre useful if you have a very long web link including many query parameters, access tokens, session ids, special characters and the like and want to share them. Often special characters break the linking or your chat application has a maximum length for hyperlinks. Or you just want to keep clarity in your document or emails. In this case it can be very helpful to make the links as short as any possible  to be precise of a length of 22 bytes with Anchr.</p>\n<p>Anchrs focus is on ease and quickness of use  short loading times, flat menu hierarchies, etc.</p>\n<p>At the end just a few words about the technical aspect in addition. Anchr.io is separated clearly into backend and frontend or server- and client-application. Both are kept as modular as possible, trying to follow the MVC pattern. Interface between front- and backend is a REST API. The server is based in Node.js with a bunch of very cool new frameworks while the client is an Angular application. Concluding the development was both fun and such a good practice to me and the result will definitely make some of my daily processes a small little easier. I hope you give the app a try and leave me some feedback to dev(at)anchr.io.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>I want to present my latest project called <a href=\"https://anchr.io/\">Anchr.io</a>. It claims to be a useful little helper or toolbox or the like for common tasks on the internet. The idea arised when someday I considered it useful to have a collection of web links or bookmarks  like those you have in Chrome or Firefox  accessible from everywhere without needing to synchronize your browser profile. Just like if youre anywhere on an other PC, find a useful article on the internet and want to save it quickly for later at home. This is what Anchrs <strong>collections</strong> feature does. It saves links  with an optional description for easier search and separated into seperate categories / collections.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/anchr_2.jpg\" alt=\"Anchr collections\"></p>\n<p>The second feature is to <strong>upload images</strong>. You can easily upload one or more photos from your computer oder mobile device and send them to friends or include them into forum posts or the like. Special with Anchrs image hosting is that users are given the opportunity to client-sided <strong>encrypt images</strong> with a password. As a result no one without the password will ever see their photos content.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/anchr_1.jpg\" alt=\"Anchr images\"></p>\n<p>The last feature are <strong>shortlinks</strong>  actually not any different from those you know from <a href=\"http://goo.gl/\">goo.gl</a> or <a href=\"http://bit.ly/\">bit.ly</a>. Theyre useful if you have a very long web link including many query parameters, access tokens, session ids, special characters and the like and want to share them. Often special characters break the linking or your chat application has a maximum length for hyperlinks. Or you just want to keep clarity in your document or emails. In this case it can be very helpful to make the links as short as any possible  to be precise of a length of 22 bytes with Anchr.</p>\n<p>Anchrs focus is on ease and quickness of use  short loading times, flat menu hierarchies, etc.</p>\n<p>At the end just a few words about the technical aspect in addition. Anchr.io is separated clearly into backend and frontend or server- and client-application. Both are kept as modular as possible, trying to follow the MVC pattern. Interface between front- and backend is a REST API. The server is based in Node.js with a bunch of very cool new frameworks while the client is an Angular application. Concluding the development was both fun and such a good practice to me and the result will definitely make some of my daily processes a small little easier. I hope you give the app a try and leave me some feedback to dev(at)anchr.io.</p>\n"},{"title":"Basic benchmarks of 5 different MQTT brokers","date":"2019-07-17T20:16:53.000Z","descriptions":"This article briefly benchmarks the performance of five different, commonly used MQTT brokers.","_content":"\nIn the context of my Master's thesis I conducted a very basic performance comparison of several different MQTT brokers and quickly wanted to share my insights. Please note that these benchmarks are quite superficial only. I did not aim to perform an in-depth evaluation, but rather get a basic idea of their performance in general. \n\n## Setup\n* To perform load tests in a _publish_ scenario, I used [takanorig/mqtt-bench](https://github.com/takanorig/mqtt-bench), an MQTT benchmarking tool written in Go.\n* All tests were run with the options `-count 10000`, `-clients 25` and `-size 4096`, which means to simulate 25 concurrent MQTT clients, each sending 10,000 messages of 4 KBytes size each.\n* Both load testing tool as well as the respective broker were run locally on a 6-core, 12-thread, 3.6 Ghz machine with Ubuntu 18.04.\n* Unless otherwise stated, the brokers were started with default configuration.\n\n## Brokers\nThe following brokers were tested.\n\n| Broker        | Written In | Version | Runtime         | Additional Info                                                                                                                                                                             |\n|---------------|------------|---------|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| hbmqtt        | Python     | 0.8     | CPython 3.6     |                                                                                                                                                                                            |\n| hbmqtt (PyPy) | Python     | 0.8     | PyPy 3.6 v7.1.1 |                                                                                                                                                                                            |\n| HiveMQ CE     | Java       | 2019.1  | Oracle JDK 12   |                                                                                                                                                                                            |\n| Mosca         | JavaScript | 2.8.1   | Node 4.8.0      |                                                                                                                                                                                            |\n| Mosquitto     | C          | 1.6.3   |                |                                                                                                                                                                                            |\n| RabbitMQ      | Erlang     | 3.7.4   |                | enabled_plugins=[rabbitmq_management, rabbitmq_management_agent, rabbitmq_management_visualiser, rabbitmq_shovel_management, rabbitmq_stomp, rabbitmq_mqtt,rabbitmq_web_stomp, rabbitmq_web_mqtt] |\n\n## Results\nThese are the results that I obtained. Higher is better.\n\n![MQTT benchmark results](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/mqtt_bench_2.png)","source":"_posts/basic-benchmarks-of-5-different-mqtt-brokers.md","raw":"---\ntitle: Basic benchmarks of 5 different MQTT brokers\ndate: 2019-07-17 22:16:53\ntags:\ndescriptions: This article briefly benchmarks the performance of five different, commonly used MQTT brokers.\n---\n\nIn the context of my Master's thesis I conducted a very basic performance comparison of several different MQTT brokers and quickly wanted to share my insights. Please note that these benchmarks are quite superficial only. I did not aim to perform an in-depth evaluation, but rather get a basic idea of their performance in general. \n\n## Setup\n* To perform load tests in a _publish_ scenario, I used [takanorig/mqtt-bench](https://github.com/takanorig/mqtt-bench), an MQTT benchmarking tool written in Go.\n* All tests were run with the options `-count 10000`, `-clients 25` and `-size 4096`, which means to simulate 25 concurrent MQTT clients, each sending 10,000 messages of 4 KBytes size each.\n* Both load testing tool as well as the respective broker were run locally on a 6-core, 12-thread, 3.6 Ghz machine with Ubuntu 18.04.\n* Unless otherwise stated, the brokers were started with default configuration.\n\n## Brokers\nThe following brokers were tested.\n\n| Broker        | Written In | Version | Runtime         | Additional Info                                                                                                                                                                             |\n|---------------|------------|---------|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| hbmqtt        | Python     | 0.8     | CPython 3.6     |                                                                                                                                                                                            |\n| hbmqtt (PyPy) | Python     | 0.8     | PyPy 3.6 v7.1.1 |                                                                                                                                                                                            |\n| HiveMQ CE     | Java       | 2019.1  | Oracle JDK 12   |                                                                                                                                                                                            |\n| Mosca         | JavaScript | 2.8.1   | Node 4.8.0      |                                                                                                                                                                                            |\n| Mosquitto     | C          | 1.6.3   |                |                                                                                                                                                                                            |\n| RabbitMQ      | Erlang     | 3.7.4   |                | enabled_plugins=[rabbitmq_management, rabbitmq_management_agent, rabbitmq_management_visualiser, rabbitmq_shovel_management, rabbitmq_stomp, rabbitmq_mqtt,rabbitmq_web_stomp, rabbitmq_web_mqtt] |\n\n## Results\nThese are the results that I obtained. Higher is better.\n\n![MQTT benchmark results](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/mqtt_bench_2.png)","slug":"basic-benchmarks-of-5-different-mqtt-brokers","published":1,"updated":"2020-10-30T20:05:40.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqle0003o2e026h37unq","content":"<p>In the context of my Masters thesis I conducted a very basic performance comparison of several different MQTT brokers and quickly wanted to share my insights. Please note that these benchmarks are quite superficial only. I did not aim to perform an in-depth evaluation, but rather get a basic idea of their performance in general. </p>\n<h2 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h2><ul>\n<li>To perform load tests in a <em>publish</em> scenario, I used <a href=\"https://github.com/takanorig/mqtt-bench\">takanorig/mqtt-bench</a>, an MQTT benchmarking tool written in Go.</li>\n<li>All tests were run with the options <code>-count 10000</code>, <code>-clients 25</code> and <code>-size 4096</code>, which means to simulate 25 concurrent MQTT clients, each sending 10,000 messages of 4 KBytes size each.</li>\n<li>Both load testing tool as well as the respective broker were run locally on a 6-core, 12-thread, 3.6 Ghz machine with Ubuntu 18.04.</li>\n<li>Unless otherwise stated, the brokers were started with default configuration.</li>\n</ul>\n<h2 id=\"Brokers\"><a href=\"#Brokers\" class=\"headerlink\" title=\"Brokers\"></a>Brokers</h2><p>The following brokers were tested.</p>\n<table>\n<thead>\n<tr>\n<th>Broker</th>\n<th>Written In</th>\n<th>Version</th>\n<th>Runtime</th>\n<th>Additional Info</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>hbmqtt</td>\n<td>Python</td>\n<td>0.8</td>\n<td>CPython 3.6</td>\n<td></td>\n</tr>\n<tr>\n<td>hbmqtt (PyPy)</td>\n<td>Python</td>\n<td>0.8</td>\n<td>PyPy 3.6 v7.1.1</td>\n<td></td>\n</tr>\n<tr>\n<td>HiveMQ CE</td>\n<td>Java</td>\n<td>2019.1</td>\n<td>Oracle JDK 12</td>\n<td></td>\n</tr>\n<tr>\n<td>Mosca</td>\n<td>JavaScript</td>\n<td>2.8.1</td>\n<td>Node 4.8.0</td>\n<td></td>\n</tr>\n<tr>\n<td>Mosquitto</td>\n<td>C</td>\n<td>1.6.3</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>RabbitMQ</td>\n<td>Erlang</td>\n<td>3.7.4</td>\n<td></td>\n<td>enabled_plugins=[rabbitmq_management, rabbitmq_management_agent, rabbitmq_management_visualiser, rabbitmq_shovel_management, rabbitmq_stomp, rabbitmq_mqtt,rabbitmq_web_stomp, rabbitmq_web_mqtt]</td>\n</tr>\n</tbody></table>\n<h2 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h2><p>These are the results that I obtained. Higher is better.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/mqtt_bench_2.png\" alt=\"MQTT benchmark results\"></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>In the context of my Masters thesis I conducted a very basic performance comparison of several different MQTT brokers and quickly wanted to share my insights. Please note that these benchmarks are quite superficial only. I did not aim to perform an in-depth evaluation, but rather get a basic idea of their performance in general. </p>\n<h2 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h2><ul>\n<li>To perform load tests in a <em>publish</em> scenario, I used <a href=\"https://github.com/takanorig/mqtt-bench\">takanorig/mqtt-bench</a>, an MQTT benchmarking tool written in Go.</li>\n<li>All tests were run with the options <code>-count 10000</code>, <code>-clients 25</code> and <code>-size 4096</code>, which means to simulate 25 concurrent MQTT clients, each sending 10,000 messages of 4 KBytes size each.</li>\n<li>Both load testing tool as well as the respective broker were run locally on a 6-core, 12-thread, 3.6 Ghz machine with Ubuntu 18.04.</li>\n<li>Unless otherwise stated, the brokers were started with default configuration.</li>\n</ul>\n<h2 id=\"Brokers\"><a href=\"#Brokers\" class=\"headerlink\" title=\"Brokers\"></a>Brokers</h2><p>The following brokers were tested.</p>\n<table>\n<thead>\n<tr>\n<th>Broker</th>\n<th>Written In</th>\n<th>Version</th>\n<th>Runtime</th>\n<th>Additional Info</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>hbmqtt</td>\n<td>Python</td>\n<td>0.8</td>\n<td>CPython 3.6</td>\n<td></td>\n</tr>\n<tr>\n<td>hbmqtt (PyPy)</td>\n<td>Python</td>\n<td>0.8</td>\n<td>PyPy 3.6 v7.1.1</td>\n<td></td>\n</tr>\n<tr>\n<td>HiveMQ CE</td>\n<td>Java</td>\n<td>2019.1</td>\n<td>Oracle JDK 12</td>\n<td></td>\n</tr>\n<tr>\n<td>Mosca</td>\n<td>JavaScript</td>\n<td>2.8.1</td>\n<td>Node 4.8.0</td>\n<td></td>\n</tr>\n<tr>\n<td>Mosquitto</td>\n<td>C</td>\n<td>1.6.3</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>RabbitMQ</td>\n<td>Erlang</td>\n<td>3.7.4</td>\n<td></td>\n<td>enabled_plugins=[rabbitmq_management, rabbitmq_management_agent, rabbitmq_management_visualiser, rabbitmq_shovel_management, rabbitmq_stomp, rabbitmq_mqtt,rabbitmq_web_stomp, rabbitmq_web_mqtt]</td>\n</tr>\n</tbody></table>\n<h2 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h2><p>These are the results that I obtained. Higher is better.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/mqtt_bench_2.png\" alt=\"MQTT benchmark results\"></p>\n"},{"title":"Building a cloud-native web scraper using 8 different AWS services","date":"2018-12-01T13:52:28.000Z","_content":"\nSounds like overkill, right? It is. Obviously, you don't need a whole bunch of cloud services to build a simple web scraper, especially since there is already a lot of them out there. However, this describes my personal journey of exploring cloud-native development on AWS by building a simple, yet useful application.\n\n# The Goal\nWhat I wanted to build was a web scraper that runs entirely on cloud infrastructure. More precisely, I wanted to build a scraper using [Selenium WebDriver](https://www.seleniumhq.org/projects/webdriver/), because it should be able to scrape not only static HTML pages, but also dynamic, JavaScript-powered single-page apps. With this requirement in mind, a simple Python script incorporating [requests](http://docs.python-requests.org/en/master/) or [urllib](https://docs.python.org/3.7/library/urllib.html#module-urllib) is not sufficient anymore. Instead, you would need at least a headless browser (like Firefox, Chrome or the out-dated [PhantomJS](http://phantomjs.org/)).\n\n## Example Use Case\nTo get a better idea of what I had been building, imagine the following use case. You are a student and your university provides a JavaScript-based website where exam results are published as soon as they are available. To retrieve your results you need to enter your student id and select a department from a drop-down list. You are curious about your grade in the most recent exam, but since you're lazy, you do not want manually check the website every day. That's where a totally over-engineered web scraper comes to play.\n\n## Requirements \nHere are some notes on what the application was supposed to be able to do (and how) - just to get a slightly better understanding.\n* Different crawl tasks are pre-defined as WebDriver scripts in Java.\n* Users can add subscriptions for pre-defined crawling jobs. They will result in a certain crawl task being executed with certain parameters (e.g. form input field values to be filled by Selenium) at a regular interval (e.g. every 24 hours).\n* When adding a subscription for a certain task (corresponding to a certain webpage), users provide their e-mail address and are getting notified once the scraper detects a change.\n* The state of a web-site is persisted in the Dynamo item for the respective subscription and compared to the most recent state that is retrieved when the scraper runs.\n\n# Architecture\nBelow you can see a high-level overview of all components and the corresponding AWS services, as well as basic interactions between the components. Please note that the diagram is not proper UML, but it should help getting an idea of the overall architecture. And it looks kind of fancy at first sight.)\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/crawlbuddy2.png)\n([Click to view large](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/crawlbuddy2.png))\n\n\n## AWS services\nThe cloud services used are:\n* **AWS Lambda** for Serverless NodeJS functions to perform stateless tasks\n* **AWS Fargate** as an on-demand Docker container runtime to execute longer-running, more resource-intense tasks\n* **AWS DynamoDB** as a schema-less data store to manage subscriptions and website states\n* **AWS SQS** as a asynchronous messaging channel for communication between components and to trigger Lambdas\n* **AWS S3** to host a static HTML page containing a form to be used for adding new subscriptions thorugh a UI\n* **AWS API Gateway** to provide an HTTP endpoint for adding new subscriptions. It is called by the \"frontend\"-side script and subsequently triggers a Lambda to add the new subscription to Dynamo.\n* **AWS CloudWatch** to regularly trigger the execution of the scraper on Fargate in a crontab-like fashion\n* **AWS SES** to send notification e-mails when something has changed\n\n## Components\nLet's take a very brief look at what the several components are doing.\n\n## crawling-core\nThis is essentially the core part of the whole application, the actual scraper / crawler. I implemented it as a **Java** command-line application, which has the WebDriver as a dependency to be able to interact with webpages dynamically.\n\nThe program is responsible for executing an actual crawling task itself, for detecting changes by comparing the task's result to the latest state in the database, for updating the database item and for potentially pushing a change notification message to a queue. \n\nScraping tasks are defined as Java classes extending the `AbstractTask` class. For instance, you could create sub-classes `AmazonPriceTask` and `ExamsResultsTask`. While implementing these classes, you would essentially need to define input parameters (e.g. your student ID number to be filled in to a search form on the university website later on) for the crawling script and a series of commands in the `run()` method to be executed by WebDriver. \n\n`crawling-core` is developed as a standalone Java command-line application, where the name of the task to be executed (e.g. `EXAM_RESULT_TASK`) and the input parameters (e.g. `VAR_STUDENT_ID`, `VAR_DEPARTMENT_NAME`) are provided as run arguments or environment variables.\n\nIn addition to the Java program, packaged as a simple JAR, we need a browser the WebDriver can use to browse the web. I decided to use Firefox in headless mode. Ultimately, the JAR and the Firefox binary are packaged together into a Docker images based on [selenium/standalone-firefox](https://hub.docker.com/r/selenium/standalone-firefox/) and pushed to **AWS ECR** (AWS' container registry). \n\nTo execute a scraping task, e.g. our `ExamsResultsTask`, **AWS Fargate** will pull the latest Docker image from the registry, create a new container from it, set the required input parameters as environment variables and eventually run the entrypoint, which is our JAR file. \n\n##  crawling-crawl\n... is a very simple Lambda function written in **NodeJS**, which is responsible for launching a crawling job. It is triggered regularly through a **CloudWatch** event. First, it fetches all crawling tasks from Dynamo. A crawling task is a unique combination of a task name and a set of input parameters. Afterwards it requests Fargate to start a new new instance of `crawling-core` for every task and passes the input parameters contained in the database item. \n\n##  crawling-notify\n... is another Lambda, which stands at the very end of one iteration of our crawling process. It is invoked through messages in the `crawling-changes` **SQS** queue and responsible for sending out notification e-mails to subscribers. It reads change information from the invoking event, including task name, the subscriber's e-mail address and the task's output parameters (e.g. your exam grade) and composes an e-mail message that eventually gets sent through the Simple E-Mail Service (**SES**).\n\n##  crawling-web-subscribe\nThe last of our three Lambdas is not directly related to the crawling itself. Instead, it is used for handling HTTP requests sent by a user who wants to add a new subscription. Initiated by a simple script on an HTML page called `subscribe.html`, a POST is sent to the `/subscriptions` endpoint in the **API Gateway**, then forwarded to the `crawling-web-subscribe` and ultimately added to the Dynamo database as a new item in the `subscriptions` table.\n\n# Okay, cool. And now?\nAs I mentioned before, this project was rather a learning playground for me than a reasonable architecture for a web scraper. Although this one should, in fact, be quite scalable, you could definitely build a scraper script with much less effort. However, I learned a lot about cloud development and AWS specifically and I really like how easy things can be and how well all these different components play together. Maybe I was able to encourage the less cloud-experienced developers among you to start playing around with AWS (or some other cloud provider) as well and I hope you liked my (very spontaneously written) article. ","source":"_posts/building-a-cloud-native-web-scraper-using-8-different-aws-services.md","raw":"---\ntitle: Building a cloud-native web scraper using 8 different AWS services\ndate: 2018-12-01 14:52:28\ntags:\n---\n\nSounds like overkill, right? It is. Obviously, you don't need a whole bunch of cloud services to build a simple web scraper, especially since there is already a lot of them out there. However, this describes my personal journey of exploring cloud-native development on AWS by building a simple, yet useful application.\n\n# The Goal\nWhat I wanted to build was a web scraper that runs entirely on cloud infrastructure. More precisely, I wanted to build a scraper using [Selenium WebDriver](https://www.seleniumhq.org/projects/webdriver/), because it should be able to scrape not only static HTML pages, but also dynamic, JavaScript-powered single-page apps. With this requirement in mind, a simple Python script incorporating [requests](http://docs.python-requests.org/en/master/) or [urllib](https://docs.python.org/3.7/library/urllib.html#module-urllib) is not sufficient anymore. Instead, you would need at least a headless browser (like Firefox, Chrome or the out-dated [PhantomJS](http://phantomjs.org/)).\n\n## Example Use Case\nTo get a better idea of what I had been building, imagine the following use case. You are a student and your university provides a JavaScript-based website where exam results are published as soon as they are available. To retrieve your results you need to enter your student id and select a department from a drop-down list. You are curious about your grade in the most recent exam, but since you're lazy, you do not want manually check the website every day. That's where a totally over-engineered web scraper comes to play.\n\n## Requirements \nHere are some notes on what the application was supposed to be able to do (and how) - just to get a slightly better understanding.\n* Different crawl tasks are pre-defined as WebDriver scripts in Java.\n* Users can add subscriptions for pre-defined crawling jobs. They will result in a certain crawl task being executed with certain parameters (e.g. form input field values to be filled by Selenium) at a regular interval (e.g. every 24 hours).\n* When adding a subscription for a certain task (corresponding to a certain webpage), users provide their e-mail address and are getting notified once the scraper detects a change.\n* The state of a web-site is persisted in the Dynamo item for the respective subscription and compared to the most recent state that is retrieved when the scraper runs.\n\n# Architecture\nBelow you can see a high-level overview of all components and the corresponding AWS services, as well as basic interactions between the components. Please note that the diagram is not proper UML, but it should help getting an idea of the overall architecture. And it looks kind of fancy at first sight.)\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/crawlbuddy2.png)\n([Click to view large](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/crawlbuddy2.png))\n\n\n## AWS services\nThe cloud services used are:\n* **AWS Lambda** for Serverless NodeJS functions to perform stateless tasks\n* **AWS Fargate** as an on-demand Docker container runtime to execute longer-running, more resource-intense tasks\n* **AWS DynamoDB** as a schema-less data store to manage subscriptions and website states\n* **AWS SQS** as a asynchronous messaging channel for communication between components and to trigger Lambdas\n* **AWS S3** to host a static HTML page containing a form to be used for adding new subscriptions thorugh a UI\n* **AWS API Gateway** to provide an HTTP endpoint for adding new subscriptions. It is called by the \"frontend\"-side script and subsequently triggers a Lambda to add the new subscription to Dynamo.\n* **AWS CloudWatch** to regularly trigger the execution of the scraper on Fargate in a crontab-like fashion\n* **AWS SES** to send notification e-mails when something has changed\n\n## Components\nLet's take a very brief look at what the several components are doing.\n\n## crawling-core\nThis is essentially the core part of the whole application, the actual scraper / crawler. I implemented it as a **Java** command-line application, which has the WebDriver as a dependency to be able to interact with webpages dynamically.\n\nThe program is responsible for executing an actual crawling task itself, for detecting changes by comparing the task's result to the latest state in the database, for updating the database item and for potentially pushing a change notification message to a queue. \n\nScraping tasks are defined as Java classes extending the `AbstractTask` class. For instance, you could create sub-classes `AmazonPriceTask` and `ExamsResultsTask`. While implementing these classes, you would essentially need to define input parameters (e.g. your student ID number to be filled in to a search form on the university website later on) for the crawling script and a series of commands in the `run()` method to be executed by WebDriver. \n\n`crawling-core` is developed as a standalone Java command-line application, where the name of the task to be executed (e.g. `EXAM_RESULT_TASK`) and the input parameters (e.g. `VAR_STUDENT_ID`, `VAR_DEPARTMENT_NAME`) are provided as run arguments or environment variables.\n\nIn addition to the Java program, packaged as a simple JAR, we need a browser the WebDriver can use to browse the web. I decided to use Firefox in headless mode. Ultimately, the JAR and the Firefox binary are packaged together into a Docker images based on [selenium/standalone-firefox](https://hub.docker.com/r/selenium/standalone-firefox/) and pushed to **AWS ECR** (AWS' container registry). \n\nTo execute a scraping task, e.g. our `ExamsResultsTask`, **AWS Fargate** will pull the latest Docker image from the registry, create a new container from it, set the required input parameters as environment variables and eventually run the entrypoint, which is our JAR file. \n\n##  crawling-crawl\n... is a very simple Lambda function written in **NodeJS**, which is responsible for launching a crawling job. It is triggered regularly through a **CloudWatch** event. First, it fetches all crawling tasks from Dynamo. A crawling task is a unique combination of a task name and a set of input parameters. Afterwards it requests Fargate to start a new new instance of `crawling-core` for every task and passes the input parameters contained in the database item. \n\n##  crawling-notify\n... is another Lambda, which stands at the very end of one iteration of our crawling process. It is invoked through messages in the `crawling-changes` **SQS** queue and responsible for sending out notification e-mails to subscribers. It reads change information from the invoking event, including task name, the subscriber's e-mail address and the task's output parameters (e.g. your exam grade) and composes an e-mail message that eventually gets sent through the Simple E-Mail Service (**SES**).\n\n##  crawling-web-subscribe\nThe last of our three Lambdas is not directly related to the crawling itself. Instead, it is used for handling HTTP requests sent by a user who wants to add a new subscription. Initiated by a simple script on an HTML page called `subscribe.html`, a POST is sent to the `/subscriptions` endpoint in the **API Gateway**, then forwarded to the `crawling-web-subscribe` and ultimately added to the Dynamo database as a new item in the `subscriptions` table.\n\n# Okay, cool. And now?\nAs I mentioned before, this project was rather a learning playground for me than a reasonable architecture for a web scraper. Although this one should, in fact, be quite scalable, you could definitely build a scraper script with much less effort. However, I learned a lot about cloud development and AWS specifically and I really like how easy things can be and how well all these different components play together. Maybe I was able to encourage the less cloud-experienced developers among you to start playing around with AWS (or some other cloud provider) as well and I hope you liked my (very spontaneously written) article. ","slug":"building-a-cloud-native-web-scraper-using-8-different-aws-services","published":1,"updated":"2020-10-30T20:05:40.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlf0004o2e06q326qcs","content":"<p>Sounds like overkill, right? It is. Obviously, you dont need a whole bunch of cloud services to build a simple web scraper, especially since there is already a lot of them out there. However, this describes my personal journey of exploring cloud-native development on AWS by building a simple, yet useful application.</p>\n<h1 id=\"The-Goal\"><a href=\"#The-Goal\" class=\"headerlink\" title=\"The Goal\"></a>The Goal</h1><p>What I wanted to build was a web scraper that runs entirely on cloud infrastructure. More precisely, I wanted to build a scraper using <a href=\"https://www.seleniumhq.org/projects/webdriver/\">Selenium WebDriver</a>, because it should be able to scrape not only static HTML pages, but also dynamic, JavaScript-powered single-page apps. With this requirement in mind, a simple Python script incorporating <a href=\"http://docs.python-requests.org/en/master/\">requests</a> or <a href=\"https://docs.python.org/3.7/library/urllib.html#module-urllib\">urllib</a> is not sufficient anymore. Instead, you would need at least a headless browser (like Firefox, Chrome or the out-dated <a href=\"http://phantomjs.org/\">PhantomJS</a>).</p>\n<h2 id=\"Example-Use-Case\"><a href=\"#Example-Use-Case\" class=\"headerlink\" title=\"Example Use Case\"></a>Example Use Case</h2><p>To get a better idea of what I had been building, imagine the following use case. You are a student and your university provides a JavaScript-based website where exam results are published as soon as they are available. To retrieve your results you need to enter your student id and select a department from a drop-down list. You are curious about your grade in the most recent exam, but since youre lazy, you do not want manually check the website every day. Thats where a totally over-engineered web scraper comes to play.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><p>Here are some notes on what the application was supposed to be able to do (and how) - just to get a slightly better understanding.</p>\n<ul>\n<li>Different crawl tasks are pre-defined as WebDriver scripts in Java.</li>\n<li>Users can add subscriptions for pre-defined crawling jobs. They will result in a certain crawl task being executed with certain parameters (e.g. form input field values to be filled by Selenium) at a regular interval (e.g. every 24 hours).</li>\n<li>When adding a subscription for a certain task (corresponding to a certain webpage), users provide their e-mail address and are getting notified once the scraper detects a change.</li>\n<li>The state of a web-site is persisted in the Dynamo item for the respective subscription and compared to the most recent state that is retrieved when the scraper runs.</li>\n</ul>\n<h1 id=\"Architecture\"><a href=\"#Architecture\" class=\"headerlink\" title=\"Architecture\"></a>Architecture</h1><p>Below you can see a high-level overview of all components and the corresponding AWS services, as well as basic interactions between the components. Please note that the diagram is not proper UML, but it should help getting an idea of the overall architecture. And it looks kind of fancy at first sight.)</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/crawlbuddy2.png\"><br>(<a href=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/crawlbuddy2.png\">Click to view large</a>)</p>\n<h2 id=\"AWS-services\"><a href=\"#AWS-services\" class=\"headerlink\" title=\"AWS services\"></a>AWS services</h2><p>The cloud services used are:</p>\n<ul>\n<li><strong>AWS Lambda</strong> for Serverless NodeJS functions to perform stateless tasks</li>\n<li><strong>AWS Fargate</strong> as an on-demand Docker container runtime to execute longer-running, more resource-intense tasks</li>\n<li><strong>AWS DynamoDB</strong> as a schema-less data store to manage subscriptions and website states</li>\n<li><strong>AWS SQS</strong> as a asynchronous messaging channel for communication between components and to trigger Lambdas</li>\n<li><strong>AWS S3</strong> to host a static HTML page containing a form to be used for adding new subscriptions thorugh a UI</li>\n<li><strong>AWS API Gateway</strong> to provide an HTTP endpoint for adding new subscriptions. It is called by the frontend-side script and subsequently triggers a Lambda to add the new subscription to Dynamo.</li>\n<li><strong>AWS CloudWatch</strong> to regularly trigger the execution of the scraper on Fargate in a crontab-like fashion</li>\n<li><strong>AWS SES</strong> to send notification e-mails when something has changed</li>\n</ul>\n<h2 id=\"Components\"><a href=\"#Components\" class=\"headerlink\" title=\"Components\"></a>Components</h2><p>Lets take a very brief look at what the several components are doing.</p>\n<h2 id=\"crawling-core\"><a href=\"#crawling-core\" class=\"headerlink\" title=\"crawling-core\"></a>crawling-core</h2><p>This is essentially the core part of the whole application, the actual scraper / crawler. I implemented it as a <strong>Java</strong> command-line application, which has the WebDriver as a dependency to be able to interact with webpages dynamically.</p>\n<p>The program is responsible for executing an actual crawling task itself, for detecting changes by comparing the tasks result to the latest state in the database, for updating the database item and for potentially pushing a change notification message to a queue. </p>\n<p>Scraping tasks are defined as Java classes extending the <code>AbstractTask</code> class. For instance, you could create sub-classes <code>AmazonPriceTask</code> and <code>ExamsResultsTask</code>. While implementing these classes, you would essentially need to define input parameters (e.g. your student ID number to be filled in to a search form on the university website later on) for the crawling script and a series of commands in the <code>run()</code> method to be executed by WebDriver. </p>\n<p><code>crawling-core</code> is developed as a standalone Java command-line application, where the name of the task to be executed (e.g. <code>EXAM_RESULT_TASK</code>) and the input parameters (e.g. <code>VAR_STUDENT_ID</code>, <code>VAR_DEPARTMENT_NAME</code>) are provided as run arguments or environment variables.</p>\n<p>In addition to the Java program, packaged as a simple JAR, we need a browser the WebDriver can use to browse the web. I decided to use Firefox in headless mode. Ultimately, the JAR and the Firefox binary are packaged together into a Docker images based on <a href=\"https://hub.docker.com/r/selenium/standalone-firefox/\">selenium/standalone-firefox</a> and pushed to <strong>AWS ECR</strong> (AWS container registry). </p>\n<p>To execute a scraping task, e.g. our <code>ExamsResultsTask</code>, <strong>AWS Fargate</strong> will pull the latest Docker image from the registry, create a new container from it, set the required input parameters as environment variables and eventually run the entrypoint, which is our JAR file. </p>\n<h2 id=\"-crawling-crawl\"><a href=\"#-crawling-crawl\" class=\"headerlink\" title=\" crawling-crawl\"></a> crawling-crawl</h2><p> is a very simple Lambda function written in <strong>NodeJS</strong>, which is responsible for launching a crawling job. It is triggered regularly through a <strong>CloudWatch</strong> event. First, it fetches all crawling tasks from Dynamo. A crawling task is a unique combination of a task name and a set of input parameters. Afterwards it requests Fargate to start a new new instance of <code>crawling-core</code> for every task and passes the input parameters contained in the database item. </p>\n<h2 id=\"-crawling-notify\"><a href=\"#-crawling-notify\" class=\"headerlink\" title=\" crawling-notify\"></a> crawling-notify</h2><p> is another Lambda, which stands at the very end of one iteration of our crawling process. It is invoked through messages in the <code>crawling-changes</code> <strong>SQS</strong> queue and responsible for sending out notification e-mails to subscribers. It reads change information from the invoking event, including task name, the subscribers e-mail address and the tasks output parameters (e.g. your exam grade) and composes an e-mail message that eventually gets sent through the Simple E-Mail Service (<strong>SES</strong>).</p>\n<h2 id=\"-crawling-web-subscribe\"><a href=\"#-crawling-web-subscribe\" class=\"headerlink\" title=\" crawling-web-subscribe\"></a> crawling-web-subscribe</h2><p>The last of our three Lambdas is not directly related to the crawling itself. Instead, it is used for handling HTTP requests sent by a user who wants to add a new subscription. Initiated by a simple script on an HTML page called <code>subscribe.html</code>, a POST is sent to the <code>/subscriptions</code> endpoint in the <strong>API Gateway</strong>, then forwarded to the <code>crawling-web-subscribe</code> and ultimately added to the Dynamo database as a new item in the <code>subscriptions</code> table.</p>\n<h1 id=\"Okay-cool-And-now\"><a href=\"#Okay-cool-And-now\" class=\"headerlink\" title=\"Okay, cool. And now?\"></a>Okay, cool. And now?</h1><p>As I mentioned before, this project was rather a learning playground for me than a reasonable architecture for a web scraper. Although this one should, in fact, be quite scalable, you could definitely build a scraper script with much less effort. However, I learned a lot about cloud development and AWS specifically and I really like how easy things can be and how well all these different components play together. Maybe I was able to encourage the less cloud-experienced developers among you to start playing around with AWS (or some other cloud provider) as well and I hope you liked my (very spontaneously written) article. </p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Sounds like overkill, right? It is. Obviously, you dont need a whole bunch of cloud services to build a simple web scraper, especially since there is already a lot of them out there. However, this describes my personal journey of exploring cloud-native development on AWS by building a simple, yet useful application.</p>\n<h1 id=\"The-Goal\"><a href=\"#The-Goal\" class=\"headerlink\" title=\"The Goal\"></a>The Goal</h1><p>What I wanted to build was a web scraper that runs entirely on cloud infrastructure. More precisely, I wanted to build a scraper using <a href=\"https://www.seleniumhq.org/projects/webdriver/\">Selenium WebDriver</a>, because it should be able to scrape not only static HTML pages, but also dynamic, JavaScript-powered single-page apps. With this requirement in mind, a simple Python script incorporating <a href=\"http://docs.python-requests.org/en/master/\">requests</a> or <a href=\"https://docs.python.org/3.7/library/urllib.html#module-urllib\">urllib</a> is not sufficient anymore. Instead, you would need at least a headless browser (like Firefox, Chrome or the out-dated <a href=\"http://phantomjs.org/\">PhantomJS</a>).</p>\n<h2 id=\"Example-Use-Case\"><a href=\"#Example-Use-Case\" class=\"headerlink\" title=\"Example Use Case\"></a>Example Use Case</h2><p>To get a better idea of what I had been building, imagine the following use case. You are a student and your university provides a JavaScript-based website where exam results are published as soon as they are available. To retrieve your results you need to enter your student id and select a department from a drop-down list. You are curious about your grade in the most recent exam, but since youre lazy, you do not want manually check the website every day. Thats where a totally over-engineered web scraper comes to play.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><p>Here are some notes on what the application was supposed to be able to do (and how) - just to get a slightly better understanding.</p>\n<ul>\n<li>Different crawl tasks are pre-defined as WebDriver scripts in Java.</li>\n<li>Users can add subscriptions for pre-defined crawling jobs. They will result in a certain crawl task being executed with certain parameters (e.g. form input field values to be filled by Selenium) at a regular interval (e.g. every 24 hours).</li>\n<li>When adding a subscription for a certain task (corresponding to a certain webpage), users provide their e-mail address and are getting notified once the scraper detects a change.</li>\n<li>The state of a web-site is persisted in the Dynamo item for the respective subscription and compared to the most recent state that is retrieved when the scraper runs.</li>\n</ul>\n<h1 id=\"Architecture\"><a href=\"#Architecture\" class=\"headerlink\" title=\"Architecture\"></a>Architecture</h1><p>Below you can see a high-level overview of all components and the corresponding AWS services, as well as basic interactions between the components. Please note that the diagram is not proper UML, but it should help getting an idea of the overall architecture. And it looks kind of fancy at first sight.)</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/crawlbuddy2.png\"><br>(<a href=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/crawlbuddy2.png\">Click to view large</a>)</p>\n<h2 id=\"AWS-services\"><a href=\"#AWS-services\" class=\"headerlink\" title=\"AWS services\"></a>AWS services</h2><p>The cloud services used are:</p>\n<ul>\n<li><strong>AWS Lambda</strong> for Serverless NodeJS functions to perform stateless tasks</li>\n<li><strong>AWS Fargate</strong> as an on-demand Docker container runtime to execute longer-running, more resource-intense tasks</li>\n<li><strong>AWS DynamoDB</strong> as a schema-less data store to manage subscriptions and website states</li>\n<li><strong>AWS SQS</strong> as a asynchronous messaging channel for communication between components and to trigger Lambdas</li>\n<li><strong>AWS S3</strong> to host a static HTML page containing a form to be used for adding new subscriptions thorugh a UI</li>\n<li><strong>AWS API Gateway</strong> to provide an HTTP endpoint for adding new subscriptions. It is called by the frontend-side script and subsequently triggers a Lambda to add the new subscription to Dynamo.</li>\n<li><strong>AWS CloudWatch</strong> to regularly trigger the execution of the scraper on Fargate in a crontab-like fashion</li>\n<li><strong>AWS SES</strong> to send notification e-mails when something has changed</li>\n</ul>\n<h2 id=\"Components\"><a href=\"#Components\" class=\"headerlink\" title=\"Components\"></a>Components</h2><p>Lets take a very brief look at what the several components are doing.</p>\n<h2 id=\"crawling-core\"><a href=\"#crawling-core\" class=\"headerlink\" title=\"crawling-core\"></a>crawling-core</h2><p>This is essentially the core part of the whole application, the actual scraper / crawler. I implemented it as a <strong>Java</strong> command-line application, which has the WebDriver as a dependency to be able to interact with webpages dynamically.</p>\n<p>The program is responsible for executing an actual crawling task itself, for detecting changes by comparing the tasks result to the latest state in the database, for updating the database item and for potentially pushing a change notification message to a queue. </p>\n<p>Scraping tasks are defined as Java classes extending the <code>AbstractTask</code> class. For instance, you could create sub-classes <code>AmazonPriceTask</code> and <code>ExamsResultsTask</code>. While implementing these classes, you would essentially need to define input parameters (e.g. your student ID number to be filled in to a search form on the university website later on) for the crawling script and a series of commands in the <code>run()</code> method to be executed by WebDriver. </p>\n<p><code>crawling-core</code> is developed as a standalone Java command-line application, where the name of the task to be executed (e.g. <code>EXAM_RESULT_TASK</code>) and the input parameters (e.g. <code>VAR_STUDENT_ID</code>, <code>VAR_DEPARTMENT_NAME</code>) are provided as run arguments or environment variables.</p>\n<p>In addition to the Java program, packaged as a simple JAR, we need a browser the WebDriver can use to browse the web. I decided to use Firefox in headless mode. Ultimately, the JAR and the Firefox binary are packaged together into a Docker images based on <a href=\"https://hub.docker.com/r/selenium/standalone-firefox/\">selenium/standalone-firefox</a> and pushed to <strong>AWS ECR</strong> (AWS container registry). </p>\n<p>To execute a scraping task, e.g. our <code>ExamsResultsTask</code>, <strong>AWS Fargate</strong> will pull the latest Docker image from the registry, create a new container from it, set the required input parameters as environment variables and eventually run the entrypoint, which is our JAR file. </p>\n<h2 id=\"-crawling-crawl\"><a href=\"#-crawling-crawl\" class=\"headerlink\" title=\" crawling-crawl\"></a> crawling-crawl</h2><p> is a very simple Lambda function written in <strong>NodeJS</strong>, which is responsible for launching a crawling job. It is triggered regularly through a <strong>CloudWatch</strong> event. First, it fetches all crawling tasks from Dynamo. A crawling task is a unique combination of a task name and a set of input parameters. Afterwards it requests Fargate to start a new new instance of <code>crawling-core</code> for every task and passes the input parameters contained in the database item. </p>\n<h2 id=\"-crawling-notify\"><a href=\"#-crawling-notify\" class=\"headerlink\" title=\" crawling-notify\"></a> crawling-notify</h2><p> is another Lambda, which stands at the very end of one iteration of our crawling process. It is invoked through messages in the <code>crawling-changes</code> <strong>SQS</strong> queue and responsible for sending out notification e-mails to subscribers. It reads change information from the invoking event, including task name, the subscribers e-mail address and the tasks output parameters (e.g. your exam grade) and composes an e-mail message that eventually gets sent through the Simple E-Mail Service (<strong>SES</strong>).</p>\n<h2 id=\"-crawling-web-subscribe\"><a href=\"#-crawling-web-subscribe\" class=\"headerlink\" title=\" crawling-web-subscribe\"></a> crawling-web-subscribe</h2><p>The last of our three Lambdas is not directly related to the crawling itself. Instead, it is used for handling HTTP requests sent by a user who wants to add a new subscription. Initiated by a simple script on an HTML page called <code>subscribe.html</code>, a POST is sent to the <code>/subscriptions</code> endpoint in the <strong>API Gateway</strong>, then forwarded to the <code>crawling-web-subscribe</code> and ultimately added to the Dynamo database as a new item in the <code>subscriptions</code> table.</p>\n<h1 id=\"Okay-cool-And-now\"><a href=\"#Okay-cool-And-now\" class=\"headerlink\" title=\"Okay, cool. And now?\"></a>Okay, cool. And now?</h1><p>As I mentioned before, this project was rather a learning playground for me than a reasonable architecture for a web scraper. Although this one should, in fact, be quite scalable, you could definitely build a scraper script with much less effort. However, I learned a lot about cloud development and AWS specifically and I really like how easy things can be and how well all these different components play together. Maybe I was able to encourage the less cloud-experienced developers among you to start playing around with AWS (or some other cloud provider) as well and I hope you liked my (very spontaneously written) article. </p>\n"},{"title":"Caddy - a modern web server (vs. nginx)","date":"2017-01-09T22:07:55.000Z","_content":"\n__Update:__ I'm glad to tell that this article made it to the front page of [Hacker News](https://news.ycombinator.com/news) only a few hours after publication .\n\nAt the time of writing this article the web is effectively powered by three different major web server software packages. A web server, as covered in this article, basically has two purposes. One is to serve static (no dynamic functionality, no backend, no databse, ...) web sites, usually consisting of HTML, JavaScript and CSS plus images etc. The other is to act as a [reverse-proxy](https://en.wikipedia.org/wiki/Reverse_proxy) to web application backends. The three servers I just mentioned have a combined market share of 94.7 % (according to [this statistic](https://w3techs.com/technologies/overview/web_server/all)) and are named [Apache 2](https://httpd.apache.org/) (or _httpd_) (written in C), [nginx](https://www.nginx.com/solutions/web-server/) (say _\"engine ex\"_) (also written in C) and [Microsoft IIS](https://www.iis.net/) (written in C++). While the first two are platform independent and open-source, the latter is a proprietary, commercial, Windows-only Microsoft product and therefore more interesting at enterprise level rather than for smaller indie projects. Consequently I won't cover IIS further in the following. \n\n![Most popular web servers on the internet](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/webservers.png)\n\n_Most popular web servers on the internet ([Source](https://w3techs.com/technologies/overview/web_server/all))_\n\nnginx' first release was in 2004 and Apache2's roots even date back to 1995. Of course both projects are getting updates regularly, but their base concepts still remain the same. And at some point they might not perfectly fit today's requirements anymore. \n\nPersonally I switched from Apache2 to nginx a few months ago mainly because of two reasons. The first one was that I had really been annoyed by [Apache2's extremely high memory overhead](https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison). The second reason was that Apache2 still didn't have HTTP/2.0 support in 2016.\n\n![Apache2 vs. nginx memory usage](https://objects-us-west-1.dream.io/kbhttps://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/images/Webserver_memory_graph.jpg)\n\n_Apache2 vs. nginx memory usage ([Source](https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison))_\n\nI was pretty happy with nginx and especially its performance as well as the large amount of documentation and forum posts on the web about every conceivable problem were great. But since I'm a developer and not a sysadmin there's one thing I didn't like. The configuration is not that intuitive and you really need to get into the syntax and concepts to get an understanding of knobs to turn in order to achieve a certain goal. It's also much more fine-grained than necessary for the average user. Personally I just want a simple config file with an intuitive syntax where I can tell my web server which static content to display or which backend to reverse-proxy for which route / domain. This, plus some additional features like handling compression, TLS encryption, authentication and maybe some basic rewrites, is fine. Looking for a more modern web server that fulfills these requirements I found [Caddy](https://caddyserver.com). As it turned out, it can even do a lot more cool things, while still being easy to use.\n\nCaddy is written is Go, open-source and pretty actively (according to commit history) developed on [GitHub](https://github.com/mholt/caddy). The goal when developing Caddy was exactly what I was looking for: easy configuration and fitness for today's web applications. It comes with HTTP/2.0 (and even QUIC) support out of the box and serves via HTTPS by default (HTTP to HTTPS redirection is also handled automatically, while you manually had to tell Apache2 or nginx to do so). It even obtains (and regularly renews!) [Let's Encrypt](https://letsencrypt.org/) certificates for every (sub)domain you specified in the config file. While enabling HTTPS for a site was really a pain some years ago, it's done completely automatically now. You don't need to run any script. You don't even need to create a Let's Encrypt account or install the _certbot_. At the center of Caddy are is the _middleware_ (or _directives_), which are added to the config as a one-liner. The [list of such](https://caddyserver.com/docs/) is long and you will find a middleware for almost everything. For instance there are middleware components for logging, gzipping, header modification, (basic or [JWT](https://jwt.io)-based) authentication and load balancing. But also more fancy things like automatically serving Markdown as HTML, a plug-and-play file browser GUI, HTML minification, IP filtering or pattern-based text replacement in HTML files are available as middlewares. Caddy also aligns well with PHP, using _php-fpm_, just as nginx does. As usual with Go applications, the entire program is shipped as a single binary (available for Windows, Mac, Linux and BSD), which includes all of its dependencies. Consequently you don't need to install any further libraries to be linked (-> no version conflicts), what really makes the installation a no-brainer. However, this introduces one little drawback in comparison to nginx modules: every middleware you want to use needs to be included into the binary and if it's not, you need to re-compile the program (which is done for you by the download script at Caddy website, actually). \n\nI migrated all of my websites and -apps from nginx to Caddy (which took me hardly more than an hour) and so far I'm happy with the setup. But what about performance?\n\nTo measure a very basic performance benchmark, I took [this script](https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh), which I used in [an earlier benchmark scenario](https://muetsch.io/http-performance-java-jersey-vs-go-vs-nodejs.html). This script uses the [h2load](https://github.com/nghttp2/nghttp2#benchmarking-tool) load test tool and I adjusted the parameters in a way that it performs a total of __100,000 requests__ against a specific route at my webserver with a number of __32 concurrent clients__ (each performing 3,125 requests) on __4 CPU threads__. I ran both servers with almost their default configuration, except that I turned on HTTP/2.0 with a self-signed certificate. The file served was a static HTML file containing 6.2 kBytes of data. Both h2load and the respective web server were executed locally on the same machine with the following specs.\n\n```\n===CPU:\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\n \n===RAM: \n              total        used        free      shared  buff/cache   available\nMem:           7,7G        1,8G        4,2G        316M        1,6G        5,3G\nSwap:           29G          0B         29G\n \n===OS: \nLinux ferdinand-ubuntu 4.8.0-32-generic #34-Ubuntu SMP Tue Dec 13 14:30:43 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nThe results look like this.\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/webserver_performance.png)\n\n_Caddy vs. nginx performance comparison_\n\nAs you can clearly see, nginx still performs way better, at least in this very simple scenario. However, Caddy is much more easy to use, in my opinion. Seems like we are having a classical trade-off here. Anyway, you should really give Caddy a try (and I'm not getting paid for this ). Concerning memory usage: I didn't observe that in detail, but suprisingly I found that neither the Caddy process nor the sum of nginx worker processes exceeded 10 MB of RAM usage (may I have done something wrong?).\n\nPlease note that I only measured one specific figure (concurrent req/s) in one specific scenario. One can think of other benchmark setups where results might be the complete opposite, potentially.\n\nBy the way, Apache2 was not included to this benchmark, because I wanted to use HTTP/2.0. Actually in the meantime there is a `mod_http2` for Apache2, but it's not included in the majority of the builds, yet, and to be honest, I didn't want to make an own one. If you're interested in that, you can get a rough idea of Apache2 vs. nginx performance in [this article](https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison) (spoiler: it's pretty poor).\n\nSo to conclude the discussion: Should you use Caddy in preference to nginx or Apache2? For private projects definitely yes, if you asked me. For more _serious_ projects you should probably wait until it's even a little more mature (e.g. when a 1.x.x version is out) and maybe also incorporating dynamic module loading. Until then I'd stick with nginx. Besides that I can't figure out a reason for preferring Apache2 over nginx, except for being too lazy to do the migration.\n\nPlease let me know if you liked my article and also if you don't agree with some of my arguments and insights.","source":"_posts/caddy-a-modern-web-server-vs-nginx.md","raw":"---\ntitle: Caddy - a modern web server (vs. nginx)\ndate: 2017-01-09 23:07:55\ntags:\n---\n\n__Update:__ I'm glad to tell that this article made it to the front page of [Hacker News](https://news.ycombinator.com/news) only a few hours after publication .\n\nAt the time of writing this article the web is effectively powered by three different major web server software packages. A web server, as covered in this article, basically has two purposes. One is to serve static (no dynamic functionality, no backend, no databse, ...) web sites, usually consisting of HTML, JavaScript and CSS plus images etc. The other is to act as a [reverse-proxy](https://en.wikipedia.org/wiki/Reverse_proxy) to web application backends. The three servers I just mentioned have a combined market share of 94.7 % (according to [this statistic](https://w3techs.com/technologies/overview/web_server/all)) and are named [Apache 2](https://httpd.apache.org/) (or _httpd_) (written in C), [nginx](https://www.nginx.com/solutions/web-server/) (say _\"engine ex\"_) (also written in C) and [Microsoft IIS](https://www.iis.net/) (written in C++). While the first two are platform independent and open-source, the latter is a proprietary, commercial, Windows-only Microsoft product and therefore more interesting at enterprise level rather than for smaller indie projects. Consequently I won't cover IIS further in the following. \n\n![Most popular web servers on the internet](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/webservers.png)\n\n_Most popular web servers on the internet ([Source](https://w3techs.com/technologies/overview/web_server/all))_\n\nnginx' first release was in 2004 and Apache2's roots even date back to 1995. Of course both projects are getting updates regularly, but their base concepts still remain the same. And at some point they might not perfectly fit today's requirements anymore. \n\nPersonally I switched from Apache2 to nginx a few months ago mainly because of two reasons. The first one was that I had really been annoyed by [Apache2's extremely high memory overhead](https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison). The second reason was that Apache2 still didn't have HTTP/2.0 support in 2016.\n\n![Apache2 vs. nginx memory usage](https://objects-us-west-1.dream.io/kbhttps://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/images/Webserver_memory_graph.jpg)\n\n_Apache2 vs. nginx memory usage ([Source](https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison))_\n\nI was pretty happy with nginx and especially its performance as well as the large amount of documentation and forum posts on the web about every conceivable problem were great. But since I'm a developer and not a sysadmin there's one thing I didn't like. The configuration is not that intuitive and you really need to get into the syntax and concepts to get an understanding of knobs to turn in order to achieve a certain goal. It's also much more fine-grained than necessary for the average user. Personally I just want a simple config file with an intuitive syntax where I can tell my web server which static content to display or which backend to reverse-proxy for which route / domain. This, plus some additional features like handling compression, TLS encryption, authentication and maybe some basic rewrites, is fine. Looking for a more modern web server that fulfills these requirements I found [Caddy](https://caddyserver.com). As it turned out, it can even do a lot more cool things, while still being easy to use.\n\nCaddy is written is Go, open-source and pretty actively (according to commit history) developed on [GitHub](https://github.com/mholt/caddy). The goal when developing Caddy was exactly what I was looking for: easy configuration and fitness for today's web applications. It comes with HTTP/2.0 (and even QUIC) support out of the box and serves via HTTPS by default (HTTP to HTTPS redirection is also handled automatically, while you manually had to tell Apache2 or nginx to do so). It even obtains (and regularly renews!) [Let's Encrypt](https://letsencrypt.org/) certificates for every (sub)domain you specified in the config file. While enabling HTTPS for a site was really a pain some years ago, it's done completely automatically now. You don't need to run any script. You don't even need to create a Let's Encrypt account or install the _certbot_. At the center of Caddy are is the _middleware_ (or _directives_), which are added to the config as a one-liner. The [list of such](https://caddyserver.com/docs/) is long and you will find a middleware for almost everything. For instance there are middleware components for logging, gzipping, header modification, (basic or [JWT](https://jwt.io)-based) authentication and load balancing. But also more fancy things like automatically serving Markdown as HTML, a plug-and-play file browser GUI, HTML minification, IP filtering or pattern-based text replacement in HTML files are available as middlewares. Caddy also aligns well with PHP, using _php-fpm_, just as nginx does. As usual with Go applications, the entire program is shipped as a single binary (available for Windows, Mac, Linux and BSD), which includes all of its dependencies. Consequently you don't need to install any further libraries to be linked (-> no version conflicts), what really makes the installation a no-brainer. However, this introduces one little drawback in comparison to nginx modules: every middleware you want to use needs to be included into the binary and if it's not, you need to re-compile the program (which is done for you by the download script at Caddy website, actually). \n\nI migrated all of my websites and -apps from nginx to Caddy (which took me hardly more than an hour) and so far I'm happy with the setup. But what about performance?\n\nTo measure a very basic performance benchmark, I took [this script](https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh), which I used in [an earlier benchmark scenario](https://muetsch.io/http-performance-java-jersey-vs-go-vs-nodejs.html). This script uses the [h2load](https://github.com/nghttp2/nghttp2#benchmarking-tool) load test tool and I adjusted the parameters in a way that it performs a total of __100,000 requests__ against a specific route at my webserver with a number of __32 concurrent clients__ (each performing 3,125 requests) on __4 CPU threads__. I ran both servers with almost their default configuration, except that I turned on HTTP/2.0 with a self-signed certificate. The file served was a static HTML file containing 6.2 kBytes of data. Both h2load and the respective web server were executed locally on the same machine with the following specs.\n\n```\n===CPU:\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\nmodel name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores : 2\n \n===RAM: \n              total        used        free      shared  buff/cache   available\nMem:           7,7G        1,8G        4,2G        316M        1,6G        5,3G\nSwap:           29G          0B         29G\n \n===OS: \nLinux ferdinand-ubuntu 4.8.0-32-generic #34-Ubuntu SMP Tue Dec 13 14:30:43 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nThe results look like this.\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/webserver_performance.png)\n\n_Caddy vs. nginx performance comparison_\n\nAs you can clearly see, nginx still performs way better, at least in this very simple scenario. However, Caddy is much more easy to use, in my opinion. Seems like we are having a classical trade-off here. Anyway, you should really give Caddy a try (and I'm not getting paid for this ). Concerning memory usage: I didn't observe that in detail, but suprisingly I found that neither the Caddy process nor the sum of nginx worker processes exceeded 10 MB of RAM usage (may I have done something wrong?).\n\nPlease note that I only measured one specific figure (concurrent req/s) in one specific scenario. One can think of other benchmark setups where results might be the complete opposite, potentially.\n\nBy the way, Apache2 was not included to this benchmark, because I wanted to use HTTP/2.0. Actually in the meantime there is a `mod_http2` for Apache2, but it's not included in the majority of the builds, yet, and to be honest, I didn't want to make an own one. If you're interested in that, you can get a rough idea of Apache2 vs. nginx performance in [this article](https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison) (spoiler: it's pretty poor).\n\nSo to conclude the discussion: Should you use Caddy in preference to nginx or Apache2? For private projects definitely yes, if you asked me. For more _serious_ projects you should probably wait until it's even a little more mature (e.g. when a 1.x.x version is out) and maybe also incorporating dynamic module loading. Until then I'd stick with nginx. Besides that I can't figure out a reason for preferring Apache2 over nginx, except for being too lazy to do the migration.\n\nPlease let me know if you liked my article and also if you don't agree with some of my arguments and insights.","slug":"caddy-a-modern-web-server-vs-nginx","published":1,"updated":"2020-10-30T20:05:40.281Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlg0005o2e0e20tbtev","content":"<p><strong>Update:</strong> Im glad to tell that this article made it to the front page of <a href=\"https://news.ycombinator.com/news\">Hacker News</a> only a few hours after publication .</p>\n<p>At the time of writing this article the web is effectively powered by three different major web server software packages. A web server, as covered in this article, basically has two purposes. One is to serve static (no dynamic functionality, no backend, no databse, ) web sites, usually consisting of HTML, JavaScript and CSS plus images etc. The other is to act as a <a href=\"https://en.wikipedia.org/wiki/Reverse_proxy\">reverse-proxy</a> to web application backends. The three servers I just mentioned have a combined market share of 94.7 % (according to <a href=\"https://w3techs.com/technologies/overview/web_server/all\">this statistic</a>) and are named <a href=\"https://httpd.apache.org/\">Apache 2</a> (or <em>httpd</em>) (written in C), <a href=\"https://www.nginx.com/solutions/web-server/\">nginx</a> (say <em>engine ex</em>) (also written in C) and <a href=\"https://www.iis.net/\">Microsoft IIS</a> (written in C++). While the first two are platform independent and open-source, the latter is a proprietary, commercial, Windows-only Microsoft product and therefore more interesting at enterprise level rather than for smaller indie projects. Consequently I wont cover IIS further in the following. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/webservers.png\" alt=\"Most popular web servers on the internet\"></p>\n<p><em>Most popular web servers on the internet (<a href=\"https://w3techs.com/technologies/overview/web_server/all\">Source</a>)</em></p>\n<p>nginx first release was in 2004 and Apache2s roots even date back to 1995. Of course both projects are getting updates regularly, but their base concepts still remain the same. And at some point they might not perfectly fit todays requirements anymore. </p>\n<p>Personally I switched from Apache2 to nginx a few months ago mainly because of two reasons. The first one was that I had really been annoyed by <a href=\"https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison\">Apache2s extremely high memory overhead</a>. The second reason was that Apache2 still didnt have HTTP/2.0 support in 2016.</p>\n<p><img src=\"https://objects-us-west-1.dream.io/kbhttps://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/images/Webserver_memory_graph.jpg\" alt=\"Apache2 vs. nginx memory usage\"></p>\n<p><em>Apache2 vs. nginx memory usage (<a href=\"https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison\">Source</a>)</em></p>\n<p>I was pretty happy with nginx and especially its performance as well as the large amount of documentation and forum posts on the web about every conceivable problem were great. But since Im a developer and not a sysadmin theres one thing I didnt like. The configuration is not that intuitive and you really need to get into the syntax and concepts to get an understanding of knobs to turn in order to achieve a certain goal. Its also much more fine-grained than necessary for the average user. Personally I just want a simple config file with an intuitive syntax where I can tell my web server which static content to display or which backend to reverse-proxy for which route / domain. This, plus some additional features like handling compression, TLS encryption, authentication and maybe some basic rewrites, is fine. Looking for a more modern web server that fulfills these requirements I found <a href=\"https://caddyserver.com/\">Caddy</a>. As it turned out, it can even do a lot more cool things, while still being easy to use.</p>\n<p>Caddy is written is Go, open-source and pretty actively (according to commit history) developed on <a href=\"https://github.com/mholt/caddy\">GitHub</a>. The goal when developing Caddy was exactly what I was looking for: easy configuration and fitness for todays web applications. It comes with HTTP/2.0 (and even QUIC) support out of the box and serves via HTTPS by default (HTTP to HTTPS redirection is also handled automatically, while you manually had to tell Apache2 or nginx to do so). It even obtains (and regularly renews!) <a href=\"https://letsencrypt.org/\">Lets Encrypt</a> certificates for every (sub)domain you specified in the config file. While enabling HTTPS for a site was really a pain some years ago, its done completely automatically now. You dont need to run any script. You dont even need to create a Lets Encrypt account or install the <em>certbot</em>. At the center of Caddy are is the <em>middleware</em> (or <em>directives</em>), which are added to the config as a one-liner. The <a href=\"https://caddyserver.com/docs/\">list of such</a> is long and you will find a middleware for almost everything. For instance there are middleware components for logging, gzipping, header modification, (basic or <a href=\"https://jwt.io/\">JWT</a>-based) authentication and load balancing. But also more fancy things like automatically serving Markdown as HTML, a plug-and-play file browser GUI, HTML minification, IP filtering or pattern-based text replacement in HTML files are available as middlewares. Caddy also aligns well with PHP, using <em>php-fpm</em>, just as nginx does. As usual with Go applications, the entire program is shipped as a single binary (available for Windows, Mac, Linux and BSD), which includes all of its dependencies. Consequently you dont need to install any further libraries to be linked (-&gt; no version conflicts), what really makes the installation a no-brainer. However, this introduces one little drawback in comparison to nginx modules: every middleware you want to use needs to be included into the binary and if its not, you need to re-compile the program (which is done for you by the download script at Caddy website, actually). </p>\n<p>I migrated all of my websites and -apps from nginx to Caddy (which took me hardly more than an hour) and so far Im happy with the setup. But what about performance?</p>\n<p>To measure a very basic performance benchmark, I took <a href=\"https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh\">this script</a>, which I used in <a href=\"https://muetsch.io/http-performance-java-jersey-vs-go-vs-nodejs.html\">an earlier benchmark scenario</a>. This script uses the <a href=\"https://github.com/nghttp2/nghttp2#benchmarking-tool\">h2load</a> load test tool and I adjusted the parameters in a way that it performs a total of <strong>100,000 requests</strong> against a specific route at my webserver with a number of <strong>32 concurrent clients</strong> (each performing 3,125 requests) on <strong>4 CPU threads</strong>. I ran both servers with almost their default configuration, except that I turned on HTTP/2.0 with a self-signed certificate. The file served was a static HTML file containing 6.2 kBytes of data. Both h2load and the respective web server were executed locally on the same machine with the following specs.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x3D;&#x3D;&#x3D;CPU:</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\"> </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;RAM: </span><br><span class=\"line\">              total        used        free      shared  buff&#x2F;cache   available</span><br><span class=\"line\">Mem:           7,7G        1,8G        4,2G        316M        1,6G        5,3G</span><br><span class=\"line\">Swap:           29G          0B         29G</span><br><span class=\"line\"> </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;OS: </span><br><span class=\"line\">Linux ferdinand-ubuntu 4.8.0-32-generic #34-Ubuntu SMP Tue Dec 13 14:30:43 UTC 2016 x86_64 x86_64 x86_64 GNU&#x2F;Linux</span><br></pre></td></tr></table></figure>\n\n<p>The results look like this.<br><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/webserver_performance.png\"></p>\n<p><em>Caddy vs. nginx performance comparison</em></p>\n<p>As you can clearly see, nginx still performs way better, at least in this very simple scenario. However, Caddy is much more easy to use, in my opinion. Seems like we are having a classical trade-off here. Anyway, you should really give Caddy a try (and Im not getting paid for this ). Concerning memory usage: I didnt observe that in detail, but suprisingly I found that neither the Caddy process nor the sum of nginx worker processes exceeded 10 MB of RAM usage (may I have done something wrong?).</p>\n<p>Please note that I only measured one specific figure (concurrent req/s) in one specific scenario. One can think of other benchmark setups where results might be the complete opposite, potentially.</p>\n<p>By the way, Apache2 was not included to this benchmark, because I wanted to use HTTP/2.0. Actually in the meantime there is a <code>mod_http2</code> for Apache2, but its not included in the majority of the builds, yet, and to be honest, I didnt want to make an own one. If youre interested in that, you can get a rough idea of Apache2 vs. nginx performance in <a href=\"https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison\">this article</a> (spoiler: its pretty poor).</p>\n<p>So to conclude the discussion: Should you use Caddy in preference to nginx or Apache2? For private projects definitely yes, if you asked me. For more <em>serious</em> projects you should probably wait until its even a little more mature (e.g. when a 1.x.x version is out) and maybe also incorporating dynamic module loading. Until then Id stick with nginx. Besides that I cant figure out a reason for preferring Apache2 over nginx, except for being too lazy to do the migration.</p>\n<p>Please let me know if you liked my article and also if you dont agree with some of my arguments and insights.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><strong>Update:</strong> Im glad to tell that this article made it to the front page of <a href=\"https://news.ycombinator.com/news\">Hacker News</a> only a few hours after publication .</p>\n<p>At the time of writing this article the web is effectively powered by three different major web server software packages. A web server, as covered in this article, basically has two purposes. One is to serve static (no dynamic functionality, no backend, no databse, ) web sites, usually consisting of HTML, JavaScript and CSS plus images etc. The other is to act as a <a href=\"https://en.wikipedia.org/wiki/Reverse_proxy\">reverse-proxy</a> to web application backends. The three servers I just mentioned have a combined market share of 94.7 % (according to <a href=\"https://w3techs.com/technologies/overview/web_server/all\">this statistic</a>) and are named <a href=\"https://httpd.apache.org/\">Apache 2</a> (or <em>httpd</em>) (written in C), <a href=\"https://www.nginx.com/solutions/web-server/\">nginx</a> (say <em>engine ex</em>) (also written in C) and <a href=\"https://www.iis.net/\">Microsoft IIS</a> (written in C++). While the first two are platform independent and open-source, the latter is a proprietary, commercial, Windows-only Microsoft product and therefore more interesting at enterprise level rather than for smaller indie projects. Consequently I wont cover IIS further in the following. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/webservers.png\" alt=\"Most popular web servers on the internet\"></p>\n<p><em>Most popular web servers on the internet (<a href=\"https://w3techs.com/technologies/overview/web_server/all\">Source</a>)</em></p>\n<p>nginx first release was in 2004 and Apache2s roots even date back to 1995. Of course both projects are getting updates regularly, but their base concepts still remain the same. And at some point they might not perfectly fit todays requirements anymore. </p>\n<p>Personally I switched from Apache2 to nginx a few months ago mainly because of two reasons. The first one was that I had really been annoyed by <a href=\"https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison\">Apache2s extremely high memory overhead</a>. The second reason was that Apache2 still didnt have HTTP/2.0 support in 2016.</p>\n<p><img src=\"https://objects-us-west-1.dream.io/kbhttps://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/images/Webserver_memory_graph.jpg\" alt=\"Apache2 vs. nginx memory usage\"></p>\n<p><em>Apache2 vs. nginx memory usage (<a href=\"https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison\">Source</a>)</em></p>\n<p>I was pretty happy with nginx and especially its performance as well as the large amount of documentation and forum posts on the web about every conceivable problem were great. But since Im a developer and not a sysadmin theres one thing I didnt like. The configuration is not that intuitive and you really need to get into the syntax and concepts to get an understanding of knobs to turn in order to achieve a certain goal. Its also much more fine-grained than necessary for the average user. Personally I just want a simple config file with an intuitive syntax where I can tell my web server which static content to display or which backend to reverse-proxy for which route / domain. This, plus some additional features like handling compression, TLS encryption, authentication and maybe some basic rewrites, is fine. Looking for a more modern web server that fulfills these requirements I found <a href=\"https://caddyserver.com/\">Caddy</a>. As it turned out, it can even do a lot more cool things, while still being easy to use.</p>\n<p>Caddy is written is Go, open-source and pretty actively (according to commit history) developed on <a href=\"https://github.com/mholt/caddy\">GitHub</a>. The goal when developing Caddy was exactly what I was looking for: easy configuration and fitness for todays web applications. It comes with HTTP/2.0 (and even QUIC) support out of the box and serves via HTTPS by default (HTTP to HTTPS redirection is also handled automatically, while you manually had to tell Apache2 or nginx to do so). It even obtains (and regularly renews!) <a href=\"https://letsencrypt.org/\">Lets Encrypt</a> certificates for every (sub)domain you specified in the config file. While enabling HTTPS for a site was really a pain some years ago, its done completely automatically now. You dont need to run any script. You dont even need to create a Lets Encrypt account or install the <em>certbot</em>. At the center of Caddy are is the <em>middleware</em> (or <em>directives</em>), which are added to the config as a one-liner. The <a href=\"https://caddyserver.com/docs/\">list of such</a> is long and you will find a middleware for almost everything. For instance there are middleware components for logging, gzipping, header modification, (basic or <a href=\"https://jwt.io/\">JWT</a>-based) authentication and load balancing. But also more fancy things like automatically serving Markdown as HTML, a plug-and-play file browser GUI, HTML minification, IP filtering or pattern-based text replacement in HTML files are available as middlewares. Caddy also aligns well with PHP, using <em>php-fpm</em>, just as nginx does. As usual with Go applications, the entire program is shipped as a single binary (available for Windows, Mac, Linux and BSD), which includes all of its dependencies. Consequently you dont need to install any further libraries to be linked (-&gt; no version conflicts), what really makes the installation a no-brainer. However, this introduces one little drawback in comparison to nginx modules: every middleware you want to use needs to be included into the binary and if its not, you need to re-compile the program (which is done for you by the download script at Caddy website, actually). </p>\n<p>I migrated all of my websites and -apps from nginx to Caddy (which took me hardly more than an hour) and so far Im happy with the setup. But what about performance?</p>\n<p>To measure a very basic performance benchmark, I took <a href=\"https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh\">this script</a>, which I used in <a href=\"https://muetsch.io/http-performance-java-jersey-vs-go-vs-nodejs.html\">an earlier benchmark scenario</a>. This script uses the <a href=\"https://github.com/nghttp2/nghttp2#benchmarking-tool\">h2load</a> load test tool and I adjusted the parameters in a way that it performs a total of <strong>100,000 requests</strong> against a specific route at my webserver with a number of <strong>32 concurrent clients</strong> (each performing 3,125 requests) on <strong>4 CPU threads</strong>. I ran both servers with almost their default configuration, except that I turned on HTTP/2.0 with a self-signed certificate. The file served was a static HTML file containing 6.2 kBytes of data. Both h2load and the respective web server were executed locally on the same machine with the following specs.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x3D;&#x3D;&#x3D;CPU:</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\">model name : Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores : 2</span><br><span class=\"line\"> </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;RAM: </span><br><span class=\"line\">              total        used        free      shared  buff&#x2F;cache   available</span><br><span class=\"line\">Mem:           7,7G        1,8G        4,2G        316M        1,6G        5,3G</span><br><span class=\"line\">Swap:           29G          0B         29G</span><br><span class=\"line\"> </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;OS: </span><br><span class=\"line\">Linux ferdinand-ubuntu 4.8.0-32-generic #34-Ubuntu SMP Tue Dec 13 14:30:43 UTC 2016 x86_64 x86_64 x86_64 GNU&#x2F;Linux</span><br></pre></td></tr></table></figure>\n\n<p>The results look like this.<br><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/webserver_performance.png\"></p>\n<p><em>Caddy vs. nginx performance comparison</em></p>\n<p>As you can clearly see, nginx still performs way better, at least in this very simple scenario. However, Caddy is much more easy to use, in my opinion. Seems like we are having a classical trade-off here. Anyway, you should really give Caddy a try (and Im not getting paid for this ). Concerning memory usage: I didnt observe that in detail, but suprisingly I found that neither the Caddy process nor the sum of nginx worker processes exceeded 10 MB of RAM usage (may I have done something wrong?).</p>\n<p>Please note that I only measured one specific figure (concurrent req/s) in one specific scenario. One can think of other benchmark setups where results might be the complete opposite, potentially.</p>\n<p>By the way, Apache2 was not included to this benchmark, because I wanted to use HTTP/2.0. Actually in the meantime there is a <code>mod_http2</code> for Apache2, but its not included in the majority of the builds, yet, and to be honest, I didnt want to make an own one. If youre interested in that, you can get a rough idea of Apache2 vs. nginx performance in <a href=\"https://help.dreamhost.com/hc/en-us/articles/215945987-Web-server-performance-comparison\">this article</a> (spoiler: its pretty poor).</p>\n<p>So to conclude the discussion: Should you use Caddy in preference to nginx or Apache2? For private projects definitely yes, if you asked me. For more <em>serious</em> projects you should probably wait until its even a little more mature (e.g. when a 1.x.x version is out) and maybe also incorporating dynamic module loading. Until then Id stick with nginx. Besides that I cant figure out a reason for preferring Apache2 over nginx, except for being too lazy to do the migration.</p>\n<p>Please let me know if you liked my article and also if you dont agree with some of my arguments and insights.</p>\n"},{"title":"CartPole with a Deep Q-Network","date":"2017-09-11T16:47:39.000Z","_content":"In my [last post](https://muetsch.io/cartpole-with-qlearning-first-experiences-with-openai-gym.html) I developed a solution to [OpenAI Gym's CartPole environment](https://gym.openai.com/envs/CartPole-v0), based on a classical Q-Learning algorithm. **The best score I achieved with it was 120**, although the score I uploaded to the [leaderboard](https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q) was 188. While this is certainly not a bad result, I wondered if I could do better using more advanced techniques. Besides that I also wanted to practice the concepts I had recently learned in the [Machine Learning 2](http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en) course at university. By the way, to all the students among you: I found that one of the best way to learn about new algorithms etc. is to actually **try to implement them in code**!\n\n## Motivation\nOne major limitation of my classical Q-Learning approach was that the number of possible states had to be reduced from basically infinity (due to the observations' continuous nature) to, in my case, `1 * 1 * 6 * 12 = 72` discrete states. Considering this extreme simpification, the results were astonishingly good! However, what if we could utilize more of the information, the observations give us? One solution is to combine Q-Learning with a (deep) neural network, which results in a technique called Deep Q-Learning (DQN). Neural networks are inherently efficient when handling very high dimensional problems. That's why they are doing so well with image-, video- and audio data. Additionally, they can easily handle continuous inputs, whereas with our classical approach we needed the Q-table to be a finite (in this case (4+1)-dimensional) matrix (or tensor). Accordingly, with DQN we don't need discrete buckets anymore, but are able to directly use the raw observations.\n\n## Deep Q-Learning\nBut how does this even work? While I don't want to explain DQN in detail here, the basic idea is to replace the Q-table by a neural network, which is trained to predict Q-values for a state. The input is a state-vector (or a batch of such) - consisting of four features in this case (which corresponds to four input neurons). The output is a vector of Q-values, one for each possible action - two in our case (corresponding to two output neurons). The training is done using **experience replay**. Basically, the agent begins to try some random actions and stores its \"experiences\" into a memory. An experience is a tuple like `(old_state, performed_action, received_reward, new_state)`. At fixed intervals (e.g. after each training episode, but NOT after each step), batches are sampled from memory and used as training data for the network. Consequently, the network (hopefully) improves every episode and predicts more precise Q-values for state-action pairs. \n\n## My implementation\nMy implementation is essentially based on [this great blog post](https://keon.io/deep-q-learning/) by [Keon](https://github.com/keon). It uses [Keras](http://keras.io) as a high-level abstraction on top of [TensorFlow](http://tensorflow.com). However, while I adopted the general structure, I made several tweaks and fixes to massively improve performance. \n\n### Tweak #1: More hidden neurons\nI slightly modified the network layout by **doubling the number of hidden neurons** in the second hidden layer. While randomly experimenting with some layouts I found that this one seemed to work better on average. Generally, determining a neural network's layout is basically trial and error for the most parts. Mainly you want to master the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), but there is no rule on how to choose network structure in order to do so. Mine looks like this now:\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn4.png)\n\n### Tweak #2: Larger replay memory\nIn [Keon](https://github.com/keon)'s original implementation, the replay memory had a maxmimum size of 2,000. Assuming an average \"survival\" time of 100 steps, it would only hold experiences from 20 episodes, which is not much. I didn't see any reason why they shouldn't be a greater variety in training examples, so I increased the **memory size to 100,000**.\n\n### Tweak #3: Mini-batch training\nWhile originally the network trained from batches of 32 examples in each episode, the way the training was conducted was not efficient in my opinion. Instead of giving TensorFlow a **32 x 4 matrix**, it was given a **1 x 4 matrix 32 times**, so the actual training procedure effectively used a mini-batch size of 1. Without having technical knowledge on how TensorFlow works, I'm still pretty sure that training the network with one large batch instead of 32 small ones is faster - especially when using a GPU. \n\n### Tweak #4: Setting  = 1\nAs already mentioned in my last post, I'm of the opinion that it wouldn't make sense to set the gamma parameter to less than one. Its purpose is to \"penalize\" the agent if it takes long to reach its goal. However, in CartPole **its even our goal** to do as many steps as possible. \n\n### Tweak #5: Logarithmic -decay\nSince the adaptive exploration rate from [@tuzzer's solution](https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947) was very effective in my last implementation, I simply adopted it for this one, too. I didn't cross-validate whether it's better or worse than [Keon](https://github.com/keon)'s epsilon decay, but at least it doesn't seem to do bad.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn3.png)\n\n### Tweak #6: tanh activation function\nI'm really not sure about this point, so please correct me if I'm wrong. The original implementation used the ReLU activation function, which is a linear function that maps the input to itself, but thesholded at zero.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn2.png)\n\nHowever, since the input features can be negative, ReLU might cause dead neurons, doesn't it? To overcome that problem, I decided to _tanh_ as an activation function.\n\n### Tweak #7: Cross-validate hyperparameters\nEventually, I conducted a grid search (using my [script](https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033) from the last time) to find good values for `alpha` (learning rate), `alpha_decay` and `epsilon_min` (minimum exploration rate). It turned out that `alpha=0.01`, `alpha_decay=0.01` and `epsilon_min=0.01` seem to work best among all tested values on average.\n\n## Results\nAfter all these optimizations, I ran the algorithm several times and the best score I achieved was actually **24**. However, I didn't record that run, so my best score in the [leaderboard](https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g) in **85**, which is better then with my classical Q-Learning approach.\nHowever, I found that although the DQN approach _can_ converge faster, it also seems to be way more unstable at the same time. Performing two consecutive runs of the exact same algorithm and configuration often resulted in one score to be extremely good, while the second one didn't even solve the environment at all (> 999 episodes). In other words, DQN has a way larger variance, than the table-based approach, as depicted in the chart below, where I performed 25 runs of each algorithm. Additionally, as expected, the neural network is slower. Running DQN 25 times took **744 seconds**, while running table-based Q-Learning 25 times only took **24 seconds** on my machine's CPU using four threads on four cores. \n\n[>> Code on GitHub (dqn_cartpole.py)](https://gist.github.com/muety/2a6722407117e4d668921fce53845432)\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn1.png)\n\n**Q**: `Min 120, Max 999, Mean 197.16, Std: 183.223`\n**DQN**: `Min 56, Max 999, Mean 600.04, Std: 356.046`","source":"_posts/cartpole-with-a-deep-q-network.md","raw":"---\ntitle: CartPole with a Deep Q-Network\ndate: 2017-09-11 18:47:39\ntags:\n---\nIn my [last post](https://muetsch.io/cartpole-with-qlearning-first-experiences-with-openai-gym.html) I developed a solution to [OpenAI Gym's CartPole environment](https://gym.openai.com/envs/CartPole-v0), based on a classical Q-Learning algorithm. **The best score I achieved with it was 120**, although the score I uploaded to the [leaderboard](https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q) was 188. While this is certainly not a bad result, I wondered if I could do better using more advanced techniques. Besides that I also wanted to practice the concepts I had recently learned in the [Machine Learning 2](http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en) course at university. By the way, to all the students among you: I found that one of the best way to learn about new algorithms etc. is to actually **try to implement them in code**!\n\n## Motivation\nOne major limitation of my classical Q-Learning approach was that the number of possible states had to be reduced from basically infinity (due to the observations' continuous nature) to, in my case, `1 * 1 * 6 * 12 = 72` discrete states. Considering this extreme simpification, the results were astonishingly good! However, what if we could utilize more of the information, the observations give us? One solution is to combine Q-Learning with a (deep) neural network, which results in a technique called Deep Q-Learning (DQN). Neural networks are inherently efficient when handling very high dimensional problems. That's why they are doing so well with image-, video- and audio data. Additionally, they can easily handle continuous inputs, whereas with our classical approach we needed the Q-table to be a finite (in this case (4+1)-dimensional) matrix (or tensor). Accordingly, with DQN we don't need discrete buckets anymore, but are able to directly use the raw observations.\n\n## Deep Q-Learning\nBut how does this even work? While I don't want to explain DQN in detail here, the basic idea is to replace the Q-table by a neural network, which is trained to predict Q-values for a state. The input is a state-vector (or a batch of such) - consisting of four features in this case (which corresponds to four input neurons). The output is a vector of Q-values, one for each possible action - two in our case (corresponding to two output neurons). The training is done using **experience replay**. Basically, the agent begins to try some random actions and stores its \"experiences\" into a memory. An experience is a tuple like `(old_state, performed_action, received_reward, new_state)`. At fixed intervals (e.g. after each training episode, but NOT after each step), batches are sampled from memory and used as training data for the network. Consequently, the network (hopefully) improves every episode and predicts more precise Q-values for state-action pairs. \n\n## My implementation\nMy implementation is essentially based on [this great blog post](https://keon.io/deep-q-learning/) by [Keon](https://github.com/keon). It uses [Keras](http://keras.io) as a high-level abstraction on top of [TensorFlow](http://tensorflow.com). However, while I adopted the general structure, I made several tweaks and fixes to massively improve performance. \n\n### Tweak #1: More hidden neurons\nI slightly modified the network layout by **doubling the number of hidden neurons** in the second hidden layer. While randomly experimenting with some layouts I found that this one seemed to work better on average. Generally, determining a neural network's layout is basically trial and error for the most parts. Mainly you want to master the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), but there is no rule on how to choose network structure in order to do so. Mine looks like this now:\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn4.png)\n\n### Tweak #2: Larger replay memory\nIn [Keon](https://github.com/keon)'s original implementation, the replay memory had a maxmimum size of 2,000. Assuming an average \"survival\" time of 100 steps, it would only hold experiences from 20 episodes, which is not much. I didn't see any reason why they shouldn't be a greater variety in training examples, so I increased the **memory size to 100,000**.\n\n### Tweak #3: Mini-batch training\nWhile originally the network trained from batches of 32 examples in each episode, the way the training was conducted was not efficient in my opinion. Instead of giving TensorFlow a **32 x 4 matrix**, it was given a **1 x 4 matrix 32 times**, so the actual training procedure effectively used a mini-batch size of 1. Without having technical knowledge on how TensorFlow works, I'm still pretty sure that training the network with one large batch instead of 32 small ones is faster - especially when using a GPU. \n\n### Tweak #4: Setting  = 1\nAs already mentioned in my last post, I'm of the opinion that it wouldn't make sense to set the gamma parameter to less than one. Its purpose is to \"penalize\" the agent if it takes long to reach its goal. However, in CartPole **its even our goal** to do as many steps as possible. \n\n### Tweak #5: Logarithmic -decay\nSince the adaptive exploration rate from [@tuzzer's solution](https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947) was very effective in my last implementation, I simply adopted it for this one, too. I didn't cross-validate whether it's better or worse than [Keon](https://github.com/keon)'s epsilon decay, but at least it doesn't seem to do bad.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn3.png)\n\n### Tweak #6: tanh activation function\nI'm really not sure about this point, so please correct me if I'm wrong. The original implementation used the ReLU activation function, which is a linear function that maps the input to itself, but thesholded at zero.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn2.png)\n\nHowever, since the input features can be negative, ReLU might cause dead neurons, doesn't it? To overcome that problem, I decided to _tanh_ as an activation function.\n\n### Tweak #7: Cross-validate hyperparameters\nEventually, I conducted a grid search (using my [script](https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033) from the last time) to find good values for `alpha` (learning rate), `alpha_decay` and `epsilon_min` (minimum exploration rate). It turned out that `alpha=0.01`, `alpha_decay=0.01` and `epsilon_min=0.01` seem to work best among all tested values on average.\n\n## Results\nAfter all these optimizations, I ran the algorithm several times and the best score I achieved was actually **24**. However, I didn't record that run, so my best score in the [leaderboard](https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g) in **85**, which is better then with my classical Q-Learning approach.\nHowever, I found that although the DQN approach _can_ converge faster, it also seems to be way more unstable at the same time. Performing two consecutive runs of the exact same algorithm and configuration often resulted in one score to be extremely good, while the second one didn't even solve the environment at all (> 999 episodes). In other words, DQN has a way larger variance, than the table-based approach, as depicted in the chart below, where I performed 25 runs of each algorithm. Additionally, as expected, the neural network is slower. Running DQN 25 times took **744 seconds**, while running table-based Q-Learning 25 times only took **24 seconds** on my machine's CPU using four threads on four cores. \n\n[>> Code on GitHub (dqn_cartpole.py)](https://gist.github.com/muety/2a6722407117e4d668921fce53845432)\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn1.png)\n\n**Q**: `Min 120, Max 999, Mean 197.16, Std: 183.223`\n**DQN**: `Min 56, Max 999, Mean 600.04, Std: 356.046`","slug":"cartpole-with-a-deep-q-network","published":1,"updated":"2020-10-30T20:05:40.281Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlh0006o2e0ab490fc3","content":"<p>In my <a href=\"https://muetsch.io/cartpole-with-qlearning-first-experiences-with-openai-gym.html\">last post</a> I developed a solution to <a href=\"https://gym.openai.com/envs/CartPole-v0\">OpenAI Gyms CartPole environment</a>, based on a classical Q-Learning algorithm. <strong>The best score I achieved with it was 120</strong>, although the score I uploaded to the <a href=\"https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q\">leaderboard</a> was 188. While this is certainly not a bad result, I wondered if I could do better using more advanced techniques. Besides that I also wanted to practice the concepts I had recently learned in the <a href=\"http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en\">Machine Learning 2</a> course at university. By the way, to all the students among you: I found that one of the best way to learn about new algorithms etc. is to actually <strong>try to implement them in code</strong>!</p>\n<h2 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h2><p>One major limitation of my classical Q-Learning approach was that the number of possible states had to be reduced from basically infinity (due to the observations continuous nature) to, in my case, <code>1 * 1 * 6 * 12 = 72</code> discrete states. Considering this extreme simpification, the results were astonishingly good! However, what if we could utilize more of the information, the observations give us? One solution is to combine Q-Learning with a (deep) neural network, which results in a technique called Deep Q-Learning (DQN). Neural networks are inherently efficient when handling very high dimensional problems. Thats why they are doing so well with image-, video- and audio data. Additionally, they can easily handle continuous inputs, whereas with our classical approach we needed the Q-table to be a finite (in this case (4+1)-dimensional) matrix (or tensor). Accordingly, with DQN we dont need discrete buckets anymore, but are able to directly use the raw observations.</p>\n<h2 id=\"Deep-Q-Learning\"><a href=\"#Deep-Q-Learning\" class=\"headerlink\" title=\"Deep Q-Learning\"></a>Deep Q-Learning</h2><p>But how does this even work? While I dont want to explain DQN in detail here, the basic idea is to replace the Q-table by a neural network, which is trained to predict Q-values for a state. The input is a state-vector (or a batch of such) - consisting of four features in this case (which corresponds to four input neurons). The output is a vector of Q-values, one for each possible action - two in our case (corresponding to two output neurons). The training is done using <strong>experience replay</strong>. Basically, the agent begins to try some random actions and stores its experiences into a memory. An experience is a tuple like <code>(old_state, performed_action, received_reward, new_state)</code>. At fixed intervals (e.g. after each training episode, but NOT after each step), batches are sampled from memory and used as training data for the network. Consequently, the network (hopefully) improves every episode and predicts more precise Q-values for state-action pairs. </p>\n<h2 id=\"My-implementation\"><a href=\"#My-implementation\" class=\"headerlink\" title=\"My implementation\"></a>My implementation</h2><p>My implementation is essentially based on <a href=\"https://keon.io/deep-q-learning/\">this great blog post</a> by <a href=\"https://github.com/keon\">Keon</a>. It uses <a href=\"http://keras.io/\">Keras</a> as a high-level abstraction on top of <a href=\"http://tensorflow.com/\">TensorFlow</a>. However, while I adopted the general structure, I made several tweaks and fixes to massively improve performance. </p>\n<h3 id=\"Tweak-1-More-hidden-neurons\"><a href=\"#Tweak-1-More-hidden-neurons\" class=\"headerlink\" title=\"Tweak #1: More hidden neurons\"></a>Tweak #1: More hidden neurons</h3><p>I slightly modified the network layout by <strong>doubling the number of hidden neurons</strong> in the second hidden layer. While randomly experimenting with some layouts I found that this one seemed to work better on average. Generally, determining a neural networks layout is basically trial and error for the most parts. Mainly you want to master the <a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\">bias-variance tradeoff</a>, but there is no rule on how to choose network structure in order to do so. Mine looks like this now:</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn4.png\"></p>\n<h3 id=\"Tweak-2-Larger-replay-memory\"><a href=\"#Tweak-2-Larger-replay-memory\" class=\"headerlink\" title=\"Tweak #2: Larger replay memory\"></a>Tweak #2: Larger replay memory</h3><p>In <a href=\"https://github.com/keon\">Keon</a>s original implementation, the replay memory had a maxmimum size of 2,000. Assuming an average survival time of 100 steps, it would only hold experiences from 20 episodes, which is not much. I didnt see any reason why they shouldnt be a greater variety in training examples, so I increased the <strong>memory size to 100,000</strong>.</p>\n<h3 id=\"Tweak-3-Mini-batch-training\"><a href=\"#Tweak-3-Mini-batch-training\" class=\"headerlink\" title=\"Tweak #3: Mini-batch training\"></a>Tweak #3: Mini-batch training</h3><p>While originally the network trained from batches of 32 examples in each episode, the way the training was conducted was not efficient in my opinion. Instead of giving TensorFlow a <strong>32 x 4 matrix</strong>, it was given a <strong>1 x 4 matrix 32 times</strong>, so the actual training procedure effectively used a mini-batch size of 1. Without having technical knowledge on how TensorFlow works, Im still pretty sure that training the network with one large batch instead of 32 small ones is faster - especially when using a GPU. </p>\n<h3 id=\"Tweak-4-Setting--1\"><a href=\"#Tweak-4-Setting--1\" class=\"headerlink\" title=\"Tweak #4: Setting  = 1\"></a>Tweak #4: Setting  = 1</h3><p>As already mentioned in my last post, Im of the opinion that it wouldnt make sense to set the gamma parameter to less than one. Its purpose is to penalize the agent if it takes long to reach its goal. However, in CartPole <strong>its even our goal</strong> to do as many steps as possible. </p>\n<h3 id=\"Tweak-5-Logarithmic--decay\"><a href=\"#Tweak-5-Logarithmic--decay\" class=\"headerlink\" title=\"Tweak #5: Logarithmic -decay\"></a>Tweak #5: Logarithmic -decay</h3><p>Since the adaptive exploration rate from <a href=\"https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947\">@tuzzers solution</a> was very effective in my last implementation, I simply adopted it for this one, too. I didnt cross-validate whether its better or worse than <a href=\"https://github.com/keon\">Keon</a>s epsilon decay, but at least it doesnt seem to do bad.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn3.png\"></p>\n<h3 id=\"Tweak-6-tanh-activation-function\"><a href=\"#Tweak-6-tanh-activation-function\" class=\"headerlink\" title=\"Tweak #6: tanh activation function\"></a>Tweak #6: tanh activation function</h3><p>Im really not sure about this point, so please correct me if Im wrong. The original implementation used the ReLU activation function, which is a linear function that maps the input to itself, but thesholded at zero.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn2.png\"></p>\n<p>However, since the input features can be negative, ReLU might cause dead neurons, doesnt it? To overcome that problem, I decided to <em>tanh</em> as an activation function.</p>\n<h3 id=\"Tweak-7-Cross-validate-hyperparameters\"><a href=\"#Tweak-7-Cross-validate-hyperparameters\" class=\"headerlink\" title=\"Tweak #7: Cross-validate hyperparameters\"></a>Tweak #7: Cross-validate hyperparameters</h3><p>Eventually, I conducted a grid search (using my <a href=\"https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033\">script</a> from the last time) to find good values for <code>alpha</code> (learning rate), <code>alpha_decay</code> and <code>epsilon_min</code> (minimum exploration rate). It turned out that <code>alpha=0.01</code>, <code>alpha_decay=0.01</code> and <code>epsilon_min=0.01</code> seem to work best among all tested values on average.</p>\n<h2 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h2><p>After all these optimizations, I ran the algorithm several times and the best score I achieved was actually <strong>24</strong>. However, I didnt record that run, so my best score in the <a href=\"https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g\">leaderboard</a> in <strong>85</strong>, which is better then with my classical Q-Learning approach.<br>However, I found that although the DQN approach <em>can</em> converge faster, it also seems to be way more unstable at the same time. Performing two consecutive runs of the exact same algorithm and configuration often resulted in one score to be extremely good, while the second one didnt even solve the environment at all (&gt; 999 episodes). In other words, DQN has a way larger variance, than the table-based approach, as depicted in the chart below, where I performed 25 runs of each algorithm. Additionally, as expected, the neural network is slower. Running DQN 25 times took <strong>744 seconds</strong>, while running table-based Q-Learning 25 times only took <strong>24 seconds</strong> on my machines CPU using four threads on four cores. </p>\n<p><a href=\"https://gist.github.com/muety/2a6722407117e4d668921fce53845432\">&gt;&gt; Code on GitHub (dqn_cartpole.py)</a></p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn1.png\"></p>\n<p><strong>Q</strong>: <code>Min 120, Max 999, Mean 197.16, Std: 183.223</code><br><strong>DQN</strong>: <code>Min 56, Max 999, Mean 600.04, Std: 356.046</code></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>In my <a href=\"https://muetsch.io/cartpole-with-qlearning-first-experiences-with-openai-gym.html\">last post</a> I developed a solution to <a href=\"https://gym.openai.com/envs/CartPole-v0\">OpenAI Gyms CartPole environment</a>, based on a classical Q-Learning algorithm. <strong>The best score I achieved with it was 120</strong>, although the score I uploaded to the <a href=\"https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q\">leaderboard</a> was 188. While this is certainly not a bad result, I wondered if I could do better using more advanced techniques. Besides that I also wanted to practice the concepts I had recently learned in the <a href=\"http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en\">Machine Learning 2</a> course at university. By the way, to all the students among you: I found that one of the best way to learn about new algorithms etc. is to actually <strong>try to implement them in code</strong>!</p>\n<h2 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h2><p>One major limitation of my classical Q-Learning approach was that the number of possible states had to be reduced from basically infinity (due to the observations continuous nature) to, in my case, <code>1 * 1 * 6 * 12 = 72</code> discrete states. Considering this extreme simpification, the results were astonishingly good! However, what if we could utilize more of the information, the observations give us? One solution is to combine Q-Learning with a (deep) neural network, which results in a technique called Deep Q-Learning (DQN). Neural networks are inherently efficient when handling very high dimensional problems. Thats why they are doing so well with image-, video- and audio data. Additionally, they can easily handle continuous inputs, whereas with our classical approach we needed the Q-table to be a finite (in this case (4+1)-dimensional) matrix (or tensor). Accordingly, with DQN we dont need discrete buckets anymore, but are able to directly use the raw observations.</p>\n<h2 id=\"Deep-Q-Learning\"><a href=\"#Deep-Q-Learning\" class=\"headerlink\" title=\"Deep Q-Learning\"></a>Deep Q-Learning</h2><p>But how does this even work? While I dont want to explain DQN in detail here, the basic idea is to replace the Q-table by a neural network, which is trained to predict Q-values for a state. The input is a state-vector (or a batch of such) - consisting of four features in this case (which corresponds to four input neurons). The output is a vector of Q-values, one for each possible action - two in our case (corresponding to two output neurons). The training is done using <strong>experience replay</strong>. Basically, the agent begins to try some random actions and stores its experiences into a memory. An experience is a tuple like <code>(old_state, performed_action, received_reward, new_state)</code>. At fixed intervals (e.g. after each training episode, but NOT after each step), batches are sampled from memory and used as training data for the network. Consequently, the network (hopefully) improves every episode and predicts more precise Q-values for state-action pairs. </p>\n<h2 id=\"My-implementation\"><a href=\"#My-implementation\" class=\"headerlink\" title=\"My implementation\"></a>My implementation</h2><p>My implementation is essentially based on <a href=\"https://keon.io/deep-q-learning/\">this great blog post</a> by <a href=\"https://github.com/keon\">Keon</a>. It uses <a href=\"http://keras.io/\">Keras</a> as a high-level abstraction on top of <a href=\"http://tensorflow.com/\">TensorFlow</a>. However, while I adopted the general structure, I made several tweaks and fixes to massively improve performance. </p>\n<h3 id=\"Tweak-1-More-hidden-neurons\"><a href=\"#Tweak-1-More-hidden-neurons\" class=\"headerlink\" title=\"Tweak #1: More hidden neurons\"></a>Tweak #1: More hidden neurons</h3><p>I slightly modified the network layout by <strong>doubling the number of hidden neurons</strong> in the second hidden layer. While randomly experimenting with some layouts I found that this one seemed to work better on average. Generally, determining a neural networks layout is basically trial and error for the most parts. Mainly you want to master the <a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\">bias-variance tradeoff</a>, but there is no rule on how to choose network structure in order to do so. Mine looks like this now:</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn4.png\"></p>\n<h3 id=\"Tweak-2-Larger-replay-memory\"><a href=\"#Tweak-2-Larger-replay-memory\" class=\"headerlink\" title=\"Tweak #2: Larger replay memory\"></a>Tweak #2: Larger replay memory</h3><p>In <a href=\"https://github.com/keon\">Keon</a>s original implementation, the replay memory had a maxmimum size of 2,000. Assuming an average survival time of 100 steps, it would only hold experiences from 20 episodes, which is not much. I didnt see any reason why they shouldnt be a greater variety in training examples, so I increased the <strong>memory size to 100,000</strong>.</p>\n<h3 id=\"Tweak-3-Mini-batch-training\"><a href=\"#Tweak-3-Mini-batch-training\" class=\"headerlink\" title=\"Tweak #3: Mini-batch training\"></a>Tweak #3: Mini-batch training</h3><p>While originally the network trained from batches of 32 examples in each episode, the way the training was conducted was not efficient in my opinion. Instead of giving TensorFlow a <strong>32 x 4 matrix</strong>, it was given a <strong>1 x 4 matrix 32 times</strong>, so the actual training procedure effectively used a mini-batch size of 1. Without having technical knowledge on how TensorFlow works, Im still pretty sure that training the network with one large batch instead of 32 small ones is faster - especially when using a GPU. </p>\n<h3 id=\"Tweak-4-Setting--1\"><a href=\"#Tweak-4-Setting--1\" class=\"headerlink\" title=\"Tweak #4: Setting  = 1\"></a>Tweak #4: Setting  = 1</h3><p>As already mentioned in my last post, Im of the opinion that it wouldnt make sense to set the gamma parameter to less than one. Its purpose is to penalize the agent if it takes long to reach its goal. However, in CartPole <strong>its even our goal</strong> to do as many steps as possible. </p>\n<h3 id=\"Tweak-5-Logarithmic--decay\"><a href=\"#Tweak-5-Logarithmic--decay\" class=\"headerlink\" title=\"Tweak #5: Logarithmic -decay\"></a>Tweak #5: Logarithmic -decay</h3><p>Since the adaptive exploration rate from <a href=\"https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947\">@tuzzers solution</a> was very effective in my last implementation, I simply adopted it for this one, too. I didnt cross-validate whether its better or worse than <a href=\"https://github.com/keon\">Keon</a>s epsilon decay, but at least it doesnt seem to do bad.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn3.png\"></p>\n<h3 id=\"Tweak-6-tanh-activation-function\"><a href=\"#Tweak-6-tanh-activation-function\" class=\"headerlink\" title=\"Tweak #6: tanh activation function\"></a>Tweak #6: tanh activation function</h3><p>Im really not sure about this point, so please correct me if Im wrong. The original implementation used the ReLU activation function, which is a linear function that maps the input to itself, but thesholded at zero.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn2.png\"></p>\n<p>However, since the input features can be negative, ReLU might cause dead neurons, doesnt it? To overcome that problem, I decided to <em>tanh</em> as an activation function.</p>\n<h3 id=\"Tweak-7-Cross-validate-hyperparameters\"><a href=\"#Tweak-7-Cross-validate-hyperparameters\" class=\"headerlink\" title=\"Tweak #7: Cross-validate hyperparameters\"></a>Tweak #7: Cross-validate hyperparameters</h3><p>Eventually, I conducted a grid search (using my <a href=\"https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033\">script</a> from the last time) to find good values for <code>alpha</code> (learning rate), <code>alpha_decay</code> and <code>epsilon_min</code> (minimum exploration rate). It turned out that <code>alpha=0.01</code>, <code>alpha_decay=0.01</code> and <code>epsilon_min=0.01</code> seem to work best among all tested values on average.</p>\n<h2 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h2><p>After all these optimizations, I ran the algorithm several times and the best score I achieved was actually <strong>24</strong>. However, I didnt record that run, so my best score in the <a href=\"https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g\">leaderboard</a> in <strong>85</strong>, which is better then with my classical Q-Learning approach.<br>However, I found that although the DQN approach <em>can</em> converge faster, it also seems to be way more unstable at the same time. Performing two consecutive runs of the exact same algorithm and configuration often resulted in one score to be extremely good, while the second one didnt even solve the environment at all (&gt; 999 episodes). In other words, DQN has a way larger variance, than the table-based approach, as depicted in the chart below, where I performed 25 runs of each algorithm. Additionally, as expected, the neural network is slower. Running DQN 25 times took <strong>744 seconds</strong>, while running table-based Q-Learning 25 times only took <strong>24 seconds</strong> on my machines CPU using four threads on four cores. </p>\n<p><a href=\"https://gist.github.com/muety/2a6722407117e4d668921fce53845432\">&gt;&gt; Code on GitHub (dqn_cartpole.py)</a></p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dqn1.png\"></p>\n<p><strong>Q</strong>: <code>Min 120, Max 999, Mean 197.16, Std: 183.223</code><br><strong>DQN</strong>: <code>Min 56, Max 999, Mean 600.04, Std: 356.046</code></p>\n"},{"title":"CartPole with Q-Learning - First experiences with OpenAI Gym","date":"2017-08-24T14:50:57.000Z","_content":"## OpenAI Gym\nToday I made my first experiences with the [OpenAI gym](https://gym.openai.com), more specifically with the [CartPole](https://gym.openai.com/envs/CartPole-v0) environment. Gym is basically a Python library that includes several machine learning challenges, in which an autonomous agent should be learned to fulfill different tasks, e.g. to master a simple game itself. One of the simplest and most popular challenges is CartPole. It's basically a 2D game in which the agent has to control, i.e. move left or right, a cart to balance a pole standing perpendicularly on the cart. This is a classical reinforcement learning problem, e.g. a scenario, in which the agents starts by trying random actions as a consequence to which it gets rewarded (or not). Based on the rewards, it continuously \"learns\", which action is good in which specific situation. Doing so, it learns how to master the game without ever being told how the game even works. The main advantage of this type of learning is, that it's completely generic and not bound to a specific problem. E.g. to learn a chess agent, you don't need to \"tell\" it the rules of chess, but just let it do trial & error, whereas \"error\" means giving it a negative (or small positive) reward.\n\n## CartPole-v0\nIn machine learning terms, CartPole is basically a binary classification problem. There are four features as inputs, which include the cart position, its velocity, the pole's angle to the cart and its derivative (i.e. how fast the pole is \"falling\"). The output is binary, i.e. either 0 or 1, corresponding to \"left\" or \"right\". One challenge is the fact that all four features are continuous values (floating point numbers), which, naively, implies an infinitely large feature space.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole1.jpg)\n\n## Approach: Basic Q-Learning\nIn the [Machine Learning 1](https://his.anthropomatik.kit.edu/english/28_315.php) course at my university, I got to know one of the most basic, yet widely-used reinforcement learning approaches, which is [Q-Learning](http://mnemstudio.org/path-finding-q-learning-tutorial.htm). The core of Q-Learning is to estimate a value for every possible pare of a state (s) and an action (a) by getting rewarded. Imagine the following graph, which consists of three states, while your agent is currently in _s0_. It can choose between two actions, one of which results in a good state _s1_ (e.g. having won the game), the other one results in a bad state _s2_ (e.g. having lost the game). Accordingly, the transition leading to the good (bad) state gives a reward of 100 (-100). If the agent performs action _a0_, the q-value of _s0_ will probably become negative (Q(s0, a0) < 0)), while Q(s0, a1) > 0.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole2.png)\n\nThe update of the q-value is done according to the following equation.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole3.png)\n\nBasically, a (S, A)-tuple's new q-value depends on its old q-value, the immediate reward received for the action and the maximum q-value achievable in the following state. So a (S, A)-pair's q-value indirectly depends on all its successors' q-values, which is expressed in the recursive function definition. By repeatedly walking through all nodes and transistions, the agent can update any (S, A)-pairs q-value, while the results of good and bad actions are slowly \"backpropagated\" from terminal nodes to early nodes. The agent ends up with a (usually multidimensional) table mapping states and actions to q-values, so that given any state, the best action can be picked by choosing the highest respective q-value.\n\n## My implementation\nSince Q-Learning is pretty straightforward to understand and implement, I decided on picking that algorithm as a starting point for my CartPole challenge. I looked for other solutions, that also use CartPole and found [this blog post](https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947) by [@tuzzer](https://medium.com/@tuzzer), which had partially inspired me during my implementation. \n\n[>> Code on GitHub (qcartpole.py)](https://gist.github.com/muety/af0b8476ae4106ec098fea1dfe57f578)\n\n### Transforming the feature space\nActually the main challenge was to convert the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states. The less states we have, the smaller the Q-table will be, the less steps the agent will need to properly learn its values. However, too few states might not hold enough information to properly represent the environment. The original domains of the input features are these.\n\n* __x__ (cart position)  [-4.8, 4.8]\n* __x'__ (cart velocity)  [-3.4 * 10^38, 3.4 * 10^38]\n* __theta__ (angle)  [-0.42, 0.42]\n* __theta'__ (angle velocity)  [-3.4 * 10^38, 3.4 * 10^38]\n\n### Finding the parameters\nAs can be seen, especially the velocities' domains are extermely large. However, from [@tuzzer](https://medium.com/@tuzzer)'s post I found that astonishingly small target intervals are sufficient. Therefore, I initially started by scaling __theta__ down to a discrete interval `theta  [0, 6]   ` (which is, to be precise, just a set of integers {0..6}) and __theta'__ to `theta'  [0, 12]   `. Inspired by [@tuzzer](https://medium.com/@tuzzer/)'s post, I dropped the __x__ and __x'__ features completely, which corresponds to mapping any of their values to a single scalar. The motivation behind this is that the probability of the cart leaving the environment at the left or right border in only 200 time steps (after 200 steps, the environment automatically resets itself) is pretty slow and the resulting reduction in dimensionality more worthy. \n\nThe implementation of the actual Q-Learning algorithm is straightfoward and consits of a function to fetch the best action for a state from the q-table and another function to update the q-table based on the last action. Nothing special here.\n\nMore interesing are the algorithm's hyperparameters, which include __alpha (= learning rate)__, __epsilon (= exploration rate)__ and __gamma (= discount factor)__.\n\nAlpha is used to \"smooth\" the updates and make them less radical, which, in the first place, prevents from errors caused by noise. Epsilon regulates between __exploitation and exploration__. Accordingly, instead of picking the _best_ action in a state, with a chance of  a _random_ action is picked. This should prevent the algorithm from getting stuck in local minima. E.g. if a bad choice was made in the beginning, without  the agent would continue on evaluating that suboptimal path and would never discover any other, potentially better, path. Gamma is used to penalize the agent if it takes long to reach its goal. However, in this case, gamma is set to constant 1 (no discount), since it's even our goal to \"survive\" as long as possible. \n\nFirst I tried to choose the epsilon and alpha parameters as constants and experimented with various combinations. However, I always achieved only a very poor score (~ between 20 and 50 ticks). Then, again inspired by [@tuzzer](https://medium.com/@tuzzer/) I decided to introduce an adaptive learning- and exploration rate, which starts with a high value and decreases by time (with each training episode). Astonishingly, this made a huge difference. Suddenly, my algorithm converged in about ~ 200 steps. Since I never thought that these hyperparameters made such an extreme difference (from not solving the challenge at all to doing pretty well), this was probably the most interesting finding from my CartPole project. I simply adpoted [@tuzzer](https://medium.com/@tuzzer/)'s adaptive function, which is visualized in the figure below (the minimum of _0.1_ is a hyperparameter to be optimized).\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole4.png)\n\n### Grid search\nAs my Q-Learning implementation with adaptive learning- and exploration rates was finished, I implemented an additional grid search to do some hyperparameter tuning. Goal was to find the optimal combination of feature interval lengths and lower bounds for alpha and epsilon. The following parameters turned out to perform best: `'buckets': (1, 1, 6, 12), 'min_alpha': 0.1, 'min_epsilon': 0.1`. Additionally, I also could have evaluated different functions for calculating the adaptive rate, but I haven't, yet. \n\n[>> Code on GitHub (qcartpole_gridsearch.py)](https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033)\n\n## Result & Future Work\nMy final score was __188__, [as can be seen in the leaderboard](https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q). As I progress with my knowledge on machine learning, while practicing for the upcoming [Machine Learning 2](http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en) exam, I want to continue improving my CartPole algorithm. Probably the next step will be to incorporate deep Q-Learning, which basically is Q-Learning with the only difference that the q-values are estimated by a deep neural net. The main (and probably only) advantage is the ability to handle way larger feature spaces. I'll keep you guys up-to-date. I hope that I could encourage you to get started with Gym, or machine learning in general, too!","source":"_posts/cartpole-with-qlearning-first-experiences-with-openai-gym.md","raw":"---\ntitle: CartPole with Q-Learning - First experiences with OpenAI Gym\ndate: 2017-08-24 16:50:57\ntags:\n---\n## OpenAI Gym\nToday I made my first experiences with the [OpenAI gym](https://gym.openai.com), more specifically with the [CartPole](https://gym.openai.com/envs/CartPole-v0) environment. Gym is basically a Python library that includes several machine learning challenges, in which an autonomous agent should be learned to fulfill different tasks, e.g. to master a simple game itself. One of the simplest and most popular challenges is CartPole. It's basically a 2D game in which the agent has to control, i.e. move left or right, a cart to balance a pole standing perpendicularly on the cart. This is a classical reinforcement learning problem, e.g. a scenario, in which the agents starts by trying random actions as a consequence to which it gets rewarded (or not). Based on the rewards, it continuously \"learns\", which action is good in which specific situation. Doing so, it learns how to master the game without ever being told how the game even works. The main advantage of this type of learning is, that it's completely generic and not bound to a specific problem. E.g. to learn a chess agent, you don't need to \"tell\" it the rules of chess, but just let it do trial & error, whereas \"error\" means giving it a negative (or small positive) reward.\n\n## CartPole-v0\nIn machine learning terms, CartPole is basically a binary classification problem. There are four features as inputs, which include the cart position, its velocity, the pole's angle to the cart and its derivative (i.e. how fast the pole is \"falling\"). The output is binary, i.e. either 0 or 1, corresponding to \"left\" or \"right\". One challenge is the fact that all four features are continuous values (floating point numbers), which, naively, implies an infinitely large feature space.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole1.jpg)\n\n## Approach: Basic Q-Learning\nIn the [Machine Learning 1](https://his.anthropomatik.kit.edu/english/28_315.php) course at my university, I got to know one of the most basic, yet widely-used reinforcement learning approaches, which is [Q-Learning](http://mnemstudio.org/path-finding-q-learning-tutorial.htm). The core of Q-Learning is to estimate a value for every possible pare of a state (s) and an action (a) by getting rewarded. Imagine the following graph, which consists of three states, while your agent is currently in _s0_. It can choose between two actions, one of which results in a good state _s1_ (e.g. having won the game), the other one results in a bad state _s2_ (e.g. having lost the game). Accordingly, the transition leading to the good (bad) state gives a reward of 100 (-100). If the agent performs action _a0_, the q-value of _s0_ will probably become negative (Q(s0, a0) < 0)), while Q(s0, a1) > 0.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole2.png)\n\nThe update of the q-value is done according to the following equation.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole3.png)\n\nBasically, a (S, A)-tuple's new q-value depends on its old q-value, the immediate reward received for the action and the maximum q-value achievable in the following state. So a (S, A)-pair's q-value indirectly depends on all its successors' q-values, which is expressed in the recursive function definition. By repeatedly walking through all nodes and transistions, the agent can update any (S, A)-pairs q-value, while the results of good and bad actions are slowly \"backpropagated\" from terminal nodes to early nodes. The agent ends up with a (usually multidimensional) table mapping states and actions to q-values, so that given any state, the best action can be picked by choosing the highest respective q-value.\n\n## My implementation\nSince Q-Learning is pretty straightforward to understand and implement, I decided on picking that algorithm as a starting point for my CartPole challenge. I looked for other solutions, that also use CartPole and found [this blog post](https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947) by [@tuzzer](https://medium.com/@tuzzer), which had partially inspired me during my implementation. \n\n[>> Code on GitHub (qcartpole.py)](https://gist.github.com/muety/af0b8476ae4106ec098fea1dfe57f578)\n\n### Transforming the feature space\nActually the main challenge was to convert the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states. The less states we have, the smaller the Q-table will be, the less steps the agent will need to properly learn its values. However, too few states might not hold enough information to properly represent the environment. The original domains of the input features are these.\n\n* __x__ (cart position)  [-4.8, 4.8]\n* __x'__ (cart velocity)  [-3.4 * 10^38, 3.4 * 10^38]\n* __theta__ (angle)  [-0.42, 0.42]\n* __theta'__ (angle velocity)  [-3.4 * 10^38, 3.4 * 10^38]\n\n### Finding the parameters\nAs can be seen, especially the velocities' domains are extermely large. However, from [@tuzzer](https://medium.com/@tuzzer)'s post I found that astonishingly small target intervals are sufficient. Therefore, I initially started by scaling __theta__ down to a discrete interval `theta  [0, 6]   ` (which is, to be precise, just a set of integers {0..6}) and __theta'__ to `theta'  [0, 12]   `. Inspired by [@tuzzer](https://medium.com/@tuzzer/)'s post, I dropped the __x__ and __x'__ features completely, which corresponds to mapping any of their values to a single scalar. The motivation behind this is that the probability of the cart leaving the environment at the left or right border in only 200 time steps (after 200 steps, the environment automatically resets itself) is pretty slow and the resulting reduction in dimensionality more worthy. \n\nThe implementation of the actual Q-Learning algorithm is straightfoward and consits of a function to fetch the best action for a state from the q-table and another function to update the q-table based on the last action. Nothing special here.\n\nMore interesing are the algorithm's hyperparameters, which include __alpha (= learning rate)__, __epsilon (= exploration rate)__ and __gamma (= discount factor)__.\n\nAlpha is used to \"smooth\" the updates and make them less radical, which, in the first place, prevents from errors caused by noise. Epsilon regulates between __exploitation and exploration__. Accordingly, instead of picking the _best_ action in a state, with a chance of  a _random_ action is picked. This should prevent the algorithm from getting stuck in local minima. E.g. if a bad choice was made in the beginning, without  the agent would continue on evaluating that suboptimal path and would never discover any other, potentially better, path. Gamma is used to penalize the agent if it takes long to reach its goal. However, in this case, gamma is set to constant 1 (no discount), since it's even our goal to \"survive\" as long as possible. \n\nFirst I tried to choose the epsilon and alpha parameters as constants and experimented with various combinations. However, I always achieved only a very poor score (~ between 20 and 50 ticks). Then, again inspired by [@tuzzer](https://medium.com/@tuzzer/) I decided to introduce an adaptive learning- and exploration rate, which starts with a high value and decreases by time (with each training episode). Astonishingly, this made a huge difference. Suddenly, my algorithm converged in about ~ 200 steps. Since I never thought that these hyperparameters made such an extreme difference (from not solving the challenge at all to doing pretty well), this was probably the most interesting finding from my CartPole project. I simply adpoted [@tuzzer](https://medium.com/@tuzzer/)'s adaptive function, which is visualized in the figure below (the minimum of _0.1_ is a hyperparameter to be optimized).\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole4.png)\n\n### Grid search\nAs my Q-Learning implementation with adaptive learning- and exploration rates was finished, I implemented an additional grid search to do some hyperparameter tuning. Goal was to find the optimal combination of feature interval lengths and lower bounds for alpha and epsilon. The following parameters turned out to perform best: `'buckets': (1, 1, 6, 12), 'min_alpha': 0.1, 'min_epsilon': 0.1`. Additionally, I also could have evaluated different functions for calculating the adaptive rate, but I haven't, yet. \n\n[>> Code on GitHub (qcartpole_gridsearch.py)](https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033)\n\n## Result & Future Work\nMy final score was __188__, [as can be seen in the leaderboard](https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q). As I progress with my knowledge on machine learning, while practicing for the upcoming [Machine Learning 2](http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en) exam, I want to continue improving my CartPole algorithm. Probably the next step will be to incorporate deep Q-Learning, which basically is Q-Learning with the only difference that the q-values are estimated by a deep neural net. The main (and probably only) advantage is the ability to handle way larger feature spaces. I'll keep you guys up-to-date. I hope that I could encourage you to get started with Gym, or machine learning in general, too!","slug":"cartpole-with-qlearning-first-experiences-with-openai-gym","published":1,"updated":"2020-10-30T20:05:40.281Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlm0007o2e06ukm6r0k","content":"<h2 id=\"OpenAI-Gym\"><a href=\"#OpenAI-Gym\" class=\"headerlink\" title=\"OpenAI Gym\"></a>OpenAI Gym</h2><p>Today I made my first experiences with the <a href=\"https://gym.openai.com/\">OpenAI gym</a>, more specifically with the <a href=\"https://gym.openai.com/envs/CartPole-v0\">CartPole</a> environment. Gym is basically a Python library that includes several machine learning challenges, in which an autonomous agent should be learned to fulfill different tasks, e.g. to master a simple game itself. One of the simplest and most popular challenges is CartPole. Its basically a 2D game in which the agent has to control, i.e. move left or right, a cart to balance a pole standing perpendicularly on the cart. This is a classical reinforcement learning problem, e.g. a scenario, in which the agents starts by trying random actions as a consequence to which it gets rewarded (or not). Based on the rewards, it continuously learns, which action is good in which specific situation. Doing so, it learns how to master the game without ever being told how the game even works. The main advantage of this type of learning is, that its completely generic and not bound to a specific problem. E.g. to learn a chess agent, you dont need to tell it the rules of chess, but just let it do trial &amp; error, whereas error means giving it a negative (or small positive) reward.</p>\n<h2 id=\"CartPole-v0\"><a href=\"#CartPole-v0\" class=\"headerlink\" title=\"CartPole-v0\"></a>CartPole-v0</h2><p>In machine learning terms, CartPole is basically a binary classification problem. There are four features as inputs, which include the cart position, its velocity, the poles angle to the cart and its derivative (i.e. how fast the pole is falling). The output is binary, i.e. either 0 or 1, corresponding to left or right. One challenge is the fact that all four features are continuous values (floating point numbers), which, naively, implies an infinitely large feature space.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole1.jpg\"></p>\n<h2 id=\"Approach-Basic-Q-Learning\"><a href=\"#Approach-Basic-Q-Learning\" class=\"headerlink\" title=\"Approach: Basic Q-Learning\"></a>Approach: Basic Q-Learning</h2><p>In the <a href=\"https://his.anthropomatik.kit.edu/english/28_315.php\">Machine Learning 1</a> course at my university, I got to know one of the most basic, yet widely-used reinforcement learning approaches, which is <a href=\"http://mnemstudio.org/path-finding-q-learning-tutorial.htm\">Q-Learning</a>. The core of Q-Learning is to estimate a value for every possible pare of a state (s) and an action (a) by getting rewarded. Imagine the following graph, which consists of three states, while your agent is currently in <em>s0</em>. It can choose between two actions, one of which results in a good state <em>s1</em> (e.g. having won the game), the other one results in a bad state <em>s2</em> (e.g. having lost the game). Accordingly, the transition leading to the good (bad) state gives a reward of 100 (-100). If the agent performs action <em>a0</em>, the q-value of <em>s0</em> will probably become negative (Q(s0, a0) &lt; 0)), while Q(s0, a1) &gt; 0.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole2.png\"></p>\n<p>The update of the q-value is done according to the following equation.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole3.png\"></p>\n<p>Basically, a (S, A)-tuples new q-value depends on its old q-value, the immediate reward received for the action and the maximum q-value achievable in the following state. So a (S, A)-pairs q-value indirectly depends on all its successors q-values, which is expressed in the recursive function definition. By repeatedly walking through all nodes and transistions, the agent can update any (S, A)-pairs q-value, while the results of good and bad actions are slowly backpropagated from terminal nodes to early nodes. The agent ends up with a (usually multidimensional) table mapping states and actions to q-values, so that given any state, the best action can be picked by choosing the highest respective q-value.</p>\n<h2 id=\"My-implementation\"><a href=\"#My-implementation\" class=\"headerlink\" title=\"My implementation\"></a>My implementation</h2><p>Since Q-Learning is pretty straightforward to understand and implement, I decided on picking that algorithm as a starting point for my CartPole challenge. I looked for other solutions, that also use CartPole and found <a href=\"https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947\">this blog post</a> by <a href=\"https://medium.com/@tuzzer\">@tuzzer</a>, which had partially inspired me during my implementation. </p>\n<p><a href=\"https://gist.github.com/muety/af0b8476ae4106ec098fea1dfe57f578\">&gt;&gt; Code on GitHub (qcartpole.py)</a></p>\n<h3 id=\"Transforming-the-feature-space\"><a href=\"#Transforming-the-feature-space\" class=\"headerlink\" title=\"Transforming the feature space\"></a>Transforming the feature space</h3><p>Actually the main challenge was to convert the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states. The less states we have, the smaller the Q-table will be, the less steps the agent will need to properly learn its values. However, too few states might not hold enough information to properly represent the environment. The original domains of the input features are these.</p>\n<ul>\n<li><strong>x</strong> (cart position)  [-4.8, 4.8]</li>\n<li><strong>x</strong> (cart velocity)  [-3.4 * 10^38, 3.4 * 10^38]</li>\n<li><strong>theta</strong> (angle)  [-0.42, 0.42]</li>\n<li><strong>theta</strong> (angle velocity)  [-3.4 * 10^38, 3.4 * 10^38]</li>\n</ul>\n<h3 id=\"Finding-the-parameters\"><a href=\"#Finding-the-parameters\" class=\"headerlink\" title=\"Finding the parameters\"></a>Finding the parameters</h3><p>As can be seen, especially the velocities domains are extermely large. However, from <a href=\"https://medium.com/@tuzzer\">@tuzzer</a>s post I found that astonishingly small target intervals are sufficient. Therefore, I initially started by scaling <strong>theta</strong> down to a discrete interval <code>theta  [0, 6]   </code> (which is, to be precise, just a set of integers {0..6}) and <strong>theta</strong> to <code>theta&#39;  [0, 12]   </code>. Inspired by <a href=\"https://medium.com/@tuzzer/\">@tuzzer</a>s post, I dropped the <strong>x</strong> and <strong>x</strong> features completely, which corresponds to mapping any of their values to a single scalar. The motivation behind this is that the probability of the cart leaving the environment at the left or right border in only 200 time steps (after 200 steps, the environment automatically resets itself) is pretty slow and the resulting reduction in dimensionality more worthy. </p>\n<p>The implementation of the actual Q-Learning algorithm is straightfoward and consits of a function to fetch the best action for a state from the q-table and another function to update the q-table based on the last action. Nothing special here.</p>\n<p>More interesing are the algorithms hyperparameters, which include <strong>alpha (= learning rate)</strong>, <strong>epsilon (= exploration rate)</strong> and <strong>gamma (= discount factor)</strong>.</p>\n<p>Alpha is used to smooth the updates and make them less radical, which, in the first place, prevents from errors caused by noise. Epsilon regulates between <strong>exploitation and exploration</strong>. Accordingly, instead of picking the <em>best</em> action in a state, with a chance of  a <em>random</em> action is picked. This should prevent the algorithm from getting stuck in local minima. E.g. if a bad choice was made in the beginning, without  the agent would continue on evaluating that suboptimal path and would never discover any other, potentially better, path. Gamma is used to penalize the agent if it takes long to reach its goal. However, in this case, gamma is set to constant 1 (no discount), since its even our goal to survive as long as possible. </p>\n<p>First I tried to choose the epsilon and alpha parameters as constants and experimented with various combinations. However, I always achieved only a very poor score (~ between 20 and 50 ticks). Then, again inspired by <a href=\"https://medium.com/@tuzzer/\">@tuzzer</a> I decided to introduce an adaptive learning- and exploration rate, which starts with a high value and decreases by time (with each training episode). Astonishingly, this made a huge difference. Suddenly, my algorithm converged in about ~ 200 steps. Since I never thought that these hyperparameters made such an extreme difference (from not solving the challenge at all to doing pretty well), this was probably the most interesting finding from my CartPole project. I simply adpoted <a href=\"https://medium.com/@tuzzer/\">@tuzzer</a>s adaptive function, which is visualized in the figure below (the minimum of <em>0.1</em> is a hyperparameter to be optimized).</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole4.png\"></p>\n<h3 id=\"Grid-search\"><a href=\"#Grid-search\" class=\"headerlink\" title=\"Grid search\"></a>Grid search</h3><p>As my Q-Learning implementation with adaptive learning- and exploration rates was finished, I implemented an additional grid search to do some hyperparameter tuning. Goal was to find the optimal combination of feature interval lengths and lower bounds for alpha and epsilon. The following parameters turned out to perform best: <code>&#39;buckets&#39;: (1, 1, 6, 12), &#39;min_alpha&#39;: 0.1, &#39;min_epsilon&#39;: 0.1</code>. Additionally, I also could have evaluated different functions for calculating the adaptive rate, but I havent, yet. </p>\n<p><a href=\"https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033\">&gt;&gt; Code on GitHub (qcartpole_gridsearch.py)</a></p>\n<h2 id=\"Result-amp-Future-Work\"><a href=\"#Result-amp-Future-Work\" class=\"headerlink\" title=\"Result &amp; Future Work\"></a>Result &amp; Future Work</h2><p>My final score was <strong>188</strong>, <a href=\"https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q\">as can be seen in the leaderboard</a>. As I progress with my knowledge on machine learning, while practicing for the upcoming <a href=\"http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en\">Machine Learning 2</a> exam, I want to continue improving my CartPole algorithm. Probably the next step will be to incorporate deep Q-Learning, which basically is Q-Learning with the only difference that the q-values are estimated by a deep neural net. The main (and probably only) advantage is the ability to handle way larger feature spaces. Ill keep you guys up-to-date. I hope that I could encourage you to get started with Gym, or machine learning in general, too!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h2 id=\"OpenAI-Gym\"><a href=\"#OpenAI-Gym\" class=\"headerlink\" title=\"OpenAI Gym\"></a>OpenAI Gym</h2><p>Today I made my first experiences with the <a href=\"https://gym.openai.com/\">OpenAI gym</a>, more specifically with the <a href=\"https://gym.openai.com/envs/CartPole-v0\">CartPole</a> environment. Gym is basically a Python library that includes several machine learning challenges, in which an autonomous agent should be learned to fulfill different tasks, e.g. to master a simple game itself. One of the simplest and most popular challenges is CartPole. Its basically a 2D game in which the agent has to control, i.e. move left or right, a cart to balance a pole standing perpendicularly on the cart. This is a classical reinforcement learning problem, e.g. a scenario, in which the agents starts by trying random actions as a consequence to which it gets rewarded (or not). Based on the rewards, it continuously learns, which action is good in which specific situation. Doing so, it learns how to master the game without ever being told how the game even works. The main advantage of this type of learning is, that its completely generic and not bound to a specific problem. E.g. to learn a chess agent, you dont need to tell it the rules of chess, but just let it do trial &amp; error, whereas error means giving it a negative (or small positive) reward.</p>\n<h2 id=\"CartPole-v0\"><a href=\"#CartPole-v0\" class=\"headerlink\" title=\"CartPole-v0\"></a>CartPole-v0</h2><p>In machine learning terms, CartPole is basically a binary classification problem. There are four features as inputs, which include the cart position, its velocity, the poles angle to the cart and its derivative (i.e. how fast the pole is falling). The output is binary, i.e. either 0 or 1, corresponding to left or right. One challenge is the fact that all four features are continuous values (floating point numbers), which, naively, implies an infinitely large feature space.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole1.jpg\"></p>\n<h2 id=\"Approach-Basic-Q-Learning\"><a href=\"#Approach-Basic-Q-Learning\" class=\"headerlink\" title=\"Approach: Basic Q-Learning\"></a>Approach: Basic Q-Learning</h2><p>In the <a href=\"https://his.anthropomatik.kit.edu/english/28_315.php\">Machine Learning 1</a> course at my university, I got to know one of the most basic, yet widely-used reinforcement learning approaches, which is <a href=\"http://mnemstudio.org/path-finding-q-learning-tutorial.htm\">Q-Learning</a>. The core of Q-Learning is to estimate a value for every possible pare of a state (s) and an action (a) by getting rewarded. Imagine the following graph, which consists of three states, while your agent is currently in <em>s0</em>. It can choose between two actions, one of which results in a good state <em>s1</em> (e.g. having won the game), the other one results in a bad state <em>s2</em> (e.g. having lost the game). Accordingly, the transition leading to the good (bad) state gives a reward of 100 (-100). If the agent performs action <em>a0</em>, the q-value of <em>s0</em> will probably become negative (Q(s0, a0) &lt; 0)), while Q(s0, a1) &gt; 0.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole2.png\"></p>\n<p>The update of the q-value is done according to the following equation.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole3.png\"></p>\n<p>Basically, a (S, A)-tuples new q-value depends on its old q-value, the immediate reward received for the action and the maximum q-value achievable in the following state. So a (S, A)-pairs q-value indirectly depends on all its successors q-values, which is expressed in the recursive function definition. By repeatedly walking through all nodes and transistions, the agent can update any (S, A)-pairs q-value, while the results of good and bad actions are slowly backpropagated from terminal nodes to early nodes. The agent ends up with a (usually multidimensional) table mapping states and actions to q-values, so that given any state, the best action can be picked by choosing the highest respective q-value.</p>\n<h2 id=\"My-implementation\"><a href=\"#My-implementation\" class=\"headerlink\" title=\"My implementation\"></a>My implementation</h2><p>Since Q-Learning is pretty straightforward to understand and implement, I decided on picking that algorithm as a starting point for my CartPole challenge. I looked for other solutions, that also use CartPole and found <a href=\"https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947\">this blog post</a> by <a href=\"https://medium.com/@tuzzer\">@tuzzer</a>, which had partially inspired me during my implementation. </p>\n<p><a href=\"https://gist.github.com/muety/af0b8476ae4106ec098fea1dfe57f578\">&gt;&gt; Code on GitHub (qcartpole.py)</a></p>\n<h3 id=\"Transforming-the-feature-space\"><a href=\"#Transforming-the-feature-space\" class=\"headerlink\" title=\"Transforming the feature space\"></a>Transforming the feature space</h3><p>Actually the main challenge was to convert the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states. The less states we have, the smaller the Q-table will be, the less steps the agent will need to properly learn its values. However, too few states might not hold enough information to properly represent the environment. The original domains of the input features are these.</p>\n<ul>\n<li><strong>x</strong> (cart position)  [-4.8, 4.8]</li>\n<li><strong>x</strong> (cart velocity)  [-3.4 * 10^38, 3.4 * 10^38]</li>\n<li><strong>theta</strong> (angle)  [-0.42, 0.42]</li>\n<li><strong>theta</strong> (angle velocity)  [-3.4 * 10^38, 3.4 * 10^38]</li>\n</ul>\n<h3 id=\"Finding-the-parameters\"><a href=\"#Finding-the-parameters\" class=\"headerlink\" title=\"Finding the parameters\"></a>Finding the parameters</h3><p>As can be seen, especially the velocities domains are extermely large. However, from <a href=\"https://medium.com/@tuzzer\">@tuzzer</a>s post I found that astonishingly small target intervals are sufficient. Therefore, I initially started by scaling <strong>theta</strong> down to a discrete interval <code>theta  [0, 6]   </code> (which is, to be precise, just a set of integers {0..6}) and <strong>theta</strong> to <code>theta&#39;  [0, 12]   </code>. Inspired by <a href=\"https://medium.com/@tuzzer/\">@tuzzer</a>s post, I dropped the <strong>x</strong> and <strong>x</strong> features completely, which corresponds to mapping any of their values to a single scalar. The motivation behind this is that the probability of the cart leaving the environment at the left or right border in only 200 time steps (after 200 steps, the environment automatically resets itself) is pretty slow and the resulting reduction in dimensionality more worthy. </p>\n<p>The implementation of the actual Q-Learning algorithm is straightfoward and consits of a function to fetch the best action for a state from the q-table and another function to update the q-table based on the last action. Nothing special here.</p>\n<p>More interesing are the algorithms hyperparameters, which include <strong>alpha (= learning rate)</strong>, <strong>epsilon (= exploration rate)</strong> and <strong>gamma (= discount factor)</strong>.</p>\n<p>Alpha is used to smooth the updates and make them less radical, which, in the first place, prevents from errors caused by noise. Epsilon regulates between <strong>exploitation and exploration</strong>. Accordingly, instead of picking the <em>best</em> action in a state, with a chance of  a <em>random</em> action is picked. This should prevent the algorithm from getting stuck in local minima. E.g. if a bad choice was made in the beginning, without  the agent would continue on evaluating that suboptimal path and would never discover any other, potentially better, path. Gamma is used to penalize the agent if it takes long to reach its goal. However, in this case, gamma is set to constant 1 (no discount), since its even our goal to survive as long as possible. </p>\n<p>First I tried to choose the epsilon and alpha parameters as constants and experimented with various combinations. However, I always achieved only a very poor score (~ between 20 and 50 ticks). Then, again inspired by <a href=\"https://medium.com/@tuzzer/\">@tuzzer</a> I decided to introduce an adaptive learning- and exploration rate, which starts with a high value and decreases by time (with each training episode). Astonishingly, this made a huge difference. Suddenly, my algorithm converged in about ~ 200 steps. Since I never thought that these hyperparameters made such an extreme difference (from not solving the challenge at all to doing pretty well), this was probably the most interesting finding from my CartPole project. I simply adpoted <a href=\"https://medium.com/@tuzzer/\">@tuzzer</a>s adaptive function, which is visualized in the figure below (the minimum of <em>0.1</em> is a hyperparameter to be optimized).</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cartpole4.png\"></p>\n<h3 id=\"Grid-search\"><a href=\"#Grid-search\" class=\"headerlink\" title=\"Grid search\"></a>Grid search</h3><p>As my Q-Learning implementation with adaptive learning- and exploration rates was finished, I implemented an additional grid search to do some hyperparameter tuning. Goal was to find the optimal combination of feature interval lengths and lower bounds for alpha and epsilon. The following parameters turned out to perform best: <code>&#39;buckets&#39;: (1, 1, 6, 12), &#39;min_alpha&#39;: 0.1, &#39;min_epsilon&#39;: 0.1</code>. Additionally, I also could have evaluated different functions for calculating the adaptive rate, but I havent, yet. </p>\n<p><a href=\"https://gist.github.com/muety/87b442fce7f7d58606f462191c6d6033\">&gt;&gt; Code on GitHub (qcartpole_gridsearch.py)</a></p>\n<h2 id=\"Result-amp-Future-Work\"><a href=\"#Result-amp-Future-Work\" class=\"headerlink\" title=\"Result &amp; Future Work\"></a>Result &amp; Future Work</h2><p>My final score was <strong>188</strong>, <a href=\"https://gym.openai.com/evaluations/eval_emRbuGdHRnWoJuMUnPwd1Q\">as can be seen in the leaderboard</a>. As I progress with my knowledge on machine learning, while practicing for the upcoming <a href=\"http://www.aifb.kit.edu/web/Lehre/Vorlesung_Maschinelles_Lernen_2_%E2%80%93_Fortgeschrittene_Verfahren/en\">Machine Learning 2</a> exam, I want to continue improving my CartPole algorithm. Probably the next step will be to incorporate deep Q-Learning, which basically is Q-Learning with the only difference that the q-values are estimated by a deep neural net. The main (and probably only) advantage is the ability to handle way larger feature spaces. Ill keep you guys up-to-date. I hope that I could encourage you to get started with Gym, or machine learning in general, too!</p>\n"},{"title":"Design of a Linked Data-enabled Microservice Platform for the Industrial Internet of Things","date":"2016-10-19T21:03:07.000Z","_content":"\nAs the topic of my bachelor's thesis at the [TECO](http://teco.edu) and part of the [ScaleIT](https://scale-it.org) research project I've designed and developed an IoT software platform with focus on uniformity, openess and ease of adoption to tackle to problem of bringing flexible IT infrastructures to the industrial shopfloor.\nHere's my thesis' abstract to get an idea of the topic.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/thesis_mockup.png)\n\n### Abstract\n\nWhile recent trends towards highly digitized, smart factories entail substantial chances for manufacturing companies to boost their performance, flexibility and productivity, the industry commonly struggles to adopt suitable technology. One major problem is a lack of uniform, standardized solutions, which could be integrated to the shopfloor without the necessity of highly specialized technical expert knowledge and a large amount of planning and restructuring.\\par \nAddressing that problem, this work proposes an architecture design as well as a concrete implementation of a Internet Of Things software platform, which mainly focuses on technological uniformity and ease of adoption and usage. As a guideline, a real-world use case elaborated in cooperation with industry partners is presented. Further on, it is shown, how general purpose web technology can be combined advantageously with recent architectural-, operational- and cultural trends in software design, powerful machine-to-machine interaction techniques and common user experience concepts. In-depth thoughts on software structure, real-time communication, machine-to-machine interaction, uniform data integration and user experience are conducted to finally obtain a working proof-of-concept software alongside recommendations and best practices for adopting smart technology on the shopfloor. An eventual evaluation investigates the designed platform regarding both performance and suitability for particular real-world scenarios and recommends further endeavor to be conducted towards achieving an actual product. \n\n### Tech stack\n\nWe've used general-purpose web technology, Linked Data techniques, semantic hypermedia APIs with Hydra and state-of-the-art software development practices and cloud technology to fulfil our goal. We also picked up on the commonly understood UX concept of apps to abstract underlying technical details away from the end-user and created kind of an app store for on-shoploor purposes. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/thesis_stack.png)\n\nMy work is published at the [KITopen repository](https://publikationen.bibliothek.kit.edu/1000061764).\nIf you got curious and are interested in more details or in case you have questions or criticism, please feel free to contact me!\n","source":"_posts/design-of-a-linked-dataenabled-microservice-platform-for-the-industrial-internet-of-things.md","raw":"---\ntitle: >-\n  Design of a Linked Data-enabled Microservice Platform for the Industrial\n  Internet of Things\ndate: 2016-10-19 23:03:07\ntags:\n---\n\nAs the topic of my bachelor's thesis at the [TECO](http://teco.edu) and part of the [ScaleIT](https://scale-it.org) research project I've designed and developed an IoT software platform with focus on uniformity, openess and ease of adoption to tackle to problem of bringing flexible IT infrastructures to the industrial shopfloor.\nHere's my thesis' abstract to get an idea of the topic.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/thesis_mockup.png)\n\n### Abstract\n\nWhile recent trends towards highly digitized, smart factories entail substantial chances for manufacturing companies to boost their performance, flexibility and productivity, the industry commonly struggles to adopt suitable technology. One major problem is a lack of uniform, standardized solutions, which could be integrated to the shopfloor without the necessity of highly specialized technical expert knowledge and a large amount of planning and restructuring.\\par \nAddressing that problem, this work proposes an architecture design as well as a concrete implementation of a Internet Of Things software platform, which mainly focuses on technological uniformity and ease of adoption and usage. As a guideline, a real-world use case elaborated in cooperation with industry partners is presented. Further on, it is shown, how general purpose web technology can be combined advantageously with recent architectural-, operational- and cultural trends in software design, powerful machine-to-machine interaction techniques and common user experience concepts. In-depth thoughts on software structure, real-time communication, machine-to-machine interaction, uniform data integration and user experience are conducted to finally obtain a working proof-of-concept software alongside recommendations and best practices for adopting smart technology on the shopfloor. An eventual evaluation investigates the designed platform regarding both performance and suitability for particular real-world scenarios and recommends further endeavor to be conducted towards achieving an actual product. \n\n### Tech stack\n\nWe've used general-purpose web technology, Linked Data techniques, semantic hypermedia APIs with Hydra and state-of-the-art software development practices and cloud technology to fulfil our goal. We also picked up on the commonly understood UX concept of apps to abstract underlying technical details away from the end-user and created kind of an app store for on-shoploor purposes. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/thesis_stack.png)\n\nMy work is published at the [KITopen repository](https://publikationen.bibliothek.kit.edu/1000061764).\nIf you got curious and are interested in more details or in case you have questions or criticism, please feel free to contact me!\n","slug":"design-of-a-linked-dataenabled-microservice-platform-for-the-industrial-internet-of-things","published":1,"updated":"2020-10-30T20:05:40.281Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlp0008o2e0c7vdg6un","content":"<p>As the topic of my bachelors thesis at the <a href=\"http://teco.edu/\">TECO</a> and part of the <a href=\"https://scale-it.org/\">ScaleIT</a> research project Ive designed and developed an IoT software platform with focus on uniformity, openess and ease of adoption to tackle to problem of bringing flexible IT infrastructures to the industrial shopfloor.<br>Heres my thesis abstract to get an idea of the topic.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/thesis_mockup.png\"></p>\n<h3 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h3><p>While recent trends towards highly digitized, smart factories entail substantial chances for manufacturing companies to boost their performance, flexibility and productivity, the industry commonly struggles to adopt suitable technology. One major problem is a lack of uniform, standardized solutions, which could be integrated to the shopfloor without the necessity of highly specialized technical expert knowledge and a large amount of planning and restructuring.\\par<br>Addressing that problem, this work proposes an architecture design as well as a concrete implementation of a Internet Of Things software platform, which mainly focuses on technological uniformity and ease of adoption and usage. As a guideline, a real-world use case elaborated in cooperation with industry partners is presented. Further on, it is shown, how general purpose web technology can be combined advantageously with recent architectural-, operational- and cultural trends in software design, powerful machine-to-machine interaction techniques and common user experience concepts. In-depth thoughts on software structure, real-time communication, machine-to-machine interaction, uniform data integration and user experience are conducted to finally obtain a working proof-of-concept software alongside recommendations and best practices for adopting smart technology on the shopfloor. An eventual evaluation investigates the designed platform regarding both performance and suitability for particular real-world scenarios and recommends further endeavor to be conducted towards achieving an actual product. </p>\n<h3 id=\"Tech-stack\"><a href=\"#Tech-stack\" class=\"headerlink\" title=\"Tech stack\"></a>Tech stack</h3><p>Weve used general-purpose web technology, Linked Data techniques, semantic hypermedia APIs with Hydra and state-of-the-art software development practices and cloud technology to fulfil our goal. We also picked up on the commonly understood UX concept of apps to abstract underlying technical details away from the end-user and created kind of an app store for on-shoploor purposes. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/thesis_stack.png\"></p>\n<p>My work is published at the <a href=\"https://publikationen.bibliothek.kit.edu/1000061764\">KITopen repository</a>.<br>If you got curious and are interested in more details or in case you have questions or criticism, please feel free to contact me!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>As the topic of my bachelors thesis at the <a href=\"http://teco.edu/\">TECO</a> and part of the <a href=\"https://scale-it.org/\">ScaleIT</a> research project Ive designed and developed an IoT software platform with focus on uniformity, openess and ease of adoption to tackle to problem of bringing flexible IT infrastructures to the industrial shopfloor.<br>Heres my thesis abstract to get an idea of the topic.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/thesis_mockup.png\"></p>\n<h3 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h3><p>While recent trends towards highly digitized, smart factories entail substantial chances for manufacturing companies to boost their performance, flexibility and productivity, the industry commonly struggles to adopt suitable technology. One major problem is a lack of uniform, standardized solutions, which could be integrated to the shopfloor without the necessity of highly specialized technical expert knowledge and a large amount of planning and restructuring.\\par<br>Addressing that problem, this work proposes an architecture design as well as a concrete implementation of a Internet Of Things software platform, which mainly focuses on technological uniformity and ease of adoption and usage. As a guideline, a real-world use case elaborated in cooperation with industry partners is presented. Further on, it is shown, how general purpose web technology can be combined advantageously with recent architectural-, operational- and cultural trends in software design, powerful machine-to-machine interaction techniques and common user experience concepts. In-depth thoughts on software structure, real-time communication, machine-to-machine interaction, uniform data integration and user experience are conducted to finally obtain a working proof-of-concept software alongside recommendations and best practices for adopting smart technology on the shopfloor. An eventual evaluation investigates the designed platform regarding both performance and suitability for particular real-world scenarios and recommends further endeavor to be conducted towards achieving an actual product. </p>\n<h3 id=\"Tech-stack\"><a href=\"#Tech-stack\" class=\"headerlink\" title=\"Tech stack\"></a>Tech stack</h3><p>Weve used general-purpose web technology, Linked Data techniques, semantic hypermedia APIs with Hydra and state-of-the-art software development practices and cloud technology to fulfil our goal. We also picked up on the commonly understood UX concept of apps to abstract underlying technical details away from the end-user and created kind of an app store for on-shoploor purposes. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/thesis_stack.png\"></p>\n<p>My work is published at the <a href=\"https://publikationen.bibliothek.kit.edu/1000061764\">KITopen repository</a>.<br>If you got curious and are interested in more details or in case you have questions or criticism, please feel free to contact me!</p>\n"},{"title":"Digitalocean  My preferred Cloud Hosting Provider","date":"2016-04-06T20:55:18.000Z","_content":"\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/do.png)\n[DigitalOcean.com](https://digitalocean.com) is a service that offers you on-demand virtual server instances that you can use to host any server application, be it a simple webpage, a Node.js written backend, any Docker container or anything else.\n\nIt is especially useful if you have developed a web application and want to bring it to the internet without owning a root server. In this case you can go to DigitalOcean, choose any boilerplate (or Droplet, as they call it) for your new virtual, cloud-hosted machine, additionally choose a the datacenter region which is closest to you or your customers, add your SSH keys for quick access and hit the create button. Within less than a minute your machine is up and running with a dedicated IPv4 assigned where you can ssh in.\n\nAs a template / boilerplate you can either choose from the common, plain Linux distributions (even CoreOS) in almost any version or take one of the various pre-configured environments like a machine already running Docker, Node.js, ownCloud, Joomla or plenty other runtimes and applications.\n\nFor scalability you can choose between different sizes, which basically means different memory capacity, cpu cores, ssd capacity and amount of traffic.\n\nA feature which I only know from DigitalOcean by now is the ability to create a cluster of multiple machines (Droplets) with private networking, meaning they can communicate with every other node in the cluster but are kind of isolated from the internet. I havent tried this feature too much but it is similar to what you might know from linking multiple Docker containers together.\n\nWhat I also like about the service is the ultra simple-to-use, minimalistic and intuitive web-interface that abstracts away this entire technical complexity running in the background when users do a single click on a button until a pre-installed machine comes up.\n\nDigitalOcean is my personal favorite service of this type, but I also want to mention some alternatives which are [Microsoft Azure](https://azure.microsoft.com/en-us/), [Google Compute Engine](https://cloud.google.com/compute/), [Amazon EC2](https://aws.amazon.com/de/ec2), [Linode](https://www.linode.com/) or in a wider sense also [JiffyBox.de](http://jiffybox.de).\n\nIf you want to give DigitalOcean a try (and support me), follow [this referral link](https://m.do.co/c/4abee7f659ad) where you get $10 in credits, which is enough for running the smallest container for two months. I will get $25 in credits in case you in turn spend $25 for credits. Of course I would be very pleased if you did so.\n\n![](assets/img/simple-smile.png)","source":"_posts/digitalocean-my-preferred-cloud-hosting-provider.md","raw":"---\ntitle: Digitalocean  My preferred Cloud Hosting Provider\ndate: 2016-04-06 22:55:18\ntags:\n---\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/do.png)\n[DigitalOcean.com](https://digitalocean.com) is a service that offers you on-demand virtual server instances that you can use to host any server application, be it a simple webpage, a Node.js written backend, any Docker container or anything else.\n\nIt is especially useful if you have developed a web application and want to bring it to the internet without owning a root server. In this case you can go to DigitalOcean, choose any boilerplate (or Droplet, as they call it) for your new virtual, cloud-hosted machine, additionally choose a the datacenter region which is closest to you or your customers, add your SSH keys for quick access and hit the create button. Within less than a minute your machine is up and running with a dedicated IPv4 assigned where you can ssh in.\n\nAs a template / boilerplate you can either choose from the common, plain Linux distributions (even CoreOS) in almost any version or take one of the various pre-configured environments like a machine already running Docker, Node.js, ownCloud, Joomla or plenty other runtimes and applications.\n\nFor scalability you can choose between different sizes, which basically means different memory capacity, cpu cores, ssd capacity and amount of traffic.\n\nA feature which I only know from DigitalOcean by now is the ability to create a cluster of multiple machines (Droplets) with private networking, meaning they can communicate with every other node in the cluster but are kind of isolated from the internet. I havent tried this feature too much but it is similar to what you might know from linking multiple Docker containers together.\n\nWhat I also like about the service is the ultra simple-to-use, minimalistic and intuitive web-interface that abstracts away this entire technical complexity running in the background when users do a single click on a button until a pre-installed machine comes up.\n\nDigitalOcean is my personal favorite service of this type, but I also want to mention some alternatives which are [Microsoft Azure](https://azure.microsoft.com/en-us/), [Google Compute Engine](https://cloud.google.com/compute/), [Amazon EC2](https://aws.amazon.com/de/ec2), [Linode](https://www.linode.com/) or in a wider sense also [JiffyBox.de](http://jiffybox.de).\n\nIf you want to give DigitalOcean a try (and support me), follow [this referral link](https://m.do.co/c/4abee7f659ad) where you get $10 in credits, which is enough for running the smallest container for two months. I will get $25 in credits in case you in turn spend $25 for credits. Of course I would be very pleased if you did so.\n\n![](assets/img/simple-smile.png)","slug":"digitalocean-my-preferred-cloud-hosting-provider","published":1,"updated":"2020-10-30T20:05:40.282Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlr0009o2e0f2v6dd2r","content":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/do.png\"><br><a href=\"https://digitalocean.com/\">DigitalOcean.com</a> is a service that offers you on-demand virtual server instances that you can use to host any server application, be it a simple webpage, a Node.js written backend, any Docker container or anything else.</p>\n<p>It is especially useful if you have developed a web application and want to bring it to the internet without owning a root server. In this case you can go to DigitalOcean, choose any boilerplate (or Droplet, as they call it) for your new virtual, cloud-hosted machine, additionally choose a the datacenter region which is closest to you or your customers, add your SSH keys for quick access and hit the create button. Within less than a minute your machine is up and running with a dedicated IPv4 assigned where you can ssh in.</p>\n<p>As a template / boilerplate you can either choose from the common, plain Linux distributions (even CoreOS) in almost any version or take one of the various pre-configured environments like a machine already running Docker, Node.js, ownCloud, Joomla or plenty other runtimes and applications.</p>\n<p>For scalability you can choose between different sizes, which basically means different memory capacity, cpu cores, ssd capacity and amount of traffic.</p>\n<p>A feature which I only know from DigitalOcean by now is the ability to create a cluster of multiple machines (Droplets) with private networking, meaning they can communicate with every other node in the cluster but are kind of isolated from the internet. I havent tried this feature too much but it is similar to what you might know from linking multiple Docker containers together.</p>\n<p>What I also like about the service is the ultra simple-to-use, minimalistic and intuitive web-interface that abstracts away this entire technical complexity running in the background when users do a single click on a button until a pre-installed machine comes up.</p>\n<p>DigitalOcean is my personal favorite service of this type, but I also want to mention some alternatives which are <a href=\"https://azure.microsoft.com/en-us/\">Microsoft Azure</a>, <a href=\"https://cloud.google.com/compute/\">Google Compute Engine</a>, <a href=\"https://aws.amazon.com/de/ec2\">Amazon EC2</a>, <a href=\"https://www.linode.com/\">Linode</a> or in a wider sense also <a href=\"http://jiffybox.de/\">JiffyBox.de</a>.</p>\n<p>If you want to give DigitalOcean a try (and support me), follow <a href=\"https://m.do.co/c/4abee7f659ad\">this referral link</a> where you get $10 in credits, which is enough for running the smallest container for two months. I will get $25 in credits in case you in turn spend $25 for credits. Of course I would be very pleased if you did so.</p>\n<p><img src=\"assets/img/simple-smile.png\"></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/do.png\"><br><a href=\"https://digitalocean.com/\">DigitalOcean.com</a> is a service that offers you on-demand virtual server instances that you can use to host any server application, be it a simple webpage, a Node.js written backend, any Docker container or anything else.</p>\n<p>It is especially useful if you have developed a web application and want to bring it to the internet without owning a root server. In this case you can go to DigitalOcean, choose any boilerplate (or Droplet, as they call it) for your new virtual, cloud-hosted machine, additionally choose a the datacenter region which is closest to you or your customers, add your SSH keys for quick access and hit the create button. Within less than a minute your machine is up and running with a dedicated IPv4 assigned where you can ssh in.</p>\n<p>As a template / boilerplate you can either choose from the common, plain Linux distributions (even CoreOS) in almost any version or take one of the various pre-configured environments like a machine already running Docker, Node.js, ownCloud, Joomla or plenty other runtimes and applications.</p>\n<p>For scalability you can choose between different sizes, which basically means different memory capacity, cpu cores, ssd capacity and amount of traffic.</p>\n<p>A feature which I only know from DigitalOcean by now is the ability to create a cluster of multiple machines (Droplets) with private networking, meaning they can communicate with every other node in the cluster but are kind of isolated from the internet. I havent tried this feature too much but it is similar to what you might know from linking multiple Docker containers together.</p>\n<p>What I also like about the service is the ultra simple-to-use, minimalistic and intuitive web-interface that abstracts away this entire technical complexity running in the background when users do a single click on a button until a pre-installed machine comes up.</p>\n<p>DigitalOcean is my personal favorite service of this type, but I also want to mention some alternatives which are <a href=\"https://azure.microsoft.com/en-us/\">Microsoft Azure</a>, <a href=\"https://cloud.google.com/compute/\">Google Compute Engine</a>, <a href=\"https://aws.amazon.com/de/ec2\">Amazon EC2</a>, <a href=\"https://www.linode.com/\">Linode</a> or in a wider sense also <a href=\"http://jiffybox.de/\">JiffyBox.de</a>.</p>\n<p>If you want to give DigitalOcean a try (and support me), follow <a href=\"https://m.do.co/c/4abee7f659ad\">this referral link</a> where you get $10 in credits, which is enough for running the smallest container for two months. I will get $25 in credits in case you in turn spend $25 for credits. Of course I would be very pleased if you did so.</p>\n<p><img src=\"assets/img/simple-smile.png\"></p>\n"},{"title":"Exploratory Analysis on GitHub Data","date":"2019-04-11T19:00:59.000Z","image":"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_location_langs.png","_content":"\n# Background\nA few days ago, I wrote a crawler (with NodeJS and [Sequelize](http://docs.sequelizejs.com/)) that fetches publicly available data from GitHub's [GraphQL API](https://developer.github.com/v4/). More precisely, I downloaded information about users, repositories, programming languages and topics.\n\nAfter running the crawler for a few days, I ended up with **154,248 user profiles**, **993,919 repositories** and **351 languages**, many of which I had never heard of (e.g. did you know about _PogoScript_?). However, although my MySQL database is already 953 MB in size with only these data, I barely crawled 0.4 % of all user profiles (~ 31 million).\n\nThe first (less extensive) version of my database  which I performed the following analyses on  looked like this.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_eer.png)\n\nWhile one could argue that the data I collected is not of a representative sample size, I still wanted to do some data analysis on it  just for fun.\n\n# Analyses\nTo perform the analyses, I used Python 3 with Pandas and Matplotlib. \n```python\nimport apriori\nimport pymysql\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sqlalchemy import create_engine\n\n%matplotlib inline\n\npymysql.install_as_MySQLdb()\n\nsql_engine = create_engine('mysql://user:heheyouwish@localhost:3306/github_data', echo=False)\nconnection = sql_engine.raw_connection()\n```\n\n## Most popular programming languages\nOne of the first and most obvious thing to check (for the sake of brevity I'll skip basic data set statistics like count, mean, variance, ...) is which languages are most widely used.\n\n```python\ndf_top_langs = pd.read_sql_query('''\n    select LanguageName, count(LanguageName) as count from RepositoryLanguages\n    group by LanguageName\n    order by count(LanguageName) desc\n    limit 10;\n''', con=connection)\ndf_top_langs.set_index('LanguageName').plot.bar(figsize=(12,8))\n```\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_popular_lang.png)\n\nNot too surprisingly, the typical web stack consisting of JavaScript, HTML and CSS, is among the most popular programming languages, according to how often they appear in repositories.\n\n## Least popular programming languages\nA little more interesting is to see, which programming languages occur least.\n```python\ndf_last_langs = pd.read_sql_query('''\n    select LanguageName, count(LanguageName) as count from RepositoryLanguages\n    group by LanguageName\n    order by count(LanguageName) asc\n    limit 10;\n''', con=connection)\nprint(df_last_langs)\n```\n\nHere are the results. Have you heard of any one of them? I didn't.\n```\n  LanguageName  count\n0          Nit      1\n1       Myghty      1\n2   Public Key      1\n3  DCPU-16 ASM      1\n4   TI Program      1\n5        Genie      1\n6           Ox      1\n7   PogoScript      1\n8        Cirru      1\n9        JFlex      2\n```\n\n# User Skills\nLet's analyze the users' skills in terms of languages. I decided to consider a user being \"skilled\" in a certain language if at least 10 % of her repositories' code is in that language. \n\n```python\nN = int(1e7)\nMIN_SUPP = .0005\nMIN_CONF = .45\nMIN_LANG_RATIO = .1\n\ndf_skills = pd.read_sql_query(f'''\n    select RepositoryLanguages.LanguageName, RepositoryLanguages.size, Users.login, Users.location from RepositoryLanguages\n    left join Repositories on Repositories.id = RepositoryLanguages.RepositoryId\n    right join Users on Users.login = Repositories.userLogin\n    limit {N}\n''', con=connection)\n\ndf_skills = df_skills.merge(pd.DataFrame(df_skills.groupby('login')['size'].sum()), how='left', on='login').rename(columns={'size_x': 'size', 'size_y': 'totalSize'})\ndf_skills = df_skills[df_skills['totalSize'] > 0]\ndf_skills['sizeRatio'] = df_skills['size'] / df_skills['totalSize']\n\nprint(f\"{df_skills['login'].unique().size} users\")\nprint(f\"{df_skills['LanguageName'].unique().size} languages\")\n\n# Output:\n# 130402 users\n# 351 languages\n```\n\n### Association Rules\nWhat I wanted to look at is combinations of different skills, i.e. languages that usually occur together as developer skills. One approach to get insights like these is to mine the data for _association rules_, e.g. using an algorithm like [Apriori](https://en.wikipedia.org/wiki/Apriori_algorithm) (as I did). The implementation I used was [asaini/Apriori](https://github.com/asaini/Apriori).\n\n```python\nuser_langs = df_skills[df_skills['sizeRatio'] >= MIN_LANG_RATIO].groupby('login')['LanguageName'].apply(set).values\nitems1, rules1 = apriori.runApriori(user_langs, MIN_SUPP, MIN_CONF)\nrules1 = sorted(rules1, key=lambda e: e[1], reverse=True)\nprint(rules1)\n```\n\n**Output:**\n```\n[((('ShaderLab',), ('C#',)), 0.904),\n ((('Vue',), ('JavaScript',)), 0.671277997364954),\n ((('Vue', 'CSS'), ('JavaScript',)), 0.656140350877193),\n ((('GLSL',), ('C#',)), 0.625),\n ((('CMake',), ('C++',)), 0.6229508196721312),\n ((('CSS',), ('JavaScript',)), 0.5807683959192532),\n ((('Tcl',), ('Python',)), 0.5658914728682171),\n ((('Kotlin',), ('Java',)), 0.5655375552282769),\n ((('ASP',), ('C#',)), 0.5488215488215488),\n ((('Vue', 'HTML'), ('JavaScript',)), 0.5404411764705882),\n ((('CoffeeScript',), ('JavaScript',)), 0.5339578454332553),\n ((('CSS', 'PHP'), ('JavaScript',)), 0.5116117850953206),\n ((('Elm',), ('JavaScript',)), 0.4951923076923077),\n ((('CSS', 'HTML'), ('JavaScript',)), 0.4906486271388778),\n ((('Smarty',), ('PHP',)), 0.4788732394366197),\n ((('TypeScript',), ('JavaScript',)), 0.4739540607054964),\n ((('CSS', 'C#'), ('JavaScript',)), 0.464926590538336),\n ((('Groovy',), ('Java',)), 0.4604651162790698)]\n```\n\nThe left part of each row is a tuple of tuples of programming languages that represent an association rule. The right part is the [confidence](https://en.wikipedia.org/wiki/Association_rule_learning#Confidence) of that rule.\n\n**For example:**\nRead `((('ShaderLab',), ('C#',)), 0.904)` as \"90 % of all people who know _ShaderLab_ also know C#\".\n\nThe results reflect common sense. For instance, the rule that developers, who know _VueJS_, also know _JavaScript_ seems to make sense, given that VueJS is a JavaScript framework. Analogously, _CMake_ is a common build tool for _C++_, etc. Nothing too fancy here, except for that I didn't know about _ShaderLab_ and _GLSL_.\n\n## Locations\nLet's take a look at where most GitHub users are from. Obviously, this only respects profiles where users have set their locations.\n\n```python\ndf_locations = df1.reindex(['location'], axis=1).groupby('location').size()\ndf_locations = df_locations.sort_values(ascending=False)[:20]\ndf_locations.plot.bar(figsize=(12,8))\n```\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_locations.png)\n\nClearly, San Francisco seems to be the most popular city for developers (or at least for those who are active on GitHub). \n\n## Skills by location\nTo take this a step further, let's take a look at which skills users tend to have in what cities.\n\n```python\ndef language_replace(df):\n    df = df.copy()\n    # Little bit of manual cleaning\n    replace = {'San Francisco': 'San Francisco, CA',\n               'Berlin': 'Berlin, Germany',\n               'New York': 'New York, NY',\n               'London': 'London, UK',\n               'Beijing': 'Beijing, China',\n               'Paris': 'Paris, France'}\n    for (k, v) in replace.items():\n        if isinstance(df, pd.DataFrame):\n            if k in df.columns and v in df.columns:\n                df[k] = df[k] + df[v]\n                df = df.drop([v], axis=1, errors='ignore')\n        else:\n            if k in df.index and v in df.index:\n                df[k] = df[k] + df[v]\n                #df = df.drop([v], axis=1)\n                del df[v]\n    return df\n\nlangs_by_loc = {}\nfor l in df_locations.index:\n    langs_by_loc[l] = df1[df1['location'] == l][['LanguageName']].groupby('LanguageName').size()\ndf_loc_langs = pd.DataFrame.from_dict(langs_by_loc).fillna(0)\n\ndf_loc_langs = language_replace(df_loc_langs)\ndf_loc_langs = df_loc_langs.T\ndf_loc_langs = df_loc_langs.drop([c for c in df_loc_langs.columns if c not in df_top_langs['LanguageName'].values], axis=1)\n\ndf_loc_langs = (df_loc_langs.T / df_loc_langs.T.sum()).T # normalize heights\ndf_loc_langs.plot.bar(stacked=True, figsize=(16,10))\n```\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_location_langs.png)\n\nLook like there are no real outliers in the distribution of developer skills between different cities of the world. Maybe you could say that, e.g., Indians like web frontends a little more than command-line hacking.\n\n## Skills: Karlsruhe vs. the World\nWhile an overview is cool, I found it even more interesting to specifically compare between to cities. So in the following chart I compare language-specific programming skills in Karlsruhe (the city where I live and study) to the rest of the world's average.\n\n```python\ndf_ka = df1[df1['location'] == 'Karlsruhe'][['LanguageName']].groupby('LanguageName').size()\ndf_ka = pd.DataFrame(df_ka, index=df_ka.index, columns=['Karlsruhe']) / df_ka.sum()\ndf_world = pd.DataFrame(df_loc_langs.mean(), index=df_loc_langs.mean().index, columns=['World'])\ndf_compare = df_world.merge(df_ka, how='left', left_index=True, right_index=True)\nax = df_compare.plot.barh(title='Languages: World vs. Karlsruhe', legend=True, figsize=(10,5))\nax.set_xlabel('Percentage (Top 10)')\nax.set_ylabel('Programming Language Skills')\n```\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_ka_world.png) \n\nThese results are a bit surprising to me. Clearly, Karlsruhe-based developers seem to dislike JavaScript compared to the world. However, this is different from what I experienced in several student jobs and internships here.\n\n## Project Tech Stacks\nLast but not least, let's apply Apriori once more, but this time in a slightly different way. Instead of looking at user skills, let's look at languages that occur together on a per-repository basis. And instead of trying to find rules, let's only look at _frequent item sets_ (which are the basis for rules). My expectation was to get back sets of commonly used tech stacks. \n\n```python\nN = int(1e7)\nMIN_SUPP = .0005\nMIN_CONF = .45\nMIN_LANG_RATIO = .1\n\ndf_stacks = pd.read_sql_query(f'''\n    select LanguageName, size, RepositoryId from RepositoryLanguages\n    order by RepositoryId\n    limit {N}\n''', con=connection)\n\ndf_stacks = df_stacks.merge(pd.DataFrame(df_stacks.groupby('RepositoryId')['size'].sum()), how='left', on='RepositoryId').rename(columns={'size_x': 'size', 'size_y': 'totalSize'})\ndf_stacks = df_stacks[df_stacks['totalSize'] > 0]\ndf_stacks['sizeRatio'] = df_stacks['size'] / df_stacks['totalSize']\n\nprint(f\"{df_stacks['RepositoryId'].unique().size} repositories\")\nprint(f\"{df_stacks['LanguageName'].unique().size} languages\")\n\n# Output: \n# 853114 repositories\n# 351 languages\n```\n\n```python\nrepo_langs = df_stacks[df_stacks['sizeRatio'] >= MIN_LANG_RATIO].groupby('RepositoryId')['LanguageName'].apply(set).values\nitems2, rules2 = apriori.runApriori(repo_langs, MIN_SUPP, MIN_CONF)\nitemsets2 = sorted(list(filter(lambda i: len(i[0]) > 2, items2)), key=lambda i: i[1], reverse=True)\nprint(itemsets2)\n```\n\n**Output:**\n```\n[(('CSS', 'JavaScript', 'HTML'), 0.04360026913167525),\n (('CSS', 'JavaScript', 'PHP'), 0.0045574213997191465),\n (('Ruby', 'CSS', 'HTML'), 0.004456614239128651),\n (('TypeScript', 'JavaScript', 'HTML'), 0.0042034241613664765),\n (('TypeScript', 'HTML', 'CSS'), 0.0035024627423767517),\n (('Python', 'JavaScript', 'HTML'), 0.002962089474560258),\n (('Python', 'HTML', 'CSS'), 0.002769852563666755),\n (('Ruby', 'JavaScript', 'HTML'), 0.0022400288824236856),\n (('JavaScript', 'HTML', 'PHP'), 0.0022154131804190294),\n (('Ruby', 'CSS', 'JavaScript'), 0.0021532878372644217),\n (('CSS', 'HTML', 'PHP'), 0.0019915275098052547),\n (('JavaScript', 'Objective-C', 'Java'), 0.0018614159420663593),\n (('CSS', 'JavaScript', 'Python'), 0.0017992905989117516),\n (('Python', 'JavaScript', 'Objective-C'), 0.0017735027206211597),\n (('Python', 'JavaScript', 'Java'), 0.001508590879999625),\n (('CSS', 'JavaScript', 'TypeScript'), 0.0014745977677074812),\n (('Python', 'Objective-C', 'Java'), 0.0014066115431231934),\n (('Python', 'JavaScript', 'Objective-C', 'Java'), 0.0013222148505358019),\n (('Vue', 'CSS', 'JavaScript'), 0.0012554008022374501)]\n```\n\nHere, the left side is sets of frequently occurring combinations of languages. The right side is the set's [support](https://en.wikipedia.org/wiki/Association_rule_learning#Support), which is the relative occurrences of that set among the whole data set.\nObviously, many of these are actually common \"tech stacks\" and almost all of them are web technologies. I guess GitHub is most popular among web developers. \n\n# Conclusion\nThere is a lot of more complex analyses that could be might on rich data like this and probably tools like [BigQuery](https://cloud.google.com/bigquery/public-data/) are better suitable than Pandas, operating on a tiny sample. However, I used this little project to improve my EDA skills and hopefully give you guys an interesting article to read. Let me know if you like it!","source":"_posts/exploratory-analysis-on-github-data.md","raw":"---\ntitle: Exploratory Analysis on GitHub Data\ndate: 2019-04-11 21:00:59\ntags:\nimage: https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_location_langs.png\n---\n\n# Background\nA few days ago, I wrote a crawler (with NodeJS and [Sequelize](http://docs.sequelizejs.com/)) that fetches publicly available data from GitHub's [GraphQL API](https://developer.github.com/v4/). More precisely, I downloaded information about users, repositories, programming languages and topics.\n\nAfter running the crawler for a few days, I ended up with **154,248 user profiles**, **993,919 repositories** and **351 languages**, many of which I had never heard of (e.g. did you know about _PogoScript_?). However, although my MySQL database is already 953 MB in size with only these data, I barely crawled 0.4 % of all user profiles (~ 31 million).\n\nThe first (less extensive) version of my database  which I performed the following analyses on  looked like this.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_eer.png)\n\nWhile one could argue that the data I collected is not of a representative sample size, I still wanted to do some data analysis on it  just for fun.\n\n# Analyses\nTo perform the analyses, I used Python 3 with Pandas and Matplotlib. \n```python\nimport apriori\nimport pymysql\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sqlalchemy import create_engine\n\n%matplotlib inline\n\npymysql.install_as_MySQLdb()\n\nsql_engine = create_engine('mysql://user:heheyouwish@localhost:3306/github_data', echo=False)\nconnection = sql_engine.raw_connection()\n```\n\n## Most popular programming languages\nOne of the first and most obvious thing to check (for the sake of brevity I'll skip basic data set statistics like count, mean, variance, ...) is which languages are most widely used.\n\n```python\ndf_top_langs = pd.read_sql_query('''\n    select LanguageName, count(LanguageName) as count from RepositoryLanguages\n    group by LanguageName\n    order by count(LanguageName) desc\n    limit 10;\n''', con=connection)\ndf_top_langs.set_index('LanguageName').plot.bar(figsize=(12,8))\n```\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_popular_lang.png)\n\nNot too surprisingly, the typical web stack consisting of JavaScript, HTML and CSS, is among the most popular programming languages, according to how often they appear in repositories.\n\n## Least popular programming languages\nA little more interesting is to see, which programming languages occur least.\n```python\ndf_last_langs = pd.read_sql_query('''\n    select LanguageName, count(LanguageName) as count from RepositoryLanguages\n    group by LanguageName\n    order by count(LanguageName) asc\n    limit 10;\n''', con=connection)\nprint(df_last_langs)\n```\n\nHere are the results. Have you heard of any one of them? I didn't.\n```\n  LanguageName  count\n0          Nit      1\n1       Myghty      1\n2   Public Key      1\n3  DCPU-16 ASM      1\n4   TI Program      1\n5        Genie      1\n6           Ox      1\n7   PogoScript      1\n8        Cirru      1\n9        JFlex      2\n```\n\n# User Skills\nLet's analyze the users' skills in terms of languages. I decided to consider a user being \"skilled\" in a certain language if at least 10 % of her repositories' code is in that language. \n\n```python\nN = int(1e7)\nMIN_SUPP = .0005\nMIN_CONF = .45\nMIN_LANG_RATIO = .1\n\ndf_skills = pd.read_sql_query(f'''\n    select RepositoryLanguages.LanguageName, RepositoryLanguages.size, Users.login, Users.location from RepositoryLanguages\n    left join Repositories on Repositories.id = RepositoryLanguages.RepositoryId\n    right join Users on Users.login = Repositories.userLogin\n    limit {N}\n''', con=connection)\n\ndf_skills = df_skills.merge(pd.DataFrame(df_skills.groupby('login')['size'].sum()), how='left', on='login').rename(columns={'size_x': 'size', 'size_y': 'totalSize'})\ndf_skills = df_skills[df_skills['totalSize'] > 0]\ndf_skills['sizeRatio'] = df_skills['size'] / df_skills['totalSize']\n\nprint(f\"{df_skills['login'].unique().size} users\")\nprint(f\"{df_skills['LanguageName'].unique().size} languages\")\n\n# Output:\n# 130402 users\n# 351 languages\n```\n\n### Association Rules\nWhat I wanted to look at is combinations of different skills, i.e. languages that usually occur together as developer skills. One approach to get insights like these is to mine the data for _association rules_, e.g. using an algorithm like [Apriori](https://en.wikipedia.org/wiki/Apriori_algorithm) (as I did). The implementation I used was [asaini/Apriori](https://github.com/asaini/Apriori).\n\n```python\nuser_langs = df_skills[df_skills['sizeRatio'] >= MIN_LANG_RATIO].groupby('login')['LanguageName'].apply(set).values\nitems1, rules1 = apriori.runApriori(user_langs, MIN_SUPP, MIN_CONF)\nrules1 = sorted(rules1, key=lambda e: e[1], reverse=True)\nprint(rules1)\n```\n\n**Output:**\n```\n[((('ShaderLab',), ('C#',)), 0.904),\n ((('Vue',), ('JavaScript',)), 0.671277997364954),\n ((('Vue', 'CSS'), ('JavaScript',)), 0.656140350877193),\n ((('GLSL',), ('C#',)), 0.625),\n ((('CMake',), ('C++',)), 0.6229508196721312),\n ((('CSS',), ('JavaScript',)), 0.5807683959192532),\n ((('Tcl',), ('Python',)), 0.5658914728682171),\n ((('Kotlin',), ('Java',)), 0.5655375552282769),\n ((('ASP',), ('C#',)), 0.5488215488215488),\n ((('Vue', 'HTML'), ('JavaScript',)), 0.5404411764705882),\n ((('CoffeeScript',), ('JavaScript',)), 0.5339578454332553),\n ((('CSS', 'PHP'), ('JavaScript',)), 0.5116117850953206),\n ((('Elm',), ('JavaScript',)), 0.4951923076923077),\n ((('CSS', 'HTML'), ('JavaScript',)), 0.4906486271388778),\n ((('Smarty',), ('PHP',)), 0.4788732394366197),\n ((('TypeScript',), ('JavaScript',)), 0.4739540607054964),\n ((('CSS', 'C#'), ('JavaScript',)), 0.464926590538336),\n ((('Groovy',), ('Java',)), 0.4604651162790698)]\n```\n\nThe left part of each row is a tuple of tuples of programming languages that represent an association rule. The right part is the [confidence](https://en.wikipedia.org/wiki/Association_rule_learning#Confidence) of that rule.\n\n**For example:**\nRead `((('ShaderLab',), ('C#',)), 0.904)` as \"90 % of all people who know _ShaderLab_ also know C#\".\n\nThe results reflect common sense. For instance, the rule that developers, who know _VueJS_, also know _JavaScript_ seems to make sense, given that VueJS is a JavaScript framework. Analogously, _CMake_ is a common build tool for _C++_, etc. Nothing too fancy here, except for that I didn't know about _ShaderLab_ and _GLSL_.\n\n## Locations\nLet's take a look at where most GitHub users are from. Obviously, this only respects profiles where users have set their locations.\n\n```python\ndf_locations = df1.reindex(['location'], axis=1).groupby('location').size()\ndf_locations = df_locations.sort_values(ascending=False)[:20]\ndf_locations.plot.bar(figsize=(12,8))\n```\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_locations.png)\n\nClearly, San Francisco seems to be the most popular city for developers (or at least for those who are active on GitHub). \n\n## Skills by location\nTo take this a step further, let's take a look at which skills users tend to have in what cities.\n\n```python\ndef language_replace(df):\n    df = df.copy()\n    # Little bit of manual cleaning\n    replace = {'San Francisco': 'San Francisco, CA',\n               'Berlin': 'Berlin, Germany',\n               'New York': 'New York, NY',\n               'London': 'London, UK',\n               'Beijing': 'Beijing, China',\n               'Paris': 'Paris, France'}\n    for (k, v) in replace.items():\n        if isinstance(df, pd.DataFrame):\n            if k in df.columns and v in df.columns:\n                df[k] = df[k] + df[v]\n                df = df.drop([v], axis=1, errors='ignore')\n        else:\n            if k in df.index and v in df.index:\n                df[k] = df[k] + df[v]\n                #df = df.drop([v], axis=1)\n                del df[v]\n    return df\n\nlangs_by_loc = {}\nfor l in df_locations.index:\n    langs_by_loc[l] = df1[df1['location'] == l][['LanguageName']].groupby('LanguageName').size()\ndf_loc_langs = pd.DataFrame.from_dict(langs_by_loc).fillna(0)\n\ndf_loc_langs = language_replace(df_loc_langs)\ndf_loc_langs = df_loc_langs.T\ndf_loc_langs = df_loc_langs.drop([c for c in df_loc_langs.columns if c not in df_top_langs['LanguageName'].values], axis=1)\n\ndf_loc_langs = (df_loc_langs.T / df_loc_langs.T.sum()).T # normalize heights\ndf_loc_langs.plot.bar(stacked=True, figsize=(16,10))\n```\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_location_langs.png)\n\nLook like there are no real outliers in the distribution of developer skills between different cities of the world. Maybe you could say that, e.g., Indians like web frontends a little more than command-line hacking.\n\n## Skills: Karlsruhe vs. the World\nWhile an overview is cool, I found it even more interesting to specifically compare between to cities. So in the following chart I compare language-specific programming skills in Karlsruhe (the city where I live and study) to the rest of the world's average.\n\n```python\ndf_ka = df1[df1['location'] == 'Karlsruhe'][['LanguageName']].groupby('LanguageName').size()\ndf_ka = pd.DataFrame(df_ka, index=df_ka.index, columns=['Karlsruhe']) / df_ka.sum()\ndf_world = pd.DataFrame(df_loc_langs.mean(), index=df_loc_langs.mean().index, columns=['World'])\ndf_compare = df_world.merge(df_ka, how='left', left_index=True, right_index=True)\nax = df_compare.plot.barh(title='Languages: World vs. Karlsruhe', legend=True, figsize=(10,5))\nax.set_xlabel('Percentage (Top 10)')\nax.set_ylabel('Programming Language Skills')\n```\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_ka_world.png) \n\nThese results are a bit surprising to me. Clearly, Karlsruhe-based developers seem to dislike JavaScript compared to the world. However, this is different from what I experienced in several student jobs and internships here.\n\n## Project Tech Stacks\nLast but not least, let's apply Apriori once more, but this time in a slightly different way. Instead of looking at user skills, let's look at languages that occur together on a per-repository basis. And instead of trying to find rules, let's only look at _frequent item sets_ (which are the basis for rules). My expectation was to get back sets of commonly used tech stacks. \n\n```python\nN = int(1e7)\nMIN_SUPP = .0005\nMIN_CONF = .45\nMIN_LANG_RATIO = .1\n\ndf_stacks = pd.read_sql_query(f'''\n    select LanguageName, size, RepositoryId from RepositoryLanguages\n    order by RepositoryId\n    limit {N}\n''', con=connection)\n\ndf_stacks = df_stacks.merge(pd.DataFrame(df_stacks.groupby('RepositoryId')['size'].sum()), how='left', on='RepositoryId').rename(columns={'size_x': 'size', 'size_y': 'totalSize'})\ndf_stacks = df_stacks[df_stacks['totalSize'] > 0]\ndf_stacks['sizeRatio'] = df_stacks['size'] / df_stacks['totalSize']\n\nprint(f\"{df_stacks['RepositoryId'].unique().size} repositories\")\nprint(f\"{df_stacks['LanguageName'].unique().size} languages\")\n\n# Output: \n# 853114 repositories\n# 351 languages\n```\n\n```python\nrepo_langs = df_stacks[df_stacks['sizeRatio'] >= MIN_LANG_RATIO].groupby('RepositoryId')['LanguageName'].apply(set).values\nitems2, rules2 = apriori.runApriori(repo_langs, MIN_SUPP, MIN_CONF)\nitemsets2 = sorted(list(filter(lambda i: len(i[0]) > 2, items2)), key=lambda i: i[1], reverse=True)\nprint(itemsets2)\n```\n\n**Output:**\n```\n[(('CSS', 'JavaScript', 'HTML'), 0.04360026913167525),\n (('CSS', 'JavaScript', 'PHP'), 0.0045574213997191465),\n (('Ruby', 'CSS', 'HTML'), 0.004456614239128651),\n (('TypeScript', 'JavaScript', 'HTML'), 0.0042034241613664765),\n (('TypeScript', 'HTML', 'CSS'), 0.0035024627423767517),\n (('Python', 'JavaScript', 'HTML'), 0.002962089474560258),\n (('Python', 'HTML', 'CSS'), 0.002769852563666755),\n (('Ruby', 'JavaScript', 'HTML'), 0.0022400288824236856),\n (('JavaScript', 'HTML', 'PHP'), 0.0022154131804190294),\n (('Ruby', 'CSS', 'JavaScript'), 0.0021532878372644217),\n (('CSS', 'HTML', 'PHP'), 0.0019915275098052547),\n (('JavaScript', 'Objective-C', 'Java'), 0.0018614159420663593),\n (('CSS', 'JavaScript', 'Python'), 0.0017992905989117516),\n (('Python', 'JavaScript', 'Objective-C'), 0.0017735027206211597),\n (('Python', 'JavaScript', 'Java'), 0.001508590879999625),\n (('CSS', 'JavaScript', 'TypeScript'), 0.0014745977677074812),\n (('Python', 'Objective-C', 'Java'), 0.0014066115431231934),\n (('Python', 'JavaScript', 'Objective-C', 'Java'), 0.0013222148505358019),\n (('Vue', 'CSS', 'JavaScript'), 0.0012554008022374501)]\n```\n\nHere, the left side is sets of frequently occurring combinations of languages. The right side is the set's [support](https://en.wikipedia.org/wiki/Association_rule_learning#Support), which is the relative occurrences of that set among the whole data set.\nObviously, many of these are actually common \"tech stacks\" and almost all of them are web technologies. I guess GitHub is most popular among web developers. \n\n# Conclusion\nThere is a lot of more complex analyses that could be might on rich data like this and probably tools like [BigQuery](https://cloud.google.com/bigquery/public-data/) are better suitable than Pandas, operating on a tiny sample. However, I used this little project to improve my EDA skills and hopefully give you guys an interesting article to read. Let me know if you like it!","slug":"exploratory-analysis-on-github-data","published":1,"updated":"2020-10-30T20:05:40.282Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqls000ao2e07djic0xr","content":"<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><p>A few days ago, I wrote a crawler (with NodeJS and <a href=\"http://docs.sequelizejs.com/\">Sequelize</a>) that fetches publicly available data from GitHubs <a href=\"https://developer.github.com/v4/\">GraphQL API</a>. More precisely, I downloaded information about users, repositories, programming languages and topics.</p>\n<p>After running the crawler for a few days, I ended up with <strong>154,248 user profiles</strong>, <strong>993,919 repositories</strong> and <strong>351 languages</strong>, many of which I had never heard of (e.g. did you know about <em>PogoScript</em>?). However, although my MySQL database is already 953 MB in size with only these data, I barely crawled 0.4 % of all user profiles (~ 31 million).</p>\n<p>The first (less extensive) version of my database  which I performed the following analyses on  looked like this.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_eer.png\"></p>\n<p>While one could argue that the data I collected is not of a representative sample size, I still wanted to do some data analysis on it  just for fun.</p>\n<h1 id=\"Analyses\"><a href=\"#Analyses\" class=\"headerlink\" title=\"Analyses\"></a>Analyses</h1><p>To perform the analyses, I used Python 3 with Pandas and Matplotlib. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> apriori</span><br><span class=\"line\"><span class=\"keyword\">import</span> pymysql</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sqlalchemy <span class=\"keyword\">import</span> create_engine</span><br><span class=\"line\"></span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"></span><br><span class=\"line\">pymysql.install_as_MySQLdb()</span><br><span class=\"line\"></span><br><span class=\"line\">sql_engine = create_engine(<span class=\"string\">&#x27;mysql://user:heheyouwish@localhost:3306/github_data&#x27;</span>, echo=<span class=\"literal\">False</span>)</span><br><span class=\"line\">connection = sql_engine.raw_connection()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Most-popular-programming-languages\"><a href=\"#Most-popular-programming-languages\" class=\"headerlink\" title=\"Most popular programming languages\"></a>Most popular programming languages</h2><p>One of the first and most obvious thing to check (for the sake of brevity Ill skip basic data set statistics like count, mean, variance, ) is which languages are most widely used.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_top_langs = pd.read_sql_query(<span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    select LanguageName, count(LanguageName) as count from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    group by LanguageName</span></span><br><span class=\"line\"><span class=\"string\">    order by count(LanguageName) desc</span></span><br><span class=\"line\"><span class=\"string\">    limit 10;</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span>, con=connection)</span><br><span class=\"line\">df_top_langs.set_index(<span class=\"string\">&#x27;LanguageName&#x27;</span>).plot.bar(figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_popular_lang.png\"></p>\n<p>Not too surprisingly, the typical web stack consisting of JavaScript, HTML and CSS, is among the most popular programming languages, according to how often they appear in repositories.</p>\n<h2 id=\"Least-popular-programming-languages\"><a href=\"#Least-popular-programming-languages\" class=\"headerlink\" title=\"Least popular programming languages\"></a>Least popular programming languages</h2><p>A little more interesting is to see, which programming languages occur least.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_last_langs = pd.read_sql_query(<span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    select LanguageName, count(LanguageName) as count from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    group by LanguageName</span></span><br><span class=\"line\"><span class=\"string\">    order by count(LanguageName) asc</span></span><br><span class=\"line\"><span class=\"string\">    limit 10;</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span>, con=connection)</span><br><span class=\"line\">print(df_last_langs)</span><br></pre></td></tr></table></figure>\n\n<p>Here are the results. Have you heard of any one of them? I didnt.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  LanguageName  count</span><br><span class=\"line\">0          Nit      1</span><br><span class=\"line\">1       Myghty      1</span><br><span class=\"line\">2   Public Key      1</span><br><span class=\"line\">3  DCPU-16 ASM      1</span><br><span class=\"line\">4   TI Program      1</span><br><span class=\"line\">5        Genie      1</span><br><span class=\"line\">6           Ox      1</span><br><span class=\"line\">7   PogoScript      1</span><br><span class=\"line\">8        Cirru      1</span><br><span class=\"line\">9        JFlex      2</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"User-Skills\"><a href=\"#User-Skills\" class=\"headerlink\" title=\"User Skills\"></a>User Skills</h1><p>Lets analyze the users skills in terms of languages. I decided to consider a user being skilled in a certain language if at least 10 % of her repositories code is in that language. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">N = <span class=\"built_in\">int</span>(<span class=\"number\">1e7</span>)</span><br><span class=\"line\">MIN_SUPP = <span class=\"number\">.0005</span></span><br><span class=\"line\">MIN_CONF = <span class=\"number\">.45</span></span><br><span class=\"line\">MIN_LANG_RATIO = <span class=\"number\">.1</span></span><br><span class=\"line\"></span><br><span class=\"line\">df_skills = pd.read_sql_query(<span class=\"string\">f&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    select RepositoryLanguages.LanguageName, RepositoryLanguages.size, Users.login, Users.location from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    left join Repositories on Repositories.id = RepositoryLanguages.RepositoryId</span></span><br><span class=\"line\"><span class=\"string\">    right join Users on Users.login = Repositories.userLogin</span></span><br><span class=\"line\"><span class=\"string\">    limit <span class=\"subst\">&#123;N&#125;</span></span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span>, con=connection)</span><br><span class=\"line\"></span><br><span class=\"line\">df_skills = df_skills.merge(pd.DataFrame(df_skills.groupby(<span class=\"string\">&#x27;login&#x27;</span>)[<span class=\"string\">&#x27;size&#x27;</span>].<span class=\"built_in\">sum</span>()), how=<span class=\"string\">&#x27;left&#x27;</span>, on=<span class=\"string\">&#x27;login&#x27;</span>).rename(columns=&#123;<span class=\"string\">&#x27;size_x&#x27;</span>: <span class=\"string\">&#x27;size&#x27;</span>, <span class=\"string\">&#x27;size_y&#x27;</span>: <span class=\"string\">&#x27;totalSize&#x27;</span>&#125;)</span><br><span class=\"line\">df_skills = df_skills[df_skills[<span class=\"string\">&#x27;totalSize&#x27;</span>] &gt; <span class=\"number\">0</span>]</span><br><span class=\"line\">df_skills[<span class=\"string\">&#x27;sizeRatio&#x27;</span>] = df_skills[<span class=\"string\">&#x27;size&#x27;</span>] / df_skills[<span class=\"string\">&#x27;totalSize&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">f&quot;<span class=\"subst\">&#123;df_skills[<span class=\"string\">&#x27;login&#x27;</span>].unique().size&#125;</span> users&quot;</span>)</span><br><span class=\"line\">print(<span class=\"string\">f&quot;<span class=\"subst\">&#123;df_skills[<span class=\"string\">&#x27;LanguageName&#x27;</span>].unique().size&#125;</span> languages&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Output:</span></span><br><span class=\"line\"><span class=\"comment\"># 130402 users</span></span><br><span class=\"line\"><span class=\"comment\"># 351 languages</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Association-Rules\"><a href=\"#Association-Rules\" class=\"headerlink\" title=\"Association Rules\"></a>Association Rules</h3><p>What I wanted to look at is combinations of different skills, i.e. languages that usually occur together as developer skills. One approach to get insights like these is to mine the data for <em>association rules</em>, e.g. using an algorithm like <a href=\"https://en.wikipedia.org/wiki/Apriori_algorithm\">Apriori</a> (as I did). The implementation I used was <a href=\"https://github.com/asaini/Apriori\">asaini/Apriori</a>.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">user_langs = df_skills[df_skills[<span class=\"string\">&#x27;sizeRatio&#x27;</span>] &gt;= MIN_LANG_RATIO].groupby(<span class=\"string\">&#x27;login&#x27;</span>)[<span class=\"string\">&#x27;LanguageName&#x27;</span>].apply(<span class=\"built_in\">set</span>).values</span><br><span class=\"line\">items1, rules1 = apriori.runApriori(user_langs, MIN_SUPP, MIN_CONF)</span><br><span class=\"line\">rules1 = <span class=\"built_in\">sorted</span>(rules1, key=<span class=\"keyword\">lambda</span> e: e[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">print(rules1)</span><br></pre></td></tr></table></figure>\n\n<p><strong>Output:</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[(((&#39;ShaderLab&#39;,), (&#39;C#&#39;,)), 0.904),</span><br><span class=\"line\"> (((&#39;Vue&#39;,), (&#39;JavaScript&#39;,)), 0.671277997364954),</span><br><span class=\"line\"> (((&#39;Vue&#39;, &#39;CSS&#39;), (&#39;JavaScript&#39;,)), 0.656140350877193),</span><br><span class=\"line\"> (((&#39;GLSL&#39;,), (&#39;C#&#39;,)), 0.625),</span><br><span class=\"line\"> (((&#39;CMake&#39;,), (&#39;C++&#39;,)), 0.6229508196721312),</span><br><span class=\"line\"> (((&#39;CSS&#39;,), (&#39;JavaScript&#39;,)), 0.5807683959192532),</span><br><span class=\"line\"> (((&#39;Tcl&#39;,), (&#39;Python&#39;,)), 0.5658914728682171),</span><br><span class=\"line\"> (((&#39;Kotlin&#39;,), (&#39;Java&#39;,)), 0.5655375552282769),</span><br><span class=\"line\"> (((&#39;ASP&#39;,), (&#39;C#&#39;,)), 0.5488215488215488),</span><br><span class=\"line\"> (((&#39;Vue&#39;, &#39;HTML&#39;), (&#39;JavaScript&#39;,)), 0.5404411764705882),</span><br><span class=\"line\"> (((&#39;CoffeeScript&#39;,), (&#39;JavaScript&#39;,)), 0.5339578454332553),</span><br><span class=\"line\"> (((&#39;CSS&#39;, &#39;PHP&#39;), (&#39;JavaScript&#39;,)), 0.5116117850953206),</span><br><span class=\"line\"> (((&#39;Elm&#39;,), (&#39;JavaScript&#39;,)), 0.4951923076923077),</span><br><span class=\"line\"> (((&#39;CSS&#39;, &#39;HTML&#39;), (&#39;JavaScript&#39;,)), 0.4906486271388778),</span><br><span class=\"line\"> (((&#39;Smarty&#39;,), (&#39;PHP&#39;,)), 0.4788732394366197),</span><br><span class=\"line\"> (((&#39;TypeScript&#39;,), (&#39;JavaScript&#39;,)), 0.4739540607054964),</span><br><span class=\"line\"> (((&#39;CSS&#39;, &#39;C#&#39;), (&#39;JavaScript&#39;,)), 0.464926590538336),</span><br><span class=\"line\"> (((&#39;Groovy&#39;,), (&#39;Java&#39;,)), 0.4604651162790698)]</span><br></pre></td></tr></table></figure>\n\n<p>The left part of each row is a tuple of tuples of programming languages that represent an association rule. The right part is the <a href=\"https://en.wikipedia.org/wiki/Association_rule_learning#Confidence\">confidence</a> of that rule.</p>\n<p><strong>For example:</strong><br>Read <code>(((&#39;ShaderLab&#39;,), (&#39;C#&#39;,)), 0.904)</code> as 90 % of all people who know <em>ShaderLab</em> also know C#.</p>\n<p>The results reflect common sense. For instance, the rule that developers, who know <em>VueJS</em>, also know <em>JavaScript</em> seems to make sense, given that VueJS is a JavaScript framework. Analogously, <em>CMake</em> is a common build tool for <em>C++</em>, etc. Nothing too fancy here, except for that I didnt know about <em>ShaderLab</em> and <em>GLSL</em>.</p>\n<h2 id=\"Locations\"><a href=\"#Locations\" class=\"headerlink\" title=\"Locations\"></a>Locations</h2><p>Lets take a look at where most GitHub users are from. Obviously, this only respects profiles where users have set their locations.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_locations = df1.reindex([<span class=\"string\">&#x27;location&#x27;</span>], axis=<span class=\"number\">1</span>).groupby(<span class=\"string\">&#x27;location&#x27;</span>).size()</span><br><span class=\"line\">df_locations = df_locations.sort_values(ascending=<span class=\"literal\">False</span>)[:<span class=\"number\">20</span>]</span><br><span class=\"line\">df_locations.plot.bar(figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_locations.png\"></p>\n<p>Clearly, San Francisco seems to be the most popular city for developers (or at least for those who are active on GitHub). </p>\n<h2 id=\"Skills-by-location\"><a href=\"#Skills-by-location\" class=\"headerlink\" title=\"Skills by location\"></a>Skills by location</h2><p>To take this a step further, lets take a look at which skills users tend to have in what cities.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">language_replace</span>(<span class=\"params\">df</span>):</span></span><br><span class=\"line\">    df = df.copy()</span><br><span class=\"line\">    <span class=\"comment\"># Little bit of manual cleaning</span></span><br><span class=\"line\">    replace = &#123;<span class=\"string\">&#x27;San Francisco&#x27;</span>: <span class=\"string\">&#x27;San Francisco, CA&#x27;</span>,</span><br><span class=\"line\">               <span class=\"string\">&#x27;Berlin&#x27;</span>: <span class=\"string\">&#x27;Berlin, Germany&#x27;</span>,</span><br><span class=\"line\">               <span class=\"string\">&#x27;New York&#x27;</span>: <span class=\"string\">&#x27;New York, NY&#x27;</span>,</span><br><span class=\"line\">               <span class=\"string\">&#x27;London&#x27;</span>: <span class=\"string\">&#x27;London, UK&#x27;</span>,</span><br><span class=\"line\">               <span class=\"string\">&#x27;Beijing&#x27;</span>: <span class=\"string\">&#x27;Beijing, China&#x27;</span>,</span><br><span class=\"line\">               <span class=\"string\">&#x27;Paris&#x27;</span>: <span class=\"string\">&#x27;Paris, France&#x27;</span>&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (k, v) <span class=\"keyword\">in</span> replace.items():</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(df, pd.DataFrame):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> k <span class=\"keyword\">in</span> df.columns <span class=\"keyword\">and</span> v <span class=\"keyword\">in</span> df.columns:</span><br><span class=\"line\">                df[k] = df[k] + df[v]</span><br><span class=\"line\">                df = df.drop([v], axis=<span class=\"number\">1</span>, errors=<span class=\"string\">&#x27;ignore&#x27;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> k <span class=\"keyword\">in</span> df.index <span class=\"keyword\">and</span> v <span class=\"keyword\">in</span> df.index:</span><br><span class=\"line\">                df[k] = df[k] + df[v]</span><br><span class=\"line\">                <span class=\"comment\">#df = df.drop([v], axis=1)</span></span><br><span class=\"line\">                <span class=\"keyword\">del</span> df[v]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> df</span><br><span class=\"line\"></span><br><span class=\"line\">langs_by_loc = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> df_locations.index:</span><br><span class=\"line\">    langs_by_loc[l] = df1[df1[<span class=\"string\">&#x27;location&#x27;</span>] == l][[<span class=\"string\">&#x27;LanguageName&#x27;</span>]].groupby(<span class=\"string\">&#x27;LanguageName&#x27;</span>).size()</span><br><span class=\"line\">df_loc_langs = pd.DataFrame.from_dict(langs_by_loc).fillna(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">df_loc_langs = language_replace(df_loc_langs)</span><br><span class=\"line\">df_loc_langs = df_loc_langs.T</span><br><span class=\"line\">df_loc_langs = df_loc_langs.drop([c <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> df_loc_langs.columns <span class=\"keyword\">if</span> c <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> df_top_langs[<span class=\"string\">&#x27;LanguageName&#x27;</span>].values], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">df_loc_langs = (df_loc_langs.T / df_loc_langs.T.<span class=\"built_in\">sum</span>()).T <span class=\"comment\"># normalize heights</span></span><br><span class=\"line\">df_loc_langs.plot.bar(stacked=<span class=\"literal\">True</span>, figsize=(<span class=\"number\">16</span>,<span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_location_langs.png\"></p>\n<p>Look like there are no real outliers in the distribution of developer skills between different cities of the world. Maybe you could say that, e.g., Indians like web frontends a little more than command-line hacking.</p>\n<h2 id=\"Skills-Karlsruhe-vs-the-World\"><a href=\"#Skills-Karlsruhe-vs-the-World\" class=\"headerlink\" title=\"Skills: Karlsruhe vs. the World\"></a>Skills: Karlsruhe vs. the World</h2><p>While an overview is cool, I found it even more interesting to specifically compare between to cities. So in the following chart I compare language-specific programming skills in Karlsruhe (the city where I live and study) to the rest of the worlds average.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_ka = df1[df1[<span class=\"string\">&#x27;location&#x27;</span>] == <span class=\"string\">&#x27;Karlsruhe&#x27;</span>][[<span class=\"string\">&#x27;LanguageName&#x27;</span>]].groupby(<span class=\"string\">&#x27;LanguageName&#x27;</span>).size()</span><br><span class=\"line\">df_ka = pd.DataFrame(df_ka, index=df_ka.index, columns=[<span class=\"string\">&#x27;Karlsruhe&#x27;</span>]) / df_ka.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">df_world = pd.DataFrame(df_loc_langs.mean(), index=df_loc_langs.mean().index, columns=[<span class=\"string\">&#x27;World&#x27;</span>])</span><br><span class=\"line\">df_compare = df_world.merge(df_ka, how=<span class=\"string\">&#x27;left&#x27;</span>, left_index=<span class=\"literal\">True</span>, right_index=<span class=\"literal\">True</span>)</span><br><span class=\"line\">ax = df_compare.plot.barh(title=<span class=\"string\">&#x27;Languages: World vs. Karlsruhe&#x27;</span>, legend=<span class=\"literal\">True</span>, figsize=(<span class=\"number\">10</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">ax.set_xlabel(<span class=\"string\">&#x27;Percentage (Top 10)&#x27;</span>)</span><br><span class=\"line\">ax.set_ylabel(<span class=\"string\">&#x27;Programming Language Skills&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_ka_world.png\"> </p>\n<p>These results are a bit surprising to me. Clearly, Karlsruhe-based developers seem to dislike JavaScript compared to the world. However, this is different from what I experienced in several student jobs and internships here.</p>\n<h2 id=\"Project-Tech-Stacks\"><a href=\"#Project-Tech-Stacks\" class=\"headerlink\" title=\"Project Tech Stacks\"></a>Project Tech Stacks</h2><p>Last but not least, lets apply Apriori once more, but this time in a slightly different way. Instead of looking at user skills, lets look at languages that occur together on a per-repository basis. And instead of trying to find rules, lets only look at <em>frequent item sets</em> (which are the basis for rules). My expectation was to get back sets of commonly used tech stacks. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">N = <span class=\"built_in\">int</span>(<span class=\"number\">1e7</span>)</span><br><span class=\"line\">MIN_SUPP = <span class=\"number\">.0005</span></span><br><span class=\"line\">MIN_CONF = <span class=\"number\">.45</span></span><br><span class=\"line\">MIN_LANG_RATIO = <span class=\"number\">.1</span></span><br><span class=\"line\"></span><br><span class=\"line\">df_stacks = pd.read_sql_query(<span class=\"string\">f&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    select LanguageName, size, RepositoryId from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    order by RepositoryId</span></span><br><span class=\"line\"><span class=\"string\">    limit <span class=\"subst\">&#123;N&#125;</span></span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span>, con=connection)</span><br><span class=\"line\"></span><br><span class=\"line\">df_stacks = df_stacks.merge(pd.DataFrame(df_stacks.groupby(<span class=\"string\">&#x27;RepositoryId&#x27;</span>)[<span class=\"string\">&#x27;size&#x27;</span>].<span class=\"built_in\">sum</span>()), how=<span class=\"string\">&#x27;left&#x27;</span>, on=<span class=\"string\">&#x27;RepositoryId&#x27;</span>).rename(columns=&#123;<span class=\"string\">&#x27;size_x&#x27;</span>: <span class=\"string\">&#x27;size&#x27;</span>, <span class=\"string\">&#x27;size_y&#x27;</span>: <span class=\"string\">&#x27;totalSize&#x27;</span>&#125;)</span><br><span class=\"line\">df_stacks = df_stacks[df_stacks[<span class=\"string\">&#x27;totalSize&#x27;</span>] &gt; <span class=\"number\">0</span>]</span><br><span class=\"line\">df_stacks[<span class=\"string\">&#x27;sizeRatio&#x27;</span>] = df_stacks[<span class=\"string\">&#x27;size&#x27;</span>] / df_stacks[<span class=\"string\">&#x27;totalSize&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">f&quot;<span class=\"subst\">&#123;df_stacks[<span class=\"string\">&#x27;RepositoryId&#x27;</span>].unique().size&#125;</span> repositories&quot;</span>)</span><br><span class=\"line\">print(<span class=\"string\">f&quot;<span class=\"subst\">&#123;df_stacks[<span class=\"string\">&#x27;LanguageName&#x27;</span>].unique().size&#125;</span> languages&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Output: </span></span><br><span class=\"line\"><span class=\"comment\"># 853114 repositories</span></span><br><span class=\"line\"><span class=\"comment\"># 351 languages</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">repo_langs = df_stacks[df_stacks[<span class=\"string\">&#x27;sizeRatio&#x27;</span>] &gt;= MIN_LANG_RATIO].groupby(<span class=\"string\">&#x27;RepositoryId&#x27;</span>)[<span class=\"string\">&#x27;LanguageName&#x27;</span>].apply(<span class=\"built_in\">set</span>).values</span><br><span class=\"line\">items2, rules2 = apriori.runApriori(repo_langs, MIN_SUPP, MIN_CONF)</span><br><span class=\"line\">itemsets2 = <span class=\"built_in\">sorted</span>(<span class=\"built_in\">list</span>(<span class=\"built_in\">filter</span>(<span class=\"keyword\">lambda</span> i: <span class=\"built_in\">len</span>(i[<span class=\"number\">0</span>]) &gt; <span class=\"number\">2</span>, items2)), key=<span class=\"keyword\">lambda</span> i: i[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">print(itemsets2)</span><br></pre></td></tr></table></figure>\n\n<p><strong>Output:</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[((&#39;CSS&#39;, &#39;JavaScript&#39;, &#39;HTML&#39;), 0.04360026913167525),</span><br><span class=\"line\"> ((&#39;CSS&#39;, &#39;JavaScript&#39;, &#39;PHP&#39;), 0.0045574213997191465),</span><br><span class=\"line\"> ((&#39;Ruby&#39;, &#39;CSS&#39;, &#39;HTML&#39;), 0.004456614239128651),</span><br><span class=\"line\"> ((&#39;TypeScript&#39;, &#39;JavaScript&#39;, &#39;HTML&#39;), 0.0042034241613664765),</span><br><span class=\"line\"> ((&#39;TypeScript&#39;, &#39;HTML&#39;, &#39;CSS&#39;), 0.0035024627423767517),</span><br><span class=\"line\"> ((&#39;Python&#39;, &#39;JavaScript&#39;, &#39;HTML&#39;), 0.002962089474560258),</span><br><span class=\"line\"> ((&#39;Python&#39;, &#39;HTML&#39;, &#39;CSS&#39;), 0.002769852563666755),</span><br><span class=\"line\"> ((&#39;Ruby&#39;, &#39;JavaScript&#39;, &#39;HTML&#39;), 0.0022400288824236856),</span><br><span class=\"line\"> ((&#39;JavaScript&#39;, &#39;HTML&#39;, &#39;PHP&#39;), 0.0022154131804190294),</span><br><span class=\"line\"> ((&#39;Ruby&#39;, &#39;CSS&#39;, &#39;JavaScript&#39;), 0.0021532878372644217),</span><br><span class=\"line\"> ((&#39;CSS&#39;, &#39;HTML&#39;, &#39;PHP&#39;), 0.0019915275098052547),</span><br><span class=\"line\"> ((&#39;JavaScript&#39;, &#39;Objective-C&#39;, &#39;Java&#39;), 0.0018614159420663593),</span><br><span class=\"line\"> ((&#39;CSS&#39;, &#39;JavaScript&#39;, &#39;Python&#39;), 0.0017992905989117516),</span><br><span class=\"line\"> ((&#39;Python&#39;, &#39;JavaScript&#39;, &#39;Objective-C&#39;), 0.0017735027206211597),</span><br><span class=\"line\"> ((&#39;Python&#39;, &#39;JavaScript&#39;, &#39;Java&#39;), 0.001508590879999625),</span><br><span class=\"line\"> ((&#39;CSS&#39;, &#39;JavaScript&#39;, &#39;TypeScript&#39;), 0.0014745977677074812),</span><br><span class=\"line\"> ((&#39;Python&#39;, &#39;Objective-C&#39;, &#39;Java&#39;), 0.0014066115431231934),</span><br><span class=\"line\"> ((&#39;Python&#39;, &#39;JavaScript&#39;, &#39;Objective-C&#39;, &#39;Java&#39;), 0.0013222148505358019),</span><br><span class=\"line\"> ((&#39;Vue&#39;, &#39;CSS&#39;, &#39;JavaScript&#39;), 0.0012554008022374501)]</span><br></pre></td></tr></table></figure>\n\n<p>Here, the left side is sets of frequently occurring combinations of languages. The right side is the sets <a href=\"https://en.wikipedia.org/wiki/Association_rule_learning#Support\">support</a>, which is the relative occurrences of that set among the whole data set.<br>Obviously, many of these are actually common tech stacks and almost all of them are web technologies. I guess GitHub is most popular among web developers. </p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>There is a lot of more complex analyses that could be might on rich data like this and probably tools like <a href=\"https://cloud.google.com/bigquery/public-data/\">BigQuery</a> are better suitable than Pandas, operating on a tiny sample. However, I used this little project to improve my EDA skills and hopefully give you guys an interesting article to read. Let me know if you like it!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><p>A few days ago, I wrote a crawler (with NodeJS and <a href=\"http://docs.sequelizejs.com/\">Sequelize</a>) that fetches publicly available data from GitHubs <a href=\"https://developer.github.com/v4/\">GraphQL API</a>. More precisely, I downloaded information about users, repositories, programming languages and topics.</p>\n<p>After running the crawler for a few days, I ended up with <strong>154,248 user profiles</strong>, <strong>993,919 repositories</strong> and <strong>351 languages</strong>, many of which I had never heard of (e.g. did you know about <em>PogoScript</em>?). However, although my MySQL database is already 953 MB in size with only these data, I barely crawled 0.4 % of all user profiles (~ 31 million).</p>\n<p>The first (less extensive) version of my database  which I performed the following analyses on  looked like this.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_eer.png\"></p>\n<p>While one could argue that the data I collected is not of a representative sample size, I still wanted to do some data analysis on it  just for fun.</p>\n<h1 id=\"Analyses\"><a href=\"#Analyses\" class=\"headerlink\" title=\"Analyses\"></a>Analyses</h1><p>To perform the analyses, I used Python 3 with Pandas and Matplotlib. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> apriori</span><br><span class=\"line\"><span class=\"keyword\">import</span> pymysql</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sqlalchemy <span class=\"keyword\">import</span> create_engine</span><br><span class=\"line\"></span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"></span><br><span class=\"line\">pymysql.install_as_MySQLdb()</span><br><span class=\"line\"></span><br><span class=\"line\">sql_engine = create_engine(<span class=\"string\">&#x27;mysql://user:heheyouwish@localhost:3306/github_data&#x27;</span>, echo=<span class=\"literal\">False</span>)</span><br><span class=\"line\">connection = sql_engine.raw_connection()</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Most-popular-programming-languages\"><a href=\"#Most-popular-programming-languages\" class=\"headerlink\" title=\"Most popular programming languages\"></a>Most popular programming languages</h2><p>One of the first and most obvious thing to check (for the sake of brevity Ill skip basic data set statistics like count, mean, variance, ) is which languages are most widely used.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_top_langs = pd.read_sql_query(<span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    select LanguageName, count(LanguageName) as count from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    group by LanguageName</span></span><br><span class=\"line\"><span class=\"string\">    order by count(LanguageName) desc</span></span><br><span class=\"line\"><span class=\"string\">    limit 10;</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span>, con=connection)</span><br><span class=\"line\">df_top_langs.set_index(<span class=\"string\">&#x27;LanguageName&#x27;</span>).plot.bar(figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_popular_lang.png\"></p>\n<p>Not too surprisingly, the typical web stack consisting of JavaScript, HTML and CSS, is among the most popular programming languages, according to how often they appear in repositories.</p>\n<h2 id=\"Least-popular-programming-languages\"><a href=\"#Least-popular-programming-languages\" class=\"headerlink\" title=\"Least popular programming languages\"></a>Least popular programming languages</h2><p>A little more interesting is to see, which programming languages occur least.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_last_langs = pd.read_sql_query(<span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    select LanguageName, count(LanguageName) as count from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    group by LanguageName</span></span><br><span class=\"line\"><span class=\"string\">    order by count(LanguageName) asc</span></span><br><span class=\"line\"><span class=\"string\">    limit 10;</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span>, con=connection)</span><br><span class=\"line\">print(df_last_langs)</span><br></pre></td></tr></table></figure>\n\n<p>Here are the results. Have you heard of any one of them? I didnt.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  LanguageName  count</span><br><span class=\"line\">0          Nit      1</span><br><span class=\"line\">1       Myghty      1</span><br><span class=\"line\">2   Public Key      1</span><br><span class=\"line\">3  DCPU-16 ASM      1</span><br><span class=\"line\">4   TI Program      1</span><br><span class=\"line\">5        Genie      1</span><br><span class=\"line\">6           Ox      1</span><br><span class=\"line\">7   PogoScript      1</span><br><span class=\"line\">8        Cirru      1</span><br><span class=\"line\">9        JFlex      2</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"User-Skills\"><a href=\"#User-Skills\" class=\"headerlink\" title=\"User Skills\"></a>User Skills</h1><p>Lets analyze the users skills in terms of languages. I decided to consider a user being skilled in a certain language if at least 10 % of her repositories code is in that language. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">N = <span class=\"built_in\">int</span>(<span class=\"number\">1e7</span>)</span><br><span class=\"line\">MIN_SUPP = <span class=\"number\">.0005</span></span><br><span class=\"line\">MIN_CONF = <span class=\"number\">.45</span></span><br><span class=\"line\">MIN_LANG_RATIO = <span class=\"number\">.1</span></span><br><span class=\"line\"></span><br><span class=\"line\">df_skills = pd.read_sql_query(<span class=\"string\">f&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    select RepositoryLanguages.LanguageName, RepositoryLanguages.size, Users.login, Users.location from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    left join Repositories on Repositories.id = RepositoryLanguages.RepositoryId</span></span><br><span class=\"line\"><span class=\"string\">    right join Users on Users.login = Repositories.userLogin</span></span><br><span class=\"line\"><span class=\"string\">    limit <span class=\"subst\">&#123;N&#125;</span></span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span>, con=connection)</span><br><span class=\"line\"></span><br><span class=\"line\">df_skills = df_skills.merge(pd.DataFrame(df_skills.groupby(<span class=\"string\">&#x27;login&#x27;</span>)[<span class=\"string\">&#x27;size&#x27;</span>].<span class=\"built_in\">sum</span>()), how=<span class=\"string\">&#x27;left&#x27;</span>, on=<span class=\"string\">&#x27;login&#x27;</span>).rename(columns=&#123;<span class=\"string\">&#x27;size_x&#x27;</span>: <span class=\"string\">&#x27;size&#x27;</span>, <span class=\"string\">&#x27;size_y&#x27;</span>: <span class=\"string\">&#x27;totalSize&#x27;</span>&#125;)</span><br><span class=\"line\">df_skills = df_skills[df_skills[<span class=\"string\">&#x27;totalSize&#x27;</span>] &gt; <span class=\"number\">0</span>]</span><br><span class=\"line\">df_skills[<span class=\"string\">&#x27;sizeRatio&#x27;</span>] = df_skills[<span class=\"string\">&#x27;size&#x27;</span>] / df_skills[<span class=\"string\">&#x27;totalSize&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">f&quot;<span class=\"subst\">&#123;df_skills[<span class=\"string\">&#x27;login&#x27;</span>].unique().size&#125;</span> users&quot;</span>)</span><br><span class=\"line\">print(<span class=\"string\">f&quot;<span class=\"subst\">&#123;df_skills[<span class=\"string\">&#x27;LanguageName&#x27;</span>].unique().size&#125;</span> languages&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Output:</span></span><br><span class=\"line\"><span class=\"comment\"># 130402 users</span></span><br><span class=\"line\"><span class=\"comment\"># 351 languages</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Association-Rules\"><a href=\"#Association-Rules\" class=\"headerlink\" title=\"Association Rules\"></a>Association Rules</h3><p>What I wanted to look at is combinations of different skills, i.e. languages that usually occur together as developer skills. One approach to get insights like these is to mine the data for <em>association rules</em>, e.g. using an algorithm like <a href=\"https://en.wikipedia.org/wiki/Apriori_algorithm\">Apriori</a> (as I did). The implementation I used was <a href=\"https://github.com/asaini/Apriori\">asaini/Apriori</a>.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">user_langs = df_skills[df_skills[<span class=\"string\">&#x27;sizeRatio&#x27;</span>] &gt;= MIN_LANG_RATIO].groupby(<span class=\"string\">&#x27;login&#x27;</span>)[<span class=\"string\">&#x27;LanguageName&#x27;</span>].apply(<span class=\"built_in\">set</span>).values</span><br><span class=\"line\">items1, rules1 = apriori.runApriori(user_langs, MIN_SUPP, MIN_CONF)</span><br><span class=\"line\">rules1 = <span class=\"built_in\">sorted</span>(rules1, key=<span class=\"keyword\">lambda</span> e: e[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">print(rules1)</span><br></pre></td></tr></table></figure>\n\n<p><strong>Output:</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[(((&#39;ShaderLab&#39;,), (&#39;C#&#39;,)), 0.904),</span><br><span class=\"line\"> (((&#39;Vue&#39;,), (&#39;JavaScript&#39;,)), 0.671277997364954),</span><br><span class=\"line\"> (((&#39;Vue&#39;, &#39;CSS&#39;), (&#39;JavaScript&#39;,)), 0.656140350877193),</span><br><span class=\"line\"> (((&#39;GLSL&#39;,), (&#39;C#&#39;,)), 0.625),</span><br><span class=\"line\"> (((&#39;CMake&#39;,), (&#39;C++&#39;,)), 0.6229508196721312),</span><br><span class=\"line\"> (((&#39;CSS&#39;,), (&#39;JavaScript&#39;,)), 0.5807683959192532),</span><br><span class=\"line\"> (((&#39;Tcl&#39;,), (&#39;Python&#39;,)), 0.5658914728682171),</span><br><span class=\"line\"> (((&#39;Kotlin&#39;,), (&#39;Java&#39;,)), 0.5655375552282769),</span><br><span class=\"line\"> (((&#39;ASP&#39;,), (&#39;C#&#39;,)), 0.5488215488215488),</span><br><span class=\"line\"> (((&#39;Vue&#39;, &#39;HTML&#39;), (&#39;JavaScript&#39;,)), 0.5404411764705882),</span><br><span class=\"line\"> (((&#39;CoffeeScript&#39;,), (&#39;JavaScript&#39;,)), 0.5339578454332553),</span><br><span class=\"line\"> (((&#39;CSS&#39;, &#39;PHP&#39;), (&#39;JavaScript&#39;,)), 0.5116117850953206),</span><br><span class=\"line\"> (((&#39;Elm&#39;,), (&#39;JavaScript&#39;,)), 0.4951923076923077),</span><br><span class=\"line\"> (((&#39;CSS&#39;, &#39;HTML&#39;), (&#39;JavaScript&#39;,)), 0.4906486271388778),</span><br><span class=\"line\"> (((&#39;Smarty&#39;,), (&#39;PHP&#39;,)), 0.4788732394366197),</span><br><span class=\"line\"> (((&#39;TypeScript&#39;,), (&#39;JavaScript&#39;,)), 0.4739540607054964),</span><br><span class=\"line\"> (((&#39;CSS&#39;, &#39;C#&#39;), (&#39;JavaScript&#39;,)), 0.464926590538336),</span><br><span class=\"line\"> (((&#39;Groovy&#39;,), (&#39;Java&#39;,)), 0.4604651162790698)]</span><br></pre></td></tr></table></figure>\n\n<p>The left part of each row is a tuple of tuples of programming languages that represent an association rule. The right part is the <a href=\"https://en.wikipedia.org/wiki/Association_rule_learning#Confidence\">confidence</a> of that rule.</p>\n<p><strong>For example:</strong><br>Read <code>(((&#39;ShaderLab&#39;,), (&#39;C#&#39;,)), 0.904)</code> as 90 % of all people who know <em>ShaderLab</em> also know C#.</p>\n<p>The results reflect common sense. For instance, the rule that developers, who know <em>VueJS</em>, also know <em>JavaScript</em> seems to make sense, given that VueJS is a JavaScript framework. Analogously, <em>CMake</em> is a common build tool for <em>C++</em>, etc. Nothing too fancy here, except for that I didnt know about <em>ShaderLab</em> and <em>GLSL</em>.</p>\n<h2 id=\"Locations\"><a href=\"#Locations\" class=\"headerlink\" title=\"Locations\"></a>Locations</h2><p>Lets take a look at where most GitHub users are from. Obviously, this only respects profiles where users have set their locations.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_locations = df1.reindex([<span class=\"string\">&#x27;location&#x27;</span>], axis=<span class=\"number\">1</span>).groupby(<span class=\"string\">&#x27;location&#x27;</span>).size()</span><br><span class=\"line\">df_locations = df_locations.sort_values(ascending=<span class=\"literal\">False</span>)[:<span class=\"number\">20</span>]</span><br><span class=\"line\">df_locations.plot.bar(figsize=(<span class=\"number\">12</span>,<span class=\"number\">8</span>))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_locations.png\"></p>\n<p>Clearly, San Francisco seems to be the most popular city for developers (or at least for those who are active on GitHub). </p>\n<h2 id=\"Skills-by-location\"><a href=\"#Skills-by-location\" class=\"headerlink\" title=\"Skills by location\"></a>Skills by location</h2><p>To take this a step further, lets take a look at which skills users tend to have in what cities.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">language_replace</span>(<span class=\"params\">df</span>):</span></span><br><span class=\"line\">    df = df.copy()</span><br><span class=\"line\">    <span class=\"comment\"># Little bit of manual cleaning</span></span><br><span class=\"line\">    replace = &#123;<span class=\"string\">&#x27;San Francisco&#x27;</span>: <span class=\"string\">&#x27;San Francisco, CA&#x27;</span>,</span><br><span class=\"line\">               <span class=\"string\">&#x27;Berlin&#x27;</span>: <span class=\"string\">&#x27;Berlin, Germany&#x27;</span>,</span><br><span class=\"line\">               <span class=\"string\">&#x27;New York&#x27;</span>: <span class=\"string\">&#x27;New York, NY&#x27;</span>,</span><br><span class=\"line\">               <span class=\"string\">&#x27;London&#x27;</span>: <span class=\"string\">&#x27;London, UK&#x27;</span>,</span><br><span class=\"line\">               <span class=\"string\">&#x27;Beijing&#x27;</span>: <span class=\"string\">&#x27;Beijing, China&#x27;</span>,</span><br><span class=\"line\">               <span class=\"string\">&#x27;Paris&#x27;</span>: <span class=\"string\">&#x27;Paris, France&#x27;</span>&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (k, v) <span class=\"keyword\">in</span> replace.items():</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(df, pd.DataFrame):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> k <span class=\"keyword\">in</span> df.columns <span class=\"keyword\">and</span> v <span class=\"keyword\">in</span> df.columns:</span><br><span class=\"line\">                df[k] = df[k] + df[v]</span><br><span class=\"line\">                df = df.drop([v], axis=<span class=\"number\">1</span>, errors=<span class=\"string\">&#x27;ignore&#x27;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> k <span class=\"keyword\">in</span> df.index <span class=\"keyword\">and</span> v <span class=\"keyword\">in</span> df.index:</span><br><span class=\"line\">                df[k] = df[k] + df[v]</span><br><span class=\"line\">                <span class=\"comment\">#df = df.drop([v], axis=1)</span></span><br><span class=\"line\">                <span class=\"keyword\">del</span> df[v]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> df</span><br><span class=\"line\"></span><br><span class=\"line\">langs_by_loc = &#123;&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> df_locations.index:</span><br><span class=\"line\">    langs_by_loc[l] = df1[df1[<span class=\"string\">&#x27;location&#x27;</span>] == l][[<span class=\"string\">&#x27;LanguageName&#x27;</span>]].groupby(<span class=\"string\">&#x27;LanguageName&#x27;</span>).size()</span><br><span class=\"line\">df_loc_langs = pd.DataFrame.from_dict(langs_by_loc).fillna(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">df_loc_langs = language_replace(df_loc_langs)</span><br><span class=\"line\">df_loc_langs = df_loc_langs.T</span><br><span class=\"line\">df_loc_langs = df_loc_langs.drop([c <span class=\"keyword\">for</span> c <span class=\"keyword\">in</span> df_loc_langs.columns <span class=\"keyword\">if</span> c <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> df_top_langs[<span class=\"string\">&#x27;LanguageName&#x27;</span>].values], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">df_loc_langs = (df_loc_langs.T / df_loc_langs.T.<span class=\"built_in\">sum</span>()).T <span class=\"comment\"># normalize heights</span></span><br><span class=\"line\">df_loc_langs.plot.bar(stacked=<span class=\"literal\">True</span>, figsize=(<span class=\"number\">16</span>,<span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_location_langs.png\"></p>\n<p>Look like there are no real outliers in the distribution of developer skills between different cities of the world. Maybe you could say that, e.g., Indians like web frontends a little more than command-line hacking.</p>\n<h2 id=\"Skills-Karlsruhe-vs-the-World\"><a href=\"#Skills-Karlsruhe-vs-the-World\" class=\"headerlink\" title=\"Skills: Karlsruhe vs. the World\"></a>Skills: Karlsruhe vs. the World</h2><p>While an overview is cool, I found it even more interesting to specifically compare between to cities. So in the following chart I compare language-specific programming skills in Karlsruhe (the city where I live and study) to the rest of the worlds average.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_ka = df1[df1[<span class=\"string\">&#x27;location&#x27;</span>] == <span class=\"string\">&#x27;Karlsruhe&#x27;</span>][[<span class=\"string\">&#x27;LanguageName&#x27;</span>]].groupby(<span class=\"string\">&#x27;LanguageName&#x27;</span>).size()</span><br><span class=\"line\">df_ka = pd.DataFrame(df_ka, index=df_ka.index, columns=[<span class=\"string\">&#x27;Karlsruhe&#x27;</span>]) / df_ka.<span class=\"built_in\">sum</span>()</span><br><span class=\"line\">df_world = pd.DataFrame(df_loc_langs.mean(), index=df_loc_langs.mean().index, columns=[<span class=\"string\">&#x27;World&#x27;</span>])</span><br><span class=\"line\">df_compare = df_world.merge(df_ka, how=<span class=\"string\">&#x27;left&#x27;</span>, left_index=<span class=\"literal\">True</span>, right_index=<span class=\"literal\">True</span>)</span><br><span class=\"line\">ax = df_compare.plot.barh(title=<span class=\"string\">&#x27;Languages: World vs. Karlsruhe&#x27;</span>, legend=<span class=\"literal\">True</span>, figsize=(<span class=\"number\">10</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">ax.set_xlabel(<span class=\"string\">&#x27;Percentage (Top 10)&#x27;</span>)</span><br><span class=\"line\">ax.set_ylabel(<span class=\"string\">&#x27;Programming Language Skills&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_ka_world.png\"> </p>\n<p>These results are a bit surprising to me. Clearly, Karlsruhe-based developers seem to dislike JavaScript compared to the world. However, this is different from what I experienced in several student jobs and internships here.</p>\n<h2 id=\"Project-Tech-Stacks\"><a href=\"#Project-Tech-Stacks\" class=\"headerlink\" title=\"Project Tech Stacks\"></a>Project Tech Stacks</h2><p>Last but not least, lets apply Apriori once more, but this time in a slightly different way. Instead of looking at user skills, lets look at languages that occur together on a per-repository basis. And instead of trying to find rules, lets only look at <em>frequent item sets</em> (which are the basis for rules). My expectation was to get back sets of commonly used tech stacks. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">N = <span class=\"built_in\">int</span>(<span class=\"number\">1e7</span>)</span><br><span class=\"line\">MIN_SUPP = <span class=\"number\">.0005</span></span><br><span class=\"line\">MIN_CONF = <span class=\"number\">.45</span></span><br><span class=\"line\">MIN_LANG_RATIO = <span class=\"number\">.1</span></span><br><span class=\"line\"></span><br><span class=\"line\">df_stacks = pd.read_sql_query(<span class=\"string\">f&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">    select LanguageName, size, RepositoryId from RepositoryLanguages</span></span><br><span class=\"line\"><span class=\"string\">    order by RepositoryId</span></span><br><span class=\"line\"><span class=\"string\">    limit <span class=\"subst\">&#123;N&#125;</span></span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span>, con=connection)</span><br><span class=\"line\"></span><br><span class=\"line\">df_stacks = df_stacks.merge(pd.DataFrame(df_stacks.groupby(<span class=\"string\">&#x27;RepositoryId&#x27;</span>)[<span class=\"string\">&#x27;size&#x27;</span>].<span class=\"built_in\">sum</span>()), how=<span class=\"string\">&#x27;left&#x27;</span>, on=<span class=\"string\">&#x27;RepositoryId&#x27;</span>).rename(columns=&#123;<span class=\"string\">&#x27;size_x&#x27;</span>: <span class=\"string\">&#x27;size&#x27;</span>, <span class=\"string\">&#x27;size_y&#x27;</span>: <span class=\"string\">&#x27;totalSize&#x27;</span>&#125;)</span><br><span class=\"line\">df_stacks = df_stacks[df_stacks[<span class=\"string\">&#x27;totalSize&#x27;</span>] &gt; <span class=\"number\">0</span>]</span><br><span class=\"line\">df_stacks[<span class=\"string\">&#x27;sizeRatio&#x27;</span>] = df_stacks[<span class=\"string\">&#x27;size&#x27;</span>] / df_stacks[<span class=\"string\">&#x27;totalSize&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">f&quot;<span class=\"subst\">&#123;df_stacks[<span class=\"string\">&#x27;RepositoryId&#x27;</span>].unique().size&#125;</span> repositories&quot;</span>)</span><br><span class=\"line\">print(<span class=\"string\">f&quot;<span class=\"subst\">&#123;df_stacks[<span class=\"string\">&#x27;LanguageName&#x27;</span>].unique().size&#125;</span> languages&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Output: </span></span><br><span class=\"line\"><span class=\"comment\"># 853114 repositories</span></span><br><span class=\"line\"><span class=\"comment\"># 351 languages</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">repo_langs = df_stacks[df_stacks[<span class=\"string\">&#x27;sizeRatio&#x27;</span>] &gt;= MIN_LANG_RATIO].groupby(<span class=\"string\">&#x27;RepositoryId&#x27;</span>)[<span class=\"string\">&#x27;LanguageName&#x27;</span>].apply(<span class=\"built_in\">set</span>).values</span><br><span class=\"line\">items2, rules2 = apriori.runApriori(repo_langs, MIN_SUPP, MIN_CONF)</span><br><span class=\"line\">itemsets2 = <span class=\"built_in\">sorted</span>(<span class=\"built_in\">list</span>(<span class=\"built_in\">filter</span>(<span class=\"keyword\">lambda</span> i: <span class=\"built_in\">len</span>(i[<span class=\"number\">0</span>]) &gt; <span class=\"number\">2</span>, items2)), key=<span class=\"keyword\">lambda</span> i: i[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">print(itemsets2)</span><br></pre></td></tr></table></figure>\n\n<p><strong>Output:</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[((&#39;CSS&#39;, &#39;JavaScript&#39;, &#39;HTML&#39;), 0.04360026913167525),</span><br><span class=\"line\"> ((&#39;CSS&#39;, &#39;JavaScript&#39;, &#39;PHP&#39;), 0.0045574213997191465),</span><br><span class=\"line\"> ((&#39;Ruby&#39;, &#39;CSS&#39;, &#39;HTML&#39;), 0.004456614239128651),</span><br><span class=\"line\"> ((&#39;TypeScript&#39;, &#39;JavaScript&#39;, &#39;HTML&#39;), 0.0042034241613664765),</span><br><span class=\"line\"> ((&#39;TypeScript&#39;, &#39;HTML&#39;, &#39;CSS&#39;), 0.0035024627423767517),</span><br><span class=\"line\"> ((&#39;Python&#39;, &#39;JavaScript&#39;, &#39;HTML&#39;), 0.002962089474560258),</span><br><span class=\"line\"> ((&#39;Python&#39;, &#39;HTML&#39;, &#39;CSS&#39;), 0.002769852563666755),</span><br><span class=\"line\"> ((&#39;Ruby&#39;, &#39;JavaScript&#39;, &#39;HTML&#39;), 0.0022400288824236856),</span><br><span class=\"line\"> ((&#39;JavaScript&#39;, &#39;HTML&#39;, &#39;PHP&#39;), 0.0022154131804190294),</span><br><span class=\"line\"> ((&#39;Ruby&#39;, &#39;CSS&#39;, &#39;JavaScript&#39;), 0.0021532878372644217),</span><br><span class=\"line\"> ((&#39;CSS&#39;, &#39;HTML&#39;, &#39;PHP&#39;), 0.0019915275098052547),</span><br><span class=\"line\"> ((&#39;JavaScript&#39;, &#39;Objective-C&#39;, &#39;Java&#39;), 0.0018614159420663593),</span><br><span class=\"line\"> ((&#39;CSS&#39;, &#39;JavaScript&#39;, &#39;Python&#39;), 0.0017992905989117516),</span><br><span class=\"line\"> ((&#39;Python&#39;, &#39;JavaScript&#39;, &#39;Objective-C&#39;), 0.0017735027206211597),</span><br><span class=\"line\"> ((&#39;Python&#39;, &#39;JavaScript&#39;, &#39;Java&#39;), 0.001508590879999625),</span><br><span class=\"line\"> ((&#39;CSS&#39;, &#39;JavaScript&#39;, &#39;TypeScript&#39;), 0.0014745977677074812),</span><br><span class=\"line\"> ((&#39;Python&#39;, &#39;Objective-C&#39;, &#39;Java&#39;), 0.0014066115431231934),</span><br><span class=\"line\"> ((&#39;Python&#39;, &#39;JavaScript&#39;, &#39;Objective-C&#39;, &#39;Java&#39;), 0.0013222148505358019),</span><br><span class=\"line\"> ((&#39;Vue&#39;, &#39;CSS&#39;, &#39;JavaScript&#39;), 0.0012554008022374501)]</span><br></pre></td></tr></table></figure>\n\n<p>Here, the left side is sets of frequently occurring combinations of languages. The right side is the sets <a href=\"https://en.wikipedia.org/wiki/Association_rule_learning#Support\">support</a>, which is the relative occurrences of that set among the whole data set.<br>Obviously, many of these are actually common tech stacks and almost all of them are web technologies. I guess GitHub is most popular among web developers. </p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>There is a lot of more complex analyses that could be might on rich data like this and probably tools like <a href=\"https://cloud.google.com/bigquery/public-data/\">BigQuery</a> are better suitable than Pandas, operating on a tiny sample. However, I used this little project to improve my EDA skills and hopefully give you guys an interesting article to read. Let me know if you like it!</p>\n"},{"title":"Flying a DJI Tello Drone with Go","date":"2019-10-01T12:21:53.000Z","description":"In this post, we program a DJI Tello drone using Go and control it from the command line.","_content":"\n# The Idea\nA few months ago, I bought a [DJI Tello](https://amzn.to/2neAwVr) (affiliate link) drone on Amazon for ~ 80 , which is quite an impressive price, considering that you can also pay several hundred or even thousand Euro for a DJI drone. Of course, this one is only meant for fun and tinkering, not for professional photography or so. \nEven though I was amazed by how easy it is to control the drone from your smartphone  even with windy weather conditions - the Tello started to bore me after a few weeks. I wanted to do something more interesting with the drone  I wanted to **program it**!\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/tello1.jpg)\n\n# Using Gobot SDK\nDuring my research on how to program drones - specifically the Tello  I found an article called [\"Hello, Tello - Hacking Drones With Go\"](https://gobot.io/blog/2018/04/20/hello-tello-hacking-drones-with-go/), which referenced the documentation of a robot programming toolkit called [Gobot](https://gobot.io). As it turned out, Gobot is a Go SDK to control several different micro-robots like [GoPiGo3](https://gobot.io/documentation/platforms/gopigo3/), the [Parrot Ardrone](https://gobot.io/documentation/platforms/ardrone/), any drone using the [MAVLink protocol](https://mavlink.io/en/), [Pebble smartwatches](https://gobot.io/documentation/platforms/pebble/) and many more. It also has support for different microcontrollers, including Arduino, RaspberryPi and Intel Edison as well as communication protocols like MQTT or [NATS](https://nats.io/). Gobot's API to interface with any of the supported platforms appeared to be quite straightforward and easy to understand, so I decided to give it a try.\n\nMy first goal was to just have a little program that allows me to **control the drone from my PC** instead of the [official Tello app](https://play.google.com/store/apps/details?id=com.ryzerobotics.tello). Luckily, Gobot also provides a [keyboard driver](https://godoc.org/gobot.io/x/gobot/platforms/keyboard) (there's also an Xbox360 gamepad driver) that can be used to subscribe to certain key events, etc.\n\n# First Prototype\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/tello2.png)\n\nAfter tinkering for two hours (and having my poor drone hit the wall several times), I got a basic program that:\n - **connects** to the drone\n - prints incoming **status information** (like battery state) to the console\n - handles **keyboard events** ( for lateral and longitudinal navigation, WASD for rotation and altitude control, spacebar to start and land)\n - runs something like a **\"game loop\"** with a tick rate of 10 FPS to sync the latest user commands with the drone\n - renders the drone's **video stream** to an [MPlayer](https://wiki.debian.org/MPlayer) window with 10 FPS\n\nThe program's main method is as simple as this:\n\n```go\n// tello.go\n// [...]\nfunc main() {\n\t// Init Gobot drivers\n\tkeys := keyboard.NewDriver()\n\tdrone := tello.NewDriver(\"8890\")\n\n\twork := func() {\n\t\t// Handle keyboard inputs\n\t\tkeys.On(keyboard.Key, handleKeyboardInput(drone))\n\n\t\t// Handle drone events\n\t\tdrone.On(tello.FlightDataEvent, handleFlightData(drone))\n\t\tdrone.On(tello.ConnectedEvent, handleConnected(drone))\n\t\tdrone.On(tello.LandingEvent, handleLanding(drone))\n\t\tdrone.On(tello.VideoFrameEvent, handleVideo(drone))\n\t}\n\n\trobot := gobot.NewRobot(\n\t\t\"tello\",\n\t\t[]gobot.Connection{},\n\t\t[]gobot.Device{keys, drone},\n\t\twork,\n\t)\n\n\trobot.Start()\n}\n// [...]\n```\n\nIn addition, here's a little excerpt from the `tick()` method, that checks for the latest key input and calls the corresponding control methods.\n\n```go\n// tello.go\n// [...]\nif currentControl == keyboard.A {\n    fmt.Println(\"Going left.\")\n    if !dry {\n        drone.Left(intensity)\n    }\n// [...]\n} else if currentControl == keyboard.ArrowRight {\n    fmt.Println(\"Rotating clockwise.\")\n    if !dry {\n        drone.Clockwise(intensity)\n    }\n} else {\n    fmt.Println(\"Resetting steering.\")\n    resetSteering(drone)\n}\n// [...]\n```\n\nThe entire code is **available on GitHub** at [muety/tello](https://github.com/muety/tello).\n\n# Challenges\nOne thing that made me stuck for a while was the way API commands like `drone.Left(100)` (where the integer parameter represents the movement's \"intensity\" or speed) work. Once called, they are being applied continuously until manually stopped. In this case, the drone would go left with max speed until you explicitly send `drone.Left(0)`. This behavior combined with the fact that Gobot's keyboard driver only supports to communicate when a key **is pressed**, but **not when it's released** again, made it a little difficult to smoothly control the drone. To cope with that, I introduced a **debouncing logic**. When a key is pressed (e.g. ), a flag for that key is toggled on for 250 ms before it's automatically reset again, if the key was released in the meantime. Within the `tick()` method, the only thing done is to translate the binary key states to API commands and sync them to the device. \n\n# Future Plans\nIn case I have some time to further work in this little project, I would love to add basic **\"self-flying\" capabilities**. How cool would it be to have the ability to set a marker on a map and have the Tello fly there autonomously, while automatically avoiding obstacles in its way? A good starting point for this might be to take a look into [GoCV](https://gocv.io/), which is a Go interface to OpenCV. Alternatively, Microsoft's [AirSim](https://github.com/Microsoft/AirSim) simulator provides explicit support for training machine learning models for self-flying drones. Very cool!","source":"_posts/flying-a-dji-tello-drone-with-go.md","raw":"---\ntitle: Flying a DJI Tello Drone with Go\ndate: 2019-10-01 14:21:53\ntags:\ndescription: In this post, we program a DJI Tello drone using Go and control it from the command line.\n---\n\n# The Idea\nA few months ago, I bought a [DJI Tello](https://amzn.to/2neAwVr) (affiliate link) drone on Amazon for ~ 80 , which is quite an impressive price, considering that you can also pay several hundred or even thousand Euro for a DJI drone. Of course, this one is only meant for fun and tinkering, not for professional photography or so. \nEven though I was amazed by how easy it is to control the drone from your smartphone  even with windy weather conditions - the Tello started to bore me after a few weeks. I wanted to do something more interesting with the drone  I wanted to **program it**!\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/tello1.jpg)\n\n# Using Gobot SDK\nDuring my research on how to program drones - specifically the Tello  I found an article called [\"Hello, Tello - Hacking Drones With Go\"](https://gobot.io/blog/2018/04/20/hello-tello-hacking-drones-with-go/), which referenced the documentation of a robot programming toolkit called [Gobot](https://gobot.io). As it turned out, Gobot is a Go SDK to control several different micro-robots like [GoPiGo3](https://gobot.io/documentation/platforms/gopigo3/), the [Parrot Ardrone](https://gobot.io/documentation/platforms/ardrone/), any drone using the [MAVLink protocol](https://mavlink.io/en/), [Pebble smartwatches](https://gobot.io/documentation/platforms/pebble/) and many more. It also has support for different microcontrollers, including Arduino, RaspberryPi and Intel Edison as well as communication protocols like MQTT or [NATS](https://nats.io/). Gobot's API to interface with any of the supported platforms appeared to be quite straightforward and easy to understand, so I decided to give it a try.\n\nMy first goal was to just have a little program that allows me to **control the drone from my PC** instead of the [official Tello app](https://play.google.com/store/apps/details?id=com.ryzerobotics.tello). Luckily, Gobot also provides a [keyboard driver](https://godoc.org/gobot.io/x/gobot/platforms/keyboard) (there's also an Xbox360 gamepad driver) that can be used to subscribe to certain key events, etc.\n\n# First Prototype\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/tello2.png)\n\nAfter tinkering for two hours (and having my poor drone hit the wall several times), I got a basic program that:\n - **connects** to the drone\n - prints incoming **status information** (like battery state) to the console\n - handles **keyboard events** ( for lateral and longitudinal navigation, WASD for rotation and altitude control, spacebar to start and land)\n - runs something like a **\"game loop\"** with a tick rate of 10 FPS to sync the latest user commands with the drone\n - renders the drone's **video stream** to an [MPlayer](https://wiki.debian.org/MPlayer) window with 10 FPS\n\nThe program's main method is as simple as this:\n\n```go\n// tello.go\n// [...]\nfunc main() {\n\t// Init Gobot drivers\n\tkeys := keyboard.NewDriver()\n\tdrone := tello.NewDriver(\"8890\")\n\n\twork := func() {\n\t\t// Handle keyboard inputs\n\t\tkeys.On(keyboard.Key, handleKeyboardInput(drone))\n\n\t\t// Handle drone events\n\t\tdrone.On(tello.FlightDataEvent, handleFlightData(drone))\n\t\tdrone.On(tello.ConnectedEvent, handleConnected(drone))\n\t\tdrone.On(tello.LandingEvent, handleLanding(drone))\n\t\tdrone.On(tello.VideoFrameEvent, handleVideo(drone))\n\t}\n\n\trobot := gobot.NewRobot(\n\t\t\"tello\",\n\t\t[]gobot.Connection{},\n\t\t[]gobot.Device{keys, drone},\n\t\twork,\n\t)\n\n\trobot.Start()\n}\n// [...]\n```\n\nIn addition, here's a little excerpt from the `tick()` method, that checks for the latest key input and calls the corresponding control methods.\n\n```go\n// tello.go\n// [...]\nif currentControl == keyboard.A {\n    fmt.Println(\"Going left.\")\n    if !dry {\n        drone.Left(intensity)\n    }\n// [...]\n} else if currentControl == keyboard.ArrowRight {\n    fmt.Println(\"Rotating clockwise.\")\n    if !dry {\n        drone.Clockwise(intensity)\n    }\n} else {\n    fmt.Println(\"Resetting steering.\")\n    resetSteering(drone)\n}\n// [...]\n```\n\nThe entire code is **available on GitHub** at [muety/tello](https://github.com/muety/tello).\n\n# Challenges\nOne thing that made me stuck for a while was the way API commands like `drone.Left(100)` (where the integer parameter represents the movement's \"intensity\" or speed) work. Once called, they are being applied continuously until manually stopped. In this case, the drone would go left with max speed until you explicitly send `drone.Left(0)`. This behavior combined with the fact that Gobot's keyboard driver only supports to communicate when a key **is pressed**, but **not when it's released** again, made it a little difficult to smoothly control the drone. To cope with that, I introduced a **debouncing logic**. When a key is pressed (e.g. ), a flag for that key is toggled on for 250 ms before it's automatically reset again, if the key was released in the meantime. Within the `tick()` method, the only thing done is to translate the binary key states to API commands and sync them to the device. \n\n# Future Plans\nIn case I have some time to further work in this little project, I would love to add basic **\"self-flying\" capabilities**. How cool would it be to have the ability to set a marker on a map and have the Tello fly there autonomously, while automatically avoiding obstacles in its way? A good starting point for this might be to take a look into [GoCV](https://gocv.io/), which is a Go interface to OpenCV. Alternatively, Microsoft's [AirSim](https://github.com/Microsoft/AirSim) simulator provides explicit support for training machine learning models for self-flying drones. Very cool!","slug":"flying-a-dji-tello-drone-with-go","published":1,"updated":"2020-10-30T20:05:40.282Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlt000bo2e0gzkhgokz","content":"<h1 id=\"The-Idea\"><a href=\"#The-Idea\" class=\"headerlink\" title=\"The Idea\"></a>The Idea</h1><p>A few months ago, I bought a <a href=\"https://amzn.to/2neAwVr\">DJI Tello</a> (affiliate link) drone on Amazon for ~ 80 , which is quite an impressive price, considering that you can also pay several hundred or even thousand Euro for a DJI drone. Of course, this one is only meant for fun and tinkering, not for professional photography or so.<br>Even though I was amazed by how easy it is to control the drone from your smartphone  even with windy weather conditions - the Tello started to bore me after a few weeks. I wanted to do something more interesting with the drone  I wanted to <strong>program it</strong>!</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/tello1.jpg\"></p>\n<h1 id=\"Using-Gobot-SDK\"><a href=\"#Using-Gobot-SDK\" class=\"headerlink\" title=\"Using Gobot SDK\"></a>Using Gobot SDK</h1><p>During my research on how to program drones - specifically the Tello  I found an article called <a href=\"https://gobot.io/blog/2018/04/20/hello-tello-hacking-drones-with-go/\">Hello, Tello - Hacking Drones With Go</a>, which referenced the documentation of a robot programming toolkit called <a href=\"https://gobot.io/\">Gobot</a>. As it turned out, Gobot is a Go SDK to control several different micro-robots like <a href=\"https://gobot.io/documentation/platforms/gopigo3/\">GoPiGo3</a>, the <a href=\"https://gobot.io/documentation/platforms/ardrone/\">Parrot Ardrone</a>, any drone using the <a href=\"https://mavlink.io/en/\">MAVLink protocol</a>, <a href=\"https://gobot.io/documentation/platforms/pebble/\">Pebble smartwatches</a> and many more. It also has support for different microcontrollers, including Arduino, RaspberryPi and Intel Edison as well as communication protocols like MQTT or <a href=\"https://nats.io/\">NATS</a>. Gobots API to interface with any of the supported platforms appeared to be quite straightforward and easy to understand, so I decided to give it a try.</p>\n<p>My first goal was to just have a little program that allows me to <strong>control the drone from my PC</strong> instead of the <a href=\"https://play.google.com/store/apps/details?id=com.ryzerobotics.tello\">official Tello app</a>. Luckily, Gobot also provides a <a href=\"https://godoc.org/gobot.io/x/gobot/platforms/keyboard\">keyboard driver</a> (theres also an Xbox360 gamepad driver) that can be used to subscribe to certain key events, etc.</p>\n<h1 id=\"First-Prototype\"><a href=\"#First-Prototype\" class=\"headerlink\" title=\"First Prototype\"></a>First Prototype</h1><p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/tello2.png\"></p>\n<p>After tinkering for two hours (and having my poor drone hit the wall several times), I got a basic program that:</p>\n<ul>\n<li><strong>connects</strong> to the drone</li>\n<li>prints incoming <strong>status information</strong> (like battery state) to the console</li>\n<li>handles <strong>keyboard events</strong> ( for lateral and longitudinal navigation, WASD for rotation and altitude control, spacebar to start and land)</li>\n<li>runs something like a <strong>game loop</strong> with a tick rate of 10 FPS to sync the latest user commands with the drone</li>\n<li>renders the drones <strong>video stream</strong> to an <a href=\"https://wiki.debian.org/MPlayer\">MPlayer</a> window with 10 FPS</li>\n</ul>\n<p>The programs main method is as simple as this:</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// tello.go</span></span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">main</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"comment\">// Init Gobot drivers</span></span><br><span class=\"line\">\tkeys := keyboard.NewDriver()</span><br><span class=\"line\">\tdrone := tello.NewDriver(<span class=\"string\">&quot;8890&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\twork := <span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// Handle keyboard inputs</span></span><br><span class=\"line\">\t\tkeys.On(keyboard.Key, handleKeyboardInput(drone))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// Handle drone events</span></span><br><span class=\"line\">\t\tdrone.On(tello.FlightDataEvent, handleFlightData(drone))</span><br><span class=\"line\">\t\tdrone.On(tello.ConnectedEvent, handleConnected(drone))</span><br><span class=\"line\">\t\tdrone.On(tello.LandingEvent, handleLanding(drone))</span><br><span class=\"line\">\t\tdrone.On(tello.VideoFrameEvent, handleVideo(drone))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\trobot := gobot.NewRobot(</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;tello&quot;</span>,</span><br><span class=\"line\">\t\t[]gobot.Connection&#123;&#125;,</span><br><span class=\"line\">\t\t[]gobot.Device&#123;keys, drone&#125;,</span><br><span class=\"line\">\t\twork,</span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\trobot.Start()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br></pre></td></tr></table></figure>\n\n<p>In addition, heres a little excerpt from the <code>tick()</code> method, that checks for the latest key input and calls the corresponding control methods.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// tello.go</span></span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> currentControl == keyboard.A &#123;</span><br><span class=\"line\">    fmt.Println(<span class=\"string\">&quot;Going left.&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> !dry &#123;</span><br><span class=\"line\">        drone.Left(intensity)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> currentControl == keyboard.ArrowRight &#123;</span><br><span class=\"line\">    fmt.Println(<span class=\"string\">&quot;Rotating clockwise.&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> !dry &#123;</span><br><span class=\"line\">        drone.Clockwise(intensity)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    fmt.Println(<span class=\"string\">&quot;Resetting steering.&quot;</span>)</span><br><span class=\"line\">    resetSteering(drone)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br></pre></td></tr></table></figure>\n\n<p>The entire code is <strong>available on GitHub</strong> at <a href=\"https://github.com/muety/tello\">muety/tello</a>.</p>\n<h1 id=\"Challenges\"><a href=\"#Challenges\" class=\"headerlink\" title=\"Challenges\"></a>Challenges</h1><p>One thing that made me stuck for a while was the way API commands like <code>drone.Left(100)</code> (where the integer parameter represents the movements intensity or speed) work. Once called, they are being applied continuously until manually stopped. In this case, the drone would go left with max speed until you explicitly send <code>drone.Left(0)</code>. This behavior combined with the fact that Gobots keyboard driver only supports to communicate when a key <strong>is pressed</strong>, but <strong>not when its released</strong> again, made it a little difficult to smoothly control the drone. To cope with that, I introduced a <strong>debouncing logic</strong>. When a key is pressed (e.g. ), a flag for that key is toggled on for 250 ms before its automatically reset again, if the key was released in the meantime. Within the <code>tick()</code> method, the only thing done is to translate the binary key states to API commands and sync them to the device. </p>\n<h1 id=\"Future-Plans\"><a href=\"#Future-Plans\" class=\"headerlink\" title=\"Future Plans\"></a>Future Plans</h1><p>In case I have some time to further work in this little project, I would love to add basic <strong>self-flying capabilities</strong>. How cool would it be to have the ability to set a marker on a map and have the Tello fly there autonomously, while automatically avoiding obstacles in its way? A good starting point for this might be to take a look into <a href=\"https://gocv.io/\">GoCV</a>, which is a Go interface to OpenCV. Alternatively, Microsofts <a href=\"https://github.com/Microsoft/AirSim\">AirSim</a> simulator provides explicit support for training machine learning models for self-flying drones. Very cool!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1 id=\"The-Idea\"><a href=\"#The-Idea\" class=\"headerlink\" title=\"The Idea\"></a>The Idea</h1><p>A few months ago, I bought a <a href=\"https://amzn.to/2neAwVr\">DJI Tello</a> (affiliate link) drone on Amazon for ~ 80 , which is quite an impressive price, considering that you can also pay several hundred or even thousand Euro for a DJI drone. Of course, this one is only meant for fun and tinkering, not for professional photography or so.<br>Even though I was amazed by how easy it is to control the drone from your smartphone  even with windy weather conditions - the Tello started to bore me after a few weeks. I wanted to do something more interesting with the drone  I wanted to <strong>program it</strong>!</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/tello1.jpg\"></p>\n<h1 id=\"Using-Gobot-SDK\"><a href=\"#Using-Gobot-SDK\" class=\"headerlink\" title=\"Using Gobot SDK\"></a>Using Gobot SDK</h1><p>During my research on how to program drones - specifically the Tello  I found an article called <a href=\"https://gobot.io/blog/2018/04/20/hello-tello-hacking-drones-with-go/\">Hello, Tello - Hacking Drones With Go</a>, which referenced the documentation of a robot programming toolkit called <a href=\"https://gobot.io/\">Gobot</a>. As it turned out, Gobot is a Go SDK to control several different micro-robots like <a href=\"https://gobot.io/documentation/platforms/gopigo3/\">GoPiGo3</a>, the <a href=\"https://gobot.io/documentation/platforms/ardrone/\">Parrot Ardrone</a>, any drone using the <a href=\"https://mavlink.io/en/\">MAVLink protocol</a>, <a href=\"https://gobot.io/documentation/platforms/pebble/\">Pebble smartwatches</a> and many more. It also has support for different microcontrollers, including Arduino, RaspberryPi and Intel Edison as well as communication protocols like MQTT or <a href=\"https://nats.io/\">NATS</a>. Gobots API to interface with any of the supported platforms appeared to be quite straightforward and easy to understand, so I decided to give it a try.</p>\n<p>My first goal was to just have a little program that allows me to <strong>control the drone from my PC</strong> instead of the <a href=\"https://play.google.com/store/apps/details?id=com.ryzerobotics.tello\">official Tello app</a>. Luckily, Gobot also provides a <a href=\"https://godoc.org/gobot.io/x/gobot/platforms/keyboard\">keyboard driver</a> (theres also an Xbox360 gamepad driver) that can be used to subscribe to certain key events, etc.</p>\n<h1 id=\"First-Prototype\"><a href=\"#First-Prototype\" class=\"headerlink\" title=\"First Prototype\"></a>First Prototype</h1><p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/tello2.png\"></p>\n<p>After tinkering for two hours (and having my poor drone hit the wall several times), I got a basic program that:</p>\n<ul>\n<li><strong>connects</strong> to the drone</li>\n<li>prints incoming <strong>status information</strong> (like battery state) to the console</li>\n<li>handles <strong>keyboard events</strong> ( for lateral and longitudinal navigation, WASD for rotation and altitude control, spacebar to start and land)</li>\n<li>runs something like a <strong>game loop</strong> with a tick rate of 10 FPS to sync the latest user commands with the drone</li>\n<li>renders the drones <strong>video stream</strong> to an <a href=\"https://wiki.debian.org/MPlayer\">MPlayer</a> window with 10 FPS</li>\n</ul>\n<p>The programs main method is as simple as this:</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// tello.go</span></span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">main</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"comment\">// Init Gobot drivers</span></span><br><span class=\"line\">\tkeys := keyboard.NewDriver()</span><br><span class=\"line\">\tdrone := tello.NewDriver(<span class=\"string\">&quot;8890&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\twork := <span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">// Handle keyboard inputs</span></span><br><span class=\"line\">\t\tkeys.On(keyboard.Key, handleKeyboardInput(drone))</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// Handle drone events</span></span><br><span class=\"line\">\t\tdrone.On(tello.FlightDataEvent, handleFlightData(drone))</span><br><span class=\"line\">\t\tdrone.On(tello.ConnectedEvent, handleConnected(drone))</span><br><span class=\"line\">\t\tdrone.On(tello.LandingEvent, handleLanding(drone))</span><br><span class=\"line\">\t\tdrone.On(tello.VideoFrameEvent, handleVideo(drone))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\trobot := gobot.NewRobot(</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;tello&quot;</span>,</span><br><span class=\"line\">\t\t[]gobot.Connection&#123;&#125;,</span><br><span class=\"line\">\t\t[]gobot.Device&#123;keys, drone&#125;,</span><br><span class=\"line\">\t\twork,</span><br><span class=\"line\">\t)</span><br><span class=\"line\"></span><br><span class=\"line\">\trobot.Start()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br></pre></td></tr></table></figure>\n\n<p>In addition, heres a little excerpt from the <code>tick()</code> method, that checks for the latest key input and calls the corresponding control methods.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// tello.go</span></span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> currentControl == keyboard.A &#123;</span><br><span class=\"line\">    fmt.Println(<span class=\"string\">&quot;Going left.&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> !dry &#123;</span><br><span class=\"line\">        drone.Left(intensity)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> currentControl == keyboard.ArrowRight &#123;</span><br><span class=\"line\">    fmt.Println(<span class=\"string\">&quot;Rotating clockwise.&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> !dry &#123;</span><br><span class=\"line\">        drone.Clockwise(intensity)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    fmt.Println(<span class=\"string\">&quot;Resetting steering.&quot;</span>)</span><br><span class=\"line\">    resetSteering(drone)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// [...]</span></span><br></pre></td></tr></table></figure>\n\n<p>The entire code is <strong>available on GitHub</strong> at <a href=\"https://github.com/muety/tello\">muety/tello</a>.</p>\n<h1 id=\"Challenges\"><a href=\"#Challenges\" class=\"headerlink\" title=\"Challenges\"></a>Challenges</h1><p>One thing that made me stuck for a while was the way API commands like <code>drone.Left(100)</code> (where the integer parameter represents the movements intensity or speed) work. Once called, they are being applied continuously until manually stopped. In this case, the drone would go left with max speed until you explicitly send <code>drone.Left(0)</code>. This behavior combined with the fact that Gobots keyboard driver only supports to communicate when a key <strong>is pressed</strong>, but <strong>not when its released</strong> again, made it a little difficult to smoothly control the drone. To cope with that, I introduced a <strong>debouncing logic</strong>. When a key is pressed (e.g. ), a flag for that key is toggled on for 250 ms before its automatically reset again, if the key was released in the meantime. Within the <code>tick()</code> method, the only thing done is to translate the binary key states to API commands and sync them to the device. </p>\n<h1 id=\"Future-Plans\"><a href=\"#Future-Plans\" class=\"headerlink\" title=\"Future Plans\"></a>Future Plans</h1><p>In case I have some time to further work in this little project, I would love to add basic <strong>self-flying capabilities</strong>. How cool would it be to have the ability to set a marker on a map and have the Tello fly there autonomously, while automatically avoiding obstacles in its way? A good starting point for this might be to take a look into <a href=\"https://gocv.io/\">GoCV</a>, which is a Go interface to OpenCV. Alternatively, Microsofts <a href=\"https://github.com/Microsoft/AirSim\">AirSim</a> simulator provides explicit support for training machine learning models for self-flying drones. Very cool!</p>\n"},{"title":"Halite - A rule-based AI bot","date":"2018-01-03T07:04:08.000Z","_content":"\nAfter having spent a considerable amount of time with it last weekend, I wanted to make a short comment on [Halite.io](https://halite.io). Halite is a programming- and AI competition where people can write programs or train algorithms to control a bot that plays in a virtual 2D game environment. There is a leaderboard to track how your bot competes with other players' bots and you can watch a replay of every game your bot has played, which helps _debugging_ your bot as well as figuring out other people's strategies. Originally, I got aware of this challenge through a video by [one of my favorite](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ) YouTube channels and became slightly addicted from that moment on. \n\n## The Game\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/halite_game.png)\n\nWhile the complete rule set of Halite can be viewed in [their documentation](https://halite.io/learn-programming-challenge/), I only want to explain very basics here. In Halite you play in a space scenario comprising _ships_ and _planets_, while you (= your bot) controls your ships. A ship can do three actions: _move_, _dock_ to or _undock_ from a planet. The more ships you have docked at a planet, the faster you are _mining_ the planet, which means to produce new ships. When two ships get close enough, they can fight and only the winner's ship survives. The game is turn-based, so each of the up to four players' programs are queried (by the game environment) for a list of moves for each of their ships in every turn. Input is the current game state (player- & planet positions, ships' health, ships' current status, ...) and output is a move for each ship. The final goal is to either completely dominate (= destroy every other players' ships) or own the strongest ship fleet and the most planets after 300 turns. \n\nAt the time of writing this article, the leaderboard comprises __~ 4700 players__ from __98 countries__. Most of them are either university students or professionals, who have, in total, played __10.9 million__ games. More interesting statistics can be found at the [stats](http://stats.halite.io:3000/public/dashboard/545ebc3c-4cdb-4940-acf1-e4d1332defac) page. \n\n## My bot\nFirst of all, what I found especially cool is the fact that players are completely free in their choice of how to realize their bot (hard-coding, machine learning, ...) and what programming language to use. Halite offers community-created starter templates for C++, C#, Dart, Elixir, Go, Haskell, JavaScript, Julia, Kotlin, PHP, Ruby, Rust, Scala, Swift and more. You can choose whatever language you like - which was __Java__ for me - and eventually submit a ZIP file with your code or binary, as well as a script for executing it, to their website. The interface between your program and the game environment is _stdin_ / _stdout_. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/halite_langs.png)\n\nI decided to build my bot based on rather simple rules first, which I figured out by watching some other players' replays. Probably applying machine learning to solve Halite would be even more challenging, but at the time I got aware of this competition, it was about to last only three more weeks (until January, 22nd), so I picked up on a rather simple approach.\n\n## Strategies\nI developed three different strategies for playing Halite, while my bot could dynamically switch between them during the game. The first one is called the __BalancedStrategy__. It's a fair mixture of conquering new planets, raising ship production and destroying enemy ships. In this strategy, a ship's most preferable goal is to take empty planets. However, if the next empty planet is unavailable ot too far away, it may also dock at a planet my bot already owns in order to increase production. However, no more than three of my own ships will ever dock at the same planet. If both the next empty planet as well as the next own planet are unavailable or too far away, the ship starts chasing the nearest enemy ship. \n\nIn addition to this compromise-like strategy, I found that more extreme ones can be successful, too. For instance, in a 1 vs. 1 match, my opponents usually won because they almost instantly destroyed all of my ships in the very beginning. So I decided to further introduce the __AggressiveStrategy__, which is only applied in matched of two players. Following this strategy, all of my three initial ships immediately start chasing the enemy's three ships. If they're successful, the game is usually over after only a few turns. \n\nFinally I built the __MiningStrategy__. Goal is to maxmimize mining rate right in the beginning. My three ships are immediately docking at the nearest and largest (in terms of how many ships can dock at it) free planet and dock at it. New ships will dock at the same planet until it's full. Doing so, I can quickly produce new ships which then, after the first planet is full, switch over to applying the __BalancedStrategy__. \n\n## Results\nUsing the bot described above, the best rank I achieved in the leaderboard was __250 of ~ 4500__ (top 6 %) and I'm still ambitious to get even better . My bot is playing as [n1try](https://halite.io/user/?user_id=7481).\n\n\\>> [Source code on GitHub](https://github.com/muety/halite-bot-java). ","source":"_posts/halite-a-rule-based-ai-bot.md","raw":"---\ntitle: Halite - A rule-based AI bot\ndate: 2018-01-03 08:04:08\ntags:\n---\n\nAfter having spent a considerable amount of time with it last weekend, I wanted to make a short comment on [Halite.io](https://halite.io). Halite is a programming- and AI competition where people can write programs or train algorithms to control a bot that plays in a virtual 2D game environment. There is a leaderboard to track how your bot competes with other players' bots and you can watch a replay of every game your bot has played, which helps _debugging_ your bot as well as figuring out other people's strategies. Originally, I got aware of this challenge through a video by [one of my favorite](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ) YouTube channels and became slightly addicted from that moment on. \n\n## The Game\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/halite_game.png)\n\nWhile the complete rule set of Halite can be viewed in [their documentation](https://halite.io/learn-programming-challenge/), I only want to explain very basics here. In Halite you play in a space scenario comprising _ships_ and _planets_, while you (= your bot) controls your ships. A ship can do three actions: _move_, _dock_ to or _undock_ from a planet. The more ships you have docked at a planet, the faster you are _mining_ the planet, which means to produce new ships. When two ships get close enough, they can fight and only the winner's ship survives. The game is turn-based, so each of the up to four players' programs are queried (by the game environment) for a list of moves for each of their ships in every turn. Input is the current game state (player- & planet positions, ships' health, ships' current status, ...) and output is a move for each ship. The final goal is to either completely dominate (= destroy every other players' ships) or own the strongest ship fleet and the most planets after 300 turns. \n\nAt the time of writing this article, the leaderboard comprises __~ 4700 players__ from __98 countries__. Most of them are either university students or professionals, who have, in total, played __10.9 million__ games. More interesting statistics can be found at the [stats](http://stats.halite.io:3000/public/dashboard/545ebc3c-4cdb-4940-acf1-e4d1332defac) page. \n\n## My bot\nFirst of all, what I found especially cool is the fact that players are completely free in their choice of how to realize their bot (hard-coding, machine learning, ...) and what programming language to use. Halite offers community-created starter templates for C++, C#, Dart, Elixir, Go, Haskell, JavaScript, Julia, Kotlin, PHP, Ruby, Rust, Scala, Swift and more. You can choose whatever language you like - which was __Java__ for me - and eventually submit a ZIP file with your code or binary, as well as a script for executing it, to their website. The interface between your program and the game environment is _stdin_ / _stdout_. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/halite_langs.png)\n\nI decided to build my bot based on rather simple rules first, which I figured out by watching some other players' replays. Probably applying machine learning to solve Halite would be even more challenging, but at the time I got aware of this competition, it was about to last only three more weeks (until January, 22nd), so I picked up on a rather simple approach.\n\n## Strategies\nI developed three different strategies for playing Halite, while my bot could dynamically switch between them during the game. The first one is called the __BalancedStrategy__. It's a fair mixture of conquering new planets, raising ship production and destroying enemy ships. In this strategy, a ship's most preferable goal is to take empty planets. However, if the next empty planet is unavailable ot too far away, it may also dock at a planet my bot already owns in order to increase production. However, no more than three of my own ships will ever dock at the same planet. If both the next empty planet as well as the next own planet are unavailable or too far away, the ship starts chasing the nearest enemy ship. \n\nIn addition to this compromise-like strategy, I found that more extreme ones can be successful, too. For instance, in a 1 vs. 1 match, my opponents usually won because they almost instantly destroyed all of my ships in the very beginning. So I decided to further introduce the __AggressiveStrategy__, which is only applied in matched of two players. Following this strategy, all of my three initial ships immediately start chasing the enemy's three ships. If they're successful, the game is usually over after only a few turns. \n\nFinally I built the __MiningStrategy__. Goal is to maxmimize mining rate right in the beginning. My three ships are immediately docking at the nearest and largest (in terms of how many ships can dock at it) free planet and dock at it. New ships will dock at the same planet until it's full. Doing so, I can quickly produce new ships which then, after the first planet is full, switch over to applying the __BalancedStrategy__. \n\n## Results\nUsing the bot described above, the best rank I achieved in the leaderboard was __250 of ~ 4500__ (top 6 %) and I'm still ambitious to get even better . My bot is playing as [n1try](https://halite.io/user/?user_id=7481).\n\n\\>> [Source code on GitHub](https://github.com/muety/halite-bot-java). ","slug":"halite-a-rule-based-ai-bot","published":1,"updated":"2020-10-30T20:05:40.283Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlu000co2e02rkn0cxc","content":"<p>After having spent a considerable amount of time with it last weekend, I wanted to make a short comment on <a href=\"https://halite.io/\">Halite.io</a>. Halite is a programming- and AI competition where people can write programs or train algorithms to control a bot that plays in a virtual 2D game environment. There is a leaderboard to track how your bot competes with other players bots and you can watch a replay of every game your bot has played, which helps <em>debugging</em> your bot as well as figuring out other peoples strategies. Originally, I got aware of this challenge through a video by <a href=\"https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ\">one of my favorite</a> YouTube channels and became slightly addicted from that moment on. </p>\n<h2 id=\"The-Game\"><a href=\"#The-Game\" class=\"headerlink\" title=\"The Game\"></a>The Game</h2><p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/halite_game.png\"></p>\n<p>While the complete rule set of Halite can be viewed in <a href=\"https://halite.io/learn-programming-challenge/\">their documentation</a>, I only want to explain very basics here. In Halite you play in a space scenario comprising <em>ships</em> and <em>planets</em>, while you (= your bot) controls your ships. A ship can do three actions: <em>move</em>, <em>dock</em> to or <em>undock</em> from a planet. The more ships you have docked at a planet, the faster you are <em>mining</em> the planet, which means to produce new ships. When two ships get close enough, they can fight and only the winners ship survives. The game is turn-based, so each of the up to four players programs are queried (by the game environment) for a list of moves for each of their ships in every turn. Input is the current game state (player- &amp; planet positions, ships health, ships current status, ) and output is a move for each ship. The final goal is to either completely dominate (= destroy every other players ships) or own the strongest ship fleet and the most planets after 300 turns. </p>\n<p>At the time of writing this article, the leaderboard comprises <strong>~ 4700 players</strong> from <strong>98 countries</strong>. Most of them are either university students or professionals, who have, in total, played <strong>10.9 million</strong> games. More interesting statistics can be found at the <a href=\"http://stats.halite.io:3000/public/dashboard/545ebc3c-4cdb-4940-acf1-e4d1332defac\">stats</a> page. </p>\n<h2 id=\"My-bot\"><a href=\"#My-bot\" class=\"headerlink\" title=\"My bot\"></a>My bot</h2><p>First of all, what I found especially cool is the fact that players are completely free in their choice of how to realize their bot (hard-coding, machine learning, ) and what programming language to use. Halite offers community-created starter templates for C++, C#, Dart, Elixir, Go, Haskell, JavaScript, Julia, Kotlin, PHP, Ruby, Rust, Scala, Swift and more. You can choose whatever language you like - which was <strong>Java</strong> for me - and eventually submit a ZIP file with your code or binary, as well as a script for executing it, to their website. The interface between your program and the game environment is <em>stdin</em> / <em>stdout</em>. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/halite_langs.png\"></p>\n<p>I decided to build my bot based on rather simple rules first, which I figured out by watching some other players replays. Probably applying machine learning to solve Halite would be even more challenging, but at the time I got aware of this competition, it was about to last only three more weeks (until January, 22nd), so I picked up on a rather simple approach.</p>\n<h2 id=\"Strategies\"><a href=\"#Strategies\" class=\"headerlink\" title=\"Strategies\"></a>Strategies</h2><p>I developed three different strategies for playing Halite, while my bot could dynamically switch between them during the game. The first one is called the <strong>BalancedStrategy</strong>. Its a fair mixture of conquering new planets, raising ship production and destroying enemy ships. In this strategy, a ships most preferable goal is to take empty planets. However, if the next empty planet is unavailable ot too far away, it may also dock at a planet my bot already owns in order to increase production. However, no more than three of my own ships will ever dock at the same planet. If both the next empty planet as well as the next own planet are unavailable or too far away, the ship starts chasing the nearest enemy ship. </p>\n<p>In addition to this compromise-like strategy, I found that more extreme ones can be successful, too. For instance, in a 1 vs. 1 match, my opponents usually won because they almost instantly destroyed all of my ships in the very beginning. So I decided to further introduce the <strong>AggressiveStrategy</strong>, which is only applied in matched of two players. Following this strategy, all of my three initial ships immediately start chasing the enemys three ships. If theyre successful, the game is usually over after only a few turns. </p>\n<p>Finally I built the <strong>MiningStrategy</strong>. Goal is to maxmimize mining rate right in the beginning. My three ships are immediately docking at the nearest and largest (in terms of how many ships can dock at it) free planet and dock at it. New ships will dock at the same planet until its full. Doing so, I can quickly produce new ships which then, after the first planet is full, switch over to applying the <strong>BalancedStrategy</strong>. </p>\n<h2 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h2><p>Using the bot described above, the best rank I achieved in the leaderboard was <strong>250 of ~ 4500</strong> (top 6 %) and Im still ambitious to get even better . My bot is playing as <a href=\"https://halite.io/user/?user_id=7481\">n1try</a>.</p>\n<p>&gt;&gt; <a href=\"https://github.com/muety/halite-bot-java\">Source code on GitHub</a>. </p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>After having spent a considerable amount of time with it last weekend, I wanted to make a short comment on <a href=\"https://halite.io/\">Halite.io</a>. Halite is a programming- and AI competition where people can write programs or train algorithms to control a bot that plays in a virtual 2D game environment. There is a leaderboard to track how your bot competes with other players bots and you can watch a replay of every game your bot has played, which helps <em>debugging</em> your bot as well as figuring out other peoples strategies. Originally, I got aware of this challenge through a video by <a href=\"https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ\">one of my favorite</a> YouTube channels and became slightly addicted from that moment on. </p>\n<h2 id=\"The-Game\"><a href=\"#The-Game\" class=\"headerlink\" title=\"The Game\"></a>The Game</h2><p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/halite_game.png\"></p>\n<p>While the complete rule set of Halite can be viewed in <a href=\"https://halite.io/learn-programming-challenge/\">their documentation</a>, I only want to explain very basics here. In Halite you play in a space scenario comprising <em>ships</em> and <em>planets</em>, while you (= your bot) controls your ships. A ship can do three actions: <em>move</em>, <em>dock</em> to or <em>undock</em> from a planet. The more ships you have docked at a planet, the faster you are <em>mining</em> the planet, which means to produce new ships. When two ships get close enough, they can fight and only the winners ship survives. The game is turn-based, so each of the up to four players programs are queried (by the game environment) for a list of moves for each of their ships in every turn. Input is the current game state (player- &amp; planet positions, ships health, ships current status, ) and output is a move for each ship. The final goal is to either completely dominate (= destroy every other players ships) or own the strongest ship fleet and the most planets after 300 turns. </p>\n<p>At the time of writing this article, the leaderboard comprises <strong>~ 4700 players</strong> from <strong>98 countries</strong>. Most of them are either university students or professionals, who have, in total, played <strong>10.9 million</strong> games. More interesting statistics can be found at the <a href=\"http://stats.halite.io:3000/public/dashboard/545ebc3c-4cdb-4940-acf1-e4d1332defac\">stats</a> page. </p>\n<h2 id=\"My-bot\"><a href=\"#My-bot\" class=\"headerlink\" title=\"My bot\"></a>My bot</h2><p>First of all, what I found especially cool is the fact that players are completely free in their choice of how to realize their bot (hard-coding, machine learning, ) and what programming language to use. Halite offers community-created starter templates for C++, C#, Dart, Elixir, Go, Haskell, JavaScript, Julia, Kotlin, PHP, Ruby, Rust, Scala, Swift and more. You can choose whatever language you like - which was <strong>Java</strong> for me - and eventually submit a ZIP file with your code or binary, as well as a script for executing it, to their website. The interface between your program and the game environment is <em>stdin</em> / <em>stdout</em>. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/halite_langs.png\"></p>\n<p>I decided to build my bot based on rather simple rules first, which I figured out by watching some other players replays. Probably applying machine learning to solve Halite would be even more challenging, but at the time I got aware of this competition, it was about to last only three more weeks (until January, 22nd), so I picked up on a rather simple approach.</p>\n<h2 id=\"Strategies\"><a href=\"#Strategies\" class=\"headerlink\" title=\"Strategies\"></a>Strategies</h2><p>I developed three different strategies for playing Halite, while my bot could dynamically switch between them during the game. The first one is called the <strong>BalancedStrategy</strong>. Its a fair mixture of conquering new planets, raising ship production and destroying enemy ships. In this strategy, a ships most preferable goal is to take empty planets. However, if the next empty planet is unavailable ot too far away, it may also dock at a planet my bot already owns in order to increase production. However, no more than three of my own ships will ever dock at the same planet. If both the next empty planet as well as the next own planet are unavailable or too far away, the ship starts chasing the nearest enemy ship. </p>\n<p>In addition to this compromise-like strategy, I found that more extreme ones can be successful, too. For instance, in a 1 vs. 1 match, my opponents usually won because they almost instantly destroyed all of my ships in the very beginning. So I decided to further introduce the <strong>AggressiveStrategy</strong>, which is only applied in matched of two players. Following this strategy, all of my three initial ships immediately start chasing the enemys three ships. If theyre successful, the game is usually over after only a few turns. </p>\n<p>Finally I built the <strong>MiningStrategy</strong>. Goal is to maxmimize mining rate right in the beginning. My three ships are immediately docking at the nearest and largest (in terms of how many ships can dock at it) free planet and dock at it. New ships will dock at the same planet until its full. Doing so, I can quickly produce new ships which then, after the first planet is full, switch over to applying the <strong>BalancedStrategy</strong>. </p>\n<h2 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h2><p>Using the bot described above, the best rank I achieved in the leaderboard was <strong>250 of ~ 4500</strong> (top 6 %) and Im still ambitious to get even better . My bot is playing as <a href=\"https://halite.io/user/?user_id=7481\">n1try</a>.</p>\n<p>&gt;&gt; <a href=\"https://github.com/muety/halite-bot-java\">Source code on GitHub</a>. </p>\n"},{"title":"How to enable DNS-over-TLS on Ubuntu using CoreDNS","date":"2020-04-11T18:41:57.000Z","description":"This article describes, how to browse the web more privately using DNS-over-TLS. Therefore, it is shown how to set up CoreDNS on a Ubuntu machine.","_content":"\n# Privacy on the Web\nLuckily, most traffic on the web is encrypted today, which means nobody between your computer and the web server knows what you are sending or receiving. This includes your internet service provider (ISP), any kind of government agency or a potential attacker on your network. Since the entire HTTP packet, including its headers, is encrypted, they will not even see what website you are visiting. At least not for sure. What they can see is the target web server's IP address from the IP packet's header. However, there might be several different web servers for different web sites listening on that IP and there is no chance to find out which one you intended to visit.\n\n# The Problem with DNS\nHowever, although HTTP is usually encrypted, DNS is usually not. So before your browser performs the actual HTTP request, your operating system will perform a DNS query to resolve, for instance, _\"google.com\"_ to `216.58.207.46`. Your question  _\"What's the IP for google.com?\"_  is contained in the DNS query as plain text, so everyone between your computer and the DNS server will know that you are trying to access Google  or whatever website. And, of course, the provider of your DNS server will know as well, since it has to answer the query.\n\nUsually, your default DNS server is the one provided by your ISP. And since the ISP can directly associate your internet connection with your name and address it will know about any website that you  as a person  visit. Even if you change the default to something else (e.g. [Google's public DNS resolver](https://developers.google.com/speed/public-dns/) 8.8.8.8 or [CloudFlare's](https://1.1.1.1/) `1.1.1.1`), your ISP can still read your DNS queries as they mandatorily pass through its network. Consequently, in order to browse more privately on the web  in addition to using HTTPS - there are two steps you need to consider:\n\n1. Change your DNS provider to one that is more anonymous and does not have personal information about you\n2. Encrypt your DNS queries to prevent anyone in the middle (especially your ISP) from reading them\n\n![Example of a non-encrypted DNS request](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dns1.png)\nExample of a non-encrypted DNS request for `kit.edu` to Google's `8.8.8.8` DNS resolver \n\nLuckily, the [DNS-over-TLS](https://en.wikipedia.org/wiki/DNS_over_TLS) specification already provides a solution and it is already supported by the three largest public DNS providers [CloudFlare](https://1.1.1.1/), [Google](https://developers.google.com/speed/public-dns/) and [Quad9](https://www.quad9.net/). You only have to configure your computer to use it.\n\n# CoreDNS Setup\nIn this article, I show you how to use DNS-over-TLS with [CoreDNS](https://coredns.io/) as a local DNS recursor on your machine. It is an open-source software and primarily known for being used as a nameserver in Kubernetes networks. Please note that I decided to use CoreDNS, because it is particularly easy to configure and offers a variety of cool [plugins](https://coredns.io/plugins/), like [metrics collection with Prometheus](https://coredns.io/plugins/metrics/) and more. However, Ubuntu's default DNS recursor [systemd-resolved apparently supports DNS-over-TLS](https://www.internetsociety.org/blog/2018/12/dns-privacy-in-linux-systemd/) as well and is probably easier to get started with initially. So if you prefer to go the easy way, just head over to the previously mentioned blog post and follow its instructions.\n\n**Disclaimer:** Please use this guide at your own risk. I do not take any responsibility in case you accidentally crash your DNS setup.\n\nIn order to set up CoreDNS, there are a few steps to follow.\n\n1. Download CoreDNS from the [website](https://coredns.io), unpack the binary to `/usr/local/bin` and make it executable (`sudo chmod +x /usr/local/bin/coredns`)\n2. Install `resolvconf` as a tool to manually manage `/etc/resolv.conf`: `sudo apt install resolvconf`  \n3. Set `dns=default` in `/etc/NetworkManager/NetworkManager.conf`\n4. Add `nameserver 127.0.0.1` to `/etc/resolvconf/resolv.conf.d/head`\n5. Create `/etc/coredns/Corefile` and paste the configuration shown below. In this example, we are using CloudFlare as a DNS provider. You can use Google or Quad9 as well, just change the IPs.\n6. Create a new user for CoreDNS: `sudo useradd -d /var/lib/coredns -m coredns`\n7. Set some permissions: `sudo chown coredns:coredns /opt/coredns`\n8. Download the SystemD service unit file from [coredns/coredns](https://github.com/coredns/deployment/tree/master/systemd) to `/etc/systemd/system/coredns.service`\n9. Disable SystemD's default DNS server: `sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved`\n  1. **Please Note:** From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again\n10. Enable and start CoreDNS: `sudo systemctl enable coredns && sudo systemctl start coredns`\n11. You should be able to resolve domain names, again. E.g. try `dig +short kit.edu`. If an IP address is printed, everything works fine.\n\n```\n# /etc/coredns/Corefile\n\n.:53 {\n    forward . tls://2606:4700:4700::1111 tls://1.1.1.1\n    log\n    errors\n    cache\n}\n```\n\n![Example of an encrypted DNS request](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dns2.png)\nExample of an encrypted DNS request for `kit.edu` to CloudFlare's `1.1.1.1` DNS resolver\n\nAlternatively, use the following `forward` statement to use the independent [BlahDNS](https://blahdns.com) instead of CloudFlare as provider.\n\n```\nforward . tls://2a01:4f8:1c1c:6b4b::1 tls://159.69.198.101 {\n    tls_servername dot-de.blahdns.com\n}\n```\n\n# What happens?\nLet us examine what happens (in terms of DNS) when you type _\"kit.edu\"_ in your browser's address bar and hit enter.\n\n1. Your browser asks your operating system to resolve `kit.edu`\n2. Your OS finds out that its primary configured nameserver is `127.0.0.1:53`, i.e. your local CoreDNS, and consults that one\n3. CoreDNS checks its cache and in case of a miss consults its configured nameserver at `1.1.1.1`, i.e. CloudFlare\n4. CloudFlare, again, checks its cache and in case of a miss goes up the hierarchical chain of nameservers until one of them has an answer\n5. Eventually, your browser performs `GET / HTTP/2.0` to `129.13.40.10` with `Host: kit.edu`\n\n# Further Reading\nHere are a few additional posts about DNS, which I found very useful.\n\n* [What Is DNS? | How DNS Works](https://www.cloudflare.com/learning/dns/what-is-dns/)\n* [DNS-over-https vs. DNS-over-TLS vs DNSCrypt](https://www.reddit.com/r/privacy/comments/89pr15/dnsoverhttps_vs_dns_overtls_vs_dnscrypt/dwsosjr?utm_source=share&utm_medium=web2x)\n\nPlease let me know if my guide is missing any required steps. Good luck, have fun and browse safely!","source":"_posts/how-to-enable-dns-over-tls-on-ubuntu-using-coredns.md","raw":"---\ntitle: How to enable DNS-over-TLS on Ubuntu using CoreDNS\ndate: 2020-04-11 20:41:57\ntags:\ndescription: This article describes, how to browse the web more privately using DNS-over-TLS. Therefore, it is shown how to set up CoreDNS on a Ubuntu machine.\n---\n\n# Privacy on the Web\nLuckily, most traffic on the web is encrypted today, which means nobody between your computer and the web server knows what you are sending or receiving. This includes your internet service provider (ISP), any kind of government agency or a potential attacker on your network. Since the entire HTTP packet, including its headers, is encrypted, they will not even see what website you are visiting. At least not for sure. What they can see is the target web server's IP address from the IP packet's header. However, there might be several different web servers for different web sites listening on that IP and there is no chance to find out which one you intended to visit.\n\n# The Problem with DNS\nHowever, although HTTP is usually encrypted, DNS is usually not. So before your browser performs the actual HTTP request, your operating system will perform a DNS query to resolve, for instance, _\"google.com\"_ to `216.58.207.46`. Your question  _\"What's the IP for google.com?\"_  is contained in the DNS query as plain text, so everyone between your computer and the DNS server will know that you are trying to access Google  or whatever website. And, of course, the provider of your DNS server will know as well, since it has to answer the query.\n\nUsually, your default DNS server is the one provided by your ISP. And since the ISP can directly associate your internet connection with your name and address it will know about any website that you  as a person  visit. Even if you change the default to something else (e.g. [Google's public DNS resolver](https://developers.google.com/speed/public-dns/) 8.8.8.8 or [CloudFlare's](https://1.1.1.1/) `1.1.1.1`), your ISP can still read your DNS queries as they mandatorily pass through its network. Consequently, in order to browse more privately on the web  in addition to using HTTPS - there are two steps you need to consider:\n\n1. Change your DNS provider to one that is more anonymous and does not have personal information about you\n2. Encrypt your DNS queries to prevent anyone in the middle (especially your ISP) from reading them\n\n![Example of a non-encrypted DNS request](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dns1.png)\nExample of a non-encrypted DNS request for `kit.edu` to Google's `8.8.8.8` DNS resolver \n\nLuckily, the [DNS-over-TLS](https://en.wikipedia.org/wiki/DNS_over_TLS) specification already provides a solution and it is already supported by the three largest public DNS providers [CloudFlare](https://1.1.1.1/), [Google](https://developers.google.com/speed/public-dns/) and [Quad9](https://www.quad9.net/). You only have to configure your computer to use it.\n\n# CoreDNS Setup\nIn this article, I show you how to use DNS-over-TLS with [CoreDNS](https://coredns.io/) as a local DNS recursor on your machine. It is an open-source software and primarily known for being used as a nameserver in Kubernetes networks. Please note that I decided to use CoreDNS, because it is particularly easy to configure and offers a variety of cool [plugins](https://coredns.io/plugins/), like [metrics collection with Prometheus](https://coredns.io/plugins/metrics/) and more. However, Ubuntu's default DNS recursor [systemd-resolved apparently supports DNS-over-TLS](https://www.internetsociety.org/blog/2018/12/dns-privacy-in-linux-systemd/) as well and is probably easier to get started with initially. So if you prefer to go the easy way, just head over to the previously mentioned blog post and follow its instructions.\n\n**Disclaimer:** Please use this guide at your own risk. I do not take any responsibility in case you accidentally crash your DNS setup.\n\nIn order to set up CoreDNS, there are a few steps to follow.\n\n1. Download CoreDNS from the [website](https://coredns.io), unpack the binary to `/usr/local/bin` and make it executable (`sudo chmod +x /usr/local/bin/coredns`)\n2. Install `resolvconf` as a tool to manually manage `/etc/resolv.conf`: `sudo apt install resolvconf`  \n3. Set `dns=default` in `/etc/NetworkManager/NetworkManager.conf`\n4. Add `nameserver 127.0.0.1` to `/etc/resolvconf/resolv.conf.d/head`\n5. Create `/etc/coredns/Corefile` and paste the configuration shown below. In this example, we are using CloudFlare as a DNS provider. You can use Google or Quad9 as well, just change the IPs.\n6. Create a new user for CoreDNS: `sudo useradd -d /var/lib/coredns -m coredns`\n7. Set some permissions: `sudo chown coredns:coredns /opt/coredns`\n8. Download the SystemD service unit file from [coredns/coredns](https://github.com/coredns/deployment/tree/master/systemd) to `/etc/systemd/system/coredns.service`\n9. Disable SystemD's default DNS server: `sudo systemctl stop systemd-resolved && sudo systemctl disable systemd-resolved`\n  1. **Please Note:** From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again\n10. Enable and start CoreDNS: `sudo systemctl enable coredns && sudo systemctl start coredns`\n11. You should be able to resolve domain names, again. E.g. try `dig +short kit.edu`. If an IP address is printed, everything works fine.\n\n```\n# /etc/coredns/Corefile\n\n.:53 {\n    forward . tls://2606:4700:4700::1111 tls://1.1.1.1\n    log\n    errors\n    cache\n}\n```\n\n![Example of an encrypted DNS request](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dns2.png)\nExample of an encrypted DNS request for `kit.edu` to CloudFlare's `1.1.1.1` DNS resolver\n\nAlternatively, use the following `forward` statement to use the independent [BlahDNS](https://blahdns.com) instead of CloudFlare as provider.\n\n```\nforward . tls://2a01:4f8:1c1c:6b4b::1 tls://159.69.198.101 {\n    tls_servername dot-de.blahdns.com\n}\n```\n\n# What happens?\nLet us examine what happens (in terms of DNS) when you type _\"kit.edu\"_ in your browser's address bar and hit enter.\n\n1. Your browser asks your operating system to resolve `kit.edu`\n2. Your OS finds out that its primary configured nameserver is `127.0.0.1:53`, i.e. your local CoreDNS, and consults that one\n3. CoreDNS checks its cache and in case of a miss consults its configured nameserver at `1.1.1.1`, i.e. CloudFlare\n4. CloudFlare, again, checks its cache and in case of a miss goes up the hierarchical chain of nameservers until one of them has an answer\n5. Eventually, your browser performs `GET / HTTP/2.0` to `129.13.40.10` with `Host: kit.edu`\n\n# Further Reading\nHere are a few additional posts about DNS, which I found very useful.\n\n* [What Is DNS? | How DNS Works](https://www.cloudflare.com/learning/dns/what-is-dns/)\n* [DNS-over-https vs. DNS-over-TLS vs DNSCrypt](https://www.reddit.com/r/privacy/comments/89pr15/dnsoverhttps_vs_dns_overtls_vs_dnscrypt/dwsosjr?utm_source=share&utm_medium=web2x)\n\nPlease let me know if my guide is missing any required steps. Good luck, have fun and browse safely!","slug":"how-to-enable-dns-over-tls-on-ubuntu-using-coredns","published":1,"updated":"2020-10-30T20:05:40.283Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlv000do2e0cw9y1ly9","content":"<h1 id=\"Privacy-on-the-Web\"><a href=\"#Privacy-on-the-Web\" class=\"headerlink\" title=\"Privacy on the Web\"></a>Privacy on the Web</h1><p>Luckily, most traffic on the web is encrypted today, which means nobody between your computer and the web server knows what you are sending or receiving. This includes your internet service provider (ISP), any kind of government agency or a potential attacker on your network. Since the entire HTTP packet, including its headers, is encrypted, they will not even see what website you are visiting. At least not for sure. What they can see is the target web servers IP address from the IP packets header. However, there might be several different web servers for different web sites listening on that IP and there is no chance to find out which one you intended to visit.</p>\n<h1 id=\"The-Problem-with-DNS\"><a href=\"#The-Problem-with-DNS\" class=\"headerlink\" title=\"The Problem with DNS\"></a>The Problem with DNS</h1><p>However, although HTTP is usually encrypted, DNS is usually not. So before your browser performs the actual HTTP request, your operating system will perform a DNS query to resolve, for instance, <em>google.com</em> to <code>216.58.207.46</code>. Your question  <em>Whats the IP for google.com?</em>  is contained in the DNS query as plain text, so everyone between your computer and the DNS server will know that you are trying to access Google  or whatever website. And, of course, the provider of your DNS server will know as well, since it has to answer the query.</p>\n<p>Usually, your default DNS server is the one provided by your ISP. And since the ISP can directly associate your internet connection with your name and address it will know about any website that you  as a person  visit. Even if you change the default to something else (e.g. <a href=\"https://developers.google.com/speed/public-dns/\">Googles public DNS resolver</a> 8.8.8.8 or <a href=\"https://1.1.1.1/\">CloudFlares</a> <code>1.1.1.1</code>), your ISP can still read your DNS queries as they mandatorily pass through its network. Consequently, in order to browse more privately on the web  in addition to using HTTPS - there are two steps you need to consider:</p>\n<ol>\n<li>Change your DNS provider to one that is more anonymous and does not have personal information about you</li>\n<li>Encrypt your DNS queries to prevent anyone in the middle (especially your ISP) from reading them</li>\n</ol>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dns1.png\" alt=\"Example of a non-encrypted DNS request\"><br>Example of a non-encrypted DNS request for <code>kit.edu</code> to Googles <code>8.8.8.8</code> DNS resolver </p>\n<p>Luckily, the <a href=\"https://en.wikipedia.org/wiki/DNS_over_TLS\">DNS-over-TLS</a> specification already provides a solution and it is already supported by the three largest public DNS providers <a href=\"https://1.1.1.1/\">CloudFlare</a>, <a href=\"https://developers.google.com/speed/public-dns/\">Google</a> and <a href=\"https://www.quad9.net/\">Quad9</a>. You only have to configure your computer to use it.</p>\n<h1 id=\"CoreDNS-Setup\"><a href=\"#CoreDNS-Setup\" class=\"headerlink\" title=\"CoreDNS Setup\"></a>CoreDNS Setup</h1><p>In this article, I show you how to use DNS-over-TLS with <a href=\"https://coredns.io/\">CoreDNS</a> as a local DNS recursor on your machine. It is an open-source software and primarily known for being used as a nameserver in Kubernetes networks. Please note that I decided to use CoreDNS, because it is particularly easy to configure and offers a variety of cool <a href=\"https://coredns.io/plugins/\">plugins</a>, like <a href=\"https://coredns.io/plugins/metrics/\">metrics collection with Prometheus</a> and more. However, Ubuntus default DNS recursor <a href=\"https://www.internetsociety.org/blog/2018/12/dns-privacy-in-linux-systemd/\">systemd-resolved apparently supports DNS-over-TLS</a> as well and is probably easier to get started with initially. So if you prefer to go the easy way, just head over to the previously mentioned blog post and follow its instructions.</p>\n<p><strong>Disclaimer:</strong> Please use this guide at your own risk. I do not take any responsibility in case you accidentally crash your DNS setup.</p>\n<p>In order to set up CoreDNS, there are a few steps to follow.</p>\n<ol>\n<li>Download CoreDNS from the <a href=\"https://coredns.io/\">website</a>, unpack the binary to <code>/usr/local/bin</code> and make it executable (<code>sudo chmod +x /usr/local/bin/coredns</code>)</li>\n<li>Install <code>resolvconf</code> as a tool to manually manage <code>/etc/resolv.conf</code>: <code>sudo apt install resolvconf</code>  </li>\n<li>Set <code>dns=default</code> in <code>/etc/NetworkManager/NetworkManager.conf</code></li>\n<li>Add <code>nameserver 127.0.0.1</code> to <code>/etc/resolvconf/resolv.conf.d/head</code></li>\n<li>Create <code>/etc/coredns/Corefile</code> and paste the configuration shown below. In this example, we are using CloudFlare as a DNS provider. You can use Google or Quad9 as well, just change the IPs.</li>\n<li>Create a new user for CoreDNS: <code>sudo useradd -d /var/lib/coredns -m coredns</code></li>\n<li>Set some permissions: <code>sudo chown coredns:coredns /opt/coredns</code></li>\n<li>Download the SystemD service unit file from <a href=\"https://github.com/coredns/deployment/tree/master/systemd\">coredns/coredns</a> to <code>/etc/systemd/system/coredns.service</code></li>\n<li>Disable SystemDs default DNS server: <code>sudo systemctl stop systemd-resolved &amp;&amp; sudo systemctl disable systemd-resolved</code><ol>\n<li><strong>Please Note:</strong> From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again</li>\n</ol>\n</li>\n<li>Enable and start CoreDNS: <code>sudo systemctl enable coredns &amp;&amp; sudo systemctl start coredns</code></li>\n<li>You should be able to resolve domain names, again. E.g. try <code>dig +short kit.edu</code>. If an IP address is printed, everything works fine.</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># &#x2F;etc&#x2F;coredns&#x2F;Corefile</span><br><span class=\"line\"></span><br><span class=\"line\">.:53 &#123;</span><br><span class=\"line\">    forward . tls:&#x2F;&#x2F;2606:4700:4700::1111 tls:&#x2F;&#x2F;1.1.1.1</span><br><span class=\"line\">    log</span><br><span class=\"line\">    errors</span><br><span class=\"line\">    cache</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dns2.png\" alt=\"Example of an encrypted DNS request\"><br>Example of an encrypted DNS request for <code>kit.edu</code> to CloudFlares <code>1.1.1.1</code> DNS resolver</p>\n<p>Alternatively, use the following <code>forward</code> statement to use the independent <a href=\"https://blahdns.com/\">BlahDNS</a> instead of CloudFlare as provider.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">forward . tls:&#x2F;&#x2F;2a01:4f8:1c1c:6b4b::1 tls:&#x2F;&#x2F;159.69.198.101 &#123;</span><br><span class=\"line\">    tls_servername dot-de.blahdns.com</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"What-happens\"><a href=\"#What-happens\" class=\"headerlink\" title=\"What happens?\"></a>What happens?</h1><p>Let us examine what happens (in terms of DNS) when you type <em>kit.edu</em> in your browsers address bar and hit enter.</p>\n<ol>\n<li>Your browser asks your operating system to resolve <code>kit.edu</code></li>\n<li>Your OS finds out that its primary configured nameserver is <code>127.0.0.1:53</code>, i.e. your local CoreDNS, and consults that one</li>\n<li>CoreDNS checks its cache and in case of a miss consults its configured nameserver at <code>1.1.1.1</code>, i.e. CloudFlare</li>\n<li>CloudFlare, again, checks its cache and in case of a miss goes up the hierarchical chain of nameservers until one of them has an answer</li>\n<li>Eventually, your browser performs <code>GET / HTTP/2.0</code> to <code>129.13.40.10</code> with <code>Host: kit.edu</code></li>\n</ol>\n<h1 id=\"Further-Reading\"><a href=\"#Further-Reading\" class=\"headerlink\" title=\"Further Reading\"></a>Further Reading</h1><p>Here are a few additional posts about DNS, which I found very useful.</p>\n<ul>\n<li><a href=\"https://www.cloudflare.com/learning/dns/what-is-dns/\">What Is DNS? | How DNS Works</a></li>\n<li><a href=\"https://www.reddit.com/r/privacy/comments/89pr15/dnsoverhttps_vs_dns_overtls_vs_dnscrypt/dwsosjr?utm_source=share&utm_medium=web2x\">DNS-over-https vs. DNS-over-TLS vs DNSCrypt</a></li>\n</ul>\n<p>Please let me know if my guide is missing any required steps. Good luck, have fun and browse safely!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1 id=\"Privacy-on-the-Web\"><a href=\"#Privacy-on-the-Web\" class=\"headerlink\" title=\"Privacy on the Web\"></a>Privacy on the Web</h1><p>Luckily, most traffic on the web is encrypted today, which means nobody between your computer and the web server knows what you are sending or receiving. This includes your internet service provider (ISP), any kind of government agency or a potential attacker on your network. Since the entire HTTP packet, including its headers, is encrypted, they will not even see what website you are visiting. At least not for sure. What they can see is the target web servers IP address from the IP packets header. However, there might be several different web servers for different web sites listening on that IP and there is no chance to find out which one you intended to visit.</p>\n<h1 id=\"The-Problem-with-DNS\"><a href=\"#The-Problem-with-DNS\" class=\"headerlink\" title=\"The Problem with DNS\"></a>The Problem with DNS</h1><p>However, although HTTP is usually encrypted, DNS is usually not. So before your browser performs the actual HTTP request, your operating system will perform a DNS query to resolve, for instance, <em>google.com</em> to <code>216.58.207.46</code>. Your question  <em>Whats the IP for google.com?</em>  is contained in the DNS query as plain text, so everyone between your computer and the DNS server will know that you are trying to access Google  or whatever website. And, of course, the provider of your DNS server will know as well, since it has to answer the query.</p>\n<p>Usually, your default DNS server is the one provided by your ISP. And since the ISP can directly associate your internet connection with your name and address it will know about any website that you  as a person  visit. Even if you change the default to something else (e.g. <a href=\"https://developers.google.com/speed/public-dns/\">Googles public DNS resolver</a> 8.8.8.8 or <a href=\"https://1.1.1.1/\">CloudFlares</a> <code>1.1.1.1</code>), your ISP can still read your DNS queries as they mandatorily pass through its network. Consequently, in order to browse more privately on the web  in addition to using HTTPS - there are two steps you need to consider:</p>\n<ol>\n<li>Change your DNS provider to one that is more anonymous and does not have personal information about you</li>\n<li>Encrypt your DNS queries to prevent anyone in the middle (especially your ISP) from reading them</li>\n</ol>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dns1.png\" alt=\"Example of a non-encrypted DNS request\"><br>Example of a non-encrypted DNS request for <code>kit.edu</code> to Googles <code>8.8.8.8</code> DNS resolver </p>\n<p>Luckily, the <a href=\"https://en.wikipedia.org/wiki/DNS_over_TLS\">DNS-over-TLS</a> specification already provides a solution and it is already supported by the three largest public DNS providers <a href=\"https://1.1.1.1/\">CloudFlare</a>, <a href=\"https://developers.google.com/speed/public-dns/\">Google</a> and <a href=\"https://www.quad9.net/\">Quad9</a>. You only have to configure your computer to use it.</p>\n<h1 id=\"CoreDNS-Setup\"><a href=\"#CoreDNS-Setup\" class=\"headerlink\" title=\"CoreDNS Setup\"></a>CoreDNS Setup</h1><p>In this article, I show you how to use DNS-over-TLS with <a href=\"https://coredns.io/\">CoreDNS</a> as a local DNS recursor on your machine. It is an open-source software and primarily known for being used as a nameserver in Kubernetes networks. Please note that I decided to use CoreDNS, because it is particularly easy to configure and offers a variety of cool <a href=\"https://coredns.io/plugins/\">plugins</a>, like <a href=\"https://coredns.io/plugins/metrics/\">metrics collection with Prometheus</a> and more. However, Ubuntus default DNS recursor <a href=\"https://www.internetsociety.org/blog/2018/12/dns-privacy-in-linux-systemd/\">systemd-resolved apparently supports DNS-over-TLS</a> as well and is probably easier to get started with initially. So if you prefer to go the easy way, just head over to the previously mentioned blog post and follow its instructions.</p>\n<p><strong>Disclaimer:</strong> Please use this guide at your own risk. I do not take any responsibility in case you accidentally crash your DNS setup.</p>\n<p>In order to set up CoreDNS, there are a few steps to follow.</p>\n<ol>\n<li>Download CoreDNS from the <a href=\"https://coredns.io/\">website</a>, unpack the binary to <code>/usr/local/bin</code> and make it executable (<code>sudo chmod +x /usr/local/bin/coredns</code>)</li>\n<li>Install <code>resolvconf</code> as a tool to manually manage <code>/etc/resolv.conf</code>: <code>sudo apt install resolvconf</code>  </li>\n<li>Set <code>dns=default</code> in <code>/etc/NetworkManager/NetworkManager.conf</code></li>\n<li>Add <code>nameserver 127.0.0.1</code> to <code>/etc/resolvconf/resolv.conf.d/head</code></li>\n<li>Create <code>/etc/coredns/Corefile</code> and paste the configuration shown below. In this example, we are using CloudFlare as a DNS provider. You can use Google or Quad9 as well, just change the IPs.</li>\n<li>Create a new user for CoreDNS: <code>sudo useradd -d /var/lib/coredns -m coredns</code></li>\n<li>Set some permissions: <code>sudo chown coredns:coredns /opt/coredns</code></li>\n<li>Download the SystemD service unit file from <a href=\"https://github.com/coredns/deployment/tree/master/systemd\">coredns/coredns</a> to <code>/etc/systemd/system/coredns.service</code></li>\n<li>Disable SystemDs default DNS server: <code>sudo systemctl stop systemd-resolved &amp;&amp; sudo systemctl disable systemd-resolved</code><ol>\n<li><strong>Please Note:</strong> From that moment on, you will not be able to resolve any web pages anymore, unless you enable DNS again</li>\n</ol>\n</li>\n<li>Enable and start CoreDNS: <code>sudo systemctl enable coredns &amp;&amp; sudo systemctl start coredns</code></li>\n<li>You should be able to resolve domain names, again. E.g. try <code>dig +short kit.edu</code>. If an IP address is printed, everything works fine.</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># &#x2F;etc&#x2F;coredns&#x2F;Corefile</span><br><span class=\"line\"></span><br><span class=\"line\">.:53 &#123;</span><br><span class=\"line\">    forward . tls:&#x2F;&#x2F;2606:4700:4700::1111 tls:&#x2F;&#x2F;1.1.1.1</span><br><span class=\"line\">    log</span><br><span class=\"line\">    errors</span><br><span class=\"line\">    cache</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/dns2.png\" alt=\"Example of an encrypted DNS request\"><br>Example of an encrypted DNS request for <code>kit.edu</code> to CloudFlares <code>1.1.1.1</code> DNS resolver</p>\n<p>Alternatively, use the following <code>forward</code> statement to use the independent <a href=\"https://blahdns.com/\">BlahDNS</a> instead of CloudFlare as provider.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">forward . tls:&#x2F;&#x2F;2a01:4f8:1c1c:6b4b::1 tls:&#x2F;&#x2F;159.69.198.101 &#123;</span><br><span class=\"line\">    tls_servername dot-de.blahdns.com</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"What-happens\"><a href=\"#What-happens\" class=\"headerlink\" title=\"What happens?\"></a>What happens?</h1><p>Let us examine what happens (in terms of DNS) when you type <em>kit.edu</em> in your browsers address bar and hit enter.</p>\n<ol>\n<li>Your browser asks your operating system to resolve <code>kit.edu</code></li>\n<li>Your OS finds out that its primary configured nameserver is <code>127.0.0.1:53</code>, i.e. your local CoreDNS, and consults that one</li>\n<li>CoreDNS checks its cache and in case of a miss consults its configured nameserver at <code>1.1.1.1</code>, i.e. CloudFlare</li>\n<li>CloudFlare, again, checks its cache and in case of a miss goes up the hierarchical chain of nameservers until one of them has an answer</li>\n<li>Eventually, your browser performs <code>GET / HTTP/2.0</code> to <code>129.13.40.10</code> with <code>Host: kit.edu</code></li>\n</ol>\n<h1 id=\"Further-Reading\"><a href=\"#Further-Reading\" class=\"headerlink\" title=\"Further Reading\"></a>Further Reading</h1><p>Here are a few additional posts about DNS, which I found very useful.</p>\n<ul>\n<li><a href=\"https://www.cloudflare.com/learning/dns/what-is-dns/\">What Is DNS? | How DNS Works</a></li>\n<li><a href=\"https://www.reddit.com/r/privacy/comments/89pr15/dnsoverhttps_vs_dns_overtls_vs_dnscrypt/dwsosjr?utm_source=share&utm_medium=web2x\">DNS-over-https vs. DNS-over-TLS vs DNSCrypt</a></li>\n</ul>\n<p>Please let me know if my guide is missing any required steps. Good luck, have fun and browse safely!</p>\n"},{"title":"How to load SVG into ImageView by URL in Android","date":"2018-07-12T22:19:19.000Z","_content":"\nI am writing this short article since I had a pretty hard time figuring out on how to do what the title says: fetching an SVG from the web and displaying it in an app.\n\nBy default, Android's `ImageView` does not support SVGs (why?). After googling for a while I found a complicated solution for the above problem using [Glide](https://github.com/bumptech/glide) with a custom module in combination with [AndroidSVG](http://bigbadaboom.github.io/androidsvg/). However, the latter library is quite outdated and caused some - apparently randomly occuring - errors on API level 28. Finally I found [Pixplicity/sharp](https://github.com/Pixplicity/sharp), which seemed to be a light-weight library for almost exactly my purpose with a minimal API. The only thing I needed to add is the ability to fetch the SVG from the web instead of from a local resource. I built a small example as can be seen below.\n\n## Code\n### build.gradle\n```\ndependencies {\n    // ...\n    implementation 'com.squareup.okhttp3:okhttp:3.10.0'\n    implementation 'com.pixplicity.sharp:library:1.1.0'\n    // ...\n}\n```\n\n### MainActivity.java\n```java\n    // ...\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        ImageView userAvatarView = findViewById(R.id.user_avatar);\n        String userAvatarUrl = \"https://avatars.dicebear.com/v2/female/anna.svg\";\n        Utils.fetchSvg(this, userAvatarUrl, userAvatarView);\n    }\n    // ...\n```\n\n### Utils.java\nA simple util class with static methods that receive an Android Context. Having a static `OkHttpClient` that gets conditionally initialized from within the static methods might arguably not be the best solution, but it works for this example. Alternatives would be to have it being autowired, passed as a parameter or to make `Utils` a singleton and the http client a member variable.\n```java\npublic class Utils {\n    private static OkHttpClient httpClient;\n\n    public static void fetchSvg(Context context, String url, final ImageView target) {\n        if (httpClient == null) {\n            // Use cache for performance and basic offline capability\n            httpClient = new OkHttpClient.Builder()\n                    .cache(new Cache(context.getCacheDir(), 5 * 1024 * 1014))\n                    .build();\n        }\n\n        Request request = new Request.Builder().url(url).build();\n        httpClient.newCall(request).enqueue(new Callback() {\n            @Override\n            public void onFailure(Call call, IOException e) {\n                target.setImageDrawable(R.drawable.fallback_image);\n            }\n\n            @Override\n            public void onResponse(Call call, Response response) throws IOException {\n                InputStream stream = response.body().byteStream();\n                Sharp.loadInputStream(stream).into(target);\n                stream.close();\n            }\n        });\n    }\n}\n```\n\nIf you know a simpler solution to load SVGs, please let me know!","source":"_posts/how-to-load-svg-into-imageview-by-url-in-android.md","raw":"---\ntitle: How to load SVG into ImageView by URL in Android\ndate: 2018-07-13 00:19:19\ntags:\n---\n\nI am writing this short article since I had a pretty hard time figuring out on how to do what the title says: fetching an SVG from the web and displaying it in an app.\n\nBy default, Android's `ImageView` does not support SVGs (why?). After googling for a while I found a complicated solution for the above problem using [Glide](https://github.com/bumptech/glide) with a custom module in combination with [AndroidSVG](http://bigbadaboom.github.io/androidsvg/). However, the latter library is quite outdated and caused some - apparently randomly occuring - errors on API level 28. Finally I found [Pixplicity/sharp](https://github.com/Pixplicity/sharp), which seemed to be a light-weight library for almost exactly my purpose with a minimal API. The only thing I needed to add is the ability to fetch the SVG from the web instead of from a local resource. I built a small example as can be seen below.\n\n## Code\n### build.gradle\n```\ndependencies {\n    // ...\n    implementation 'com.squareup.okhttp3:okhttp:3.10.0'\n    implementation 'com.pixplicity.sharp:library:1.1.0'\n    // ...\n}\n```\n\n### MainActivity.java\n```java\n    // ...\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        ImageView userAvatarView = findViewById(R.id.user_avatar);\n        String userAvatarUrl = \"https://avatars.dicebear.com/v2/female/anna.svg\";\n        Utils.fetchSvg(this, userAvatarUrl, userAvatarView);\n    }\n    // ...\n```\n\n### Utils.java\nA simple util class with static methods that receive an Android Context. Having a static `OkHttpClient` that gets conditionally initialized from within the static methods might arguably not be the best solution, but it works for this example. Alternatives would be to have it being autowired, passed as a parameter or to make `Utils` a singleton and the http client a member variable.\n```java\npublic class Utils {\n    private static OkHttpClient httpClient;\n\n    public static void fetchSvg(Context context, String url, final ImageView target) {\n        if (httpClient == null) {\n            // Use cache for performance and basic offline capability\n            httpClient = new OkHttpClient.Builder()\n                    .cache(new Cache(context.getCacheDir(), 5 * 1024 * 1014))\n                    .build();\n        }\n\n        Request request = new Request.Builder().url(url).build();\n        httpClient.newCall(request).enqueue(new Callback() {\n            @Override\n            public void onFailure(Call call, IOException e) {\n                target.setImageDrawable(R.drawable.fallback_image);\n            }\n\n            @Override\n            public void onResponse(Call call, Response response) throws IOException {\n                InputStream stream = response.body().byteStream();\n                Sharp.loadInputStream(stream).into(target);\n                stream.close();\n            }\n        });\n    }\n}\n```\n\nIf you know a simpler solution to load SVGs, please let me know!","slug":"how-to-load-svg-into-imageview-by-url-in-android","published":1,"updated":"2020-10-30T20:05:40.291Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlw000eo2e053fdby7f","content":"<p>I am writing this short article since I had a pretty hard time figuring out on how to do what the title says: fetching an SVG from the web and displaying it in an app.</p>\n<p>By default, Androids <code>ImageView</code> does not support SVGs (why?). After googling for a while I found a complicated solution for the above problem using <a href=\"https://github.com/bumptech/glide\">Glide</a> with a custom module in combination with <a href=\"http://bigbadaboom.github.io/androidsvg/\">AndroidSVG</a>. However, the latter library is quite outdated and caused some - apparently randomly occuring - errors on API level 28. Finally I found <a href=\"https://github.com/Pixplicity/sharp\">Pixplicity/sharp</a>, which seemed to be a light-weight library for almost exactly my purpose with a minimal API. The only thing I needed to add is the ability to fetch the SVG from the web instead of from a local resource. I built a small example as can be seen below.</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><h3 id=\"build-gradle\"><a href=\"#build-gradle\" class=\"headerlink\" title=\"build.gradle\"></a>build.gradle</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dependencies &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; ...</span><br><span class=\"line\">    implementation &#39;com.squareup.okhttp3:okhttp:3.10.0&#39;</span><br><span class=\"line\">    implementation &#39;com.pixplicity.sharp:library:1.1.0&#39;</span><br><span class=\"line\">    &#x2F;&#x2F; ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"MainActivity-java\"><a href=\"#MainActivity-java\" class=\"headerlink\" title=\"MainActivity.java\"></a>MainActivity.java</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// ...</span></span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">onCreate</span><span class=\"params\">(Bundle savedInstanceState)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">super</span>.onCreate(savedInstanceState);</span><br><span class=\"line\">    setContentView(R.layout.activity_main);</span><br><span class=\"line\"></span><br><span class=\"line\">    ImageView userAvatarView = findViewById(R.id.user_avatar);</span><br><span class=\"line\">    String userAvatarUrl = <span class=\"string\">&quot;https://avatars.dicebear.com/v2/female/anna.svg&quot;</span>;</span><br><span class=\"line\">    Utils.fetchSvg(<span class=\"keyword\">this</span>, userAvatarUrl, userAvatarView);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// ...</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Utils-java\"><a href=\"#Utils-java\" class=\"headerlink\" title=\"Utils.java\"></a>Utils.java</h3><p>A simple util class with static methods that receive an Android Context. Having a static <code>OkHttpClient</code> that gets conditionally initialized from within the static methods might arguably not be the best solution, but it works for this example. Alternatives would be to have it being autowired, passed as a parameter or to make <code>Utils</code> a singleton and the http client a member variable.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Utils</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> OkHttpClient httpClient;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">fetchSvg</span><span class=\"params\">(Context context, String url, <span class=\"keyword\">final</span> ImageView target)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (httpClient == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Use cache for performance and basic offline capability</span></span><br><span class=\"line\">            httpClient = <span class=\"keyword\">new</span> OkHttpClient.Builder()</span><br><span class=\"line\">                    .cache(<span class=\"keyword\">new</span> Cache(context.getCacheDir(), <span class=\"number\">5</span> * <span class=\"number\">1024</span> * <span class=\"number\">1014</span>))</span><br><span class=\"line\">                    .build();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        Request request = <span class=\"keyword\">new</span> Request.Builder().url(url).build();</span><br><span class=\"line\">        httpClient.newCall(request).enqueue(<span class=\"keyword\">new</span> Callback() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onFailure</span><span class=\"params\">(Call call, IOException e)</span> </span>&#123;</span><br><span class=\"line\">                target.setImageDrawable(R.drawable.fallback_image);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onResponse</span><span class=\"params\">(Call call, Response response)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">                InputStream stream = response.body().byteStream();</span><br><span class=\"line\">                Sharp.loadInputStream(stream).into(target);</span><br><span class=\"line\">                stream.close();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>If you know a simpler solution to load SVGs, please let me know!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>I am writing this short article since I had a pretty hard time figuring out on how to do what the title says: fetching an SVG from the web and displaying it in an app.</p>\n<p>By default, Androids <code>ImageView</code> does not support SVGs (why?). After googling for a while I found a complicated solution for the above problem using <a href=\"https://github.com/bumptech/glide\">Glide</a> with a custom module in combination with <a href=\"http://bigbadaboom.github.io/androidsvg/\">AndroidSVG</a>. However, the latter library is quite outdated and caused some - apparently randomly occuring - errors on API level 28. Finally I found <a href=\"https://github.com/Pixplicity/sharp\">Pixplicity/sharp</a>, which seemed to be a light-weight library for almost exactly my purpose with a minimal API. The only thing I needed to add is the ability to fetch the SVG from the web instead of from a local resource. I built a small example as can be seen below.</p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><h3 id=\"build-gradle\"><a href=\"#build-gradle\" class=\"headerlink\" title=\"build.gradle\"></a>build.gradle</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dependencies &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; ...</span><br><span class=\"line\">    implementation &#39;com.squareup.okhttp3:okhttp:3.10.0&#39;</span><br><span class=\"line\">    implementation &#39;com.pixplicity.sharp:library:1.1.0&#39;</span><br><span class=\"line\">    &#x2F;&#x2F; ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"MainActivity-java\"><a href=\"#MainActivity-java\" class=\"headerlink\" title=\"MainActivity.java\"></a>MainActivity.java</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// ...</span></span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">onCreate</span><span class=\"params\">(Bundle savedInstanceState)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">super</span>.onCreate(savedInstanceState);</span><br><span class=\"line\">    setContentView(R.layout.activity_main);</span><br><span class=\"line\"></span><br><span class=\"line\">    ImageView userAvatarView = findViewById(R.id.user_avatar);</span><br><span class=\"line\">    String userAvatarUrl = <span class=\"string\">&quot;https://avatars.dicebear.com/v2/female/anna.svg&quot;</span>;</span><br><span class=\"line\">    Utils.fetchSvg(<span class=\"keyword\">this</span>, userAvatarUrl, userAvatarView);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// ...</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Utils-java\"><a href=\"#Utils-java\" class=\"headerlink\" title=\"Utils.java\"></a>Utils.java</h3><p>A simple util class with static methods that receive an Android Context. Having a static <code>OkHttpClient</code> that gets conditionally initialized from within the static methods might arguably not be the best solution, but it works for this example. Alternatives would be to have it being autowired, passed as a parameter or to make <code>Utils</code> a singleton and the http client a member variable.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Utils</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> OkHttpClient httpClient;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">fetchSvg</span><span class=\"params\">(Context context, String url, <span class=\"keyword\">final</span> ImageView target)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (httpClient == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// Use cache for performance and basic offline capability</span></span><br><span class=\"line\">            httpClient = <span class=\"keyword\">new</span> OkHttpClient.Builder()</span><br><span class=\"line\">                    .cache(<span class=\"keyword\">new</span> Cache(context.getCacheDir(), <span class=\"number\">5</span> * <span class=\"number\">1024</span> * <span class=\"number\">1014</span>))</span><br><span class=\"line\">                    .build();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        Request request = <span class=\"keyword\">new</span> Request.Builder().url(url).build();</span><br><span class=\"line\">        httpClient.newCall(request).enqueue(<span class=\"keyword\">new</span> Callback() &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onFailure</span><span class=\"params\">(Call call, IOException e)</span> </span>&#123;</span><br><span class=\"line\">                target.setImageDrawable(R.drawable.fallback_image);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onResponse</span><span class=\"params\">(Call call, Response response)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">                InputStream stream = response.body().byteStream();</span><br><span class=\"line\">                Sharp.loadInputStream(stream).into(target);</span><br><span class=\"line\">                stream.close();</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>If you know a simpler solution to load SVGs, please let me know!</p>\n"},{"title":"How to load Yago into Apache Jena / Fuseki","date":"2016-11-11T22:04:09.000Z","_content":"\nThis article describes how to load the [Yago](http://yago-knowledge.org) Linked Data knowledge collection into an [Apache Jena](https://jena.apache.org/) triple store database on Windows 10 as well as on Linux.\n\n1. At very first, please make sure you have Java 8 Runtime Environment installed on your system.\n\n2. Download all Yago graphs you need from the [downloads section](http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/) as .ttl files. In my case I took all graphs from _TAXONOMY_, _CORE_ and additonally the _yagoDBpediaInstances_ and _yagoDBpediaClasses_ collections to have relations from Yago entities to DBpedia ones. Download the files to a folder on your system, let's say `/home/ferdinand/yago/` on Linux or `C:\\Users\\Ferdinand\\yago` on Windows and extract them using 7zip.\n\n3. Delete all `.7z` files.\n\n4. Download `apache-jena-3.1.1.zip` (or newer version) and `apache-jena-fuseki-2.4.1.zip` from [here](https://jena.apache.org/download/index.cgi) and extract them to, let's say `/home/ferdinand/jena/` and `/home/ferdinand/fuseki/` (or the analogue directories on Windows).\n\n5. Now the .ttl files needs to get some kind of preprocessed, where non-unicode characters are replaced in order for Jena to accept the data. On Linux run `sed -i 's/|/-/g' ./* && sed -i 's/\\\\\\\\/-/g' ./* && sed -i 's//-/g' ./*` from within the directory where your .ttl files are. On Windows, start the Ubuntu Bash, navigate to the respective directory (e.g. `/mnt/c/Users/Ferdinand/yago`) and do the same command. It will take several minutes. I mean, really several...\n\n6. Create a folder to be used for the database later, e.g. `/home/ferdinand/yago/data`.\n\n7. Add the Fuseki root directory (e.g. `/home/ferdinand/fuseki`) and the Jena _bin_ (or _bat_ on Win) (e.g. `/home/ferdinand/jena/bin`) to your `PATH` environment variable. On Linux you would do this by editing your `~/.bash_profile`, on Windows you can search for _\"envionment variables\"_ and then use the Windows system settings dialog.\n\n7. Load the graphs using _tdbloader_: `tdbloader.bat --loc data ./*` from the directory where your .ttl files are located. This may take several hours. Not joking...\n\n8. Start Fuseki typing `java -jar fuseki-server.jar --update --loc /home/ferdinand/yago/data /myGraph` to run fuseki with your entire Yago graph available under the _myGraph_ alias.\n\n9. Open [http://localhost:3030](http://localhost:3030) in your browser and start making queries.\n\nIf you're about to run really expensive queries, consider the following.\n\n1. Set the `JVM_ARGS` environment variable to `-Xms512m -Xmx2048M -XX:-UseGCOverheadLimit -XX:+UseParallelGC`. This will basically prevent you from getting _OutOfMemory_ errors.\n\n2. Use _tdbquery_ since it might be a little more performant than the web SPARQL endpoint. An example _tdbquery_ command might look like this, assuming you have a file `q.txt` that contains your SPARQL query: `tdbquery --loc=/home/ferdinand/yago/data --time --results=CSV --query=q.txt > output.txt`","source":"_posts/how-to-load-yago-into-apache-jena-fuseki.md","raw":"---\ntitle: How to load Yago into Apache Jena / Fuseki\ndate: 2016-11-11 23:04:09\ntags:\n---\n\nThis article describes how to load the [Yago](http://yago-knowledge.org) Linked Data knowledge collection into an [Apache Jena](https://jena.apache.org/) triple store database on Windows 10 as well as on Linux.\n\n1. At very first, please make sure you have Java 8 Runtime Environment installed on your system.\n\n2. Download all Yago graphs you need from the [downloads section](http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/) as .ttl files. In my case I took all graphs from _TAXONOMY_, _CORE_ and additonally the _yagoDBpediaInstances_ and _yagoDBpediaClasses_ collections to have relations from Yago entities to DBpedia ones. Download the files to a folder on your system, let's say `/home/ferdinand/yago/` on Linux or `C:\\Users\\Ferdinand\\yago` on Windows and extract them using 7zip.\n\n3. Delete all `.7z` files.\n\n4. Download `apache-jena-3.1.1.zip` (or newer version) and `apache-jena-fuseki-2.4.1.zip` from [here](https://jena.apache.org/download/index.cgi) and extract them to, let's say `/home/ferdinand/jena/` and `/home/ferdinand/fuseki/` (or the analogue directories on Windows).\n\n5. Now the .ttl files needs to get some kind of preprocessed, where non-unicode characters are replaced in order for Jena to accept the data. On Linux run `sed -i 's/|/-/g' ./* && sed -i 's/\\\\\\\\/-/g' ./* && sed -i 's//-/g' ./*` from within the directory where your .ttl files are. On Windows, start the Ubuntu Bash, navigate to the respective directory (e.g. `/mnt/c/Users/Ferdinand/yago`) and do the same command. It will take several minutes. I mean, really several...\n\n6. Create a folder to be used for the database later, e.g. `/home/ferdinand/yago/data`.\n\n7. Add the Fuseki root directory (e.g. `/home/ferdinand/fuseki`) and the Jena _bin_ (or _bat_ on Win) (e.g. `/home/ferdinand/jena/bin`) to your `PATH` environment variable. On Linux you would do this by editing your `~/.bash_profile`, on Windows you can search for _\"envionment variables\"_ and then use the Windows system settings dialog.\n\n7. Load the graphs using _tdbloader_: `tdbloader.bat --loc data ./*` from the directory where your .ttl files are located. This may take several hours. Not joking...\n\n8. Start Fuseki typing `java -jar fuseki-server.jar --update --loc /home/ferdinand/yago/data /myGraph` to run fuseki with your entire Yago graph available under the _myGraph_ alias.\n\n9. Open [http://localhost:3030](http://localhost:3030) in your browser and start making queries.\n\nIf you're about to run really expensive queries, consider the following.\n\n1. Set the `JVM_ARGS` environment variable to `-Xms512m -Xmx2048M -XX:-UseGCOverheadLimit -XX:+UseParallelGC`. This will basically prevent you from getting _OutOfMemory_ errors.\n\n2. Use _tdbquery_ since it might be a little more performant than the web SPARQL endpoint. An example _tdbquery_ command might look like this, assuming you have a file `q.txt` that contains your SPARQL query: `tdbquery --loc=/home/ferdinand/yago/data --time --results=CSV --query=q.txt > output.txt`","slug":"how-to-load-yago-into-apache-jena-fuseki","published":1,"updated":"2020-10-30T20:05:40.286Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlx000fo2e09v1b9w9r","content":"<p>This article describes how to load the <a href=\"http://yago-knowledge.org/\">Yago</a> Linked Data knowledge collection into an <a href=\"https://jena.apache.org/\">Apache Jena</a> triple store database on Windows 10 as well as on Linux.</p>\n<ol>\n<li><p>At very first, please make sure you have Java 8 Runtime Environment installed on your system.</p>\n</li>\n<li><p>Download all Yago graphs you need from the <a href=\"http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/\">downloads section</a> as .ttl files. In my case I took all graphs from <em>TAXONOMY</em>, <em>CORE</em> and additonally the <em>yagoDBpediaInstances</em> and <em>yagoDBpediaClasses</em> collections to have relations from Yago entities to DBpedia ones. Download the files to a folder on your system, lets say <code>/home/ferdinand/yago/</code> on Linux or <code>C:\\Users\\Ferdinand\\yago</code> on Windows and extract them using 7zip.</p>\n</li>\n<li><p>Delete all <code>.7z</code> files.</p>\n</li>\n<li><p>Download <code>apache-jena-3.1.1.zip</code> (or newer version) and <code>apache-jena-fuseki-2.4.1.zip</code> from <a href=\"https://jena.apache.org/download/index.cgi\">here</a> and extract them to, lets say <code>/home/ferdinand/jena/</code> and <code>/home/ferdinand/fuseki/</code> (or the analogue directories on Windows).</p>\n</li>\n<li><p>Now the .ttl files needs to get some kind of preprocessed, where non-unicode characters are replaced in order for Jena to accept the data. On Linux run <code>sed -i &#39;s/|/-/g&#39; ./* &amp;&amp; sed -i &#39;s/\\\\\\\\/-/g&#39; ./* &amp;&amp; sed -i &#39;s//-/g&#39; ./*</code> from within the directory where your .ttl files are. On Windows, start the Ubuntu Bash, navigate to the respective directory (e.g. <code>/mnt/c/Users/Ferdinand/yago</code>) and do the same command. It will take several minutes. I mean, really several</p>\n</li>\n<li><p>Create a folder to be used for the database later, e.g. <code>/home/ferdinand/yago/data</code>.</p>\n</li>\n<li><p>Add the Fuseki root directory (e.g. <code>/home/ferdinand/fuseki</code>) and the Jena <em>bin</em> (or <em>bat</em> on Win) (e.g. <code>/home/ferdinand/jena/bin</code>) to your <code>PATH</code> environment variable. On Linux you would do this by editing your <code>~/.bash_profile</code>, on Windows you can search for <em>envionment variables</em> and then use the Windows system settings dialog.</p>\n</li>\n<li><p>Load the graphs using <em>tdbloader</em>: <code>tdbloader.bat --loc data ./*</code> from the directory where your .ttl files are located. This may take several hours. Not joking</p>\n</li>\n<li><p>Start Fuseki typing <code>java -jar fuseki-server.jar --update --loc /home/ferdinand/yago/data /myGraph</code> to run fuseki with your entire Yago graph available under the <em>myGraph</em> alias.</p>\n</li>\n<li><p>Open <a href=\"http://localhost:3030/\">http://localhost:3030</a> in your browser and start making queries.</p>\n</li>\n</ol>\n<p>If youre about to run really expensive queries, consider the following.</p>\n<ol>\n<li><p>Set the <code>JVM_ARGS</code> environment variable to <code>-Xms512m -Xmx2048M -XX:-UseGCOverheadLimit -XX:+UseParallelGC</code>. This will basically prevent you from getting <em>OutOfMemory</em> errors.</p>\n</li>\n<li><p>Use <em>tdbquery</em> since it might be a little more performant than the web SPARQL endpoint. An example <em>tdbquery</em> command might look like this, assuming you have a file <code>q.txt</code> that contains your SPARQL query: <code>tdbquery --loc=/home/ferdinand/yago/data --time --results=CSV --query=q.txt &gt; output.txt</code></p>\n</li>\n</ol>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>This article describes how to load the <a href=\"http://yago-knowledge.org/\">Yago</a> Linked Data knowledge collection into an <a href=\"https://jena.apache.org/\">Apache Jena</a> triple store database on Windows 10 as well as on Linux.</p>\n<ol>\n<li><p>At very first, please make sure you have Java 8 Runtime Environment installed on your system.</p>\n</li>\n<li><p>Download all Yago graphs you need from the <a href=\"http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/\">downloads section</a> as .ttl files. In my case I took all graphs from <em>TAXONOMY</em>, <em>CORE</em> and additonally the <em>yagoDBpediaInstances</em> and <em>yagoDBpediaClasses</em> collections to have relations from Yago entities to DBpedia ones. Download the files to a folder on your system, lets say <code>/home/ferdinand/yago/</code> on Linux or <code>C:\\Users\\Ferdinand\\yago</code> on Windows and extract them using 7zip.</p>\n</li>\n<li><p>Delete all <code>.7z</code> files.</p>\n</li>\n<li><p>Download <code>apache-jena-3.1.1.zip</code> (or newer version) and <code>apache-jena-fuseki-2.4.1.zip</code> from <a href=\"https://jena.apache.org/download/index.cgi\">here</a> and extract them to, lets say <code>/home/ferdinand/jena/</code> and <code>/home/ferdinand/fuseki/</code> (or the analogue directories on Windows).</p>\n</li>\n<li><p>Now the .ttl files needs to get some kind of preprocessed, where non-unicode characters are replaced in order for Jena to accept the data. On Linux run <code>sed -i &#39;s/|/-/g&#39; ./* &amp;&amp; sed -i &#39;s/\\\\\\\\/-/g&#39; ./* &amp;&amp; sed -i &#39;s//-/g&#39; ./*</code> from within the directory where your .ttl files are. On Windows, start the Ubuntu Bash, navigate to the respective directory (e.g. <code>/mnt/c/Users/Ferdinand/yago</code>) and do the same command. It will take several minutes. I mean, really several</p>\n</li>\n<li><p>Create a folder to be used for the database later, e.g. <code>/home/ferdinand/yago/data</code>.</p>\n</li>\n<li><p>Add the Fuseki root directory (e.g. <code>/home/ferdinand/fuseki</code>) and the Jena <em>bin</em> (or <em>bat</em> on Win) (e.g. <code>/home/ferdinand/jena/bin</code>) to your <code>PATH</code> environment variable. On Linux you would do this by editing your <code>~/.bash_profile</code>, on Windows you can search for <em>envionment variables</em> and then use the Windows system settings dialog.</p>\n</li>\n<li><p>Load the graphs using <em>tdbloader</em>: <code>tdbloader.bat --loc data ./*</code> from the directory where your .ttl files are located. This may take several hours. Not joking</p>\n</li>\n<li><p>Start Fuseki typing <code>java -jar fuseki-server.jar --update --loc /home/ferdinand/yago/data /myGraph</code> to run fuseki with your entire Yago graph available under the <em>myGraph</em> alias.</p>\n</li>\n<li><p>Open <a href=\"http://localhost:3030/\">http://localhost:3030</a> in your browser and start making queries.</p>\n</li>\n</ol>\n<p>If youre about to run really expensive queries, consider the following.</p>\n<ol>\n<li><p>Set the <code>JVM_ARGS</code> environment variable to <code>-Xms512m -Xmx2048M -XX:-UseGCOverheadLimit -XX:+UseParallelGC</code>. This will basically prevent you from getting <em>OutOfMemory</em> errors.</p>\n</li>\n<li><p>Use <em>tdbquery</em> since it might be a little more performant than the web SPARQL endpoint. An example <em>tdbquery</em> command might look like this, assuming you have a file <code>q.txt</code> that contains your SPARQL query: <code>tdbquery --loc=/home/ferdinand/yago/data --time --results=CSV --query=q.txt &gt; output.txt</code></p>\n</li>\n</ol>\n"},{"title":"How to make Telegram Bots","date":"2015-06-28T20:39:44.000Z","_content":"\nRecently [Telegram](http://telegram.org \"Telegram\") has introduced a new feature, the bots. Basically the bots enable human Telegram users to talk to machines, i.e. customly written little programs. A bot could be a daily helper, be it a bot that you can ask for the current temperature, one that googles something for you, one to manage your todos or even a little text based game  everything within the Telegram chat. The nice thing about them is that theyre really simple to create. You can read more about the bots in general here: [https://telegram.org/blog/bot-revolution](https://telegram.org/blog/bot-revolution)\n\nThis article shouldnt cover how to create and publish a bot (which actually is the same step), but how to write its actual functionality  its backend. On how to initially set up one, please refer to this little guide: [https://core.telegram.org/bots](https://core.telegram.org/bots). Its very easy, trust me. Everything you need for that is your Telegram app.\n\nWhat you need further to program your bot basically is a favorite programming language, an IDE or at least a text editor, [this](https://core.telegram.org/bots/api \"this\") page to be open and again your app. In this tutorial we will use Node.js as programming language (or more precisely as programming platform), but you really could use any other language, as well. Probably Python or PHP would do the job well, but you could even write a Java application.\n\nFirst some basic things to understand. For the most parts your bot is your own application which we will try to create here, running on your own local PC or server. Telegram wont host any code. All functionality and data storage is kept in your program on your machine. Basically Telegram doesnt provide more than kind of an interface between your users Telegram client (app) and your bot application. The flow would be like:\n\n1.  You create a new bot with @BotFather and set its description and commands (the commands you set there are  strictly speaking  completely independent of which commands your program will actually accept  they are just strings which the user gets suggested in a chat with your bot) so that the bot gets publicly accessible as any other (human) Telegram user is via her @ nickname.\n\n2.  You write the backend and run it to be listening.\n\n3.  A user sends a message to your bot.\n\n4.  The (JSON formatted) Telegram message object gets passed to your backend via HTTPS by Telegram.\n\n5.  Your backend parses the message text, extract the commands and arguments, processes some logic, possibly does some database actions and so on.\n\n6.  Your backend passes a response message object to the Telegram API via HTTPS (the other way round now).\n\n7.  Telegram shows the message to your users chat window.\n\nAlright, now lets get into the code. I require you to already have set up a bot and got its authentication token (all using @BotFather). As I said, we will make a little Node.js program here, but for those who dont understand JavaScript too well, Ill try to explain everything as clear as you need to re-do this in your programming language.\n\nFirst, you need any library that can do HTTP requests, because basically your backend will be busy doing POSTs and GETs for the most time. For Node.js well use the [Unirest](http://unirest.io/nodejs.html \"Unirest\") library, which probably is a little bit overkill, but very simple for our purpose. The library is available for Python, PHP, Ruby, Java and many others as well.\n\nSo first step is setting up a Node application and requiring Unirest (i assume you know how to set up a Node application and youre familiar with npm).\n\n```javascript\n/* app.js */\n\nvar unirest = require('unirest');\n```\n\nNext is to receive messages from your users. Telegram offers two ways to do this. The first way would be to use Webhooks, which basically means that youre backend runs on a webserver (or for Node it actually is one itself), Telegram knows your HTTPS endpoint and does a request to your backend every time a message is sent by a user. I would consider this the more elegant way. But the minus about this is that you would need to have a valid SSL certificate (which costs monthly charge) to provide a secure HTTPS connection. Telegram wont send any data to an unsecure or unverified endpoint. This is why we will take the other approach, which works kind of the other way round. Your backend will continuously request the Telegram API if there are new messages available. In detail your backend wont do request after request after request (because this would be so inefficient!) but use [long polling](http://www.pubnub.com/blog/http-long-polling/ \"long polling\"). To put it simple long polling means that a request wont be answered instantly, but kept open until there is some data available.\n\nWe need to specify some constants. As i said, everything between your backend and Telegram happens via their HTTPS interface. We set up constants for the base API url, containing your bot token, the URL for the [getUpdates](https://core.telegram.org/bots/api#getupdates \"getUpdates\") method endpoint and the URL for the [sendMessage](https://core.telegram.org/bots/api#sendmessage \"sendMessage\") method endpoint. The *:offset:* within the URL string will get replaced by a number, specifying, which new messages to fetch from Telegram later.\n\n```javascript\n/* app.js */\n\n/* ... */\n\nvar BASE_URL = \"https://api.telegram.org/botYOUR_TOKEN_HERE/\";\nvar POLLING_URL = BASE_URL + \"getUpdates?offset=:offset:&timeout=60\";\nvar SEND_MESSAGE_URL = BASE_URL + \"sendMessage\";\n```\n\nNow well introduce a function called *poll* (you can choose any other name), which basically is kind of the main loop of our program. Heres the code for this method, explanation follows.\n\n```javascript\n/* ... */\nfunction poll(offset) {\n    var url = POLLING_URL.replace(\":offset:\", offset);\n\n    unirest.get(url)\n        .end(function(response) {\n            var body = response.raw_body;\n            if (response.status == 200) {\n                var jsonData = JSON.parse(body);\n                var result = jsonData.result;\n\n                if (result.length > 0) {\n                    for (i in result) {\n                        if (runCommand(result[i].message)) continue;\n                    }\n\n                    max_offset = parseInt(result[result.length - 1].update_id) + 1; // update max offset\n                }\n                poll(max_offset);\n            }\n        });\n};\n```\n\nAlright. The function is recursive, meaning it will call itself  namely each time, a request was answered (which is, when a new message was fetched). The http request to Telegram API to get updates will be pending until a message arrives. Then it gets answered, but has to be re-opened again instantly now to continue listening for message updates again. As a parameter it takes an offset number, which replaces the *:offset:* placeholder in the url string. To read more about this parameter, go to [https://core.telegram.org/bots/api#getupdates](https://core.telegram.org/bots/api#getupdates). Unirest opens an http request to the specified url and executes the callback function given to end(), if the request was answered. First, we extract the response body. Afterwards, we check if our request was successful. If this is the case, we parse the body (which is a JSON object, consisting of an *ok* field and a *result* array. The result array contains one or more message objects. These are the ones that are relevant for us. For each message object, we try to parse it as a command (ill give you the runCommand() function is a second) and depending on which command we got, execute the respective method. Afterwards the offset gets updates to the id of the latest message plus one (to not receive it again next time) and a new request gets opened.\n\n```javascript\n/* ... */\n\nvar dosth = function(message) {\n    // to be implemented....\n}\n\nvar COMMANDS = {\n    \"dosth\" : dosth\n};\n```\n\nNow we specify a map, which maps strings (representing the users command input  in this case */dosth* to actual functions.\n\n```javascript\n    var msgtext = message.text;\n\n    if (msgtext.indexOf(\"/\") != 0) return false; // no slash at beginning?\n    var command = msgtext.substring(1, msgtext.indexOf(\" \"));\n    if (COMMANDS[command] == null) return false; // not a valid command?\n    COMMANDS[command](message);\n    return true;\n}\n```\n\nAnd this is the runCommand method. It takes the entire Telegram message object, which we got as a response from the Telegram API above and tries to parse its text as a command. A command string always starts with a slash. So if there is not slash in the beginning, we can be sure that we didnt get a valid command. We simply return here, but we also could send a message to the user telling him Hey, please enter a valid command.. But lets keep it simple. In the following line we extract everything after the slash and before the first blank space (assuming a command mustnt contain a blank space) as the command. Afterwards we look into our map if the command actually is a key for a method and if so, we just run this method, passing it the message object as a parameter as the function will need information out of it.\n\nWhat have we done so far? We wrote a program that requests the Telegram API for new messages, parses potentially contained commands and runs functions depending on these commands.\n\nAll we still need is to implement the method belonging to the */dosth* command.\n\n```javascript\n    var caps = message.text.toUpperCase();\n    var answer = {\n        chat_id : message.chat.id,\n        text : \"You told be to do something, so I took your input and made it all caps. Look: \" + caps\n    };\n\n    unirest.post(SEND_MESSAGE_URL)\n        .send(answer)\n        .end(function (response) {\n            if (response.status == 200) console.log(\"Successfully sent message to \" + message.chat.id);\n        });\n}\n```\n\nMost times youll want to send a message as response to your user. You could also send an image, an audio, a location,  (see [https://core.telegram.org/bots/api#available-methods](https://core.telegram.org/bots/api#available-methods)). Every message object needs a *chat_id* field, containing a Telegram user id, so that Telegram knows which user to deliver your message to. We simply extract this id out of the message objects chat object we received from the user. The second mandatory field in a message object is the *text*. This is up to you. After having set up the new message object (you could add other fields, e.g. to show bot-buttons to the user  see [https://core.telegram.org/bots/api#message](https://core.telegram.org/bots/api#message) for this), we just need to HTTP POST it to Telegram using Unirest again. Finished.\n\nThis example was kept veeeery simple. Of course you could implement your bot to do really fancy things. You could process media, do location-specific operations, include a database (MongoDB suits really well!) and many, many other things. You could write any complex application you can imagine  with a Telegram chat as the text i/o interface. Please tell be your ideas  what would be a great bot?\n\nIf you like to try by bot, simply write a message to **@FavoriteBot** and share it to your friends, if you like it.\n\nIf you have any questions, contact me via mail to *ferdinand(at)muetsch.io.*","source":"_posts/how-to-make-telegram-bots.md","raw":"---\ntitle: How to make Telegram Bots\ndate: 2015-06-28 22:39:44\ntags:\n---\n\nRecently [Telegram](http://telegram.org \"Telegram\") has introduced a new feature, the bots. Basically the bots enable human Telegram users to talk to machines, i.e. customly written little programs. A bot could be a daily helper, be it a bot that you can ask for the current temperature, one that googles something for you, one to manage your todos or even a little text based game  everything within the Telegram chat. The nice thing about them is that theyre really simple to create. You can read more about the bots in general here: [https://telegram.org/blog/bot-revolution](https://telegram.org/blog/bot-revolution)\n\nThis article shouldnt cover how to create and publish a bot (which actually is the same step), but how to write its actual functionality  its backend. On how to initially set up one, please refer to this little guide: [https://core.telegram.org/bots](https://core.telegram.org/bots). Its very easy, trust me. Everything you need for that is your Telegram app.\n\nWhat you need further to program your bot basically is a favorite programming language, an IDE or at least a text editor, [this](https://core.telegram.org/bots/api \"this\") page to be open and again your app. In this tutorial we will use Node.js as programming language (or more precisely as programming platform), but you really could use any other language, as well. Probably Python or PHP would do the job well, but you could even write a Java application.\n\nFirst some basic things to understand. For the most parts your bot is your own application which we will try to create here, running on your own local PC or server. Telegram wont host any code. All functionality and data storage is kept in your program on your machine. Basically Telegram doesnt provide more than kind of an interface between your users Telegram client (app) and your bot application. The flow would be like:\n\n1.  You create a new bot with @BotFather and set its description and commands (the commands you set there are  strictly speaking  completely independent of which commands your program will actually accept  they are just strings which the user gets suggested in a chat with your bot) so that the bot gets publicly accessible as any other (human) Telegram user is via her @ nickname.\n\n2.  You write the backend and run it to be listening.\n\n3.  A user sends a message to your bot.\n\n4.  The (JSON formatted) Telegram message object gets passed to your backend via HTTPS by Telegram.\n\n5.  Your backend parses the message text, extract the commands and arguments, processes some logic, possibly does some database actions and so on.\n\n6.  Your backend passes a response message object to the Telegram API via HTTPS (the other way round now).\n\n7.  Telegram shows the message to your users chat window.\n\nAlright, now lets get into the code. I require you to already have set up a bot and got its authentication token (all using @BotFather). As I said, we will make a little Node.js program here, but for those who dont understand JavaScript too well, Ill try to explain everything as clear as you need to re-do this in your programming language.\n\nFirst, you need any library that can do HTTP requests, because basically your backend will be busy doing POSTs and GETs for the most time. For Node.js well use the [Unirest](http://unirest.io/nodejs.html \"Unirest\") library, which probably is a little bit overkill, but very simple for our purpose. The library is available for Python, PHP, Ruby, Java and many others as well.\n\nSo first step is setting up a Node application and requiring Unirest (i assume you know how to set up a Node application and youre familiar with npm).\n\n```javascript\n/* app.js */\n\nvar unirest = require('unirest');\n```\n\nNext is to receive messages from your users. Telegram offers two ways to do this. The first way would be to use Webhooks, which basically means that youre backend runs on a webserver (or for Node it actually is one itself), Telegram knows your HTTPS endpoint and does a request to your backend every time a message is sent by a user. I would consider this the more elegant way. But the minus about this is that you would need to have a valid SSL certificate (which costs monthly charge) to provide a secure HTTPS connection. Telegram wont send any data to an unsecure or unverified endpoint. This is why we will take the other approach, which works kind of the other way round. Your backend will continuously request the Telegram API if there are new messages available. In detail your backend wont do request after request after request (because this would be so inefficient!) but use [long polling](http://www.pubnub.com/blog/http-long-polling/ \"long polling\"). To put it simple long polling means that a request wont be answered instantly, but kept open until there is some data available.\n\nWe need to specify some constants. As i said, everything between your backend and Telegram happens via their HTTPS interface. We set up constants for the base API url, containing your bot token, the URL for the [getUpdates](https://core.telegram.org/bots/api#getupdates \"getUpdates\") method endpoint and the URL for the [sendMessage](https://core.telegram.org/bots/api#sendmessage \"sendMessage\") method endpoint. The *:offset:* within the URL string will get replaced by a number, specifying, which new messages to fetch from Telegram later.\n\n```javascript\n/* app.js */\n\n/* ... */\n\nvar BASE_URL = \"https://api.telegram.org/botYOUR_TOKEN_HERE/\";\nvar POLLING_URL = BASE_URL + \"getUpdates?offset=:offset:&timeout=60\";\nvar SEND_MESSAGE_URL = BASE_URL + \"sendMessage\";\n```\n\nNow well introduce a function called *poll* (you can choose any other name), which basically is kind of the main loop of our program. Heres the code for this method, explanation follows.\n\n```javascript\n/* ... */\nfunction poll(offset) {\n    var url = POLLING_URL.replace(\":offset:\", offset);\n\n    unirest.get(url)\n        .end(function(response) {\n            var body = response.raw_body;\n            if (response.status == 200) {\n                var jsonData = JSON.parse(body);\n                var result = jsonData.result;\n\n                if (result.length > 0) {\n                    for (i in result) {\n                        if (runCommand(result[i].message)) continue;\n                    }\n\n                    max_offset = parseInt(result[result.length - 1].update_id) + 1; // update max offset\n                }\n                poll(max_offset);\n            }\n        });\n};\n```\n\nAlright. The function is recursive, meaning it will call itself  namely each time, a request was answered (which is, when a new message was fetched). The http request to Telegram API to get updates will be pending until a message arrives. Then it gets answered, but has to be re-opened again instantly now to continue listening for message updates again. As a parameter it takes an offset number, which replaces the *:offset:* placeholder in the url string. To read more about this parameter, go to [https://core.telegram.org/bots/api#getupdates](https://core.telegram.org/bots/api#getupdates). Unirest opens an http request to the specified url and executes the callback function given to end(), if the request was answered. First, we extract the response body. Afterwards, we check if our request was successful. If this is the case, we parse the body (which is a JSON object, consisting of an *ok* field and a *result* array. The result array contains one or more message objects. These are the ones that are relevant for us. For each message object, we try to parse it as a command (ill give you the runCommand() function is a second) and depending on which command we got, execute the respective method. Afterwards the offset gets updates to the id of the latest message plus one (to not receive it again next time) and a new request gets opened.\n\n```javascript\n/* ... */\n\nvar dosth = function(message) {\n    // to be implemented....\n}\n\nvar COMMANDS = {\n    \"dosth\" : dosth\n};\n```\n\nNow we specify a map, which maps strings (representing the users command input  in this case */dosth* to actual functions.\n\n```javascript\n    var msgtext = message.text;\n\n    if (msgtext.indexOf(\"/\") != 0) return false; // no slash at beginning?\n    var command = msgtext.substring(1, msgtext.indexOf(\" \"));\n    if (COMMANDS[command] == null) return false; // not a valid command?\n    COMMANDS[command](message);\n    return true;\n}\n```\n\nAnd this is the runCommand method. It takes the entire Telegram message object, which we got as a response from the Telegram API above and tries to parse its text as a command. A command string always starts with a slash. So if there is not slash in the beginning, we can be sure that we didnt get a valid command. We simply return here, but we also could send a message to the user telling him Hey, please enter a valid command.. But lets keep it simple. In the following line we extract everything after the slash and before the first blank space (assuming a command mustnt contain a blank space) as the command. Afterwards we look into our map if the command actually is a key for a method and if so, we just run this method, passing it the message object as a parameter as the function will need information out of it.\n\nWhat have we done so far? We wrote a program that requests the Telegram API for new messages, parses potentially contained commands and runs functions depending on these commands.\n\nAll we still need is to implement the method belonging to the */dosth* command.\n\n```javascript\n    var caps = message.text.toUpperCase();\n    var answer = {\n        chat_id : message.chat.id,\n        text : \"You told be to do something, so I took your input and made it all caps. Look: \" + caps\n    };\n\n    unirest.post(SEND_MESSAGE_URL)\n        .send(answer)\n        .end(function (response) {\n            if (response.status == 200) console.log(\"Successfully sent message to \" + message.chat.id);\n        });\n}\n```\n\nMost times youll want to send a message as response to your user. You could also send an image, an audio, a location,  (see [https://core.telegram.org/bots/api#available-methods](https://core.telegram.org/bots/api#available-methods)). Every message object needs a *chat_id* field, containing a Telegram user id, so that Telegram knows which user to deliver your message to. We simply extract this id out of the message objects chat object we received from the user. The second mandatory field in a message object is the *text*. This is up to you. After having set up the new message object (you could add other fields, e.g. to show bot-buttons to the user  see [https://core.telegram.org/bots/api#message](https://core.telegram.org/bots/api#message) for this), we just need to HTTP POST it to Telegram using Unirest again. Finished.\n\nThis example was kept veeeery simple. Of course you could implement your bot to do really fancy things. You could process media, do location-specific operations, include a database (MongoDB suits really well!) and many, many other things. You could write any complex application you can imagine  with a Telegram chat as the text i/o interface. Please tell be your ideas  what would be a great bot?\n\nIf you like to try by bot, simply write a message to **@FavoriteBot** and share it to your friends, if you like it.\n\nIf you have any questions, contact me via mail to *ferdinand(at)muetsch.io.*","slug":"how-to-make-telegram-bots","published":1,"updated":"2020-10-30T20:05:40.287Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqly000go2e03rt5e6gx","content":"<p>Recently <a href=\"http://telegram.org/\" title=\"Telegram\">Telegram</a> has introduced a new feature, the bots. Basically the bots enable human Telegram users to talk to machines, i.e. customly written little programs. A bot could be a daily helper, be it a bot that you can ask for the current temperature, one that googles something for you, one to manage your todos or even a little text based game  everything within the Telegram chat. The nice thing about them is that theyre really simple to create. You can read more about the bots in general here: <a href=\"https://telegram.org/blog/bot-revolution\">https://telegram.org/blog/bot-revolution</a></p>\n<p>This article shouldnt cover how to create and publish a bot (which actually is the same step), but how to write its actual functionality  its backend. On how to initially set up one, please refer to this little guide: <a href=\"https://core.telegram.org/bots\">https://core.telegram.org/bots</a>. Its very easy, trust me. Everything you need for that is your Telegram app.</p>\n<p>What you need further to program your bot basically is a favorite programming language, an IDE or at least a text editor, <a href=\"https://core.telegram.org/bots/api\" title=\"this\">this</a> page to be open and again your app. In this tutorial we will use Node.js as programming language (or more precisely as programming platform), but you really could use any other language, as well. Probably Python or PHP would do the job well, but you could even write a Java application.</p>\n<p>First some basic things to understand. For the most parts your bot is your own application which we will try to create here, running on your own local PC or server. Telegram wont host any code. All functionality and data storage is kept in your program on your machine. Basically Telegram doesnt provide more than kind of an interface between your users Telegram client (app) and your bot application. The flow would be like:</p>\n<ol>\n<li><p> You create a new bot with @BotFather and set its description and commands (the commands you set there are  strictly speaking  completely independent of which commands your program will actually accept  they are just strings which the user gets suggested in a chat with your bot) so that the bot gets publicly accessible as any other (human) Telegram user is via her @ nickname.</p>\n</li>\n<li><p> You write the backend and run it to be listening.</p>\n</li>\n<li><p> A user sends a message to your bot.</p>\n</li>\n<li><p> The (JSON formatted) Telegram message object gets passed to your backend via HTTPS by Telegram.</p>\n</li>\n<li><p> Your backend parses the message text, extract the commands and arguments, processes some logic, possibly does some database actions and so on.</p>\n</li>\n<li><p> Your backend passes a response message object to the Telegram API via HTTPS (the other way round now).</p>\n</li>\n<li><p> Telegram shows the message to your users chat window.</p>\n</li>\n</ol>\n<p>Alright, now lets get into the code. I require you to already have set up a bot and got its authentication token (all using @BotFather). As I said, we will make a little Node.js program here, but for those who dont understand JavaScript too well, Ill try to explain everything as clear as you need to re-do this in your programming language.</p>\n<p>First, you need any library that can do HTTP requests, because basically your backend will be busy doing POSTs and GETs for the most time. For Node.js well use the <a href=\"http://unirest.io/nodejs.html\" title=\"Unirest\">Unirest</a> library, which probably is a little bit overkill, but very simple for our purpose. The library is available for Python, PHP, Ruby, Java and many others as well.</p>\n<p>So first step is setting up a Node application and requiring Unirest (i assume you know how to set up a Node application and youre familiar with npm).</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* app.js */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> unirest = <span class=\"built_in\">require</span>(<span class=\"string\">&#x27;unirest&#x27;</span>);</span><br></pre></td></tr></table></figure>\n\n<p>Next is to receive messages from your users. Telegram offers two ways to do this. The first way would be to use Webhooks, which basically means that youre backend runs on a webserver (or for Node it actually is one itself), Telegram knows your HTTPS endpoint and does a request to your backend every time a message is sent by a user. I would consider this the more elegant way. But the minus about this is that you would need to have a valid SSL certificate (which costs monthly charge) to provide a secure HTTPS connection. Telegram wont send any data to an unsecure or unverified endpoint. This is why we will take the other approach, which works kind of the other way round. Your backend will continuously request the Telegram API if there are new messages available. In detail your backend wont do request after request after request (because this would be so inefficient!) but use <a href=\"http://www.pubnub.com/blog/http-long-polling/\" title=\"long polling\">long polling</a>. To put it simple long polling means that a request wont be answered instantly, but kept open until there is some data available.</p>\n<p>We need to specify some constants. As i said, everything between your backend and Telegram happens via their HTTPS interface. We set up constants for the base API url, containing your bot token, the URL for the <a href=\"https://core.telegram.org/bots/api#getupdates\" title=\"getUpdates\">getUpdates</a> method endpoint and the URL for the <a href=\"https://core.telegram.org/bots/api#sendmessage\" title=\"sendMessage\">sendMessage</a> method endpoint. The <em>:offset:</em> within the URL string will get replaced by a number, specifying, which new messages to fetch from Telegram later.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* app.js */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* ... */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> BASE_URL = <span class=\"string\">&quot;https://api.telegram.org/botYOUR_TOKEN_HERE/&quot;</span>;</span><br><span class=\"line\"><span class=\"keyword\">var</span> POLLING_URL = BASE_URL + <span class=\"string\">&quot;getUpdates?offset=:offset:&amp;timeout=60&quot;</span>;</span><br><span class=\"line\"><span class=\"keyword\">var</span> SEND_MESSAGE_URL = BASE_URL + <span class=\"string\">&quot;sendMessage&quot;</span>;</span><br></pre></td></tr></table></figure>\n\n<p>Now well introduce a function called <em>poll</em> (you can choose any other name), which basically is kind of the main loop of our program. Heres the code for this method, explanation follows.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* ... */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">poll</span>(<span class=\"params\">offset</span>) </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> url = POLLING_URL.replace(<span class=\"string\">&quot;:offset:&quot;</span>, offset);</span><br><span class=\"line\"></span><br><span class=\"line\">    unirest.get(url)</span><br><span class=\"line\">        .end(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">var</span> body = response.raw_body;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.status == <span class=\"number\">200</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">var</span> jsonData = <span class=\"built_in\">JSON</span>.parse(body);</span><br><span class=\"line\">                <span class=\"keyword\">var</span> result = jsonData.result;</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (result.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> (i <span class=\"keyword\">in</span> result) &#123;</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (runCommand(result[i].message)) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">                    max_offset = <span class=\"built_in\">parseInt</span>(result[result.length - <span class=\"number\">1</span>].update_id) + <span class=\"number\">1</span>; <span class=\"comment\">// update max offset</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                poll(max_offset);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>Alright. The function is recursive, meaning it will call itself  namely each time, a request was answered (which is, when a new message was fetched). The http request to Telegram API to get updates will be pending until a message arrives. Then it gets answered, but has to be re-opened again instantly now to continue listening for message updates again. As a parameter it takes an offset number, which replaces the <em>:offset:</em> placeholder in the url string. To read more about this parameter, go to <a href=\"https://core.telegram.org/bots/api#getupdates\">https://core.telegram.org/bots/api#getupdates</a>. Unirest opens an http request to the specified url and executes the callback function given to end(), if the request was answered. First, we extract the response body. Afterwards, we check if our request was successful. If this is the case, we parse the body (which is a JSON object, consisting of an <em>ok</em> field and a <em>result</em> array. The result array contains one or more message objects. These are the ones that are relevant for us. For each message object, we try to parse it as a command (ill give you the runCommand() function is a second) and depending on which command we got, execute the respective method. Afterwards the offset gets updates to the id of the latest message plus one (to not receive it again next time) and a new request gets opened.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* ... */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> dosth = <span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">message</span>) </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// to be implemented....</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> COMMANDS = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;dosth&quot;</span> : dosth</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>Now we specify a map, which maps strings (representing the users command input  in this case <em>/dosth</em> to actual functions.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">var</span> msgtext = message.text;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (msgtext.indexOf(<span class=\"string\">&quot;/&quot;</span>) != <span class=\"number\">0</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>; <span class=\"comment\">// no slash at beginning?</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> command = msgtext.substring(<span class=\"number\">1</span>, msgtext.indexOf(<span class=\"string\">&quot; &quot;</span>));</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (COMMANDS[command] == <span class=\"literal\">null</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>; <span class=\"comment\">// not a valid command?</span></span><br><span class=\"line\">    COMMANDS[command](message);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>And this is the runCommand method. It takes the entire Telegram message object, which we got as a response from the Telegram API above and tries to parse its text as a command. A command string always starts with a slash. So if there is not slash in the beginning, we can be sure that we didnt get a valid command. We simply return here, but we also could send a message to the user telling him Hey, please enter a valid command.. But lets keep it simple. In the following line we extract everything after the slash and before the first blank space (assuming a command mustnt contain a blank space) as the command. Afterwards we look into our map if the command actually is a key for a method and if so, we just run this method, passing it the message object as a parameter as the function will need information out of it.</p>\n<p>What have we done so far? We wrote a program that requests the Telegram API for new messages, parses potentially contained commands and runs functions depending on these commands.</p>\n<p>All we still need is to implement the method belonging to the <em>/dosth</em> command.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">var</span> caps = message.text.toUpperCase();</span><br><span class=\"line\">    <span class=\"keyword\">var</span> answer = &#123;</span><br><span class=\"line\">        chat_id : message.chat.id,</span><br><span class=\"line\">        text : <span class=\"string\">&quot;You told be to do something, so I took your input and made it all caps. Look: &quot;</span> + caps</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    unirest.post(SEND_MESSAGE_URL)</span><br><span class=\"line\">        .send(answer)</span><br><span class=\"line\">        .end(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.status == <span class=\"number\">200</span>) <span class=\"built_in\">console</span>.log(<span class=\"string\">&quot;Successfully sent message to &quot;</span> + message.chat.id);</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Most times youll want to send a message as response to your user. You could also send an image, an audio, a location,  (see <a href=\"https://core.telegram.org/bots/api#available-methods\">https://core.telegram.org/bots/api#available-methods</a>). Every message object needs a <em>chat_id</em> field, containing a Telegram user id, so that Telegram knows which user to deliver your message to. We simply extract this id out of the message objects chat object we received from the user. The second mandatory field in a message object is the <em>text</em>. This is up to you. After having set up the new message object (you could add other fields, e.g. to show bot-buttons to the user  see <a href=\"https://core.telegram.org/bots/api#message\">https://core.telegram.org/bots/api#message</a> for this), we just need to HTTP POST it to Telegram using Unirest again. Finished.</p>\n<p>This example was kept veeeery simple. Of course you could implement your bot to do really fancy things. You could process media, do location-specific operations, include a database (MongoDB suits really well!) and many, many other things. You could write any complex application you can imagine  with a Telegram chat as the text i/o interface. Please tell be your ideas  what would be a great bot?</p>\n<p>If you like to try by bot, simply write a message to <strong>@FavoriteBot</strong> and share it to your friends, if you like it.</p>\n<p>If you have any questions, contact me via mail to <em>ferdinand(at)muetsch.io.</em></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Recently <a href=\"http://telegram.org/\" title=\"Telegram\">Telegram</a> has introduced a new feature, the bots. Basically the bots enable human Telegram users to talk to machines, i.e. customly written little programs. A bot could be a daily helper, be it a bot that you can ask for the current temperature, one that googles something for you, one to manage your todos or even a little text based game  everything within the Telegram chat. The nice thing about them is that theyre really simple to create. You can read more about the bots in general here: <a href=\"https://telegram.org/blog/bot-revolution\">https://telegram.org/blog/bot-revolution</a></p>\n<p>This article shouldnt cover how to create and publish a bot (which actually is the same step), but how to write its actual functionality  its backend. On how to initially set up one, please refer to this little guide: <a href=\"https://core.telegram.org/bots\">https://core.telegram.org/bots</a>. Its very easy, trust me. Everything you need for that is your Telegram app.</p>\n<p>What you need further to program your bot basically is a favorite programming language, an IDE or at least a text editor, <a href=\"https://core.telegram.org/bots/api\" title=\"this\">this</a> page to be open and again your app. In this tutorial we will use Node.js as programming language (or more precisely as programming platform), but you really could use any other language, as well. Probably Python or PHP would do the job well, but you could even write a Java application.</p>\n<p>First some basic things to understand. For the most parts your bot is your own application which we will try to create here, running on your own local PC or server. Telegram wont host any code. All functionality and data storage is kept in your program on your machine. Basically Telegram doesnt provide more than kind of an interface between your users Telegram client (app) and your bot application. The flow would be like:</p>\n<ol>\n<li><p> You create a new bot with @BotFather and set its description and commands (the commands you set there are  strictly speaking  completely independent of which commands your program will actually accept  they are just strings which the user gets suggested in a chat with your bot) so that the bot gets publicly accessible as any other (human) Telegram user is via her @ nickname.</p>\n</li>\n<li><p> You write the backend and run it to be listening.</p>\n</li>\n<li><p> A user sends a message to your bot.</p>\n</li>\n<li><p> The (JSON formatted) Telegram message object gets passed to your backend via HTTPS by Telegram.</p>\n</li>\n<li><p> Your backend parses the message text, extract the commands and arguments, processes some logic, possibly does some database actions and so on.</p>\n</li>\n<li><p> Your backend passes a response message object to the Telegram API via HTTPS (the other way round now).</p>\n</li>\n<li><p> Telegram shows the message to your users chat window.</p>\n</li>\n</ol>\n<p>Alright, now lets get into the code. I require you to already have set up a bot and got its authentication token (all using @BotFather). As I said, we will make a little Node.js program here, but for those who dont understand JavaScript too well, Ill try to explain everything as clear as you need to re-do this in your programming language.</p>\n<p>First, you need any library that can do HTTP requests, because basically your backend will be busy doing POSTs and GETs for the most time. For Node.js well use the <a href=\"http://unirest.io/nodejs.html\" title=\"Unirest\">Unirest</a> library, which probably is a little bit overkill, but very simple for our purpose. The library is available for Python, PHP, Ruby, Java and many others as well.</p>\n<p>So first step is setting up a Node application and requiring Unirest (i assume you know how to set up a Node application and youre familiar with npm).</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* app.js */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> unirest = <span class=\"built_in\">require</span>(<span class=\"string\">&#x27;unirest&#x27;</span>);</span><br></pre></td></tr></table></figure>\n\n<p>Next is to receive messages from your users. Telegram offers two ways to do this. The first way would be to use Webhooks, which basically means that youre backend runs on a webserver (or for Node it actually is one itself), Telegram knows your HTTPS endpoint and does a request to your backend every time a message is sent by a user. I would consider this the more elegant way. But the minus about this is that you would need to have a valid SSL certificate (which costs monthly charge) to provide a secure HTTPS connection. Telegram wont send any data to an unsecure or unverified endpoint. This is why we will take the other approach, which works kind of the other way round. Your backend will continuously request the Telegram API if there are new messages available. In detail your backend wont do request after request after request (because this would be so inefficient!) but use <a href=\"http://www.pubnub.com/blog/http-long-polling/\" title=\"long polling\">long polling</a>. To put it simple long polling means that a request wont be answered instantly, but kept open until there is some data available.</p>\n<p>We need to specify some constants. As i said, everything between your backend and Telegram happens via their HTTPS interface. We set up constants for the base API url, containing your bot token, the URL for the <a href=\"https://core.telegram.org/bots/api#getupdates\" title=\"getUpdates\">getUpdates</a> method endpoint and the URL for the <a href=\"https://core.telegram.org/bots/api#sendmessage\" title=\"sendMessage\">sendMessage</a> method endpoint. The <em>:offset:</em> within the URL string will get replaced by a number, specifying, which new messages to fetch from Telegram later.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* app.js */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">/* ... */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> BASE_URL = <span class=\"string\">&quot;https://api.telegram.org/botYOUR_TOKEN_HERE/&quot;</span>;</span><br><span class=\"line\"><span class=\"keyword\">var</span> POLLING_URL = BASE_URL + <span class=\"string\">&quot;getUpdates?offset=:offset:&amp;timeout=60&quot;</span>;</span><br><span class=\"line\"><span class=\"keyword\">var</span> SEND_MESSAGE_URL = BASE_URL + <span class=\"string\">&quot;sendMessage&quot;</span>;</span><br></pre></td></tr></table></figure>\n\n<p>Now well introduce a function called <em>poll</em> (you can choose any other name), which basically is kind of the main loop of our program. Heres the code for this method, explanation follows.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* ... */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">function</span> <span class=\"title\">poll</span>(<span class=\"params\">offset</span>) </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> url = POLLING_URL.replace(<span class=\"string\">&quot;:offset:&quot;</span>, offset);</span><br><span class=\"line\"></span><br><span class=\"line\">    unirest.get(url)</span><br><span class=\"line\">        .end(<span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">var</span> body = response.raw_body;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.status == <span class=\"number\">200</span>) &#123;</span><br><span class=\"line\">                <span class=\"keyword\">var</span> jsonData = <span class=\"built_in\">JSON</span>.parse(body);</span><br><span class=\"line\">                <span class=\"keyword\">var</span> result = jsonData.result;</span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (result.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">for</span> (i <span class=\"keyword\">in</span> result) &#123;</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> (runCommand(result[i].message)) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">                    max_offset = <span class=\"built_in\">parseInt</span>(result[result.length - <span class=\"number\">1</span>].update_id) + <span class=\"number\">1</span>; <span class=\"comment\">// update max offset</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">                poll(max_offset);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>Alright. The function is recursive, meaning it will call itself  namely each time, a request was answered (which is, when a new message was fetched). The http request to Telegram API to get updates will be pending until a message arrives. Then it gets answered, but has to be re-opened again instantly now to continue listening for message updates again. As a parameter it takes an offset number, which replaces the <em>:offset:</em> placeholder in the url string. To read more about this parameter, go to <a href=\"https://core.telegram.org/bots/api#getupdates\">https://core.telegram.org/bots/api#getupdates</a>. Unirest opens an http request to the specified url and executes the callback function given to end(), if the request was answered. First, we extract the response body. Afterwards, we check if our request was successful. If this is the case, we parse the body (which is a JSON object, consisting of an <em>ok</em> field and a <em>result</em> array. The result array contains one or more message objects. These are the ones that are relevant for us. For each message object, we try to parse it as a command (ill give you the runCommand() function is a second) and depending on which command we got, execute the respective method. Afterwards the offset gets updates to the id of the latest message plus one (to not receive it again next time) and a new request gets opened.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/* ... */</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> dosth = <span class=\"function\"><span class=\"keyword\">function</span>(<span class=\"params\">message</span>) </span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// to be implemented....</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> COMMANDS = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;dosth&quot;</span> : dosth</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>Now we specify a map, which maps strings (representing the users command input  in this case <em>/dosth</em> to actual functions.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">var</span> msgtext = message.text;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (msgtext.indexOf(<span class=\"string\">&quot;/&quot;</span>) != <span class=\"number\">0</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>; <span class=\"comment\">// no slash at beginning?</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> command = msgtext.substring(<span class=\"number\">1</span>, msgtext.indexOf(<span class=\"string\">&quot; &quot;</span>));</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (COMMANDS[command] == <span class=\"literal\">null</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>; <span class=\"comment\">// not a valid command?</span></span><br><span class=\"line\">    COMMANDS[command](message);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>And this is the runCommand method. It takes the entire Telegram message object, which we got as a response from the Telegram API above and tries to parse its text as a command. A command string always starts with a slash. So if there is not slash in the beginning, we can be sure that we didnt get a valid command. We simply return here, but we also could send a message to the user telling him Hey, please enter a valid command.. But lets keep it simple. In the following line we extract everything after the slash and before the first blank space (assuming a command mustnt contain a blank space) as the command. Afterwards we look into our map if the command actually is a key for a method and if so, we just run this method, passing it the message object as a parameter as the function will need information out of it.</p>\n<p>What have we done so far? We wrote a program that requests the Telegram API for new messages, parses potentially contained commands and runs functions depending on these commands.</p>\n<p>All we still need is to implement the method belonging to the <em>/dosth</em> command.</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">var</span> caps = message.text.toUpperCase();</span><br><span class=\"line\">    <span class=\"keyword\">var</span> answer = &#123;</span><br><span class=\"line\">        chat_id : message.chat.id,</span><br><span class=\"line\">        text : <span class=\"string\">&quot;You told be to do something, so I took your input and made it all caps. Look: &quot;</span> + caps</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    unirest.post(SEND_MESSAGE_URL)</span><br><span class=\"line\">        .send(answer)</span><br><span class=\"line\">        .end(<span class=\"function\"><span class=\"keyword\">function</span> (<span class=\"params\">response</span>) </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (response.status == <span class=\"number\">200</span>) <span class=\"built_in\">console</span>.log(<span class=\"string\">&quot;Successfully sent message to &quot;</span> + message.chat.id);</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Most times youll want to send a message as response to your user. You could also send an image, an audio, a location,  (see <a href=\"https://core.telegram.org/bots/api#available-methods\">https://core.telegram.org/bots/api#available-methods</a>). Every message object needs a <em>chat_id</em> field, containing a Telegram user id, so that Telegram knows which user to deliver your message to. We simply extract this id out of the message objects chat object we received from the user. The second mandatory field in a message object is the <em>text</em>. This is up to you. After having set up the new message object (you could add other fields, e.g. to show bot-buttons to the user  see <a href=\"https://core.telegram.org/bots/api#message\">https://core.telegram.org/bots/api#message</a> for this), we just need to HTTP POST it to Telegram using Unirest again. Finished.</p>\n<p>This example was kept veeeery simple. Of course you could implement your bot to do really fancy things. You could process media, do location-specific operations, include a database (MongoDB suits really well!) and many, many other things. You could write any complex application you can imagine  with a Telegram chat as the text i/o interface. Please tell be your ideas  what would be a great bot?</p>\n<p>If you like to try by bot, simply write a message to <strong>@FavoriteBot</strong> and share it to your friends, if you like it.</p>\n<p>If you have any questions, contact me via mail to <em>ferdinand(at)muetsch.io.</em></p>\n"},{"title":"How to receive sharing intents in Flutter?","date":"2019-03-11T05:56:28.000Z","_content":"\n# Use Case\nA common use case when building an Android app is to have it handle data shared from other apps. All Android users know this little dialog that pops up when you hit the _\"Share\"_ button in any app. It displays a list of applications, which are registered for receiving shared data. The user has to choose one, which is then opened up to handle the shared text, URL or whatever it is. On iOS, a similar concept exists. However, this article focused on **Android only**. \n\n![](https://cketti.de/img/share-url-to-clipboard/screenshot_share.png)\n_(Source: https://cketti.de/)_\n\nAs an example, let's imagine having a bookmark-manager written in Flutter. It is supposed to save **URLs** with their accompanying **titles**, shared from the smartphone's browser to the app, to one of your bookmark collection. This is exactly [what I just build](https://github.com/muety/anchr-android).\n\n# Background\nThe official Flutter docs already give [a good example](https://flutter.dev/docs/get-started/flutter-for/android-devs#how-do-i-handle-incoming-intents-from-external-applications-in-flutter) on how to achieve that functionality. However, I found that their piece of code only works, if you share data to an app that **is still closed**. If you had opened your app before and it idles in the background, it won't receive the [sharing intent](https://www.androidcode.ninja/android-share-intent-example/) when it is [resumed](https://developer.android.com/guide/components/activities/activity-lifecycle#onresume). Therefore, I extended the example. \n\n# Code\n## AndroidManifest.xml\nFirst, you have to add an `intent-filter` to your `AndroidManifest.xml` in the `android/` directory to register your app as a sharing target.\n\n```xml\n...\n<intent-filter>\n    <action android:name=\"android.intent.action.SEND\" />\n    <category android:name=\"android.intent.category.DEFAULT\" />\n    <data android:mimeType=\"text/plain\" />\n</intent-filter>\n...\n```\n\n## MainActivity.java\nSecondly, you will need to add some code to `MainActivity.java` (analogously for Kotlin projects). In the `onCreate()` lifecycle hook, you have to register a `MethodCallHandler()` to act as an interface between the underlying Android app and your flutter code. In addition, you have to override the `onNewIntent()` callback, which is triggered when a new sharing intent (`Intent.ACTION_SEND`) causes your app to change its lifecycle state. Lastly, you need a method to handle the actual content shared from the external app. It consists of two fields, a URL and a title, both represented as strings in a Map. In the end, your `MainActivity` looks like something like this.\n\n```java\nprivate Map<String, String> sharedData = new HashMap();\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        GeneratedPluginRegistrant.registerWith(this);\n\n        // Handle intent when app is initially opened\n        handleSendIntent(getIntent());\n\n        new MethodChannel(getFlutterView(), \"app.channel.shared.data\").setMethodCallHandler(\n            new MethodCallHandler() {\n                @Override\n                public void onMethodCall(MethodCall call, MethodChannel.Result result) {\n                    if (call.method.contentEquals(\"getSharedData\")) {\n                        result.success(sharedData);\n                        sharedData.clear();\n                    }\n                }\n            }\n        );\n    }\n\n    @Override\n    protected void onNewIntent(Intent intent) {\n        // Handle intent when app is resumed\n        super.onNewIntent(intent);\n        handleSendIntent(intent);\n    }\n\n    private void handleSendIntent(Intent intent) {\n        String action = intent.getAction();\n        String type = intent.getType();\n\n        // We only care about sharing intent that contain plain text\n        if (Intent.ACTION_SEND.equals(action) && type != null) {\n            if (\"text/plain\".equals(type)) {\n                sharedData.put(\"subject\", intent.getStringExtra(Intent.EXTRA_SUBJECT));\n                sharedData.put(\"text\", intent.getStringExtra(Intent.EXTRA_TEXT));\n            }\n        }\n    }\n}\n```\n\nNote that the shared data is \"cached\" on the Java side of your app until is it picked up by your Flutter code.\n\n## Your Flutter app\nEventually, you need to add a method to your Flutter code to interact with the native-Android `MethodHandler`. It will be called once during state initialization and  with the help of a listener  every time the underlying Android activity is resumed. \n\n```dart\nclass SampleAppPage extends StatefulWidget {\n  SampleAppPage({Key key}) : super(key: key);\n\n  @override\n  _SampleAppPageState createState() => _SampleAppPageState();\n}\n\nclass _SampleAppPageState extends State<SampleAppPage> {\n    static const platform = const MethodChannel('app.channel.shared.data');\n    Map<dynamic, dynamic> sharedData = Map();\n\n    @override\n    void initState() {\n        super.initState();\n        _init();\n    }\n\n    _init() async {\n        // Case 1: App is already running in background:\n        // Listen to lifecycle changes to subsequently call Java MethodHandler to check for shared data\n        SystemChannels.lifecycle.setMessageHandler((msg) {\n            if (msg.contains('resumed')) {\n                _getSharedData().then((d) {\n                    if (d.isEmpty) return;\n                    // Your logic here\n                    // E.g. at this place you might want to use Navigator to launch a new page and pass the shared data\n                });\n            }\n        });\n\n        // Case 2: App is started by the intent:\n        // Call Java MethodHandler on application start up to check for shared data\n        var data = await _getSharedData();\n        setState(() => sharedData = data);\n\n        // You can use sharedData in your build() method now\n    }\n\n    Future<Map> _getSharedData() async => await platform.invokeMethod('getSharedData');\n}\n```\n\nNow you're good to go! Once you extracted the sharing intent's contents, you can, for instance, show a pre-filled dialog to add the new link to one of your bookmark collections, just as [I did here](https://github.com/muety/anchr-android/blob/897395528532a03ce4e1bdba00fe4b3b35f5fe43/lib/app.dart#L39).\n\n# Conclusion\nThis approach might seem a little complicated, but in fact, it is the only working solution I could find. There is a plugin called [flutter-share](https://github.com/d-silveira/flutter-share), but unfortunately it did not work for me. Happy coding !","source":"_posts/how-to-receive-sharing-intents-in-flutter.md","raw":"---\ntitle: How to receive sharing intents in Flutter?\ndate: 2019-03-11 06:56:28\ntags:\n---\n\n# Use Case\nA common use case when building an Android app is to have it handle data shared from other apps. All Android users know this little dialog that pops up when you hit the _\"Share\"_ button in any app. It displays a list of applications, which are registered for receiving shared data. The user has to choose one, which is then opened up to handle the shared text, URL or whatever it is. On iOS, a similar concept exists. However, this article focused on **Android only**. \n\n![](https://cketti.de/img/share-url-to-clipboard/screenshot_share.png)\n_(Source: https://cketti.de/)_\n\nAs an example, let's imagine having a bookmark-manager written in Flutter. It is supposed to save **URLs** with their accompanying **titles**, shared from the smartphone's browser to the app, to one of your bookmark collection. This is exactly [what I just build](https://github.com/muety/anchr-android).\n\n# Background\nThe official Flutter docs already give [a good example](https://flutter.dev/docs/get-started/flutter-for/android-devs#how-do-i-handle-incoming-intents-from-external-applications-in-flutter) on how to achieve that functionality. However, I found that their piece of code only works, if you share data to an app that **is still closed**. If you had opened your app before and it idles in the background, it won't receive the [sharing intent](https://www.androidcode.ninja/android-share-intent-example/) when it is [resumed](https://developer.android.com/guide/components/activities/activity-lifecycle#onresume). Therefore, I extended the example. \n\n# Code\n## AndroidManifest.xml\nFirst, you have to add an `intent-filter` to your `AndroidManifest.xml` in the `android/` directory to register your app as a sharing target.\n\n```xml\n...\n<intent-filter>\n    <action android:name=\"android.intent.action.SEND\" />\n    <category android:name=\"android.intent.category.DEFAULT\" />\n    <data android:mimeType=\"text/plain\" />\n</intent-filter>\n...\n```\n\n## MainActivity.java\nSecondly, you will need to add some code to `MainActivity.java` (analogously for Kotlin projects). In the `onCreate()` lifecycle hook, you have to register a `MethodCallHandler()` to act as an interface between the underlying Android app and your flutter code. In addition, you have to override the `onNewIntent()` callback, which is triggered when a new sharing intent (`Intent.ACTION_SEND`) causes your app to change its lifecycle state. Lastly, you need a method to handle the actual content shared from the external app. It consists of two fields, a URL and a title, both represented as strings in a Map. In the end, your `MainActivity` looks like something like this.\n\n```java\nprivate Map<String, String> sharedData = new HashMap();\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        GeneratedPluginRegistrant.registerWith(this);\n\n        // Handle intent when app is initially opened\n        handleSendIntent(getIntent());\n\n        new MethodChannel(getFlutterView(), \"app.channel.shared.data\").setMethodCallHandler(\n            new MethodCallHandler() {\n                @Override\n                public void onMethodCall(MethodCall call, MethodChannel.Result result) {\n                    if (call.method.contentEquals(\"getSharedData\")) {\n                        result.success(sharedData);\n                        sharedData.clear();\n                    }\n                }\n            }\n        );\n    }\n\n    @Override\n    protected void onNewIntent(Intent intent) {\n        // Handle intent when app is resumed\n        super.onNewIntent(intent);\n        handleSendIntent(intent);\n    }\n\n    private void handleSendIntent(Intent intent) {\n        String action = intent.getAction();\n        String type = intent.getType();\n\n        // We only care about sharing intent that contain plain text\n        if (Intent.ACTION_SEND.equals(action) && type != null) {\n            if (\"text/plain\".equals(type)) {\n                sharedData.put(\"subject\", intent.getStringExtra(Intent.EXTRA_SUBJECT));\n                sharedData.put(\"text\", intent.getStringExtra(Intent.EXTRA_TEXT));\n            }\n        }\n    }\n}\n```\n\nNote that the shared data is \"cached\" on the Java side of your app until is it picked up by your Flutter code.\n\n## Your Flutter app\nEventually, you need to add a method to your Flutter code to interact with the native-Android `MethodHandler`. It will be called once during state initialization and  with the help of a listener  every time the underlying Android activity is resumed. \n\n```dart\nclass SampleAppPage extends StatefulWidget {\n  SampleAppPage({Key key}) : super(key: key);\n\n  @override\n  _SampleAppPageState createState() => _SampleAppPageState();\n}\n\nclass _SampleAppPageState extends State<SampleAppPage> {\n    static const platform = const MethodChannel('app.channel.shared.data');\n    Map<dynamic, dynamic> sharedData = Map();\n\n    @override\n    void initState() {\n        super.initState();\n        _init();\n    }\n\n    _init() async {\n        // Case 1: App is already running in background:\n        // Listen to lifecycle changes to subsequently call Java MethodHandler to check for shared data\n        SystemChannels.lifecycle.setMessageHandler((msg) {\n            if (msg.contains('resumed')) {\n                _getSharedData().then((d) {\n                    if (d.isEmpty) return;\n                    // Your logic here\n                    // E.g. at this place you might want to use Navigator to launch a new page and pass the shared data\n                });\n            }\n        });\n\n        // Case 2: App is started by the intent:\n        // Call Java MethodHandler on application start up to check for shared data\n        var data = await _getSharedData();\n        setState(() => sharedData = data);\n\n        // You can use sharedData in your build() method now\n    }\n\n    Future<Map> _getSharedData() async => await platform.invokeMethod('getSharedData');\n}\n```\n\nNow you're good to go! Once you extracted the sharing intent's contents, you can, for instance, show a pre-filled dialog to add the new link to one of your bookmark collections, just as [I did here](https://github.com/muety/anchr-android/blob/897395528532a03ce4e1bdba00fe4b3b35f5fe43/lib/app.dart#L39).\n\n# Conclusion\nThis approach might seem a little complicated, but in fact, it is the only working solution I could find. There is a plugin called [flutter-share](https://github.com/d-silveira/flutter-share), but unfortunately it did not work for me. Happy coding !","slug":"how-to-receive-sharing-intents-in-flutter","published":1,"updated":"2020-10-30T20:05:40.290Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqly000ho2e07i2u6ifa","content":"<h1 id=\"Use-Case\"><a href=\"#Use-Case\" class=\"headerlink\" title=\"Use Case\"></a>Use Case</h1><p>A common use case when building an Android app is to have it handle data shared from other apps. All Android users know this little dialog that pops up when you hit the <em>Share</em> button in any app. It displays a list of applications, which are registered for receiving shared data. The user has to choose one, which is then opened up to handle the shared text, URL or whatever it is. On iOS, a similar concept exists. However, this article focused on <strong>Android only</strong>. </p>\n<p><img src=\"https://cketti.de/img/share-url-to-clipboard/screenshot_share.png\"><br><em>(Source: <a href=\"https://cketti.de/\">https://cketti.de/</a>)</em></p>\n<p>As an example, lets imagine having a bookmark-manager written in Flutter. It is supposed to save <strong>URLs</strong> with their accompanying <strong>titles</strong>, shared from the smartphones browser to the app, to one of your bookmark collection. This is exactly <a href=\"https://github.com/muety/anchr-android\">what I just build</a>.</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><p>The official Flutter docs already give <a href=\"https://flutter.dev/docs/get-started/flutter-for/android-devs#how-do-i-handle-incoming-intents-from-external-applications-in-flutter\">a good example</a> on how to achieve that functionality. However, I found that their piece of code only works, if you share data to an app that <strong>is still closed</strong>. If you had opened your app before and it idles in the background, it wont receive the <a href=\"https://www.androidcode.ninja/android-share-intent-example/\">sharing intent</a> when it is <a href=\"https://developer.android.com/guide/components/activities/activity-lifecycle#onresume\">resumed</a>. Therefore, I extended the example. </p>\n<h1 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h1><h2 id=\"AndroidManifest-xml\"><a href=\"#AndroidManifest-xml\" class=\"headerlink\" title=\"AndroidManifest.xml\"></a>AndroidManifest.xml</h2><p>First, you have to add an <code>intent-filter</code> to your <code>AndroidManifest.xml</code> in the <code>android/</code> directory to register your app as a sharing target.</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">intent-filter</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">action</span> <span class=\"attr\">android:name</span>=<span class=\"string\">&quot;android.intent.action.SEND&quot;</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">category</span> <span class=\"attr\">android:name</span>=<span class=\"string\">&quot;android.intent.category.DEFAULT&quot;</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">data</span> <span class=\"attr\">android:mimeType</span>=<span class=\"string\">&quot;text/plain&quot;</span> /&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">intent-filter</span>&gt;</span></span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"MainActivity-java\"><a href=\"#MainActivity-java\" class=\"headerlink\" title=\"MainActivity.java\"></a>MainActivity.java</h2><p>Secondly, you will need to add some code to <code>MainActivity.java</code> (analogously for Kotlin projects). In the <code>onCreate()</code> lifecycle hook, you have to register a <code>MethodCallHandler()</code> to act as an interface between the underlying Android app and your flutter code. In addition, you have to override the <code>onNewIntent()</code> callback, which is triggered when a new sharing intent (<code>Intent.ACTION_SEND</code>) causes your app to change its lifecycle state. Lastly, you need a method to handle the actual content shared from the external app. It consists of two fields, a URL and a title, both represented as strings in a Map. In the end, your <code>MainActivity</code> looks like something like this.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> Map&lt;String, String&gt; sharedData = <span class=\"keyword\">new</span> HashMap();</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">onCreate</span><span class=\"params\">(Bundle savedInstanceState)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onCreate(savedInstanceState);</span><br><span class=\"line\">        GeneratedPluginRegistrant.registerWith(<span class=\"keyword\">this</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Handle intent when app is initially opened</span></span><br><span class=\"line\">        handleSendIntent(getIntent());</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">new</span> MethodChannel(getFlutterView(), <span class=\"string\">&quot;app.channel.shared.data&quot;</span>).setMethodCallHandler(</span><br><span class=\"line\">            <span class=\"keyword\">new</span> MethodCallHandler() &#123;</span><br><span class=\"line\">                <span class=\"meta\">@Override</span></span><br><span class=\"line\">                <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onMethodCall</span><span class=\"params\">(MethodCall call, MethodChannel.Result result)</span> </span>&#123;</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (call.method.contentEquals(<span class=\"string\">&quot;getSharedData&quot;</span>)) &#123;</span><br><span class=\"line\">                        result.success(sharedData);</span><br><span class=\"line\">                        sharedData.clear();</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">onNewIntent</span><span class=\"params\">(Intent intent)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// Handle intent when app is resumed</span></span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onNewIntent(intent);</span><br><span class=\"line\">        handleSendIntent(intent);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">handleSendIntent</span><span class=\"params\">(Intent intent)</span> </span>&#123;</span><br><span class=\"line\">        String action = intent.getAction();</span><br><span class=\"line\">        String type = intent.getType();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// We only care about sharing intent that contain plain text</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (Intent.ACTION_SEND.equals(action) &amp;&amp; type != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"string\">&quot;text/plain&quot;</span>.equals(type)) &#123;</span><br><span class=\"line\">                sharedData.put(<span class=\"string\">&quot;subject&quot;</span>, intent.getStringExtra(Intent.EXTRA_SUBJECT));</span><br><span class=\"line\">                sharedData.put(<span class=\"string\">&quot;text&quot;</span>, intent.getStringExtra(Intent.EXTRA_TEXT));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Note that the shared data is cached on the Java side of your app until is it picked up by your Flutter code.</p>\n<h2 id=\"Your-Flutter-app\"><a href=\"#Your-Flutter-app\" class=\"headerlink\" title=\"Your Flutter app\"></a>Your Flutter app</h2><p>Eventually, you need to add a method to your Flutter code to interact with the native-Android <code>MethodHandler</code>. It will be called once during state initialization and  with the help of a listener  every time the underlying Android activity is resumed. </p>\n<figure class=\"highlight dart\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SampleAppPage</span> <span class=\"keyword\">extends</span> <span class=\"title\">StatefulWidget</span> </span>&#123;</span><br><span class=\"line\">  SampleAppPage(&#123;Key key&#125;) : <span class=\"keyword\">super</span>(key: key);</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"meta\">@override</span></span><br><span class=\"line\">  _SampleAppPageState createState() =&gt; _SampleAppPageState();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">_SampleAppPageState</span> <span class=\"keyword\">extends</span> <span class=\"title\">State</span>&lt;<span class=\"title\">SampleAppPage</span>&gt; </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">const</span> platform = <span class=\"keyword\">const</span> MethodChannel(<span class=\"string\">&#x27;app.channel.shared.data&#x27;</span>);</span><br><span class=\"line\">    <span class=\"built_in\">Map</span>&lt;<span class=\"built_in\">dynamic</span>, <span class=\"built_in\">dynamic</span>&gt; sharedData = <span class=\"built_in\">Map</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@override</span></span><br><span class=\"line\">    <span class=\"keyword\">void</span> initState() &#123;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.initState();</span><br><span class=\"line\">        _init();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    _init() <span class=\"keyword\">async</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Case 1: App is already running in background:</span></span><br><span class=\"line\">        <span class=\"comment\">// Listen to lifecycle changes to subsequently call Java MethodHandler to check for shared data</span></span><br><span class=\"line\">        SystemChannels.lifecycle.setMessageHandler((msg) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (msg.contains(<span class=\"string\">&#x27;resumed&#x27;</span>)) &#123;</span><br><span class=\"line\">                _getSharedData().then((d) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (d.isEmpty) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">                    <span class=\"comment\">// Your logic here</span></span><br><span class=\"line\">                    <span class=\"comment\">// E.g. at this place you might want to use Navigator to launch a new page and pass the shared data</span></span><br><span class=\"line\">                &#125;);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Case 2: App is started by the intent:</span></span><br><span class=\"line\">        <span class=\"comment\">// Call Java MethodHandler on application start up to check for shared data</span></span><br><span class=\"line\">        <span class=\"keyword\">var</span> data = <span class=\"keyword\">await</span> _getSharedData();</span><br><span class=\"line\">        setState(() =&gt; sharedData = data);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// You can use sharedData in your build() method now</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    Future&lt;<span class=\"built_in\">Map</span>&gt; _getSharedData() <span class=\"keyword\">async</span> =&gt; <span class=\"keyword\">await</span> platform.invokeMethod(<span class=\"string\">&#x27;getSharedData&#x27;</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Now youre good to go! Once you extracted the sharing intents contents, you can, for instance, show a pre-filled dialog to add the new link to one of your bookmark collections, just as <a href=\"https://github.com/muety/anchr-android/blob/897395528532a03ce4e1bdba00fe4b3b35f5fe43/lib/app.dart#L39\">I did here</a>.</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>This approach might seem a little complicated, but in fact, it is the only working solution I could find. There is a plugin called <a href=\"https://github.com/d-silveira/flutter-share\">flutter-share</a>, but unfortunately it did not work for me. Happy coding !</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1 id=\"Use-Case\"><a href=\"#Use-Case\" class=\"headerlink\" title=\"Use Case\"></a>Use Case</h1><p>A common use case when building an Android app is to have it handle data shared from other apps. All Android users know this little dialog that pops up when you hit the <em>Share</em> button in any app. It displays a list of applications, which are registered for receiving shared data. The user has to choose one, which is then opened up to handle the shared text, URL or whatever it is. On iOS, a similar concept exists. However, this article focused on <strong>Android only</strong>. </p>\n<p><img src=\"https://cketti.de/img/share-url-to-clipboard/screenshot_share.png\"><br><em>(Source: <a href=\"https://cketti.de/\">https://cketti.de/</a>)</em></p>\n<p>As an example, lets imagine having a bookmark-manager written in Flutter. It is supposed to save <strong>URLs</strong> with their accompanying <strong>titles</strong>, shared from the smartphones browser to the app, to one of your bookmark collection. This is exactly <a href=\"https://github.com/muety/anchr-android\">what I just build</a>.</p>\n<h1 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h1><p>The official Flutter docs already give <a href=\"https://flutter.dev/docs/get-started/flutter-for/android-devs#how-do-i-handle-incoming-intents-from-external-applications-in-flutter\">a good example</a> on how to achieve that functionality. However, I found that their piece of code only works, if you share data to an app that <strong>is still closed</strong>. If you had opened your app before and it idles in the background, it wont receive the <a href=\"https://www.androidcode.ninja/android-share-intent-example/\">sharing intent</a> when it is <a href=\"https://developer.android.com/guide/components/activities/activity-lifecycle#onresume\">resumed</a>. Therefore, I extended the example. </p>\n<h1 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h1><h2 id=\"AndroidManifest-xml\"><a href=\"#AndroidManifest-xml\" class=\"headerlink\" title=\"AndroidManifest.xml\"></a>AndroidManifest.xml</h2><p>First, you have to add an <code>intent-filter</code> to your <code>AndroidManifest.xml</code> in the <code>android/</code> directory to register your app as a sharing target.</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">intent-filter</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">action</span> <span class=\"attr\">android:name</span>=<span class=\"string\">&quot;android.intent.action.SEND&quot;</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">category</span> <span class=\"attr\">android:name</span>=<span class=\"string\">&quot;android.intent.category.DEFAULT&quot;</span> /&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">data</span> <span class=\"attr\">android:mimeType</span>=<span class=\"string\">&quot;text/plain&quot;</span> /&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">intent-filter</span>&gt;</span></span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"MainActivity-java\"><a href=\"#MainActivity-java\" class=\"headerlink\" title=\"MainActivity.java\"></a>MainActivity.java</h2><p>Secondly, you will need to add some code to <code>MainActivity.java</code> (analogously for Kotlin projects). In the <code>onCreate()</code> lifecycle hook, you have to register a <code>MethodCallHandler()</code> to act as an interface between the underlying Android app and your flutter code. In addition, you have to override the <code>onNewIntent()</code> callback, which is triggered when a new sharing intent (<code>Intent.ACTION_SEND</code>) causes your app to change its lifecycle state. Lastly, you need a method to handle the actual content shared from the external app. It consists of two fields, a URL and a title, both represented as strings in a Map. In the end, your <code>MainActivity</code> looks like something like this.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> Map&lt;String, String&gt; sharedData = <span class=\"keyword\">new</span> HashMap();</span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">onCreate</span><span class=\"params\">(Bundle savedInstanceState)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onCreate(savedInstanceState);</span><br><span class=\"line\">        GeneratedPluginRegistrant.registerWith(<span class=\"keyword\">this</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Handle intent when app is initially opened</span></span><br><span class=\"line\">        handleSendIntent(getIntent());</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">new</span> MethodChannel(getFlutterView(), <span class=\"string\">&quot;app.channel.shared.data&quot;</span>).setMethodCallHandler(</span><br><span class=\"line\">            <span class=\"keyword\">new</span> MethodCallHandler() &#123;</span><br><span class=\"line\">                <span class=\"meta\">@Override</span></span><br><span class=\"line\">                <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">onMethodCall</span><span class=\"params\">(MethodCall call, MethodChannel.Result result)</span> </span>&#123;</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (call.method.contentEquals(<span class=\"string\">&quot;getSharedData&quot;</span>)) &#123;</span><br><span class=\"line\">                        result.success(sharedData);</span><br><span class=\"line\">                        sharedData.clear();</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">onNewIntent</span><span class=\"params\">(Intent intent)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"comment\">// Handle intent when app is resumed</span></span><br><span class=\"line\">        <span class=\"keyword\">super</span>.onNewIntent(intent);</span><br><span class=\"line\">        handleSendIntent(intent);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">handleSendIntent</span><span class=\"params\">(Intent intent)</span> </span>&#123;</span><br><span class=\"line\">        String action = intent.getAction();</span><br><span class=\"line\">        String type = intent.getType();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// We only care about sharing intent that contain plain text</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (Intent.ACTION_SEND.equals(action) &amp;&amp; type != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"string\">&quot;text/plain&quot;</span>.equals(type)) &#123;</span><br><span class=\"line\">                sharedData.put(<span class=\"string\">&quot;subject&quot;</span>, intent.getStringExtra(Intent.EXTRA_SUBJECT));</span><br><span class=\"line\">                sharedData.put(<span class=\"string\">&quot;text&quot;</span>, intent.getStringExtra(Intent.EXTRA_TEXT));</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Note that the shared data is cached on the Java side of your app until is it picked up by your Flutter code.</p>\n<h2 id=\"Your-Flutter-app\"><a href=\"#Your-Flutter-app\" class=\"headerlink\" title=\"Your Flutter app\"></a>Your Flutter app</h2><p>Eventually, you need to add a method to your Flutter code to interact with the native-Android <code>MethodHandler</code>. It will be called once during state initialization and  with the help of a listener  every time the underlying Android activity is resumed. </p>\n<figure class=\"highlight dart\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SampleAppPage</span> <span class=\"keyword\">extends</span> <span class=\"title\">StatefulWidget</span> </span>&#123;</span><br><span class=\"line\">  SampleAppPage(&#123;Key key&#125;) : <span class=\"keyword\">super</span>(key: key);</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"meta\">@override</span></span><br><span class=\"line\">  _SampleAppPageState createState() =&gt; _SampleAppPageState();</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">_SampleAppPageState</span> <span class=\"keyword\">extends</span> <span class=\"title\">State</span>&lt;<span class=\"title\">SampleAppPage</span>&gt; </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">const</span> platform = <span class=\"keyword\">const</span> MethodChannel(<span class=\"string\">&#x27;app.channel.shared.data&#x27;</span>);</span><br><span class=\"line\">    <span class=\"built_in\">Map</span>&lt;<span class=\"built_in\">dynamic</span>, <span class=\"built_in\">dynamic</span>&gt; sharedData = <span class=\"built_in\">Map</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@override</span></span><br><span class=\"line\">    <span class=\"keyword\">void</span> initState() &#123;</span><br><span class=\"line\">        <span class=\"keyword\">super</span>.initState();</span><br><span class=\"line\">        _init();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    _init() <span class=\"keyword\">async</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// Case 1: App is already running in background:</span></span><br><span class=\"line\">        <span class=\"comment\">// Listen to lifecycle changes to subsequently call Java MethodHandler to check for shared data</span></span><br><span class=\"line\">        SystemChannels.lifecycle.setMessageHandler((msg) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (msg.contains(<span class=\"string\">&#x27;resumed&#x27;</span>)) &#123;</span><br><span class=\"line\">                _getSharedData().then((d) &#123;</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> (d.isEmpty) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">                    <span class=\"comment\">// Your logic here</span></span><br><span class=\"line\">                    <span class=\"comment\">// E.g. at this place you might want to use Navigator to launch a new page and pass the shared data</span></span><br><span class=\"line\">                &#125;);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Case 2: App is started by the intent:</span></span><br><span class=\"line\">        <span class=\"comment\">// Call Java MethodHandler on application start up to check for shared data</span></span><br><span class=\"line\">        <span class=\"keyword\">var</span> data = <span class=\"keyword\">await</span> _getSharedData();</span><br><span class=\"line\">        setState(() =&gt; sharedData = data);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// You can use sharedData in your build() method now</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    Future&lt;<span class=\"built_in\">Map</span>&gt; _getSharedData() <span class=\"keyword\">async</span> =&gt; <span class=\"keyword\">await</span> platform.invokeMethod(<span class=\"string\">&#x27;getSharedData&#x27;</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Now youre good to go! Once you extracted the sharing intents contents, you can, for instance, show a pre-filled dialog to add the new link to one of your bookmark collections, just as <a href=\"https://github.com/muety/anchr-android/blob/897395528532a03ce4e1bdba00fe4b3b35f5fe43/lib/app.dart#L39\">I did here</a>.</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>This approach might seem a little complicated, but in fact, it is the only working solution I could find. There is a plugin called <a href=\"https://github.com/d-silveira/flutter-share\">flutter-share</a>, but unfortunately it did not work for me. Happy coding !</p>\n"},{"title":"Http performance Java (Jersey) vs. Go vs. NodeJS","date":"2016-11-19T22:06:49.000Z","_content":"\nI developed a very basic benchmark suite to compare different HTTP server's performance. It is inspired by [arcadius/java-rest-api-web-container-benchmark](https://github.com/arcadius/java-rest-api-web-container-benchmark), but uses [h2load](https://github.com/nghttp2/nghttp2#benchmarking-tool) instead of [ab](http://httpd.apache.org/docs/2.4/programs/ab.html).\n\nI implemented four very basic REST APIs with exactly one route each, which exposes a small, static todo list as JSON.\n\n## Server Implementations\n* __Java:__ [Jersey](http://jersey.java.net/) with embedded [Grizzly](https://grizzly.java.net/)\n* __Go:__ Using plain `net/http` package\n* __NodeJS:__ Using plain `http` package\n* __NodeJS:__ Using de-facto standard [Express 4](http://expressjs.com/) framework\n\n## Setup\nMy machine, where the benchmark suite was executed on, has the following specifications.\n\n```\n===CPU:\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\n \n===RAM: \n             total       used       free     shared    buffers     cached\nMem:          7.7G       6.3G       1.4G       412M       527M       2.4G\n-/+ buffers/cache:       3.3G       4.3G\nSwap:         5.6G         0B       5.6G\n\n===Java version: \njava version \"1.8.0_101\"\nJava(TM) SE Runtime Environment (build 1.8.0_101-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)\n \n===OS: \nLinux ferdinand-notebook 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux\n\n===Node: \nv6.5.0\n\n=== Go:\ngo version go1.7.3 linux/amd64\n```\n\n## Test parameters\nBasically there are three parameters to be varied for the benchmark.\n* The __number of total reqests__ to be performed against the API. I chose to set this to __100,000__\n* The __number of concurrent__ client to make those requests. I chose to have __32__ concurrent clients, each of them making 3,125 requests.\n* The __number of threads__ to be used by _h2load_. I set this parameter to four, corresponding to the number of logical CPU cores of my machine.\n\n## Results\nRunning my [benchmark script](https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh) delivered the following results.\n\n![](images/benchmarks.svg)\n\n## Discussion\nFirst of all, please notice that this is definitely not a 100 % correct, scientifical evaluation. Rather it should give basic insights on the order of magnitute of the performance differences between different language's HTTP servers.\nAs we can clearly see, Go is the fastest candidate among my test subjects. However, my implementation only utilized the plain, built-in http package without any custom ServeMux or any router or middleware on top of it. In a real-world application, one would most likely not operate on such a low level, but use frameworks like [Iris](http://iris-go.com/) on top, which add additional overhead.\n\nSecond place is Java using Grizzly as an embedded server inside a Jersey application. The reason for me picking Grizzly was that it pointed out to be the fastest among the common servers in [this benchmark](http://menelic.com/2016/01/06/java-rest-api-benchmark-tomcat-vs-jetty-vs-grizzly-vs-undertow/).\n\nBoth of my Node implementations perform worst in this benchmark, whereas Express is even only as half as good as the plain http package. Evidently, it introduces quite an amount of overhead. However, one would most likely not implement an actual REST API without a higher-level framework like Express. Consequently, the Express benchmark is probably more representative. \n\nConclusing I can say that I was pretty surprised about how large the differences between various servers are. Go is almost six times as fast as Node with Express, even though [Express still has a very great performance](https://raygun.com/blog/2016/06/node-performance/).\n\nThe full benchmark results as well as the suite's source code can be found at my [GitHub project](https://github.com/muety/http-server-benchmarks).\n\n## EDIT\nAt January 1st 2017 I did some minor adjustments to my benchmark suite. A thoughtful reader has drawn my attention to the fact that my comparison was a little unfair in the way that Go's net/http as well as Grizzly use as many threads as the host system provides CPU core by default, while Node doesn't. Using Node's `cluster` module I made both Node-based webservers use four listener threads, too and actually the results have improved by around 45 %. Furthermore I did an adjustment to the Jersey + Grizzly server by changing the `IOStrategy` from the default `WorkerThreadIOStrategy` to `SameThreadIOStrategy`, which brought around 10 % in this specific case, where we don't have any blocking computations but only spit out static JSON. If you're interested in leaarning more about different io strategies, refer to [this official documentation page](https://grizzly.java.net/iostrategies.html). Here is my updated benchmark chart.\n\n![](images/benchmarks2.svg)","source":"_posts/http-performance-java-jersey-vs-go-vs-nodejs.md","raw":"---\ntitle: Http performance Java (Jersey) vs. Go vs. NodeJS\ndate: 2016-11-19 23:06:49\ntags:\n---\n\nI developed a very basic benchmark suite to compare different HTTP server's performance. It is inspired by [arcadius/java-rest-api-web-container-benchmark](https://github.com/arcadius/java-rest-api-web-container-benchmark), but uses [h2load](https://github.com/nghttp2/nghttp2#benchmarking-tool) instead of [ab](http://httpd.apache.org/docs/2.4/programs/ab.html).\n\nI implemented four very basic REST APIs with exactly one route each, which exposes a small, static todo list as JSON.\n\n## Server Implementations\n* __Java:__ [Jersey](http://jersey.java.net/) with embedded [Grizzly](https://grizzly.java.net/)\n* __Go:__ Using plain `net/http` package\n* __NodeJS:__ Using plain `http` package\n* __NodeJS:__ Using de-facto standard [Express 4](http://expressjs.com/) framework\n\n## Setup\nMy machine, where the benchmark suite was executed on, has the following specifications.\n\n```\n===CPU:\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\nmodel name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz\ncpu cores\t: 2\n \n===RAM: \n             total       used       free     shared    buffers     cached\nMem:          7.7G       6.3G       1.4G       412M       527M       2.4G\n-/+ buffers/cache:       3.3G       4.3G\nSwap:         5.6G         0B       5.6G\n\n===Java version: \njava version \"1.8.0_101\"\nJava(TM) SE Runtime Environment (build 1.8.0_101-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)\n \n===OS: \nLinux ferdinand-notebook 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux\n\n===Node: \nv6.5.0\n\n=== Go:\ngo version go1.7.3 linux/amd64\n```\n\n## Test parameters\nBasically there are three parameters to be varied for the benchmark.\n* The __number of total reqests__ to be performed against the API. I chose to set this to __100,000__\n* The __number of concurrent__ client to make those requests. I chose to have __32__ concurrent clients, each of them making 3,125 requests.\n* The __number of threads__ to be used by _h2load_. I set this parameter to four, corresponding to the number of logical CPU cores of my machine.\n\n## Results\nRunning my [benchmark script](https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh) delivered the following results.\n\n![](images/benchmarks.svg)\n\n## Discussion\nFirst of all, please notice that this is definitely not a 100 % correct, scientifical evaluation. Rather it should give basic insights on the order of magnitute of the performance differences between different language's HTTP servers.\nAs we can clearly see, Go is the fastest candidate among my test subjects. However, my implementation only utilized the plain, built-in http package without any custom ServeMux or any router or middleware on top of it. In a real-world application, one would most likely not operate on such a low level, but use frameworks like [Iris](http://iris-go.com/) on top, which add additional overhead.\n\nSecond place is Java using Grizzly as an embedded server inside a Jersey application. The reason for me picking Grizzly was that it pointed out to be the fastest among the common servers in [this benchmark](http://menelic.com/2016/01/06/java-rest-api-benchmark-tomcat-vs-jetty-vs-grizzly-vs-undertow/).\n\nBoth of my Node implementations perform worst in this benchmark, whereas Express is even only as half as good as the plain http package. Evidently, it introduces quite an amount of overhead. However, one would most likely not implement an actual REST API without a higher-level framework like Express. Consequently, the Express benchmark is probably more representative. \n\nConclusing I can say that I was pretty surprised about how large the differences between various servers are. Go is almost six times as fast as Node with Express, even though [Express still has a very great performance](https://raygun.com/blog/2016/06/node-performance/).\n\nThe full benchmark results as well as the suite's source code can be found at my [GitHub project](https://github.com/muety/http-server-benchmarks).\n\n## EDIT\nAt January 1st 2017 I did some minor adjustments to my benchmark suite. A thoughtful reader has drawn my attention to the fact that my comparison was a little unfair in the way that Go's net/http as well as Grizzly use as many threads as the host system provides CPU core by default, while Node doesn't. Using Node's `cluster` module I made both Node-based webservers use four listener threads, too and actually the results have improved by around 45 %. Furthermore I did an adjustment to the Jersey + Grizzly server by changing the `IOStrategy` from the default `WorkerThreadIOStrategy` to `SameThreadIOStrategy`, which brought around 10 % in this specific case, where we don't have any blocking computations but only spit out static JSON. If you're interested in leaarning more about different io strategies, refer to [this official documentation page](https://grizzly.java.net/iostrategies.html). Here is my updated benchmark chart.\n\n![](images/benchmarks2.svg)","slug":"http-performance-java-jersey-vs-go-vs-nodejs","published":1,"updated":"2020-10-30T20:05:40.287Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqlz000io2e03he6459k","content":"<p>I developed a very basic benchmark suite to compare different HTTP servers performance. It is inspired by <a href=\"https://github.com/arcadius/java-rest-api-web-container-benchmark\">arcadius/java-rest-api-web-container-benchmark</a>, but uses <a href=\"https://github.com/nghttp2/nghttp2#benchmarking-tool\">h2load</a> instead of <a href=\"http://httpd.apache.org/docs/2.4/programs/ab.html\">ab</a>.</p>\n<p>I implemented four very basic REST APIs with exactly one route each, which exposes a small, static todo list as JSON.</p>\n<h2 id=\"Server-Implementations\"><a href=\"#Server-Implementations\" class=\"headerlink\" title=\"Server Implementations\"></a>Server Implementations</h2><ul>\n<li><strong>Java:</strong> <a href=\"http://jersey.java.net/\">Jersey</a> with embedded <a href=\"https://grizzly.java.net/\">Grizzly</a></li>\n<li><strong>Go:</strong> Using plain <code>net/http</code> package</li>\n<li><strong>NodeJS:</strong> Using plain <code>http</code> package</li>\n<li><strong>NodeJS:</strong> Using de-facto standard <a href=\"http://expressjs.com/\">Express 4</a> framework</li>\n</ul>\n<h2 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h2><p>My machine, where the benchmark suite was executed on, has the following specifications.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x3D;&#x3D;&#x3D;CPU:</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\"> </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;RAM: </span><br><span class=\"line\">             total       used       free     shared    buffers     cached</span><br><span class=\"line\">Mem:          7.7G       6.3G       1.4G       412M       527M       2.4G</span><br><span class=\"line\">-&#x2F;+ buffers&#x2F;cache:       3.3G       4.3G</span><br><span class=\"line\">Swap:         5.6G         0B       5.6G</span><br><span class=\"line\"></span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;Java version: </span><br><span class=\"line\">java version &quot;1.8.0_101&quot;</span><br><span class=\"line\">Java(TM) SE Runtime Environment (build 1.8.0_101-b13)</span><br><span class=\"line\">Java HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)</span><br><span class=\"line\"> </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;OS: </span><br><span class=\"line\">Linux ferdinand-notebook 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU&#x2F;Linux</span><br><span class=\"line\"></span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;Node: </span><br><span class=\"line\">v6.5.0</span><br><span class=\"line\"></span><br><span class=\"line\">&#x3D;&#x3D;&#x3D; Go:</span><br><span class=\"line\">go version go1.7.3 linux&#x2F;amd64</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Test-parameters\"><a href=\"#Test-parameters\" class=\"headerlink\" title=\"Test parameters\"></a>Test parameters</h2><p>Basically there are three parameters to be varied for the benchmark.</p>\n<ul>\n<li>The <strong>number of total reqests</strong> to be performed against the API. I chose to set this to <strong>100,000</strong></li>\n<li>The <strong>number of concurrent</strong> client to make those requests. I chose to have <strong>32</strong> concurrent clients, each of them making 3,125 requests.</li>\n<li>The <strong>number of threads</strong> to be used by <em>h2load</em>. I set this parameter to four, corresponding to the number of logical CPU cores of my machine.</li>\n</ul>\n<h2 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h2><p>Running my <a href=\"https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh\">benchmark script</a> delivered the following results.</p>\n<p><img src=\"images/benchmarks.svg\"></p>\n<h2 id=\"Discussion\"><a href=\"#Discussion\" class=\"headerlink\" title=\"Discussion\"></a>Discussion</h2><p>First of all, please notice that this is definitely not a 100 % correct, scientifical evaluation. Rather it should give basic insights on the order of magnitute of the performance differences between different languages HTTP servers.<br>As we can clearly see, Go is the fastest candidate among my test subjects. However, my implementation only utilized the plain, built-in http package without any custom ServeMux or any router or middleware on top of it. In a real-world application, one would most likely not operate on such a low level, but use frameworks like <a href=\"http://iris-go.com/\">Iris</a> on top, which add additional overhead.</p>\n<p>Second place is Java using Grizzly as an embedded server inside a Jersey application. The reason for me picking Grizzly was that it pointed out to be the fastest among the common servers in <a href=\"http://menelic.com/2016/01/06/java-rest-api-benchmark-tomcat-vs-jetty-vs-grizzly-vs-undertow/\">this benchmark</a>.</p>\n<p>Both of my Node implementations perform worst in this benchmark, whereas Express is even only as half as good as the plain http package. Evidently, it introduces quite an amount of overhead. However, one would most likely not implement an actual REST API without a higher-level framework like Express. Consequently, the Express benchmark is probably more representative. </p>\n<p>Conclusing I can say that I was pretty surprised about how large the differences between various servers are. Go is almost six times as fast as Node with Express, even though <a href=\"https://raygun.com/blog/2016/06/node-performance/\">Express still has a very great performance</a>.</p>\n<p>The full benchmark results as well as the suites source code can be found at my <a href=\"https://github.com/muety/http-server-benchmarks\">GitHub project</a>.</p>\n<h2 id=\"EDIT\"><a href=\"#EDIT\" class=\"headerlink\" title=\"EDIT\"></a>EDIT</h2><p>At January 1st 2017 I did some minor adjustments to my benchmark suite. A thoughtful reader has drawn my attention to the fact that my comparison was a little unfair in the way that Gos net/http as well as Grizzly use as many threads as the host system provides CPU core by default, while Node doesnt. Using Nodes <code>cluster</code> module I made both Node-based webservers use four listener threads, too and actually the results have improved by around 45 %. Furthermore I did an adjustment to the Jersey + Grizzly server by changing the <code>IOStrategy</code> from the default <code>WorkerThreadIOStrategy</code> to <code>SameThreadIOStrategy</code>, which brought around 10 % in this specific case, where we dont have any blocking computations but only spit out static JSON. If youre interested in leaarning more about different io strategies, refer to <a href=\"https://grizzly.java.net/iostrategies.html\">this official documentation page</a>. Here is my updated benchmark chart.</p>\n<p><img src=\"images/benchmarks2.svg\"></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>I developed a very basic benchmark suite to compare different HTTP servers performance. It is inspired by <a href=\"https://github.com/arcadius/java-rest-api-web-container-benchmark\">arcadius/java-rest-api-web-container-benchmark</a>, but uses <a href=\"https://github.com/nghttp2/nghttp2#benchmarking-tool\">h2load</a> instead of <a href=\"http://httpd.apache.org/docs/2.4/programs/ab.html\">ab</a>.</p>\n<p>I implemented four very basic REST APIs with exactly one route each, which exposes a small, static todo list as JSON.</p>\n<h2 id=\"Server-Implementations\"><a href=\"#Server-Implementations\" class=\"headerlink\" title=\"Server Implementations\"></a>Server Implementations</h2><ul>\n<li><strong>Java:</strong> <a href=\"http://jersey.java.net/\">Jersey</a> with embedded <a href=\"https://grizzly.java.net/\">Grizzly</a></li>\n<li><strong>Go:</strong> Using plain <code>net/http</code> package</li>\n<li><strong>NodeJS:</strong> Using plain <code>http</code> package</li>\n<li><strong>NodeJS:</strong> Using de-facto standard <a href=\"http://expressjs.com/\">Express 4</a> framework</li>\n</ul>\n<h2 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h2><p>My machine, where the benchmark suite was executed on, has the following specifications.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x3D;&#x3D;&#x3D;CPU:</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\">model name\t: Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz</span><br><span class=\"line\">cpu cores\t: 2</span><br><span class=\"line\"> </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;RAM: </span><br><span class=\"line\">             total       used       free     shared    buffers     cached</span><br><span class=\"line\">Mem:          7.7G       6.3G       1.4G       412M       527M       2.4G</span><br><span class=\"line\">-&#x2F;+ buffers&#x2F;cache:       3.3G       4.3G</span><br><span class=\"line\">Swap:         5.6G         0B       5.6G</span><br><span class=\"line\"></span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;Java version: </span><br><span class=\"line\">java version &quot;1.8.0_101&quot;</span><br><span class=\"line\">Java(TM) SE Runtime Environment (build 1.8.0_101-b13)</span><br><span class=\"line\">Java HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)</span><br><span class=\"line\"> </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;OS: </span><br><span class=\"line\">Linux ferdinand-notebook 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU&#x2F;Linux</span><br><span class=\"line\"></span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;Node: </span><br><span class=\"line\">v6.5.0</span><br><span class=\"line\"></span><br><span class=\"line\">&#x3D;&#x3D;&#x3D; Go:</span><br><span class=\"line\">go version go1.7.3 linux&#x2F;amd64</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Test-parameters\"><a href=\"#Test-parameters\" class=\"headerlink\" title=\"Test parameters\"></a>Test parameters</h2><p>Basically there are three parameters to be varied for the benchmark.</p>\n<ul>\n<li>The <strong>number of total reqests</strong> to be performed against the API. I chose to set this to <strong>100,000</strong></li>\n<li>The <strong>number of concurrent</strong> client to make those requests. I chose to have <strong>32</strong> concurrent clients, each of them making 3,125 requests.</li>\n<li>The <strong>number of threads</strong> to be used by <em>h2load</em>. I set this parameter to four, corresponding to the number of logical CPU cores of my machine.</li>\n</ul>\n<h2 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h2><p>Running my <a href=\"https://github.com/muety/http-server-benchmarks/blob/master/run-load.sh\">benchmark script</a> delivered the following results.</p>\n<p><img src=\"images/benchmarks.svg\"></p>\n<h2 id=\"Discussion\"><a href=\"#Discussion\" class=\"headerlink\" title=\"Discussion\"></a>Discussion</h2><p>First of all, please notice that this is definitely not a 100 % correct, scientifical evaluation. Rather it should give basic insights on the order of magnitute of the performance differences between different languages HTTP servers.<br>As we can clearly see, Go is the fastest candidate among my test subjects. However, my implementation only utilized the plain, built-in http package without any custom ServeMux or any router or middleware on top of it. In a real-world application, one would most likely not operate on such a low level, but use frameworks like <a href=\"http://iris-go.com/\">Iris</a> on top, which add additional overhead.</p>\n<p>Second place is Java using Grizzly as an embedded server inside a Jersey application. The reason for me picking Grizzly was that it pointed out to be the fastest among the common servers in <a href=\"http://menelic.com/2016/01/06/java-rest-api-benchmark-tomcat-vs-jetty-vs-grizzly-vs-undertow/\">this benchmark</a>.</p>\n<p>Both of my Node implementations perform worst in this benchmark, whereas Express is even only as half as good as the plain http package. Evidently, it introduces quite an amount of overhead. However, one would most likely not implement an actual REST API without a higher-level framework like Express. Consequently, the Express benchmark is probably more representative. </p>\n<p>Conclusing I can say that I was pretty surprised about how large the differences between various servers are. Go is almost six times as fast as Node with Express, even though <a href=\"https://raygun.com/blog/2016/06/node-performance/\">Express still has a very great performance</a>.</p>\n<p>The full benchmark results as well as the suites source code can be found at my <a href=\"https://github.com/muety/http-server-benchmarks\">GitHub project</a>.</p>\n<h2 id=\"EDIT\"><a href=\"#EDIT\" class=\"headerlink\" title=\"EDIT\"></a>EDIT</h2><p>At January 1st 2017 I did some minor adjustments to my benchmark suite. A thoughtful reader has drawn my attention to the fact that my comparison was a little unfair in the way that Gos net/http as well as Grizzly use as many threads as the host system provides CPU core by default, while Node doesnt. Using Nodes <code>cluster</code> module I made both Node-based webservers use four listener threads, too and actually the results have improved by around 45 %. Furthermore I did an adjustment to the Jersey + Grizzly server by changing the <code>IOStrategy</code> from the default <code>WorkerThreadIOStrategy</code> to <code>SameThreadIOStrategy</code>, which brought around 10 % in this specific case, where we dont have any blocking computations but only spit out static JSON. If youre interested in leaarning more about different io strategies, refer to <a href=\"https://grizzly.java.net/iostrategies.html\">this official documentation page</a>. Here is my updated benchmark chart.</p>\n<p><img src=\"images/benchmarks2.svg\"></p>\n"},{"title":"HTTP/2.0 server push proxy","date":"2016-11-14T22:05:45.000Z","_content":"\nI just released a new, little Node project on GitHub and NPM, which is called [http2-serverpush-proxy](https://www.npmjs.com/package/http2-serverpush-proxy) and does exactly what the name suggests. It spawns a reverse proxy between a web application and its clients, that serves via HTTP/2 and automatically server-pushes assets contained in the HTML. It can be used as either standalone server or as _connect_ middleware for ExpressJS.\n\n### How it works\nUsually, websites consist of multiple assets, like CSS and JS files as well as images like PNGs, JPGs and SVGs. Traditionally, a user's browser fetches the HTML first, parses it and then downloads all linked assets. However, this is slow, since the assets can't be loaded before the HTML is completely fetched and parsed. With server push, your webserver can actively send those assets to the client browser even before it requested them. To prevent you from having to implement this functionality, _http2-serverpush-proxy_ sits as a proxy between your actual webserver and the user. In contrast to some other approaches like [http2-push-manifest](https://github.com/GoogleChrome/http2-push-manifest), where the assets to be pushed are declared statically, this library __dynamically parses the HTML__ files and extracts contained assets that should be pushed.\n\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/push_screenshot1.png)\nWithout server push\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/push_screenshot2.png)\nWith server push\n\nDetails on how to use this library are to be found on the [project site](https://github.com/muety/http2-serverpush-proxy). Please feel free to give me feedback!\n","source":"_posts/http20-server-push-proxy.md","raw":"---\ntitle: HTTP/2.0 server push proxy\ndate: 2016-11-14 23:05:45\ntags:\n---\n\nI just released a new, little Node project on GitHub and NPM, which is called [http2-serverpush-proxy](https://www.npmjs.com/package/http2-serverpush-proxy) and does exactly what the name suggests. It spawns a reverse proxy between a web application and its clients, that serves via HTTP/2 and automatically server-pushes assets contained in the HTML. It can be used as either standalone server or as _connect_ middleware for ExpressJS.\n\n### How it works\nUsually, websites consist of multiple assets, like CSS and JS files as well as images like PNGs, JPGs and SVGs. Traditionally, a user's browser fetches the HTML first, parses it and then downloads all linked assets. However, this is slow, since the assets can't be loaded before the HTML is completely fetched and parsed. With server push, your webserver can actively send those assets to the client browser even before it requested them. To prevent you from having to implement this functionality, _http2-serverpush-proxy_ sits as a proxy between your actual webserver and the user. In contrast to some other approaches like [http2-push-manifest](https://github.com/GoogleChrome/http2-push-manifest), where the assets to be pushed are declared statically, this library __dynamically parses the HTML__ files and extracts contained assets that should be pushed.\n\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/push_screenshot1.png)\nWithout server push\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/push_screenshot2.png)\nWith server push\n\nDetails on how to use this library are to be found on the [project site](https://github.com/muety/http2-serverpush-proxy). Please feel free to give me feedback!\n","slug":"http20-server-push-proxy","published":1,"updated":"2020-10-30T20:05:40.283Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqm0000jo2e0egwy5eya","content":"<p>I just released a new, little Node project on GitHub and NPM, which is called <a href=\"https://www.npmjs.com/package/http2-serverpush-proxy\">http2-serverpush-proxy</a> and does exactly what the name suggests. It spawns a reverse proxy between a web application and its clients, that serves via HTTP/2 and automatically server-pushes assets contained in the HTML. It can be used as either standalone server or as <em>connect</em> middleware for ExpressJS.</p>\n<h3 id=\"How-it-works\"><a href=\"#How-it-works\" class=\"headerlink\" title=\"How it works\"></a>How it works</h3><p>Usually, websites consist of multiple assets, like CSS and JS files as well as images like PNGs, JPGs and SVGs. Traditionally, a users browser fetches the HTML first, parses it and then downloads all linked assets. However, this is slow, since the assets cant be loaded before the HTML is completely fetched and parsed. With server push, your webserver can actively send those assets to the client browser even before it requested them. To prevent you from having to implement this functionality, <em>http2-serverpush-proxy</em> sits as a proxy between your actual webserver and the user. In contrast to some other approaches like <a href=\"https://github.com/GoogleChrome/http2-push-manifest\">http2-push-manifest</a>, where the assets to be pushed are declared statically, this library <strong>dynamically parses the HTML</strong> files and extracts contained assets that should be pushed.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/push_screenshot1.png\"><br>Without server push<br><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/push_screenshot2.png\"><br>With server push</p>\n<p>Details on how to use this library are to be found on the <a href=\"https://github.com/muety/http2-serverpush-proxy\">project site</a>. Please feel free to give me feedback!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>I just released a new, little Node project on GitHub and NPM, which is called <a href=\"https://www.npmjs.com/package/http2-serverpush-proxy\">http2-serverpush-proxy</a> and does exactly what the name suggests. It spawns a reverse proxy between a web application and its clients, that serves via HTTP/2 and automatically server-pushes assets contained in the HTML. It can be used as either standalone server or as <em>connect</em> middleware for ExpressJS.</p>\n<h3 id=\"How-it-works\"><a href=\"#How-it-works\" class=\"headerlink\" title=\"How it works\"></a>How it works</h3><p>Usually, websites consist of multiple assets, like CSS and JS files as well as images like PNGs, JPGs and SVGs. Traditionally, a users browser fetches the HTML first, parses it and then downloads all linked assets. However, this is slow, since the assets cant be loaded before the HTML is completely fetched and parsed. With server push, your webserver can actively send those assets to the client browser even before it requested them. To prevent you from having to implement this functionality, <em>http2-serverpush-proxy</em> sits as a proxy between your actual webserver and the user. In contrast to some other approaches like <a href=\"https://github.com/GoogleChrome/http2-push-manifest\">http2-push-manifest</a>, where the assets to be pushed are declared statically, this library <strong>dynamically parses the HTML</strong> files and extracts contained assets that should be pushed.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/push_screenshot1.png\"><br>Without server push<br><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/push_screenshot2.png\"><br>With server push</p>\n<p>Details on how to use this library are to be found on the <a href=\"https://github.com/muety/http2-serverpush-proxy\">project site</a>. Please feel free to give me feedback!</p>\n"},{"title":"Innovation in Germany - not","date":"2016-05-19T21:00:53.000Z","_content":"\nOver last last while I got confronted with this topic quite frequently. Eventually [this german article](http://t3n.de/news/hotspots-unitymedia-abgemahnt-707512/)  or more precisely the comments below it  caused me to write my own little post on my personal perception of how innovation takes place in Germany. For the non-german readers among us I want to give a brief summary of that article. Recently the german internet provider Unitymedia came up with the idea to provide free WiFi-Hotspots for everyone, since internet connection in public places is pretty much a negative report in Germany. The key point with that is their intention of how to implement those so called WifiSpots. Every Unitymedia customer who has a Wifi-capable router in her home should become such a hotspot, while they promise that the hotspot network is completely isolated from the customers own network  in terms of both security and bandwidth. Personally I like the idea, because I consider it quite efficient. Why put effort in distributing routers in public places if there already is full Wifi coverage? Most peoples routers are nowhere near working at capacity anyway. And if its guaranteed that the public internet traffic doesnt influence you at all, why not? The article tells that the consumer protection center of North Rhine-Westphalia had admonished Unitymedia for their plans recently. One of the top comments below the article says about the following:\n\n*\"Thats exactly the reason why innovation isnt possible in Germany. As soon as a company tries to solve peoples problems, everybody goes to the barricades. One gets punished for experiments  not surprisingly nobody wants to found a company.\"*\n\nEven though the comment received a lot of bad write-up, that boils it down for me quite well. In my opinion Germany is ways too sluggish and conservative when it comes to adapting something new. Even though most representatives of the german manufacture consider themselves progressive, I actually dont think they are  at least not as much as they once were. I went to a conference on [Industry 4.0](https://en.wikipedia.org/wiki/Industry_4.0) last week where one speaker claimed that Germany was at least one to two years ahead of other countries at Industry 4.0 topics, but what I picked out between the lines is that quite the opposite is true. While the Germany are continuously trying to define standards, develop well-defined business-processes, clarify legal aspects and the like other countries are just doing it. They simply try it out, taking not too much risk, and if it fails, it fails. But if it succeeds  and I claim that if not applied totally wrong, new technology is likely to  they are given a competitive advantage, while the current big players lead is melting. Of course, with this attitude you are more likely to fail as if you examine every little aspect fussily. But youre also so much more likely to win big. As a professor at university had repeatedly said: think big!.\n\n***\"If your dreams do not scare you, they are not big enough!  Ellen Johnson Sirleaf\"***\n\nThats what often is referred to as the Silicon valley mindset. In fact, as the diagram below shows, most startups are founded in the U.S.A. and I guess if there was a ranking of really big and successful startups (like Uber, WhatsApp, Tesla, ) the contrast would be even more dramatic. Take Elon Musk  the founder of Tesla Motors and SpaceX  (I really recommend his biography) as an instance. He thought big and he obviously won (admittedly he took a really high risk). I dont think they evaluate and plan new technology (like VR and stuff) that much at SpaceX  they just do it.\n\n[![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/statista.png)](http://www.statista.com/statistics/268786/start-ups-in-leading-economic-nations/)\nSource: Statista.com\n\nAnother example for Germanys innovation power is the following. Ive worked for two different companies as a working student  both were software manufacturers. One was a typical german medium-sized company and the other was an american corporation. In one of them, the second-latest version of Internet Explorer was the only browser installed on every employees computer and if you desired another, you had to open a ticket to make the software distribution department install an outdated version of Firefox a few hours later. I dont want to blame that company  they did a great job at what they did. But the overall way of thinking there was old-fashioned, strict and not open-minded at all. Primarily its the peoples mindset that differentiates those two companies completely. I cant really imagine that this first company is a workplace where you really feel comfortable and where youre looking forward to a workday. At the time I worked there, they were about to release a little mobile app, whose development effort I estimate to only few weeks of intensive work of a small group of developers. The apps purpose was great, but unfortunately, it was modern and innovative. Therefore there were only too few people to insistently support it. And theres also the fact that processes were ways too sluggish to do a rapid development. As a result the app is still not released, but instead, two american countries each released an app with pretty much the exact same purpose. So much for that mindset in german companies  of course and as always, there definitely are exceptions (a really big global player originated in Germany at which one department prints out document to hand it over to the other department, at which a trainee typewrites it to make it digital again, not being one of them).\n\nAnother alarming fact I want to mention in this context is that the [average internet speed in Germany is even far behind countries like Sri-Lanka and Vietnam](https://en.wikipedia.org/wiki/List_of_countries_by_Internet_connection_speeds#Akamai_Q3_2015_rankings).\n\n***\"Once a new technology rolls over you, if youre not part of the steamroller, youre part of the road.Stewart Brand\"***\n\nWhat this all amounts to is that Germany should really watch out to not get passed by countries where the people are more ambitious and motivated and less conservative and formal. We should never rest on our laurels but try to permanently improve in a continuous evolution  or to quote many speakers at the conference mentioned above: to not fall into the process of disruptive self-destruction.","source":"_posts/innovation-in-germany-not.md","raw":"---\ntitle: Innovation in Germany - not\ndate: 2016-05-19 23:00:53\ntags:\n---\n\nOver last last while I got confronted with this topic quite frequently. Eventually [this german article](http://t3n.de/news/hotspots-unitymedia-abgemahnt-707512/)  or more precisely the comments below it  caused me to write my own little post on my personal perception of how innovation takes place in Germany. For the non-german readers among us I want to give a brief summary of that article. Recently the german internet provider Unitymedia came up with the idea to provide free WiFi-Hotspots for everyone, since internet connection in public places is pretty much a negative report in Germany. The key point with that is their intention of how to implement those so called WifiSpots. Every Unitymedia customer who has a Wifi-capable router in her home should become such a hotspot, while they promise that the hotspot network is completely isolated from the customers own network  in terms of both security and bandwidth. Personally I like the idea, because I consider it quite efficient. Why put effort in distributing routers in public places if there already is full Wifi coverage? Most peoples routers are nowhere near working at capacity anyway. And if its guaranteed that the public internet traffic doesnt influence you at all, why not? The article tells that the consumer protection center of North Rhine-Westphalia had admonished Unitymedia for their plans recently. One of the top comments below the article says about the following:\n\n*\"Thats exactly the reason why innovation isnt possible in Germany. As soon as a company tries to solve peoples problems, everybody goes to the barricades. One gets punished for experiments  not surprisingly nobody wants to found a company.\"*\n\nEven though the comment received a lot of bad write-up, that boils it down for me quite well. In my opinion Germany is ways too sluggish and conservative when it comes to adapting something new. Even though most representatives of the german manufacture consider themselves progressive, I actually dont think they are  at least not as much as they once were. I went to a conference on [Industry 4.0](https://en.wikipedia.org/wiki/Industry_4.0) last week where one speaker claimed that Germany was at least one to two years ahead of other countries at Industry 4.0 topics, but what I picked out between the lines is that quite the opposite is true. While the Germany are continuously trying to define standards, develop well-defined business-processes, clarify legal aspects and the like other countries are just doing it. They simply try it out, taking not too much risk, and if it fails, it fails. But if it succeeds  and I claim that if not applied totally wrong, new technology is likely to  they are given a competitive advantage, while the current big players lead is melting. Of course, with this attitude you are more likely to fail as if you examine every little aspect fussily. But youre also so much more likely to win big. As a professor at university had repeatedly said: think big!.\n\n***\"If your dreams do not scare you, they are not big enough!  Ellen Johnson Sirleaf\"***\n\nThats what often is referred to as the Silicon valley mindset. In fact, as the diagram below shows, most startups are founded in the U.S.A. and I guess if there was a ranking of really big and successful startups (like Uber, WhatsApp, Tesla, ) the contrast would be even more dramatic. Take Elon Musk  the founder of Tesla Motors and SpaceX  (I really recommend his biography) as an instance. He thought big and he obviously won (admittedly he took a really high risk). I dont think they evaluate and plan new technology (like VR and stuff) that much at SpaceX  they just do it.\n\n[![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/statista.png)](http://www.statista.com/statistics/268786/start-ups-in-leading-economic-nations/)\nSource: Statista.com\n\nAnother example for Germanys innovation power is the following. Ive worked for two different companies as a working student  both were software manufacturers. One was a typical german medium-sized company and the other was an american corporation. In one of them, the second-latest version of Internet Explorer was the only browser installed on every employees computer and if you desired another, you had to open a ticket to make the software distribution department install an outdated version of Firefox a few hours later. I dont want to blame that company  they did a great job at what they did. But the overall way of thinking there was old-fashioned, strict and not open-minded at all. Primarily its the peoples mindset that differentiates those two companies completely. I cant really imagine that this first company is a workplace where you really feel comfortable and where youre looking forward to a workday. At the time I worked there, they were about to release a little mobile app, whose development effort I estimate to only few weeks of intensive work of a small group of developers. The apps purpose was great, but unfortunately, it was modern and innovative. Therefore there were only too few people to insistently support it. And theres also the fact that processes were ways too sluggish to do a rapid development. As a result the app is still not released, but instead, two american countries each released an app with pretty much the exact same purpose. So much for that mindset in german companies  of course and as always, there definitely are exceptions (a really big global player originated in Germany at which one department prints out document to hand it over to the other department, at which a trainee typewrites it to make it digital again, not being one of them).\n\nAnother alarming fact I want to mention in this context is that the [average internet speed in Germany is even far behind countries like Sri-Lanka and Vietnam](https://en.wikipedia.org/wiki/List_of_countries_by_Internet_connection_speeds#Akamai_Q3_2015_rankings).\n\n***\"Once a new technology rolls over you, if youre not part of the steamroller, youre part of the road.Stewart Brand\"***\n\nWhat this all amounts to is that Germany should really watch out to not get passed by countries where the people are more ambitious and motivated and less conservative and formal. We should never rest on our laurels but try to permanently improve in a continuous evolution  or to quote many speakers at the conference mentioned above: to not fall into the process of disruptive self-destruction.","slug":"innovation-in-germany-not","published":1,"updated":"2020-10-30T20:05:40.283Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqm1000ko2e08y3y9vzb","content":"<p>Over last last while I got confronted with this topic quite frequently. Eventually <a href=\"http://t3n.de/news/hotspots-unitymedia-abgemahnt-707512/\">this german article</a>  or more precisely the comments below it  caused me to write my own little post on my personal perception of how innovation takes place in Germany. For the non-german readers among us I want to give a brief summary of that article. Recently the german internet provider Unitymedia came up with the idea to provide free WiFi-Hotspots for everyone, since internet connection in public places is pretty much a negative report in Germany. The key point with that is their intention of how to implement those so called WifiSpots. Every Unitymedia customer who has a Wifi-capable router in her home should become such a hotspot, while they promise that the hotspot network is completely isolated from the customers own network  in terms of both security and bandwidth. Personally I like the idea, because I consider it quite efficient. Why put effort in distributing routers in public places if there already is full Wifi coverage? Most peoples routers are nowhere near working at capacity anyway. And if its guaranteed that the public internet traffic doesnt influence you at all, why not? The article tells that the consumer protection center of North Rhine-Westphalia had admonished Unitymedia for their plans recently. One of the top comments below the article says about the following:</p>\n<p><em>Thats exactly the reason why innovation isnt possible in Germany. As soon as a company tries to solve peoples problems, everybody goes to the barricades. One gets punished for experiments  not surprisingly nobody wants to found a company.</em></p>\n<p>Even though the comment received a lot of bad write-up, that boils it down for me quite well. In my opinion Germany is ways too sluggish and conservative when it comes to adapting something new. Even though most representatives of the german manufacture consider themselves progressive, I actually dont think they are  at least not as much as they once were. I went to a conference on <a href=\"https://en.wikipedia.org/wiki/Industry_4.0\">Industry 4.0</a> last week where one speaker claimed that Germany was at least one to two years ahead of other countries at Industry 4.0 topics, but what I picked out between the lines is that quite the opposite is true. While the Germany are continuously trying to define standards, develop well-defined business-processes, clarify legal aspects and the like other countries are just doing it. They simply try it out, taking not too much risk, and if it fails, it fails. But if it succeeds  and I claim that if not applied totally wrong, new technology is likely to  they are given a competitive advantage, while the current big players lead is melting. Of course, with this attitude you are more likely to fail as if you examine every little aspect fussily. But youre also so much more likely to win big. As a professor at university had repeatedly said: think big!.</p>\n<p><strong><em>If your dreams do not scare you, they are not big enough!  Ellen Johnson Sirleaf</em></strong></p>\n<p>Thats what often is referred to as the Silicon valley mindset. In fact, as the diagram below shows, most startups are founded in the U.S.A. and I guess if there was a ranking of really big and successful startups (like Uber, WhatsApp, Tesla, ) the contrast would be even more dramatic. Take Elon Musk  the founder of Tesla Motors and SpaceX  (I really recommend his biography) as an instance. He thought big and he obviously won (admittedly he took a really high risk). I dont think they evaluate and plan new technology (like VR and stuff) that much at SpaceX  they just do it.</p>\n<p><a href=\"http://www.statista.com/statistics/268786/start-ups-in-leading-economic-nations/\"><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/statista.png\"></a><br>Source: Statista.com</p>\n<p>Another example for Germanys innovation power is the following. Ive worked for two different companies as a working student  both were software manufacturers. One was a typical german medium-sized company and the other was an american corporation. In one of them, the second-latest version of Internet Explorer was the only browser installed on every employees computer and if you desired another, you had to open a ticket to make the software distribution department install an outdated version of Firefox a few hours later. I dont want to blame that company  they did a great job at what they did. But the overall way of thinking there was old-fashioned, strict and not open-minded at all. Primarily its the peoples mindset that differentiates those two companies completely. I cant really imagine that this first company is a workplace where you really feel comfortable and where youre looking forward to a workday. At the time I worked there, they were about to release a little mobile app, whose development effort I estimate to only few weeks of intensive work of a small group of developers. The apps purpose was great, but unfortunately, it was modern and innovative. Therefore there were only too few people to insistently support it. And theres also the fact that processes were ways too sluggish to do a rapid development. As a result the app is still not released, but instead, two american countries each released an app with pretty much the exact same purpose. So much for that mindset in german companies  of course and as always, there definitely are exceptions (a really big global player originated in Germany at which one department prints out document to hand it over to the other department, at which a trainee typewrites it to make it digital again, not being one of them).</p>\n<p>Another alarming fact I want to mention in this context is that the <a href=\"https://en.wikipedia.org/wiki/List_of_countries_by_Internet_connection_speeds#Akamai_Q3_2015_rankings\">average internet speed in Germany is even far behind countries like Sri-Lanka and Vietnam</a>.</p>\n<p><strong><em>Once a new technology rolls over you, if youre not part of the steamroller, youre part of the road.Stewart Brand</em></strong></p>\n<p>What this all amounts to is that Germany should really watch out to not get passed by countries where the people are more ambitious and motivated and less conservative and formal. We should never rest on our laurels but try to permanently improve in a continuous evolution  or to quote many speakers at the conference mentioned above: to not fall into the process of disruptive self-destruction.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Over last last while I got confronted with this topic quite frequently. Eventually <a href=\"http://t3n.de/news/hotspots-unitymedia-abgemahnt-707512/\">this german article</a>  or more precisely the comments below it  caused me to write my own little post on my personal perception of how innovation takes place in Germany. For the non-german readers among us I want to give a brief summary of that article. Recently the german internet provider Unitymedia came up with the idea to provide free WiFi-Hotspots for everyone, since internet connection in public places is pretty much a negative report in Germany. The key point with that is their intention of how to implement those so called WifiSpots. Every Unitymedia customer who has a Wifi-capable router in her home should become such a hotspot, while they promise that the hotspot network is completely isolated from the customers own network  in terms of both security and bandwidth. Personally I like the idea, because I consider it quite efficient. Why put effort in distributing routers in public places if there already is full Wifi coverage? Most peoples routers are nowhere near working at capacity anyway. And if its guaranteed that the public internet traffic doesnt influence you at all, why not? The article tells that the consumer protection center of North Rhine-Westphalia had admonished Unitymedia for their plans recently. One of the top comments below the article says about the following:</p>\n<p><em>Thats exactly the reason why innovation isnt possible in Germany. As soon as a company tries to solve peoples problems, everybody goes to the barricades. One gets punished for experiments  not surprisingly nobody wants to found a company.</em></p>\n<p>Even though the comment received a lot of bad write-up, that boils it down for me quite well. In my opinion Germany is ways too sluggish and conservative when it comes to adapting something new. Even though most representatives of the german manufacture consider themselves progressive, I actually dont think they are  at least not as much as they once were. I went to a conference on <a href=\"https://en.wikipedia.org/wiki/Industry_4.0\">Industry 4.0</a> last week where one speaker claimed that Germany was at least one to two years ahead of other countries at Industry 4.0 topics, but what I picked out between the lines is that quite the opposite is true. While the Germany are continuously trying to define standards, develop well-defined business-processes, clarify legal aspects and the like other countries are just doing it. They simply try it out, taking not too much risk, and if it fails, it fails. But if it succeeds  and I claim that if not applied totally wrong, new technology is likely to  they are given a competitive advantage, while the current big players lead is melting. Of course, with this attitude you are more likely to fail as if you examine every little aspect fussily. But youre also so much more likely to win big. As a professor at university had repeatedly said: think big!.</p>\n<p><strong><em>If your dreams do not scare you, they are not big enough!  Ellen Johnson Sirleaf</em></strong></p>\n<p>Thats what often is referred to as the Silicon valley mindset. In fact, as the diagram below shows, most startups are founded in the U.S.A. and I guess if there was a ranking of really big and successful startups (like Uber, WhatsApp, Tesla, ) the contrast would be even more dramatic. Take Elon Musk  the founder of Tesla Motors and SpaceX  (I really recommend his biography) as an instance. He thought big and he obviously won (admittedly he took a really high risk). I dont think they evaluate and plan new technology (like VR and stuff) that much at SpaceX  they just do it.</p>\n<p><a href=\"http://www.statista.com/statistics/268786/start-ups-in-leading-economic-nations/\"><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/statista.png\"></a><br>Source: Statista.com</p>\n<p>Another example for Germanys innovation power is the following. Ive worked for two different companies as a working student  both were software manufacturers. One was a typical german medium-sized company and the other was an american corporation. In one of them, the second-latest version of Internet Explorer was the only browser installed on every employees computer and if you desired another, you had to open a ticket to make the software distribution department install an outdated version of Firefox a few hours later. I dont want to blame that company  they did a great job at what they did. But the overall way of thinking there was old-fashioned, strict and not open-minded at all. Primarily its the peoples mindset that differentiates those two companies completely. I cant really imagine that this first company is a workplace where you really feel comfortable and where youre looking forward to a workday. At the time I worked there, they were about to release a little mobile app, whose development effort I estimate to only few weeks of intensive work of a small group of developers. The apps purpose was great, but unfortunately, it was modern and innovative. Therefore there were only too few people to insistently support it. And theres also the fact that processes were ways too sluggish to do a rapid development. As a result the app is still not released, but instead, two american countries each released an app with pretty much the exact same purpose. So much for that mindset in german companies  of course and as always, there definitely are exceptions (a really big global player originated in Germany at which one department prints out document to hand it over to the other department, at which a trainee typewrites it to make it digital again, not being one of them).</p>\n<p>Another alarming fact I want to mention in this context is that the <a href=\"https://en.wikipedia.org/wiki/List_of_countries_by_Internet_connection_speeds#Akamai_Q3_2015_rankings\">average internet speed in Germany is even far behind countries like Sri-Lanka and Vietnam</a>.</p>\n<p><strong><em>Once a new technology rolls over you, if youre not part of the steamroller, youre part of the road.Stewart Brand</em></strong></p>\n<p>What this all amounts to is that Germany should really watch out to not get passed by countries where the people are more ambitious and motivated and less conservative and formal. We should never rest on our laurels but try to permanently improve in a continuous evolution  or to quote many speakers at the conference mentioned above: to not fall into the process of disruptive self-destruction.</p>\n"},{"title":"Instant messenger security / encryption overview","date":"2016-02-01T21:48:46.000Z","_content":"\n---\n\n**[Update: Feb 28, 2020]**\nThis post is quite old now and things change rapdily, so please keep in mind that it might not be 100 % accurate anymore. Therefore, I would like to refer to a more recent article on [The best encrypted messaging apps (and their limitations) in 2020](https://www.comparitech.com/blog/information-security/best-encrypted-messaging-apps/). Also, there was a very interesting discussion about [Why do instant messengers not simply use PGP?](https://www.reddit.com/r/crypto/comments/f87asa/why_do_instant_messengers_not_simply_use_pgp/), that helps to better understand the topic of secure messaging.\n\n---\n\nI found a very nice page showing an overview of the security features of almost any instant messenger available. Nowadays where digital privacy observation is on everyones lips it is really interesting to see in how far the messengers we use every day are actually secure and who can potentially read or messages or not.\n\n[![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/scorecard.jpg)](https://www.eff.org/pages/secure-messaging-scorecard)\n\nThe aspects Encrypted in transit? and Encrypted so the provider cant read it? are probably the most important ones and are a different way of saying if the application uses encrypted transmission (probably HTTPS for the most cases) and if it has end-to-end encryption. The last one means that keys are exchanged between sender and recipient which are used for asynchronous encryption so that nobody who potentially receives the message in the middle between them could read it  neither the provider nor the government. To prove that this is actually implemented properly it is required to have an open code which can be reviewed by anyone, because if nobody has ever seen the code it could potentially be the case that your messenger might use end-to-end encryption but the provider grabs your private key too or things the like.\n\nWhat I miss about this Secure Messaging Scorecard is a specification of whether images (and audio recordings, videos, ) get encrypted, too, by messengers that have a checkmark in the second column. Maybe I will do some research on this for some of the listed messengers.\n\nWhat I find alarming is that some commonly used apps like Skype dont even have end-to-end encryption  it is not that hard to integrate and for me this should be standard today. I dont care that extremely much about online privacy because eventually I dont have anything to hide but anyhow  why should Microsoft employees potentially be able to read my messages and view my pics?","source":"_posts/instant-messenger-security-encryption-overview.md","raw":"---\ntitle: Instant messenger security / encryption overview\ndate: 2016-02-01 22:48:46\ntags:\n---\n\n---\n\n**[Update: Feb 28, 2020]**\nThis post is quite old now and things change rapdily, so please keep in mind that it might not be 100 % accurate anymore. Therefore, I would like to refer to a more recent article on [The best encrypted messaging apps (and their limitations) in 2020](https://www.comparitech.com/blog/information-security/best-encrypted-messaging-apps/). Also, there was a very interesting discussion about [Why do instant messengers not simply use PGP?](https://www.reddit.com/r/crypto/comments/f87asa/why_do_instant_messengers_not_simply_use_pgp/), that helps to better understand the topic of secure messaging.\n\n---\n\nI found a very nice page showing an overview of the security features of almost any instant messenger available. Nowadays where digital privacy observation is on everyones lips it is really interesting to see in how far the messengers we use every day are actually secure and who can potentially read or messages or not.\n\n[![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/scorecard.jpg)](https://www.eff.org/pages/secure-messaging-scorecard)\n\nThe aspects Encrypted in transit? and Encrypted so the provider cant read it? are probably the most important ones and are a different way of saying if the application uses encrypted transmission (probably HTTPS for the most cases) and if it has end-to-end encryption. The last one means that keys are exchanged between sender and recipient which are used for asynchronous encryption so that nobody who potentially receives the message in the middle between them could read it  neither the provider nor the government. To prove that this is actually implemented properly it is required to have an open code which can be reviewed by anyone, because if nobody has ever seen the code it could potentially be the case that your messenger might use end-to-end encryption but the provider grabs your private key too or things the like.\n\nWhat I miss about this Secure Messaging Scorecard is a specification of whether images (and audio recordings, videos, ) get encrypted, too, by messengers that have a checkmark in the second column. Maybe I will do some research on this for some of the listed messengers.\n\nWhat I find alarming is that some commonly used apps like Skype dont even have end-to-end encryption  it is not that hard to integrate and for me this should be standard today. I dont care that extremely much about online privacy because eventually I dont have anything to hide but anyhow  why should Microsoft employees potentially be able to read my messages and view my pics?","slug":"instant-messenger-security-encryption-overview","published":1,"updated":"2020-10-30T20:05:40.283Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqm1000lo2e0dmoa64c3","content":"<hr>\n<p><strong>[Update: Feb 28, 2020]</strong><br>This post is quite old now and things change rapdily, so please keep in mind that it might not be 100 % accurate anymore. Therefore, I would like to refer to a more recent article on <a href=\"https://www.comparitech.com/blog/information-security/best-encrypted-messaging-apps/\">The best encrypted messaging apps (and their limitations) in 2020</a>. Also, there was a very interesting discussion about <a href=\"https://www.reddit.com/r/crypto/comments/f87asa/why_do_instant_messengers_not_simply_use_pgp/\">Why do instant messengers not simply use PGP?</a>, that helps to better understand the topic of secure messaging.</p>\n<hr>\n<p>I found a very nice page showing an overview of the security features of almost any instant messenger available. Nowadays where digital privacy observation is on everyones lips it is really interesting to see in how far the messengers we use every day are actually secure and who can potentially read or messages or not.</p>\n<p><a href=\"https://www.eff.org/pages/secure-messaging-scorecard\"><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/scorecard.jpg\"></a></p>\n<p>The aspects Encrypted in transit? and Encrypted so the provider cant read it? are probably the most important ones and are a different way of saying if the application uses encrypted transmission (probably HTTPS for the most cases) and if it has end-to-end encryption. The last one means that keys are exchanged between sender and recipient which are used for asynchronous encryption so that nobody who potentially receives the message in the middle between them could read it  neither the provider nor the government. To prove that this is actually implemented properly it is required to have an open code which can be reviewed by anyone, because if nobody has ever seen the code it could potentially be the case that your messenger might use end-to-end encryption but the provider grabs your private key too or things the like.</p>\n<p>What I miss about this Secure Messaging Scorecard is a specification of whether images (and audio recordings, videos, ) get encrypted, too, by messengers that have a checkmark in the second column. Maybe I will do some research on this for some of the listed messengers.</p>\n<p>What I find alarming is that some commonly used apps like Skype dont even have end-to-end encryption  it is not that hard to integrate and for me this should be standard today. I dont care that extremely much about online privacy because eventually I dont have anything to hide but anyhow  why should Microsoft employees potentially be able to read my messages and view my pics?</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<hr>\n<p><strong>[Update: Feb 28, 2020]</strong><br>This post is quite old now and things change rapdily, so please keep in mind that it might not be 100 % accurate anymore. Therefore, I would like to refer to a more recent article on <a href=\"https://www.comparitech.com/blog/information-security/best-encrypted-messaging-apps/\">The best encrypted messaging apps (and their limitations) in 2020</a>. Also, there was a very interesting discussion about <a href=\"https://www.reddit.com/r/crypto/comments/f87asa/why_do_instant_messengers_not_simply_use_pgp/\">Why do instant messengers not simply use PGP?</a>, that helps to better understand the topic of secure messaging.</p>\n<hr>\n<p>I found a very nice page showing an overview of the security features of almost any instant messenger available. Nowadays where digital privacy observation is on everyones lips it is really interesting to see in how far the messengers we use every day are actually secure and who can potentially read or messages or not.</p>\n<p><a href=\"https://www.eff.org/pages/secure-messaging-scorecard\"><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/scorecard.jpg\"></a></p>\n<p>The aspects Encrypted in transit? and Encrypted so the provider cant read it? are probably the most important ones and are a different way of saying if the application uses encrypted transmission (probably HTTPS for the most cases) and if it has end-to-end encryption. The last one means that keys are exchanged between sender and recipient which are used for asynchronous encryption so that nobody who potentially receives the message in the middle between them could read it  neither the provider nor the government. To prove that this is actually implemented properly it is required to have an open code which can be reviewed by anyone, because if nobody has ever seen the code it could potentially be the case that your messenger might use end-to-end encryption but the provider grabs your private key too or things the like.</p>\n<p>What I miss about this Secure Messaging Scorecard is a specification of whether images (and audio recordings, videos, ) get encrypted, too, by messengers that have a checkmark in the second column. Maybe I will do some research on this for some of the listed messengers.</p>\n<p>What I find alarming is that some commonly used apps like Skype dont even have end-to-end encryption  it is not that hard to integrate and for me this should be standard today. I dont care that extremely much about online privacy because eventually I dont have anything to hide but anyhow  why should Microsoft employees potentially be able to read my messages and view my pics?</p>\n"},{"title":"Learning Angular2: What is new?","date":"2016-02-17T21:51:59.000Z","_content":"\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/angular2_logo.png)\n\nA few days ago i started teaching myself [Angular 2](http://angular.io), which is the successor of the popular frontend web-framework [AngularJS 1.x](https://angularjs.org/). It is still in development and only released as a beta and the developers at Google recommend to not use it in production yet. But Im sure it will come some time in the near future so why not take a step ahead and already learn it now? As Angular 1 has become very successful and wide-spread for web-applications client side I have no doubts that Angular 2 will establish itself pretty quick too.\n\nFor those of you who are familiar with Angular 1 and have developed with it yet: according to what Ive seen so far you will definitely need to take some time for learning Angular 2  it is considerably different from the first version and got few major changes, at least in my eyes. Those changes include:\n\n*   **Different syntax for directives** in HTML: They have introduced parantheses (), brackets [], stars *, hashtags # and combinations of them to be used in your markup. E.g. parantheses () are used as attributes in HTML elements to bind to their events.\n*   There are no controllers anymore. Instead everything is based on (Web-)**Components** (as you may know them from [Google Polymer](https://www.polymer-project.org/1.0/)  if you dont check this out as well, it is pretty cool), which basically consist of the components logic and a view and define a new custom HTML element each. Almost everything in Angular 2 is a component, which enables the code to be even more structurable, more modular and more reusable. But it is a completely new way of thinking in comparison to Angular 1s controllers.\n*   It is based on **ES6 and [TypeScript](http://www.typescriptlang.org/)**. ES6 is the latest JavaScript standard (or version so to say) and TypeScript is even a superset of that, which basically introduces types and modifiers for variables and functions (as you may know from strongly-typed languages like Java). This brings some completely [new features](https://github.com/lukehoban/es6features) and syntax you need to get familiar with. For instance you can define classes with attributes and methods like this now:\n```javascript\n    class Greeter {\n        greeting: string;\n        constructor(message: string) {\n            this.greeting = message;\n        }\n        greet() {\n            return \"Hello, \" + this.greeting;\n        }\n    }\n\n    var greeter = new Greeter(\"world\");</pre>\n```\n\nAlso there are interfaces, import statements, a shorthand way for anonymous functions called arrow functions and many more. Before learning Angular 2 I really recommend to first learn JavaScript ES6 for which [these two videos](https://www.youtube.com/playlist?list=PLoYCgNOIyGACDQLaThEEKBAlgs4OIUGif) are really great.\n\n*   **Dependency injection** has also been reworked to be better understandable, easier to use and more modular now. You will no use @Inectable decorators for injectable services and other modules and provide in the modules by referencing to them in a providers property in components @Component decorator.\n*   Two-way data-binding is still available but the focus is now on **one-way data-binding** (if I got it right the main reason are performance considerations). One-way data-binding means that data isnt continuously updated between template and component but only based on events triggered.\n\nThose where just some of the major changes I got so far. If you want to learn Angular 2 I recommend you the following resources:\n\n * [Angulars offical Getting Started](https://angular.io/docs/ts/latest/quickstart.html)\n * [Angular 2 Fundamentals](https://www.udemy.com/angular-2-fundamentals/) (a free video course on [Udemy](http://udemy.com) for the very basic concepts)\n * This video tutorial on YouTube which I found quite good (and I really had to laugh at 30:20 minutes). You can skip the first few minutes.  \n\n[![](http://img.youtube.com/vi/KL4Yi3WtymA/0.jpg)](http://www.youtube.com/watch?v=KL4Yi3WtymA)\n\nBeing able to develop the brand new, fancy Angular 2 with the brand new JavaScript ES6 (which is so new that most of todays browsers wont even understand it and as a result it needs to get transpiled to ES5 at present) will definitely give you benefits in web development and also in finding a job in that sector  which is probably the fastest growing and most hyped one at the moment.","source":"_posts/learning-angular2-what-is-new.md","raw":"---\ntitle: 'Learning Angular2: What is new?'\ndate: 2016-02-17 22:51:59\ntags:\n---\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/angular2_logo.png)\n\nA few days ago i started teaching myself [Angular 2](http://angular.io), which is the successor of the popular frontend web-framework [AngularJS 1.x](https://angularjs.org/). It is still in development and only released as a beta and the developers at Google recommend to not use it in production yet. But Im sure it will come some time in the near future so why not take a step ahead and already learn it now? As Angular 1 has become very successful and wide-spread for web-applications client side I have no doubts that Angular 2 will establish itself pretty quick too.\n\nFor those of you who are familiar with Angular 1 and have developed with it yet: according to what Ive seen so far you will definitely need to take some time for learning Angular 2  it is considerably different from the first version and got few major changes, at least in my eyes. Those changes include:\n\n*   **Different syntax for directives** in HTML: They have introduced parantheses (), brackets [], stars *, hashtags # and combinations of them to be used in your markup. E.g. parantheses () are used as attributes in HTML elements to bind to their events.\n*   There are no controllers anymore. Instead everything is based on (Web-)**Components** (as you may know them from [Google Polymer](https://www.polymer-project.org/1.0/)  if you dont check this out as well, it is pretty cool), which basically consist of the components logic and a view and define a new custom HTML element each. Almost everything in Angular 2 is a component, which enables the code to be even more structurable, more modular and more reusable. But it is a completely new way of thinking in comparison to Angular 1s controllers.\n*   It is based on **ES6 and [TypeScript](http://www.typescriptlang.org/)**. ES6 is the latest JavaScript standard (or version so to say) and TypeScript is even a superset of that, which basically introduces types and modifiers for variables and functions (as you may know from strongly-typed languages like Java). This brings some completely [new features](https://github.com/lukehoban/es6features) and syntax you need to get familiar with. For instance you can define classes with attributes and methods like this now:\n```javascript\n    class Greeter {\n        greeting: string;\n        constructor(message: string) {\n            this.greeting = message;\n        }\n        greet() {\n            return \"Hello, \" + this.greeting;\n        }\n    }\n\n    var greeter = new Greeter(\"world\");</pre>\n```\n\nAlso there are interfaces, import statements, a shorthand way for anonymous functions called arrow functions and many more. Before learning Angular 2 I really recommend to first learn JavaScript ES6 for which [these two videos](https://www.youtube.com/playlist?list=PLoYCgNOIyGACDQLaThEEKBAlgs4OIUGif) are really great.\n\n*   **Dependency injection** has also been reworked to be better understandable, easier to use and more modular now. You will no use @Inectable decorators for injectable services and other modules and provide in the modules by referencing to them in a providers property in components @Component decorator.\n*   Two-way data-binding is still available but the focus is now on **one-way data-binding** (if I got it right the main reason are performance considerations). One-way data-binding means that data isnt continuously updated between template and component but only based on events triggered.\n\nThose where just some of the major changes I got so far. If you want to learn Angular 2 I recommend you the following resources:\n\n * [Angulars offical Getting Started](https://angular.io/docs/ts/latest/quickstart.html)\n * [Angular 2 Fundamentals](https://www.udemy.com/angular-2-fundamentals/) (a free video course on [Udemy](http://udemy.com) for the very basic concepts)\n * This video tutorial on YouTube which I found quite good (and I really had to laugh at 30:20 minutes). You can skip the first few minutes.  \n\n[![](http://img.youtube.com/vi/KL4Yi3WtymA/0.jpg)](http://www.youtube.com/watch?v=KL4Yi3WtymA)\n\nBeing able to develop the brand new, fancy Angular 2 with the brand new JavaScript ES6 (which is so new that most of todays browsers wont even understand it and as a result it needs to get transpiled to ES5 at present) will definitely give you benefits in web development and also in finding a job in that sector  which is probably the fastest growing and most hyped one at the moment.","slug":"learning-angular2-what-is-new","published":1,"updated":"2020-10-30T20:05:40.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqm2000mo2e0bwuv02yo","content":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/angular2_logo.png\"></p>\n<p>A few days ago i started teaching myself <a href=\"http://angular.io/\">Angular 2</a>, which is the successor of the popular frontend web-framework <a href=\"https://angularjs.org/\">AngularJS 1.x</a>. It is still in development and only released as a beta and the developers at Google recommend to not use it in production yet. But Im sure it will come some time in the near future so why not take a step ahead and already learn it now? As Angular 1 has become very successful and wide-spread for web-applications client side I have no doubts that Angular 2 will establish itself pretty quick too.</p>\n<p>For those of you who are familiar with Angular 1 and have developed with it yet: according to what Ive seen so far you will definitely need to take some time for learning Angular 2  it is considerably different from the first version and got few major changes, at least in my eyes. Those changes include:</p>\n<ul>\n<li>  <strong>Different syntax for directives</strong> in HTML: They have introduced parantheses (), brackets [], stars *, hashtags # and combinations of them to be used in your markup. E.g. parantheses () are used as attributes in HTML elements to bind to their events.</li>\n<li>  There are no controllers anymore. Instead everything is based on (Web-)<strong>Components</strong> (as you may know them from <a href=\"https://www.polymer-project.org/1.0/\">Google Polymer</a>  if you dont check this out as well, it is pretty cool), which basically consist of the components logic and a view and define a new custom HTML element each. Almost everything in Angular 2 is a component, which enables the code to be even more structurable, more modular and more reusable. But it is a completely new way of thinking in comparison to Angular 1s controllers.</li>\n<li>  It is based on <strong>ES6 and <a href=\"http://www.typescriptlang.org/\">TypeScript</a></strong>. ES6 is the latest JavaScript standard (or version so to say) and TypeScript is even a superset of that, which basically introduces types and modifiers for variables and functions (as you may know from strongly-typed languages like Java). This brings some completely <a href=\"https://github.com/lukehoban/es6features\">new features</a> and syntax you need to get familiar with. For instance you can define classes with attributes and methods like this now:<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Greeter</span> </span>&#123;</span><br><span class=\"line\">    greeting: string;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"title\">constructor</span>(<span class=\"params\">message: string</span>)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.greeting = message;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"title\">greet</span>(<span class=\"params\"></span>)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;Hello, &quot;</span> + <span class=\"built_in\">this</span>.greeting;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> greeter = <span class=\"keyword\">new</span> Greeter(<span class=\"string\">&quot;world&quot;</span>);&lt;/pre&gt;</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>Also there are interfaces, import statements, a shorthand way for anonymous functions called arrow functions and many more. Before learning Angular 2 I really recommend to first learn JavaScript ES6 for which <a href=\"https://www.youtube.com/playlist?list=PLoYCgNOIyGACDQLaThEEKBAlgs4OIUGif\">these two videos</a> are really great.</p>\n<ul>\n<li>  <strong>Dependency injection</strong> has also been reworked to be better understandable, easier to use and more modular now. You will no use @Inectable decorators for injectable services and other modules and provide in the modules by referencing to them in a providers property in components @Component decorator.</li>\n<li>  Two-way data-binding is still available but the focus is now on <strong>one-way data-binding</strong> (if I got it right the main reason are performance considerations). One-way data-binding means that data isnt continuously updated between template and component but only based on events triggered.</li>\n</ul>\n<p>Those where just some of the major changes I got so far. If you want to learn Angular 2 I recommend you the following resources:</p>\n<ul>\n<li><a href=\"https://angular.io/docs/ts/latest/quickstart.html\">Angulars offical Getting Started</a></li>\n<li><a href=\"https://www.udemy.com/angular-2-fundamentals/\">Angular 2 Fundamentals</a> (a free video course on <a href=\"http://udemy.com/\">Udemy</a> for the very basic concepts)</li>\n<li>This video tutorial on YouTube which I found quite good (and I really had to laugh at 30:20 minutes). You can skip the first few minutes.  </li>\n</ul>\n<p><a href=\"http://www.youtube.com/watch?v=KL4Yi3WtymA\"><img src=\"http://img.youtube.com/vi/KL4Yi3WtymA/0.jpg\"></a></p>\n<p>Being able to develop the brand new, fancy Angular 2 with the brand new JavaScript ES6 (which is so new that most of todays browsers wont even understand it and as a result it needs to get transpiled to ES5 at present) will definitely give you benefits in web development and also in finding a job in that sector  which is probably the fastest growing and most hyped one at the moment.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/angular2_logo.png\"></p>\n<p>A few days ago i started teaching myself <a href=\"http://angular.io/\">Angular 2</a>, which is the successor of the popular frontend web-framework <a href=\"https://angularjs.org/\">AngularJS 1.x</a>. It is still in development and only released as a beta and the developers at Google recommend to not use it in production yet. But Im sure it will come some time in the near future so why not take a step ahead and already learn it now? As Angular 1 has become very successful and wide-spread for web-applications client side I have no doubts that Angular 2 will establish itself pretty quick too.</p>\n<p>For those of you who are familiar with Angular 1 and have developed with it yet: according to what Ive seen so far you will definitely need to take some time for learning Angular 2  it is considerably different from the first version and got few major changes, at least in my eyes. Those changes include:</p>\n<ul>\n<li>  <strong>Different syntax for directives</strong> in HTML: They have introduced parantheses (), brackets [], stars *, hashtags # and combinations of them to be used in your markup. E.g. parantheses () are used as attributes in HTML elements to bind to their events.</li>\n<li>  There are no controllers anymore. Instead everything is based on (Web-)<strong>Components</strong> (as you may know them from <a href=\"https://www.polymer-project.org/1.0/\">Google Polymer</a>  if you dont check this out as well, it is pretty cool), which basically consist of the components logic and a view and define a new custom HTML element each. Almost everything in Angular 2 is a component, which enables the code to be even more structurable, more modular and more reusable. But it is a completely new way of thinking in comparison to Angular 1s controllers.</li>\n<li>  It is based on <strong>ES6 and <a href=\"http://www.typescriptlang.org/\">TypeScript</a></strong>. ES6 is the latest JavaScript standard (or version so to say) and TypeScript is even a superset of that, which basically introduces types and modifiers for variables and functions (as you may know from strongly-typed languages like Java). This brings some completely <a href=\"https://github.com/lukehoban/es6features\">new features</a> and syntax you need to get familiar with. For instance you can define classes with attributes and methods like this now:<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Greeter</span> </span>&#123;</span><br><span class=\"line\">    greeting: string;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"title\">constructor</span>(<span class=\"params\">message: string</span>)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.greeting = message;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"function\"><span class=\"title\">greet</span>(<span class=\"params\"></span>)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;Hello, &quot;</span> + <span class=\"built_in\">this</span>.greeting;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> greeter = <span class=\"keyword\">new</span> Greeter(<span class=\"string\">&quot;world&quot;</span>);&lt;/pre&gt;</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>Also there are interfaces, import statements, a shorthand way for anonymous functions called arrow functions and many more. Before learning Angular 2 I really recommend to first learn JavaScript ES6 for which <a href=\"https://www.youtube.com/playlist?list=PLoYCgNOIyGACDQLaThEEKBAlgs4OIUGif\">these two videos</a> are really great.</p>\n<ul>\n<li>  <strong>Dependency injection</strong> has also been reworked to be better understandable, easier to use and more modular now. You will no use @Inectable decorators for injectable services and other modules and provide in the modules by referencing to them in a providers property in components @Component decorator.</li>\n<li>  Two-way data-binding is still available but the focus is now on <strong>one-way data-binding</strong> (if I got it right the main reason are performance considerations). One-way data-binding means that data isnt continuously updated between template and component but only based on events triggered.</li>\n</ul>\n<p>Those where just some of the major changes I got so far. If you want to learn Angular 2 I recommend you the following resources:</p>\n<ul>\n<li><a href=\"https://angular.io/docs/ts/latest/quickstart.html\">Angulars offical Getting Started</a></li>\n<li><a href=\"https://www.udemy.com/angular-2-fundamentals/\">Angular 2 Fundamentals</a> (a free video course on <a href=\"http://udemy.com/\">Udemy</a> for the very basic concepts)</li>\n<li>This video tutorial on YouTube which I found quite good (and I really had to laugh at 30:20 minutes). You can skip the first few minutes.  </li>\n</ul>\n<p><a href=\"http://www.youtube.com/watch?v=KL4Yi3WtymA\"><img src=\"http://img.youtube.com/vi/KL4Yi3WtymA/0.jpg\"></a></p>\n<p>Being able to develop the brand new, fancy Angular 2 with the brand new JavaScript ES6 (which is so new that most of todays browsers wont even understand it and as a result it needs to get transpiled to ES5 at present) will definitely give you benefits in web development and also in finding a job in that sector  which is probably the fastest growing and most hyped one at the moment.</p>\n"},{"title":"LinkedData Trivia Game","date":"2017-02-01T22:09:18.000Z","_content":"\nOriginally I got inspired by [this recent post](https://news.ycombinator.com/item?id=13677748) on HackerNews, where [alex_g](https://news.ycombinator.com/user?id=alex_g) has built a quiz, which automatically generates questions from Wikipedia articles using natural language processing (NLP). However, I found the results not that satisfying, yet, and decides to build my own dynamic quiz. Instead of NLP processing I decided to use Linked Data as a base for generating questions. More precisely I'm using the [DBPedia](https://dbpedia.org) knowledge base to retreive fact information from, which mostly originates in Wikipedia articles as well. The data is structured as an RDF graph and can be queried using SPARQL. Despite from the official DBPedia SPARQL endpoint this little proof-of-concept-like app uses another webservice, [kit-lod16-knowledge-panel](https://github.com/muety/kit-lod16-knowledge-panel), which I developed in the context of the Linked Open Data seminar at university. It is responsible for ranking RDF properties for specific RDF entities by relevance in order to decide, which one to display to an end-user (or include to a quiz).\n\n[Code on GitHub](https://github.com/muety/linkeddata-trivia)\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/trivia.jpg)\n\n### Limitations\nThis project is __not a production-ready__ app at all, but rather a __proof-of-concept__ to experiment with. Currently, the __major issue is performance__. Since the app fires a bunch of rather expensive, non-optimized SPARQL queries at the public DBPedia endpoint, the whole process of generating a quiz question takes several seconds on average, sometimes even up to a minute. This could be optimized to a certain extent (e.g. currently there are at least 8 separate HTTP requests from this app plus a few more from the ranking webservice), but all in all querying RDF data is still pretty slow. \n\nAnother limitation is the way \"wrong\" answer options are generated. Currently, random values within a certain interval around the \"correct\" answer are generated for dates and numbers. For properties, whose _rdfs:range_ are entities of a class, a random set of other entities from the same class is fetched from DBPedia and shown as alternative answers. However, __string-valued answers, among others, are ignored__ completely, because it's hard to auto-generate an alternative value for a plain string. There's room for enhancement here.\n\nA third way for improvement would be to include not only DBPedia, but also [Yago](https://yago-knowledge.org), Wikidata and other sources. ","source":"_posts/linkeddata-trivia-game.md","raw":"---\ntitle: LinkedData Trivia Game\ndate: 2017-02-01 23:09:18\ntags:\n---\n\nOriginally I got inspired by [this recent post](https://news.ycombinator.com/item?id=13677748) on HackerNews, where [alex_g](https://news.ycombinator.com/user?id=alex_g) has built a quiz, which automatically generates questions from Wikipedia articles using natural language processing (NLP). However, I found the results not that satisfying, yet, and decides to build my own dynamic quiz. Instead of NLP processing I decided to use Linked Data as a base for generating questions. More precisely I'm using the [DBPedia](https://dbpedia.org) knowledge base to retreive fact information from, which mostly originates in Wikipedia articles as well. The data is structured as an RDF graph and can be queried using SPARQL. Despite from the official DBPedia SPARQL endpoint this little proof-of-concept-like app uses another webservice, [kit-lod16-knowledge-panel](https://github.com/muety/kit-lod16-knowledge-panel), which I developed in the context of the Linked Open Data seminar at university. It is responsible for ranking RDF properties for specific RDF entities by relevance in order to decide, which one to display to an end-user (or include to a quiz).\n\n[Code on GitHub](https://github.com/muety/linkeddata-trivia)\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/trivia.jpg)\n\n### Limitations\nThis project is __not a production-ready__ app at all, but rather a __proof-of-concept__ to experiment with. Currently, the __major issue is performance__. Since the app fires a bunch of rather expensive, non-optimized SPARQL queries at the public DBPedia endpoint, the whole process of generating a quiz question takes several seconds on average, sometimes even up to a minute. This could be optimized to a certain extent (e.g. currently there are at least 8 separate HTTP requests from this app plus a few more from the ranking webservice), but all in all querying RDF data is still pretty slow. \n\nAnother limitation is the way \"wrong\" answer options are generated. Currently, random values within a certain interval around the \"correct\" answer are generated for dates and numbers. For properties, whose _rdfs:range_ are entities of a class, a random set of other entities from the same class is fetched from DBPedia and shown as alternative answers. However, __string-valued answers, among others, are ignored__ completely, because it's hard to auto-generate an alternative value for a plain string. There's room for enhancement here.\n\nA third way for improvement would be to include not only DBPedia, but also [Yago](https://yago-knowledge.org), Wikidata and other sources. ","slug":"linkeddata-trivia-game","published":1,"updated":"2020-10-30T20:05:40.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqm3000no2e06wt9cjd6","content":"<p>Originally I got inspired by <a href=\"https://news.ycombinator.com/item?id=13677748\">this recent post</a> on HackerNews, where <a href=\"https://news.ycombinator.com/user?id=alex_g\">alex_g</a> has built a quiz, which automatically generates questions from Wikipedia articles using natural language processing (NLP). However, I found the results not that satisfying, yet, and decides to build my own dynamic quiz. Instead of NLP processing I decided to use Linked Data as a base for generating questions. More precisely Im using the <a href=\"https://dbpedia.org/\">DBPedia</a> knowledge base to retreive fact information from, which mostly originates in Wikipedia articles as well. The data is structured as an RDF graph and can be queried using SPARQL. Despite from the official DBPedia SPARQL endpoint this little proof-of-concept-like app uses another webservice, <a href=\"https://github.com/muety/kit-lod16-knowledge-panel\">kit-lod16-knowledge-panel</a>, which I developed in the context of the Linked Open Data seminar at university. It is responsible for ranking RDF properties for specific RDF entities by relevance in order to decide, which one to display to an end-user (or include to a quiz).</p>\n<p><a href=\"https://github.com/muety/linkeddata-trivia\">Code on GitHub</a></p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/trivia.jpg\"></p>\n<h3 id=\"Limitations\"><a href=\"#Limitations\" class=\"headerlink\" title=\"Limitations\"></a>Limitations</h3><p>This project is <strong>not a production-ready</strong> app at all, but rather a <strong>proof-of-concept</strong> to experiment with. Currently, the <strong>major issue is performance</strong>. Since the app fires a bunch of rather expensive, non-optimized SPARQL queries at the public DBPedia endpoint, the whole process of generating a quiz question takes several seconds on average, sometimes even up to a minute. This could be optimized to a certain extent (e.g. currently there are at least 8 separate HTTP requests from this app plus a few more from the ranking webservice), but all in all querying RDF data is still pretty slow. </p>\n<p>Another limitation is the way wrong answer options are generated. Currently, random values within a certain interval around the correct answer are generated for dates and numbers. For properties, whose <em>rdfs:range</em> are entities of a class, a random set of other entities from the same class is fetched from DBPedia and shown as alternative answers. However, <strong>string-valued answers, among others, are ignored</strong> completely, because its hard to auto-generate an alternative value for a plain string. Theres room for enhancement here.</p>\n<p>A third way for improvement would be to include not only DBPedia, but also <a href=\"https://yago-knowledge.org/\">Yago</a>, Wikidata and other sources. </p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Originally I got inspired by <a href=\"https://news.ycombinator.com/item?id=13677748\">this recent post</a> on HackerNews, where <a href=\"https://news.ycombinator.com/user?id=alex_g\">alex_g</a> has built a quiz, which automatically generates questions from Wikipedia articles using natural language processing (NLP). However, I found the results not that satisfying, yet, and decides to build my own dynamic quiz. Instead of NLP processing I decided to use Linked Data as a base for generating questions. More precisely Im using the <a href=\"https://dbpedia.org/\">DBPedia</a> knowledge base to retreive fact information from, which mostly originates in Wikipedia articles as well. The data is structured as an RDF graph and can be queried using SPARQL. Despite from the official DBPedia SPARQL endpoint this little proof-of-concept-like app uses another webservice, <a href=\"https://github.com/muety/kit-lod16-knowledge-panel\">kit-lod16-knowledge-panel</a>, which I developed in the context of the Linked Open Data seminar at university. It is responsible for ranking RDF properties for specific RDF entities by relevance in order to decide, which one to display to an end-user (or include to a quiz).</p>\n<p><a href=\"https://github.com/muety/linkeddata-trivia\">Code on GitHub</a></p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/trivia.jpg\"></p>\n<h3 id=\"Limitations\"><a href=\"#Limitations\" class=\"headerlink\" title=\"Limitations\"></a>Limitations</h3><p>This project is <strong>not a production-ready</strong> app at all, but rather a <strong>proof-of-concept</strong> to experiment with. Currently, the <strong>major issue is performance</strong>. Since the app fires a bunch of rather expensive, non-optimized SPARQL queries at the public DBPedia endpoint, the whole process of generating a quiz question takes several seconds on average, sometimes even up to a minute. This could be optimized to a certain extent (e.g. currently there are at least 8 separate HTTP requests from this app plus a few more from the ranking webservice), but all in all querying RDF data is still pretty slow. </p>\n<p>Another limitation is the way wrong answer options are generated. Currently, random values within a certain interval around the correct answer are generated for dates and numbers. For properties, whose <em>rdfs:range</em> are entities of a class, a random set of other entities from the same class is fetched from DBPedia and shown as alternative answers. However, <strong>string-valued answers, among others, are ignored</strong> completely, because its hard to auto-generate an alternative value for a plain string. Theres room for enhancement here.</p>\n<p>A third way for improvement would be to include not only DBPedia, but also <a href=\"https://yago-knowledge.org/\">Yago</a>, Wikidata and other sources. </p>\n"},{"title":"Linux - Cache information bash script","date":"2015-02-27T21:31:53.000Z","_content":"\nThis is a little bash script to get the CPU cache ratios on Ubuntu.\n\n```\nCache Level: (1, 2 or 3)\nCache Type: (data-, instruction or general cache)\nCapacity: of the respective cache\nAssociativity: (set size)\nBlock capacity: / capacity of a cache line\nNumber of sets: ((total capacity / block capacity) / associativity)\n```\n\nConcerning the associativity, see [https://en.wikipedia.org/wiki/CPU_cache#Associativity](https://en.wikipedia.org/wiki/CPU_cache#Associativity).\n\n```bash\nfor DIR0 in /sys/devices/system/cpu/cpu0/cache/*\n    do\n        LEVEL0=$(sudo cat $DIR0\\/level)\n        TYPE0=$(sudo cat $DIR0\\/type)\n        SIZE0=$(sudo cat $DIR0\\/size)\n        ASSOC0=$(sudo cat $DIR0\\/ways_of_associativity)\n        BLOCK0=$(sudo cat $DIR0\\/coherency_line_size)\n        SETS0=$(sudo cat $DIR0\\/number_of_sets)\n\n        printf &quot;Cache level: %s\\nCache type: %s\\nCapacity: %s Bytes\\nAssociativity: %s\\nSets: %s\\nBlock size: %s Bytes\\n\\n&quot; &quot;$LEVEL0&quot; &quot;$TYPE0&quot; &quot;$SIZE0&quot; &quot;$ASSOC0&quot; &quot;$SETS0&quot; &quot;$BLOCK0&quot;\ndone\n```\n\n**Usage:**\n\n1.  Save code to file, e.g. _~/cacheinfo.sh_\n2.  Make it executable: _chmod +x cacheinfo.sh_\n3.  Execute: _sudo sh cacheinfo.sh_","source":"_posts/linux-cache-information-bash-script.md","raw":"---\ntitle: Linux - Cache information bash script\ndate: 2015-02-27 22:31:53\ntags:\n---\n\nThis is a little bash script to get the CPU cache ratios on Ubuntu.\n\n```\nCache Level: (1, 2 or 3)\nCache Type: (data-, instruction or general cache)\nCapacity: of the respective cache\nAssociativity: (set size)\nBlock capacity: / capacity of a cache line\nNumber of sets: ((total capacity / block capacity) / associativity)\n```\n\nConcerning the associativity, see [https://en.wikipedia.org/wiki/CPU_cache#Associativity](https://en.wikipedia.org/wiki/CPU_cache#Associativity).\n\n```bash\nfor DIR0 in /sys/devices/system/cpu/cpu0/cache/*\n    do\n        LEVEL0=$(sudo cat $DIR0\\/level)\n        TYPE0=$(sudo cat $DIR0\\/type)\n        SIZE0=$(sudo cat $DIR0\\/size)\n        ASSOC0=$(sudo cat $DIR0\\/ways_of_associativity)\n        BLOCK0=$(sudo cat $DIR0\\/coherency_line_size)\n        SETS0=$(sudo cat $DIR0\\/number_of_sets)\n\n        printf &quot;Cache level: %s\\nCache type: %s\\nCapacity: %s Bytes\\nAssociativity: %s\\nSets: %s\\nBlock size: %s Bytes\\n\\n&quot; &quot;$LEVEL0&quot; &quot;$TYPE0&quot; &quot;$SIZE0&quot; &quot;$ASSOC0&quot; &quot;$SETS0&quot; &quot;$BLOCK0&quot;\ndone\n```\n\n**Usage:**\n\n1.  Save code to file, e.g. _~/cacheinfo.sh_\n2.  Make it executable: _chmod +x cacheinfo.sh_\n3.  Execute: _sudo sh cacheinfo.sh_","slug":"linux-cache-information-bash-script","published":1,"updated":"2020-10-30T20:05:40.287Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqm3000oo2e09bzvgn6c","content":"<p>This is a little bash script to get the CPU cache ratios on Ubuntu.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Cache Level: (1, 2 or 3)</span><br><span class=\"line\">Cache Type: (data-, instruction or general cache)</span><br><span class=\"line\">Capacity: of the respective cache</span><br><span class=\"line\">Associativity: (set size)</span><br><span class=\"line\">Block capacity: &#x2F; capacity of a cache line</span><br><span class=\"line\">Number of sets: ((total capacity &#x2F; block capacity) &#x2F; associativity)</span><br></pre></td></tr></table></figure>\n\n<p>Concerning the associativity, see <a href=\"https://en.wikipedia.org/wiki/CPU_cache#Associativity\">https://en.wikipedia.org/wiki/CPU_cache#Associativity</a>.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> DIR0 <span class=\"keyword\">in</span> /sys/devices/system/cpu/cpu0/cache/*</span><br><span class=\"line\">    <span class=\"keyword\">do</span></span><br><span class=\"line\">        LEVEL0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/level)</span><br><span class=\"line\">        TYPE0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/<span class=\"built_in\">type</span>)</span><br><span class=\"line\">        SIZE0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/size)</span><br><span class=\"line\">        ASSOC0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/ways_of_associativity)</span><br><span class=\"line\">        BLOCK0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/coherency_line_size)</span><br><span class=\"line\">        SETS0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/number_of_sets)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">printf</span> &amp;quot;Cache level: %s\\nCache <span class=\"built_in\">type</span>: %s\\nCapacity: %s Bytes\\nAssociativity: %s\\nSets: %s\\nBlock size: %s Bytes\\n\\n&amp;quot; &amp;quot;<span class=\"variable\">$LEVEL0</span>&amp;quot; &amp;quot;<span class=\"variable\">$TYPE0</span>&amp;quot; &amp;quot;<span class=\"variable\">$SIZE0</span>&amp;quot; &amp;quot;<span class=\"variable\">$ASSOC0</span>&amp;quot; &amp;quot;<span class=\"variable\">$SETS0</span>&amp;quot; &amp;quot;<span class=\"variable\">$BLOCK0</span>&amp;quot;</span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n\n<p><strong>Usage:</strong></p>\n<ol>\n<li> Save code to file, e.g. <em>~/cacheinfo.sh</em></li>\n<li> Make it executable: <em>chmod +x cacheinfo.sh</em></li>\n<li> Execute: <em>sudo sh cacheinfo.sh</em></li>\n</ol>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>This is a little bash script to get the CPU cache ratios on Ubuntu.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Cache Level: (1, 2 or 3)</span><br><span class=\"line\">Cache Type: (data-, instruction or general cache)</span><br><span class=\"line\">Capacity: of the respective cache</span><br><span class=\"line\">Associativity: (set size)</span><br><span class=\"line\">Block capacity: &#x2F; capacity of a cache line</span><br><span class=\"line\">Number of sets: ((total capacity &#x2F; block capacity) &#x2F; associativity)</span><br></pre></td></tr></table></figure>\n\n<p>Concerning the associativity, see <a href=\"https://en.wikipedia.org/wiki/CPU_cache#Associativity\">https://en.wikipedia.org/wiki/CPU_cache#Associativity</a>.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> DIR0 <span class=\"keyword\">in</span> /sys/devices/system/cpu/cpu0/cache/*</span><br><span class=\"line\">    <span class=\"keyword\">do</span></span><br><span class=\"line\">        LEVEL0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/level)</span><br><span class=\"line\">        TYPE0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/<span class=\"built_in\">type</span>)</span><br><span class=\"line\">        SIZE0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/size)</span><br><span class=\"line\">        ASSOC0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/ways_of_associativity)</span><br><span class=\"line\">        BLOCK0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/coherency_line_size)</span><br><span class=\"line\">        SETS0=$(sudo cat <span class=\"variable\">$DIR0</span>\\/number_of_sets)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"built_in\">printf</span> &amp;quot;Cache level: %s\\nCache <span class=\"built_in\">type</span>: %s\\nCapacity: %s Bytes\\nAssociativity: %s\\nSets: %s\\nBlock size: %s Bytes\\n\\n&amp;quot; &amp;quot;<span class=\"variable\">$LEVEL0</span>&amp;quot; &amp;quot;<span class=\"variable\">$TYPE0</span>&amp;quot; &amp;quot;<span class=\"variable\">$SIZE0</span>&amp;quot; &amp;quot;<span class=\"variable\">$ASSOC0</span>&amp;quot; &amp;quot;<span class=\"variable\">$SETS0</span>&amp;quot; &amp;quot;<span class=\"variable\">$BLOCK0</span>&amp;quot;</span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n\n<p><strong>Usage:</strong></p>\n<ol>\n<li> Save code to file, e.g. <em>~/cacheinfo.sh</em></li>\n<li> Make it executable: <em>chmod +x cacheinfo.sh</em></li>\n<li> Execute: <em>sudo sh cacheinfo.sh</em></li>\n</ol>\n"},{"title":"Migrate Maildir to new server using imapsync","date":"2016-07-23T21:01:44.000Z","_content":"\n\nThis is a little tutorial for mailserver administrators, who want to *migrate* to a new server while *keeping all e-mails*. This works for mailservers whose MDA uses the [Maildir](https://en.wikipedia.org/wiki/Maildir) format  like Dovecot by default  and have *IMAP* enabled.  \nThis tutorial does *not* cover how to set up and configure a new mailserver on a new machine, based on the old ones configuration, but only how to migrate the e-mails. Simply *tar*ing the Maildir folder and un_tar_ing it on the new machine again usually wont work. But dont worry, there is a cleaner way that abstracts away any actual mailserver or file-level considerations by only using the IMAP protocols methods. Therefore, we use a tool *imapsync*, which is written Perl. It acts as an ordinary IMAP client  just as Outlook or Thunderbird  that connects to both mailservers, the old and the new one. All information needed is how to authenticate the respective user with both servers. Actually one manual way to migrate the mails would be to set up both mail accounts in Outlook or Thunderbird, let download the mails via IMAP from the old one and Ctrl+A and Drag&Drop them over to the new one. imapsync does just that  yet automatically and without Outlook or Thunderbird.\n\nFirst we need to *install imapsync*. You could install imapsync on your local PC, just as you would with Outlook or Thunderbird, but then there would be a unnecessary detour from server 1 over your PC to server 2\\. And since your local internet connection is probably ways slower then the servers, your PC would be a bottleneck. So I recommend to install imapsync on either the old or the new mailservers host machine. Lets do it.\n\n1.  Clone the imapsync repository to any folder on your machine, e.g. `/opt/imapsync`: `git clone https://github.com/imapsync/imapsync`\n\n2.  Read the installation notes for your specific operation system at [https://github.com/imapsync/imapsync/tree/master/INSTALL.d](https://github.com/imapsync/imapsync/tree/master/INSTALL.d) and do exactly whats described there. Usually, you will need to install some dependencies and the like.\n\n3.  Now you should be able to execute `./imapsync` from within the directory where you have cloned it to, e.g. `/opt/imapsync`. You should see a description on how to use the program.\n\nLets now assume that you want to migrate mails from your old server with ip *12.34.45.78* for user *foo@example.org* with password *suchsecret* to your new server with ip *98.76.54.32*. A prerequisite is that on both machines the mailserver is up and running and the respective user is configured. Further, lets assume that on the new machine the user, as it makes sense, is called *foo@example.org* again, but his password is *ssshhhhh* now and that both MDAs require a *TLS*-secured connection, use standard *PLAIN* login method and are listening on *port 143*.\n\nTo perform the migration now, run the following command:\n\n```bash\n./imapsync --host1 12.34.45.78 --user1 foo@example.org --password1 suchsecret --authmech1 PLAIN --tls1 --host2 98.76.54.32 --user2 foo@example.org --password2 ssshhhhh --authmech2 PLAIN --tls2\n```\n\nNow all mails should be transferred from `host1` through the imapsync client to `host2`, using nothing but the IMAP protocol. If you want to test if everything is working fine first, before actually transferring data, you could add the `--dry` option to the above command.\n\nTo migrate multiple accounts at once, you could write a small scripts that takes username-password combinations from a text file, as described here: [https://wiki.ubuntuusers.de/imapsync/#Massenmigration](https://wiki.ubuntuusers.de/imapsync/#Massenmigration) (although that article is in German, the code should be clear).","source":"_posts/migrate-maildir-to-new-server-using-imapsync.md","raw":"---\ntitle: Migrate Maildir to new server using imapsync\ndate: 2016-07-23 23:01:44\ntags:\n---\n\n\nThis is a little tutorial for mailserver administrators, who want to *migrate* to a new server while *keeping all e-mails*. This works for mailservers whose MDA uses the [Maildir](https://en.wikipedia.org/wiki/Maildir) format  like Dovecot by default  and have *IMAP* enabled.  \nThis tutorial does *not* cover how to set up and configure a new mailserver on a new machine, based on the old ones configuration, but only how to migrate the e-mails. Simply *tar*ing the Maildir folder and un_tar_ing it on the new machine again usually wont work. But dont worry, there is a cleaner way that abstracts away any actual mailserver or file-level considerations by only using the IMAP protocols methods. Therefore, we use a tool *imapsync*, which is written Perl. It acts as an ordinary IMAP client  just as Outlook or Thunderbird  that connects to both mailservers, the old and the new one. All information needed is how to authenticate the respective user with both servers. Actually one manual way to migrate the mails would be to set up both mail accounts in Outlook or Thunderbird, let download the mails via IMAP from the old one and Ctrl+A and Drag&Drop them over to the new one. imapsync does just that  yet automatically and without Outlook or Thunderbird.\n\nFirst we need to *install imapsync*. You could install imapsync on your local PC, just as you would with Outlook or Thunderbird, but then there would be a unnecessary detour from server 1 over your PC to server 2\\. And since your local internet connection is probably ways slower then the servers, your PC would be a bottleneck. So I recommend to install imapsync on either the old or the new mailservers host machine. Lets do it.\n\n1.  Clone the imapsync repository to any folder on your machine, e.g. `/opt/imapsync`: `git clone https://github.com/imapsync/imapsync`\n\n2.  Read the installation notes for your specific operation system at [https://github.com/imapsync/imapsync/tree/master/INSTALL.d](https://github.com/imapsync/imapsync/tree/master/INSTALL.d) and do exactly whats described there. Usually, you will need to install some dependencies and the like.\n\n3.  Now you should be able to execute `./imapsync` from within the directory where you have cloned it to, e.g. `/opt/imapsync`. You should see a description on how to use the program.\n\nLets now assume that you want to migrate mails from your old server with ip *12.34.45.78* for user *foo@example.org* with password *suchsecret* to your new server with ip *98.76.54.32*. A prerequisite is that on both machines the mailserver is up and running and the respective user is configured. Further, lets assume that on the new machine the user, as it makes sense, is called *foo@example.org* again, but his password is *ssshhhhh* now and that both MDAs require a *TLS*-secured connection, use standard *PLAIN* login method and are listening on *port 143*.\n\nTo perform the migration now, run the following command:\n\n```bash\n./imapsync --host1 12.34.45.78 --user1 foo@example.org --password1 suchsecret --authmech1 PLAIN --tls1 --host2 98.76.54.32 --user2 foo@example.org --password2 ssshhhhh --authmech2 PLAIN --tls2\n```\n\nNow all mails should be transferred from `host1` through the imapsync client to `host2`, using nothing but the IMAP protocol. If you want to test if everything is working fine first, before actually transferring data, you could add the `--dry` option to the above command.\n\nTo migrate multiple accounts at once, you could write a small scripts that takes username-password combinations from a text file, as described here: [https://wiki.ubuntuusers.de/imapsync/#Massenmigration](https://wiki.ubuntuusers.de/imapsync/#Massenmigration) (although that article is in German, the code should be clear).","slug":"migrate-maildir-to-new-server-using-imapsync","published":1,"updated":"2020-10-30T20:05:40.287Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqm4000po2e00sop3tgv","content":"<p>This is a little tutorial for mailserver administrators, who want to <em>migrate</em> to a new server while <em>keeping all e-mails</em>. This works for mailservers whose MDA uses the <a href=\"https://en.wikipedia.org/wiki/Maildir\">Maildir</a> format  like Dovecot by default  and have <em>IMAP</em> enabled.<br>This tutorial does <em>not</em> cover how to set up and configure a new mailserver on a new machine, based on the old ones configuration, but only how to migrate the e-mails. Simply <em>tar</em>ing the Maildir folder and un_tar_ing it on the new machine again usually wont work. But dont worry, there is a cleaner way that abstracts away any actual mailserver or file-level considerations by only using the IMAP protocols methods. Therefore, we use a tool <em>imapsync</em>, which is written Perl. It acts as an ordinary IMAP client  just as Outlook or Thunderbird  that connects to both mailservers, the old and the new one. All information needed is how to authenticate the respective user with both servers. Actually one manual way to migrate the mails would be to set up both mail accounts in Outlook or Thunderbird, let download the mails via IMAP from the old one and Ctrl+A and Drag&amp;Drop them over to the new one. imapsync does just that  yet automatically and without Outlook or Thunderbird.</p>\n<p>First we need to <em>install imapsync</em>. You could install imapsync on your local PC, just as you would with Outlook or Thunderbird, but then there would be a unnecessary detour from server 1 over your PC to server 2. And since your local internet connection is probably ways slower then the servers, your PC would be a bottleneck. So I recommend to install imapsync on either the old or the new mailservers host machine. Lets do it.</p>\n<ol>\n<li><p> Clone the imapsync repository to any folder on your machine, e.g. <code>/opt/imapsync</code>: <code>git clone https://github.com/imapsync/imapsync</code></p>\n</li>\n<li><p> Read the installation notes for your specific operation system at <a href=\"https://github.com/imapsync/imapsync/tree/master/INSTALL.d\">https://github.com/imapsync/imapsync/tree/master/INSTALL.d</a> and do exactly whats described there. Usually, you will need to install some dependencies and the like.</p>\n</li>\n<li><p> Now you should be able to execute <code>./imapsync</code> from within the directory where you have cloned it to, e.g. <code>/opt/imapsync</code>. You should see a description on how to use the program.</p>\n</li>\n</ol>\n<p>Lets now assume that you want to migrate mails from your old server with ip <em>12.34.45.78</em> for user <em><a href=\"mailto:&#102;&#111;&#x6f;&#x40;&#101;&#x78;&#97;&#x6d;&#112;&#108;&#101;&#46;&#111;&#x72;&#103;\">&#102;&#111;&#x6f;&#x40;&#101;&#x78;&#97;&#x6d;&#112;&#108;&#101;&#46;&#111;&#x72;&#103;</a></em> with password <em>suchsecret</em> to your new server with ip <em>98.76.54.32</em>. A prerequisite is that on both machines the mailserver is up and running and the respective user is configured. Further, lets assume that on the new machine the user, as it makes sense, is called <em><a href=\"mailto:&#102;&#x6f;&#111;&#64;&#x65;&#x78;&#97;&#x6d;&#x70;&#x6c;&#101;&#x2e;&#x6f;&#x72;&#103;\">&#102;&#x6f;&#111;&#64;&#x65;&#x78;&#97;&#x6d;&#x70;&#x6c;&#101;&#x2e;&#x6f;&#x72;&#103;</a></em> again, but his password is <em>ssshhhhh</em> now and that both MDAs require a <em>TLS</em>-secured connection, use standard <em>PLAIN</em> login method and are listening on <em>port 143</em>.</p>\n<p>To perform the migration now, run the following command:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./imapsync --host1 12.34.45.78 --user1 foo@example.org --password1 suchsecret --authmech1 PLAIN --tls1 --host2 98.76.54.32 --user2 foo@example.org --password2 ssshhhhh --authmech2 PLAIN --tls2</span><br></pre></td></tr></table></figure>\n\n<p>Now all mails should be transferred from <code>host1</code> through the imapsync client to <code>host2</code>, using nothing but the IMAP protocol. If you want to test if everything is working fine first, before actually transferring data, you could add the <code>--dry</code> option to the above command.</p>\n<p>To migrate multiple accounts at once, you could write a small scripts that takes username-password combinations from a text file, as described here: <a href=\"https://wiki.ubuntuusers.de/imapsync/#Massenmigration\">https://wiki.ubuntuusers.de/imapsync/#Massenmigration</a> (although that article is in German, the code should be clear).</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>This is a little tutorial for mailserver administrators, who want to <em>migrate</em> to a new server while <em>keeping all e-mails</em>. This works for mailservers whose MDA uses the <a href=\"https://en.wikipedia.org/wiki/Maildir\">Maildir</a> format  like Dovecot by default  and have <em>IMAP</em> enabled.<br>This tutorial does <em>not</em> cover how to set up and configure a new mailserver on a new machine, based on the old ones configuration, but only how to migrate the e-mails. Simply <em>tar</em>ing the Maildir folder and un_tar_ing it on the new machine again usually wont work. But dont worry, there is a cleaner way that abstracts away any actual mailserver or file-level considerations by only using the IMAP protocols methods. Therefore, we use a tool <em>imapsync</em>, which is written Perl. It acts as an ordinary IMAP client  just as Outlook or Thunderbird  that connects to both mailservers, the old and the new one. All information needed is how to authenticate the respective user with both servers. Actually one manual way to migrate the mails would be to set up both mail accounts in Outlook or Thunderbird, let download the mails via IMAP from the old one and Ctrl+A and Drag&amp;Drop them over to the new one. imapsync does just that  yet automatically and without Outlook or Thunderbird.</p>\n<p>First we need to <em>install imapsync</em>. You could install imapsync on your local PC, just as you would with Outlook or Thunderbird, but then there would be a unnecessary detour from server 1 over your PC to server 2. And since your local internet connection is probably ways slower then the servers, your PC would be a bottleneck. So I recommend to install imapsync on either the old or the new mailservers host machine. Lets do it.</p>\n<ol>\n<li><p> Clone the imapsync repository to any folder on your machine, e.g. <code>/opt/imapsync</code>: <code>git clone https://github.com/imapsync/imapsync</code></p>\n</li>\n<li><p> Read the installation notes for your specific operation system at <a href=\"https://github.com/imapsync/imapsync/tree/master/INSTALL.d\">https://github.com/imapsync/imapsync/tree/master/INSTALL.d</a> and do exactly whats described there. Usually, you will need to install some dependencies and the like.</p>\n</li>\n<li><p> Now you should be able to execute <code>./imapsync</code> from within the directory where you have cloned it to, e.g. <code>/opt/imapsync</code>. You should see a description on how to use the program.</p>\n</li>\n</ol>\n<p>Lets now assume that you want to migrate mails from your old server with ip <em>12.34.45.78</em> for user <em><a href=\"mailto:&#102;&#111;&#x6f;&#x40;&#101;&#x78;&#97;&#x6d;&#112;&#108;&#101;&#46;&#111;&#x72;&#103;\">&#102;&#111;&#x6f;&#x40;&#101;&#x78;&#97;&#x6d;&#112;&#108;&#101;&#46;&#111;&#x72;&#103;</a></em> with password <em>suchsecret</em> to your new server with ip <em>98.76.54.32</em>. A prerequisite is that on both machines the mailserver is up and running and the respective user is configured. Further, lets assume that on the new machine the user, as it makes sense, is called <em><a href=\"mailto:&#102;&#x6f;&#111;&#64;&#x65;&#x78;&#97;&#x6d;&#x70;&#x6c;&#101;&#x2e;&#x6f;&#x72;&#103;\">&#102;&#x6f;&#111;&#64;&#x65;&#x78;&#97;&#x6d;&#x70;&#x6c;&#101;&#x2e;&#x6f;&#x72;&#103;</a></em> again, but his password is <em>ssshhhhh</em> now and that both MDAs require a <em>TLS</em>-secured connection, use standard <em>PLAIN</em> login method and are listening on <em>port 143</em>.</p>\n<p>To perform the migration now, run the following command:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./imapsync --host1 12.34.45.78 --user1 foo@example.org --password1 suchsecret --authmech1 PLAIN --tls1 --host2 98.76.54.32 --user2 foo@example.org --password2 ssshhhhh --authmech2 PLAIN --tls2</span><br></pre></td></tr></table></figure>\n\n<p>Now all mails should be transferred from <code>host1</code> through the imapsync client to <code>host2</code>, using nothing but the IMAP protocol. If you want to test if everything is working fine first, before actually transferring data, you could add the <code>--dry</code> option to the above command.</p>\n<p>To migrate multiple accounts at once, you could write a small scripts that takes username-password combinations from a text file, as described here: <a href=\"https://wiki.ubuntuusers.de/imapsync/#Massenmigration\">https://wiki.ubuntuusers.de/imapsync/#Massenmigration</a> (although that article is in German, the code should be clear).</p>\n"},{"title":"Migrating Nextcloud from SQLite to MySQL with Docker","date":"2020-10-02T16:05:15.000Z","_content":"\n# Introduction\nI bet there are a ton of tutorial like this out there already and also the Nextcloud documentations are brilliant themselves. However, I want to quickly document the process of migrating a Nextcloud 19 instance from SQLite to MySQL as a database backend.\n\nPlease refer to the [official guide](https://docs.nextcloud.com/server/18/admin_manual/configuration_database/db_conversion.html) for further information.\n\n# Setup\nMy starting situation is two existing, running Docker containers, one for Nextcloud ([`nextcloud:19`](https://hub.docker.com/_/nextcloud)) and one for a MySQL server ([`mysql:5.7`](https://hub.docker.com/_/mysql)), which I also use for other applications as well. \n\n```\n$ docker ps\n\n3bea098afb11        nextcloud:19                 \"/entrypoint.sh apac\"   6 weeks ago         Up 7 days           127.0.0.1:9000->80/tcp                      nextcloud\na140a0ba21d3        mysql:5.7                    \"docker-entrypoint.s\"   11 months ago       Up 8 days           127.0.0.1:3306->3306/tcp, 33060/tcp         mysql\n```\n\n# Migrating\n## Creating a new database\nFirst, a new database is needed, which can later be filled by Nextcloud. To do so, use the `mysql` command-line client provided by the Docker container to interactively create a new database and a corresponding user. \n\n```bash\n$ docker exec -it nextcloud mysql -u root -p;\n```\n\nAfter typing your `root` password, you're logged in to the interactive SQL console. Run the following queries.\n\n```sql\nCREATE DATABASE 'nextcloud';\nCREATE USER 'nextcloud_user'@'%' IDENTIFIED BY 'some-secret-password';\nGRANT ALL PRIVILEGES ON nextcloud.* TO 'nextcloud_user'@'%';\nALTER DATABASE nextcloud CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;\nFLUSH PRIVILEGES;\n```\n\nAs Nextcloud's docs say, you need to explicitly [enable MySQL 4-byte UTF-8 support](https://docs.nextcloud.com/server/18/admin_manual/configuration_database/mysql_4byte_support.html). This is done by the last of the above queries.\n\n## Configuring a Docker network\nIf you are using Docker, your Nextcloud container needs a way to talk to your database container. To enable this, they both need to be in the same virtual network. The following shows how to create a new network (called `mysql`) and join both containers to it.\n\n```bash\n$ docker network create mysql\n$ docker network connect mysql mysql\n$ docker network connect mysql nextcloud\n```\n\nAssuming the database container has the name `mysql`, like in this example, the Nextcloud container can now reference through simply using `mysql` as a DNS name.\n\n## Migrating data\nAs part of the second steps, Nextcloud's excellent `occ` command-line tool comes to play. It essentially does all the heavy lifting for you. Simply follow these steps.\n\nNote: If you are not using Docker, but a native Nextcloud installation, simply leave out the `docker exec` command and run `php occ` directly.\n\nFirst, we want to turn maintenance mode on, to \"freeze\" the data.\n```bash\n$ docker exec -it -u www-data nextcloud php occ maintenance:mode --on\n```\n\nNext, we need to do the server-side part of the above mentioned migration to 4-byte UTF-8 support.\n```bash\n$ docker exec -it -u www-data nextcloud php occ config:system:set mysql.utf8mb4 --type boolean --value=\"true\"\n$ docker exec -it -u www-data nextcloud php occ maintenance:repair\n```\n\nNow, we can perform the actual migration from SQLite to MySQL. This may take a while, depending on the size of your database. Luckily, mine was only about 27 MB in size.\n\n```bash\n$ docker exec -it -u www-data nextcloud php occ db:convert-type --all-apps --clear-schema mysql nextcloud_user mysql nextcloud\n```\n\nEventually, turn off maintenance mode again.\n```bash\n$ docker exec -it -u www-data nextcloud php occ maintenance:mode --off\n```\n\nIf everything went well, Nextcloud has updated its own config to use the MySQL database instead of SQLite. You can check this at http://your-nextcloud-server.tld/settings/admin/serverinfo.\n\n![](https://apps.muetsch.io/images/o:auto/?image=https://muetsch.io/images/nextcloud_migration.png)\n\n# Conclusion\n\nThe above process helped me  thanks to the great tooling and documentation provided by the Nextcloud community  to migrate my Nextcloud instance from the \"slow\" SQLite database engine to MySQL. I hope it works for you as well.\n\nGood luck!\n\n# P.S.\nPlease keep in mind that the start command for your Nextcloud container is different now. For instance, if you want to update your container, you have to pass different parameters than before. Here is what works for me:\n\n```bash\n$ docker run -d -v /var/data/nextcloud:/var/www/html -v /var/data/nextcloud/data/:/var/www/html/data -p 127.0.0.1:9000:80 --network mysql -e MYSQL_DATABASE=nextcloud -e MYSQL_USER=nextcloud_user -e MYSQL_PASSWORD=iwonttellyouthis -e MYSQL_HOST=mysql --name nextcloud nextcloud:19\n```\n\n(An even better practice would be to not use file system mounts but actual Docker volumes here.)","source":"_posts/migrating-nextcloud-from-sqlite-to-mysql-with-docker.md","raw":"---\ntitle: Migrating Nextcloud from SQLite to MySQL with Docker\ndate: 2020-10-02 18:05:15\ntags:\n---\n\n# Introduction\nI bet there are a ton of tutorial like this out there already and also the Nextcloud documentations are brilliant themselves. However, I want to quickly document the process of migrating a Nextcloud 19 instance from SQLite to MySQL as a database backend.\n\nPlease refer to the [official guide](https://docs.nextcloud.com/server/18/admin_manual/configuration_database/db_conversion.html) for further information.\n\n# Setup\nMy starting situation is two existing, running Docker containers, one for Nextcloud ([`nextcloud:19`](https://hub.docker.com/_/nextcloud)) and one for a MySQL server ([`mysql:5.7`](https://hub.docker.com/_/mysql)), which I also use for other applications as well. \n\n```\n$ docker ps\n\n3bea098afb11        nextcloud:19                 \"/entrypoint.sh apac\"   6 weeks ago         Up 7 days           127.0.0.1:9000->80/tcp                      nextcloud\na140a0ba21d3        mysql:5.7                    \"docker-entrypoint.s\"   11 months ago       Up 8 days           127.0.0.1:3306->3306/tcp, 33060/tcp         mysql\n```\n\n# Migrating\n## Creating a new database\nFirst, a new database is needed, which can later be filled by Nextcloud. To do so, use the `mysql` command-line client provided by the Docker container to interactively create a new database and a corresponding user. \n\n```bash\n$ docker exec -it nextcloud mysql -u root -p;\n```\n\nAfter typing your `root` password, you're logged in to the interactive SQL console. Run the following queries.\n\n```sql\nCREATE DATABASE 'nextcloud';\nCREATE USER 'nextcloud_user'@'%' IDENTIFIED BY 'some-secret-password';\nGRANT ALL PRIVILEGES ON nextcloud.* TO 'nextcloud_user'@'%';\nALTER DATABASE nextcloud CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;\nFLUSH PRIVILEGES;\n```\n\nAs Nextcloud's docs say, you need to explicitly [enable MySQL 4-byte UTF-8 support](https://docs.nextcloud.com/server/18/admin_manual/configuration_database/mysql_4byte_support.html). This is done by the last of the above queries.\n\n## Configuring a Docker network\nIf you are using Docker, your Nextcloud container needs a way to talk to your database container. To enable this, they both need to be in the same virtual network. The following shows how to create a new network (called `mysql`) and join both containers to it.\n\n```bash\n$ docker network create mysql\n$ docker network connect mysql mysql\n$ docker network connect mysql nextcloud\n```\n\nAssuming the database container has the name `mysql`, like in this example, the Nextcloud container can now reference through simply using `mysql` as a DNS name.\n\n## Migrating data\nAs part of the second steps, Nextcloud's excellent `occ` command-line tool comes to play. It essentially does all the heavy lifting for you. Simply follow these steps.\n\nNote: If you are not using Docker, but a native Nextcloud installation, simply leave out the `docker exec` command and run `php occ` directly.\n\nFirst, we want to turn maintenance mode on, to \"freeze\" the data.\n```bash\n$ docker exec -it -u www-data nextcloud php occ maintenance:mode --on\n```\n\nNext, we need to do the server-side part of the above mentioned migration to 4-byte UTF-8 support.\n```bash\n$ docker exec -it -u www-data nextcloud php occ config:system:set mysql.utf8mb4 --type boolean --value=\"true\"\n$ docker exec -it -u www-data nextcloud php occ maintenance:repair\n```\n\nNow, we can perform the actual migration from SQLite to MySQL. This may take a while, depending on the size of your database. Luckily, mine was only about 27 MB in size.\n\n```bash\n$ docker exec -it -u www-data nextcloud php occ db:convert-type --all-apps --clear-schema mysql nextcloud_user mysql nextcloud\n```\n\nEventually, turn off maintenance mode again.\n```bash\n$ docker exec -it -u www-data nextcloud php occ maintenance:mode --off\n```\n\nIf everything went well, Nextcloud has updated its own config to use the MySQL database instead of SQLite. You can check this at http://your-nextcloud-server.tld/settings/admin/serverinfo.\n\n![](https://apps.muetsch.io/images/o:auto/?image=https://muetsch.io/images/nextcloud_migration.png)\n\n# Conclusion\n\nThe above process helped me  thanks to the great tooling and documentation provided by the Nextcloud community  to migrate my Nextcloud instance from the \"slow\" SQLite database engine to MySQL. I hope it works for you as well.\n\nGood luck!\n\n# P.S.\nPlease keep in mind that the start command for your Nextcloud container is different now. For instance, if you want to update your container, you have to pass different parameters than before. Here is what works for me:\n\n```bash\n$ docker run -d -v /var/data/nextcloud:/var/www/html -v /var/data/nextcloud/data/:/var/www/html/data -p 127.0.0.1:9000:80 --network mysql -e MYSQL_DATABASE=nextcloud -e MYSQL_USER=nextcloud_user -e MYSQL_PASSWORD=iwonttellyouthis -e MYSQL_HOST=mysql --name nextcloud nextcloud:19\n```\n\n(An even better practice would be to not use file system mounts but actual Docker volumes here.)","slug":"migrating-nextcloud-from-sqlite-to-mysql-with-docker","published":1,"updated":"2020-10-30T20:05:40.291Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqm4000qo2e0hes51chd","content":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>I bet there are a ton of tutorial like this out there already and also the Nextcloud documentations are brilliant themselves. However, I want to quickly document the process of migrating a Nextcloud 19 instance from SQLite to MySQL as a database backend.</p>\n<p>Please refer to the <a href=\"https://docs.nextcloud.com/server/18/admin_manual/configuration_database/db_conversion.html\">official guide</a> for further information.</p>\n<h1 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h1><p>My starting situation is two existing, running Docker containers, one for Nextcloud (<a href=\"https://hub.docker.com/_/nextcloud\"><code>nextcloud:19</code></a>) and one for a MySQL server (<a href=\"https://hub.docker.com/_/mysql\"><code>mysql:5.7</code></a>), which I also use for other applications as well. </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker ps</span><br><span class=\"line\"></span><br><span class=\"line\">3bea098afb11        nextcloud:19                 &quot;&#x2F;entrypoint.sh apac&quot;   6 weeks ago         Up 7 days           127.0.0.1:9000-&gt;80&#x2F;tcp                      nextcloud</span><br><span class=\"line\">a140a0ba21d3        mysql:5.7                    &quot;docker-entrypoint.s&quot;   11 months ago       Up 8 days           127.0.0.1:3306-&gt;3306&#x2F;tcp, 33060&#x2F;tcp         mysql</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Migrating\"><a href=\"#Migrating\" class=\"headerlink\" title=\"Migrating\"></a>Migrating</h1><h2 id=\"Creating-a-new-database\"><a href=\"#Creating-a-new-database\" class=\"headerlink\" title=\"Creating a new database\"></a>Creating a new database</h2><p>First, a new database is needed, which can later be filled by Nextcloud. To do so, use the <code>mysql</code> command-line client provided by the Docker container to interactively create a new database and a corresponding user. </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker <span class=\"built_in\">exec</span> -it nextcloud mysql -u root -p;</span><br></pre></td></tr></table></figure>\n\n<p>After typing your <code>root</code> password, youre logged in to the interactive SQL console. Run the following queries.</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">DATABASE</span> <span class=\"string\">&#x27;nextcloud&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">USER</span> <span class=\"string\">&#x27;nextcloud_user&#x27;</span>@<span class=\"string\">&#x27;%&#x27;</span> <span class=\"keyword\">IDENTIFIED</span> <span class=\"keyword\">BY</span> <span class=\"string\">&#x27;some-secret-password&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">GRANT</span> <span class=\"keyword\">ALL</span> <span class=\"keyword\">PRIVILEGES</span> <span class=\"keyword\">ON</span> nextcloud.* <span class=\"keyword\">TO</span> <span class=\"string\">&#x27;nextcloud_user&#x27;</span>@<span class=\"string\">&#x27;%&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">DATABASE</span> nextcloud <span class=\"built_in\">CHARACTER</span> <span class=\"keyword\">SET</span> utf8mb4 <span class=\"keyword\">COLLATE</span> utf8mb4_general_ci;</span><br><span class=\"line\"><span class=\"keyword\">FLUSH</span> <span class=\"keyword\">PRIVILEGES</span>;</span><br></pre></td></tr></table></figure>\n\n<p>As Nextclouds docs say, you need to explicitly <a href=\"https://docs.nextcloud.com/server/18/admin_manual/configuration_database/mysql_4byte_support.html\">enable MySQL 4-byte UTF-8 support</a>. This is done by the last of the above queries.</p>\n<h2 id=\"Configuring-a-Docker-network\"><a href=\"#Configuring-a-Docker-network\" class=\"headerlink\" title=\"Configuring a Docker network\"></a>Configuring a Docker network</h2><p>If you are using Docker, your Nextcloud container needs a way to talk to your database container. To enable this, they both need to be in the same virtual network. The following shows how to create a new network (called <code>mysql</code>) and join both containers to it.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker network create mysql</span><br><span class=\"line\">$ docker network connect mysql mysql</span><br><span class=\"line\">$ docker network connect mysql nextcloud</span><br></pre></td></tr></table></figure>\n\n<p>Assuming the database container has the name <code>mysql</code>, like in this example, the Nextcloud container can now reference through simply using <code>mysql</code> as a DNS name.</p>\n<h2 id=\"Migrating-data\"><a href=\"#Migrating-data\" class=\"headerlink\" title=\"Migrating data\"></a>Migrating data</h2><p>As part of the second steps, Nextclouds excellent <code>occ</code> command-line tool comes to play. It essentially does all the heavy lifting for you. Simply follow these steps.</p>\n<p>Note: If you are not using Docker, but a native Nextcloud installation, simply leave out the <code>docker exec</code> command and run <code>php occ</code> directly.</p>\n<p>First, we want to turn maintenance mode on, to freeze the data.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker <span class=\"built_in\">exec</span> -it -u www-data nextcloud php occ maintenance:mode --on</span><br></pre></td></tr></table></figure>\n\n<p>Next, we need to do the server-side part of the above mentioned migration to 4-byte UTF-8 support.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker <span class=\"built_in\">exec</span> -it -u www-data nextcloud php occ config:system:<span class=\"built_in\">set</span> mysql.utf8mb4 --<span class=\"built_in\">type</span> boolean --value=<span class=\"string\">&quot;true&quot;</span></span><br><span class=\"line\">$ docker <span class=\"built_in\">exec</span> -it -u www-data nextcloud php occ maintenance:repair</span><br></pre></td></tr></table></figure>\n\n<p>Now, we can perform the actual migration from SQLite to MySQL. This may take a while, depending on the size of your database. Luckily, mine was only about 27 MB in size.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker <span class=\"built_in\">exec</span> -it -u www-data nextcloud php occ db:convert-type --all-apps --clear-schema mysql nextcloud_user mysql nextcloud</span><br></pre></td></tr></table></figure>\n\n<p>Eventually, turn off maintenance mode again.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker <span class=\"built_in\">exec</span> -it -u www-data nextcloud php occ maintenance:mode --off</span><br></pre></td></tr></table></figure>\n\n<p>If everything went well, Nextcloud has updated its own config to use the MySQL database instead of SQLite. You can check this at <a href=\"http://your-nextcloud-server.tld/settings/admin/serverinfo\">http://your-nextcloud-server.tld/settings/admin/serverinfo</a>.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto/?image=https://muetsch.io/images/nextcloud_migration.png\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>The above process helped me  thanks to the great tooling and documentation provided by the Nextcloud community  to migrate my Nextcloud instance from the slow SQLite database engine to MySQL. I hope it works for you as well.</p>\n<p>Good luck!</p>\n<h1 id=\"P-S\"><a href=\"#P-S\" class=\"headerlink\" title=\"P.S.\"></a>P.S.</h1><p>Please keep in mind that the start command for your Nextcloud container is different now. For instance, if you want to update your container, you have to pass different parameters than before. Here is what works for me:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d -v /var/data/nextcloud:/var/www/html -v /var/data/nextcloud/data/:/var/www/html/data -p 127.0.0.1:9000:80 --network mysql -e MYSQL_DATABASE=nextcloud -e MYSQL_USER=nextcloud_user -e MYSQL_PASSWORD=iwonttellyouthis -e MYSQL_HOST=mysql --name nextcloud nextcloud:19</span><br></pre></td></tr></table></figure>\n\n<p>(An even better practice would be to not use file system mounts but actual Docker volumes here.)</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>I bet there are a ton of tutorial like this out there already and also the Nextcloud documentations are brilliant themselves. However, I want to quickly document the process of migrating a Nextcloud 19 instance from SQLite to MySQL as a database backend.</p>\n<p>Please refer to the <a href=\"https://docs.nextcloud.com/server/18/admin_manual/configuration_database/db_conversion.html\">official guide</a> for further information.</p>\n<h1 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h1><p>My starting situation is two existing, running Docker containers, one for Nextcloud (<a href=\"https://hub.docker.com/_/nextcloud\"><code>nextcloud:19</code></a>) and one for a MySQL server (<a href=\"https://hub.docker.com/_/mysql\"><code>mysql:5.7</code></a>), which I also use for other applications as well. </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker ps</span><br><span class=\"line\"></span><br><span class=\"line\">3bea098afb11        nextcloud:19                 &quot;&#x2F;entrypoint.sh apac&quot;   6 weeks ago         Up 7 days           127.0.0.1:9000-&gt;80&#x2F;tcp                      nextcloud</span><br><span class=\"line\">a140a0ba21d3        mysql:5.7                    &quot;docker-entrypoint.s&quot;   11 months ago       Up 8 days           127.0.0.1:3306-&gt;3306&#x2F;tcp, 33060&#x2F;tcp         mysql</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Migrating\"><a href=\"#Migrating\" class=\"headerlink\" title=\"Migrating\"></a>Migrating</h1><h2 id=\"Creating-a-new-database\"><a href=\"#Creating-a-new-database\" class=\"headerlink\" title=\"Creating a new database\"></a>Creating a new database</h2><p>First, a new database is needed, which can later be filled by Nextcloud. To do so, use the <code>mysql</code> command-line client provided by the Docker container to interactively create a new database and a corresponding user. </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker <span class=\"built_in\">exec</span> -it nextcloud mysql -u root -p;</span><br></pre></td></tr></table></figure>\n\n<p>After typing your <code>root</code> password, youre logged in to the interactive SQL console. Run the following queries.</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">DATABASE</span> <span class=\"string\">&#x27;nextcloud&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">USER</span> <span class=\"string\">&#x27;nextcloud_user&#x27;</span>@<span class=\"string\">&#x27;%&#x27;</span> <span class=\"keyword\">IDENTIFIED</span> <span class=\"keyword\">BY</span> <span class=\"string\">&#x27;some-secret-password&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">GRANT</span> <span class=\"keyword\">ALL</span> <span class=\"keyword\">PRIVILEGES</span> <span class=\"keyword\">ON</span> nextcloud.* <span class=\"keyword\">TO</span> <span class=\"string\">&#x27;nextcloud_user&#x27;</span>@<span class=\"string\">&#x27;%&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">ALTER</span> <span class=\"keyword\">DATABASE</span> nextcloud <span class=\"built_in\">CHARACTER</span> <span class=\"keyword\">SET</span> utf8mb4 <span class=\"keyword\">COLLATE</span> utf8mb4_general_ci;</span><br><span class=\"line\"><span class=\"keyword\">FLUSH</span> <span class=\"keyword\">PRIVILEGES</span>;</span><br></pre></td></tr></table></figure>\n\n<p>As Nextclouds docs say, you need to explicitly <a href=\"https://docs.nextcloud.com/server/18/admin_manual/configuration_database/mysql_4byte_support.html\">enable MySQL 4-byte UTF-8 support</a>. This is done by the last of the above queries.</p>\n<h2 id=\"Configuring-a-Docker-network\"><a href=\"#Configuring-a-Docker-network\" class=\"headerlink\" title=\"Configuring a Docker network\"></a>Configuring a Docker network</h2><p>If you are using Docker, your Nextcloud container needs a way to talk to your database container. To enable this, they both need to be in the same virtual network. The following shows how to create a new network (called <code>mysql</code>) and join both containers to it.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker network create mysql</span><br><span class=\"line\">$ docker network connect mysql mysql</span><br><span class=\"line\">$ docker network connect mysql nextcloud</span><br></pre></td></tr></table></figure>\n\n<p>Assuming the database container has the name <code>mysql</code>, like in this example, the Nextcloud container can now reference through simply using <code>mysql</code> as a DNS name.</p>\n<h2 id=\"Migrating-data\"><a href=\"#Migrating-data\" class=\"headerlink\" title=\"Migrating data\"></a>Migrating data</h2><p>As part of the second steps, Nextclouds excellent <code>occ</code> command-line tool comes to play. It essentially does all the heavy lifting for you. Simply follow these steps.</p>\n<p>Note: If you are not using Docker, but a native Nextcloud installation, simply leave out the <code>docker exec</code> command and run <code>php occ</code> directly.</p>\n<p>First, we want to turn maintenance mode on, to freeze the data.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker <span class=\"built_in\">exec</span> -it -u www-data nextcloud php occ maintenance:mode --on</span><br></pre></td></tr></table></figure>\n\n<p>Next, we need to do the server-side part of the above mentioned migration to 4-byte UTF-8 support.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker <span class=\"built_in\">exec</span> -it -u www-data nextcloud php occ config:system:<span class=\"built_in\">set</span> mysql.utf8mb4 --<span class=\"built_in\">type</span> boolean --value=<span class=\"string\">&quot;true&quot;</span></span><br><span class=\"line\">$ docker <span class=\"built_in\">exec</span> -it -u www-data nextcloud php occ maintenance:repair</span><br></pre></td></tr></table></figure>\n\n<p>Now, we can perform the actual migration from SQLite to MySQL. This may take a while, depending on the size of your database. Luckily, mine was only about 27 MB in size.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker <span class=\"built_in\">exec</span> -it -u www-data nextcloud php occ db:convert-type --all-apps --clear-schema mysql nextcloud_user mysql nextcloud</span><br></pre></td></tr></table></figure>\n\n<p>Eventually, turn off maintenance mode again.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker <span class=\"built_in\">exec</span> -it -u www-data nextcloud php occ maintenance:mode --off</span><br></pre></td></tr></table></figure>\n\n<p>If everything went well, Nextcloud has updated its own config to use the MySQL database instead of SQLite. You can check this at <a href=\"http://your-nextcloud-server.tld/settings/admin/serverinfo\">http://your-nextcloud-server.tld/settings/admin/serverinfo</a>.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto/?image=https://muetsch.io/images/nextcloud_migration.png\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>The above process helped me  thanks to the great tooling and documentation provided by the Nextcloud community  to migrate my Nextcloud instance from the slow SQLite database engine to MySQL. I hope it works for you as well.</p>\n<p>Good luck!</p>\n<h1 id=\"P-S\"><a href=\"#P-S\" class=\"headerlink\" title=\"P.S.\"></a>P.S.</h1><p>Please keep in mind that the start command for your Nextcloud container is different now. For instance, if you want to update your container, you have to pass different parameters than before. Here is what works for me:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker run -d -v /var/data/nextcloud:/var/www/html -v /var/data/nextcloud/data/:/var/www/html/data -p 127.0.0.1:9000:80 --network mysql -e MYSQL_DATABASE=nextcloud -e MYSQL_USER=nextcloud_user -e MYSQL_PASSWORD=iwonttellyouthis -e MYSQL_HOST=mysql --name nextcloud nextcloud:19</span><br></pre></td></tr></table></figure>\n\n<p>(An even better practice would be to not use file system mounts but actual Docker volumes here.)</p>\n"},{"title":"ML: Telegram chat message classification","date":"2017-02-28T22:10:05.000Z","_content":"\n### Intro\nFirst of all, a short disclaimer: I'm not an expert in machine learning at all. In fact I'm in a rather early stage of the learning process, have basic knowledge and this project is kind of my first practical hands-on ML. I've done the [machine learning course](https://www.youtube.com/user/UWCSE/playlists?sort=dd&shelf_id=16&view=50) by [Pedro Domingos](https://homes.cs.washington.edu/~pedrod/) at University of Washington, [Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120) by Udacity and Google and the [Machine Learning 1 lecture at Karlsruhe Institute Of Technology](https://his.anthropomatik.kit.edu/english/28_315.php), all of which I can really recommend.\nAfter having gathered all that theoretical knowledge, I wanted to try something practical on my own. I decided to learn a simple classifier for chat messages from my [Telegram](https://telegram.com) messenger history. I wanted to learn a program that can, given a chat message, tell who the sender of that message is. I further got inspired after having read the papers related to Facebook's [fastText](https://github.com/facebookresearch/fastText) text classification algorithm. In their examples they classify Wikipedia abstracts / descriptions to [DBPedia](https://dbpedia.org) classes or news article headlines to their respective news categories, only based on plain, natural words. Basically these problems are very similar to mine, so I decided to give it a try. Since I found that many text classifiers are learned using the Naive Bayes algorithm (especially popular in spam detection and part of [SpamAssassin](http://spamassassin.apache.org/)) and it's really easy to understand, I decided to go for that one, too. Inspired by [this article](http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/), where the sentiment of tweets is analyzed, I chose to also use the [natural language toolkit](http://www.nltk.org/) for Python. Another option would have been [sklearn](http://scikit-learn.org/), but NLTK also provided some useful utilities beyond the pure ML scope. \n\nAll of my __code is [available on GitHub](https://github.com/muety/tg-chat-classification/)__.\n\n### Basic Steps\n1. The very first step was to download the data, namely the chat messages. Luckily, Telegram has an open API. However it's not a classical REST API, but instead they're using the [MTProto](https://core.telegram.org/mtproto) protocol. I found [vysheng/tg](https://github.com/vysheng/tg) as a cool C++-written commandline client on GitHub as well as [tvdstaaij/telegram-history-dump](https://github.com/tvdstaaij/telegram-history-dump) as a Ruby script to automate the history download for a set of users / chat partners. I told the script (ran in a Docker container, since I didn't want to install Ruby) to fetch at max 40,000 messages for my top three chat partners (let's call them _M_, _P_ and _J_). The outcome were three [JSON Lines](http://jsonlines.org/) files.\n2. To pre-process these files as needed for my learning algorithm, I wrote a Python script that extracted only message text and sender from all incoming messages and dumped these data to a JSON file. Additionally I also extracted the same information for all outgoing messages, i.e. all messages where the sender was me. Consequently, there are four classes: __C = { _M_, _P_, _J_, _F_ }__\n3. Another data-preprocessing step was to convert the JSON objects with class names as keys for message-arrays to one large list of tuples of the form _(text, label)_, where _label_ is the name of the message's sender and _text_ is the respective message text. In this step I also discarded words with a length of less than 2 characters and converted everything to lower case.\n4. Next step was to extract the features. In text classification, there is often one binary (_contains_ / _contains not_) feature for every possible word. So if all messages in total comprise X different words, there will be a X-dimensional feature vector.\n5. Last step before actually training the classifier is to compute the feature vector for every messages. For examples if the total feature set is `['in', 'case', 'of', 'fire', 'coffee', 'we', 'trust']`, the resultung feature vector for a message _\"in coffee we trust\"_ would be `('in'=True, 'case'=False, 'of'=False, 'fire'=False, 'coffee'=True, 'we'=True, 'trust'=True)`.\n6. One more minor thing: shuffle the feature set so that the order of messages and message senders is random. Also divide the feature set into training- and test data, where test data contains about 10 % of the number of messages in the train data.\n7. Train [nltk.NaiveBayesClassifier](http://www.nltk.org/api/nltk.classify.html) classifier. This is really just one line of code.\n8. Use the returned classifier to predict classes for the test messages, validate them and compute the accuracy.\n\nUsing that basic initial setup on a set of __37257 messages__, (7931 from M, 9795 from P, 9314 from F and 10217 from J), I ended up with an __accuracy of 0.58__. There seemed to be room for optimization.\n\n### Optimizations\n* Inspired by _fastText_, I decided to include n-grams. This seemed resonable to me, because intuitively I'd say that single words a way less characteristic for a person's writing style than certain phrases. I extended the feature list from step 4 by all possible __bi- and tri-grams__, which are easy to compute with NLTK. Actually I'm not taking ALL bi- and tri-grams and I'm not even take all single words as features. Reason for that is that there were approx. 35k different words in the dataset. Plus the n-grams this would make an extremely multi-dimensional feature vector and as it turned out, it was way to complex for my 16 GB MacBook Pro to compute. Consequently, I only took the __top 5000 single words, bigrams and trigrams__, ranked descending by their overall frequency. \n* Since NLTK already provides a corpus of __stopwords__ (like \"in\", \"and\", \"of\", etc.), which are obviously not characteristic for a person's style of chatting, I decided to remove them (the German ones) from the message set in step 2.\n\nWith these optimizations, I ended up with an __accuracy of 0.61__ after a training time of 348 seconds (I didn't log testing time at that point).\n\n### Conclusion\nCertainly 61 % accuracy isn't really a good classifier, but at least significantly better than random guessing (chance of 1/4 in this case). However, I trained a __fastText__ classifier on my data as a comparison baseline and it even only reached __60 % accuracy__ (but with a much better __training time of only 0.66 seconds__). \nMy intuitive explanation for these rather bad results is the complexity of the problem itself. Given only a set of words without any context and semantics, it's not only hard for a machine to predict the message's sender but also for a human. \nMoreover, given more training data (I'd need a longer message history) and more computing power to handle larger feature sets, the accuracy might further improve slightly.\nActually, the practical relevance of this project isn't quit high anyway, but it was a good practice for me to get into the basics of ML and it's really fun!\n\nPlease leave me feedback if you like to.","source":"_posts/ml-telegram-chat-message-classification.md","raw":"---\ntitle: 'ML: Telegram chat message classification'\ndate: 2017-02-28 23:10:05\ntags:\n---\n\n### Intro\nFirst of all, a short disclaimer: I'm not an expert in machine learning at all. In fact I'm in a rather early stage of the learning process, have basic knowledge and this project is kind of my first practical hands-on ML. I've done the [machine learning course](https://www.youtube.com/user/UWCSE/playlists?sort=dd&shelf_id=16&view=50) by [Pedro Domingos](https://homes.cs.washington.edu/~pedrod/) at University of Washington, [Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120) by Udacity and Google and the [Machine Learning 1 lecture at Karlsruhe Institute Of Technology](https://his.anthropomatik.kit.edu/english/28_315.php), all of which I can really recommend.\nAfter having gathered all that theoretical knowledge, I wanted to try something practical on my own. I decided to learn a simple classifier for chat messages from my [Telegram](https://telegram.com) messenger history. I wanted to learn a program that can, given a chat message, tell who the sender of that message is. I further got inspired after having read the papers related to Facebook's [fastText](https://github.com/facebookresearch/fastText) text classification algorithm. In their examples they classify Wikipedia abstracts / descriptions to [DBPedia](https://dbpedia.org) classes or news article headlines to their respective news categories, only based on plain, natural words. Basically these problems are very similar to mine, so I decided to give it a try. Since I found that many text classifiers are learned using the Naive Bayes algorithm (especially popular in spam detection and part of [SpamAssassin](http://spamassassin.apache.org/)) and it's really easy to understand, I decided to go for that one, too. Inspired by [this article](http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/), where the sentiment of tweets is analyzed, I chose to also use the [natural language toolkit](http://www.nltk.org/) for Python. Another option would have been [sklearn](http://scikit-learn.org/), but NLTK also provided some useful utilities beyond the pure ML scope. \n\nAll of my __code is [available on GitHub](https://github.com/muety/tg-chat-classification/)__.\n\n### Basic Steps\n1. The very first step was to download the data, namely the chat messages. Luckily, Telegram has an open API. However it's not a classical REST API, but instead they're using the [MTProto](https://core.telegram.org/mtproto) protocol. I found [vysheng/tg](https://github.com/vysheng/tg) as a cool C++-written commandline client on GitHub as well as [tvdstaaij/telegram-history-dump](https://github.com/tvdstaaij/telegram-history-dump) as a Ruby script to automate the history download for a set of users / chat partners. I told the script (ran in a Docker container, since I didn't want to install Ruby) to fetch at max 40,000 messages for my top three chat partners (let's call them _M_, _P_ and _J_). The outcome were three [JSON Lines](http://jsonlines.org/) files.\n2. To pre-process these files as needed for my learning algorithm, I wrote a Python script that extracted only message text and sender from all incoming messages and dumped these data to a JSON file. Additionally I also extracted the same information for all outgoing messages, i.e. all messages where the sender was me. Consequently, there are four classes: __C = { _M_, _P_, _J_, _F_ }__\n3. Another data-preprocessing step was to convert the JSON objects with class names as keys for message-arrays to one large list of tuples of the form _(text, label)_, where _label_ is the name of the message's sender and _text_ is the respective message text. In this step I also discarded words with a length of less than 2 characters and converted everything to lower case.\n4. Next step was to extract the features. In text classification, there is often one binary (_contains_ / _contains not_) feature for every possible word. So if all messages in total comprise X different words, there will be a X-dimensional feature vector.\n5. Last step before actually training the classifier is to compute the feature vector for every messages. For examples if the total feature set is `['in', 'case', 'of', 'fire', 'coffee', 'we', 'trust']`, the resultung feature vector for a message _\"in coffee we trust\"_ would be `('in'=True, 'case'=False, 'of'=False, 'fire'=False, 'coffee'=True, 'we'=True, 'trust'=True)`.\n6. One more minor thing: shuffle the feature set so that the order of messages and message senders is random. Also divide the feature set into training- and test data, where test data contains about 10 % of the number of messages in the train data.\n7. Train [nltk.NaiveBayesClassifier](http://www.nltk.org/api/nltk.classify.html) classifier. This is really just one line of code.\n8. Use the returned classifier to predict classes for the test messages, validate them and compute the accuracy.\n\nUsing that basic initial setup on a set of __37257 messages__, (7931 from M, 9795 from P, 9314 from F and 10217 from J), I ended up with an __accuracy of 0.58__. There seemed to be room for optimization.\n\n### Optimizations\n* Inspired by _fastText_, I decided to include n-grams. This seemed resonable to me, because intuitively I'd say that single words a way less characteristic for a person's writing style than certain phrases. I extended the feature list from step 4 by all possible __bi- and tri-grams__, which are easy to compute with NLTK. Actually I'm not taking ALL bi- and tri-grams and I'm not even take all single words as features. Reason for that is that there were approx. 35k different words in the dataset. Plus the n-grams this would make an extremely multi-dimensional feature vector and as it turned out, it was way to complex for my 16 GB MacBook Pro to compute. Consequently, I only took the __top 5000 single words, bigrams and trigrams__, ranked descending by their overall frequency. \n* Since NLTK already provides a corpus of __stopwords__ (like \"in\", \"and\", \"of\", etc.), which are obviously not characteristic for a person's style of chatting, I decided to remove them (the German ones) from the message set in step 2.\n\nWith these optimizations, I ended up with an __accuracy of 0.61__ after a training time of 348 seconds (I didn't log testing time at that point).\n\n### Conclusion\nCertainly 61 % accuracy isn't really a good classifier, but at least significantly better than random guessing (chance of 1/4 in this case). However, I trained a __fastText__ classifier on my data as a comparison baseline and it even only reached __60 % accuracy__ (but with a much better __training time of only 0.66 seconds__). \nMy intuitive explanation for these rather bad results is the complexity of the problem itself. Given only a set of words without any context and semantics, it's not only hard for a machine to predict the message's sender but also for a human. \nMoreover, given more training data (I'd need a longer message history) and more computing power to handle larger feature sets, the accuracy might further improve slightly.\nActually, the practical relevance of this project isn't quit high anyway, but it was a good practice for me to get into the basics of ML and it's really fun!\n\nPlease leave me feedback if you like to.","slug":"ml-telegram-chat-message-classification","published":1,"updated":"2020-10-30T20:05:40.288Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqm5000ro2e028zq6lrs","content":"<h3 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h3><p>First of all, a short disclaimer: Im not an expert in machine learning at all. In fact Im in a rather early stage of the learning process, have basic knowledge and this project is kind of my first practical hands-on ML. Ive done the <a href=\"https://www.youtube.com/user/UWCSE/playlists?sort=dd&shelf_id=16&view=50\">machine learning course</a> by <a href=\"https://homes.cs.washington.edu/~pedrod/\">Pedro Domingos</a> at University of Washington, <a href=\"https://www.udacity.com/course/intro-to-machine-learning--ud120\">Intro to Machine Learning</a> by Udacity and Google and the <a href=\"https://his.anthropomatik.kit.edu/english/28_315.php\">Machine Learning 1 lecture at Karlsruhe Institute Of Technology</a>, all of which I can really recommend.<br>After having gathered all that theoretical knowledge, I wanted to try something practical on my own. I decided to learn a simple classifier for chat messages from my <a href=\"https://telegram.com/\">Telegram</a> messenger history. I wanted to learn a program that can, given a chat message, tell who the sender of that message is. I further got inspired after having read the papers related to Facebooks <a href=\"https://github.com/facebookresearch/fastText\">fastText</a> text classification algorithm. In their examples they classify Wikipedia abstracts / descriptions to <a href=\"https://dbpedia.org/\">DBPedia</a> classes or news article headlines to their respective news categories, only based on plain, natural words. Basically these problems are very similar to mine, so I decided to give it a try. Since I found that many text classifiers are learned using the Naive Bayes algorithm (especially popular in spam detection and part of <a href=\"http://spamassassin.apache.org/\">SpamAssassin</a>) and its really easy to understand, I decided to go for that one, too. Inspired by <a href=\"http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/\">this article</a>, where the sentiment of tweets is analyzed, I chose to also use the <a href=\"http://www.nltk.org/\">natural language toolkit</a> for Python. Another option would have been <a href=\"http://scikit-learn.org/\">sklearn</a>, but NLTK also provided some useful utilities beyond the pure ML scope. </p>\n<p>All of my <strong>code is <a href=\"https://github.com/muety/tg-chat-classification/\">available on GitHub</a></strong>.</p>\n<h3 id=\"Basic-Steps\"><a href=\"#Basic-Steps\" class=\"headerlink\" title=\"Basic Steps\"></a>Basic Steps</h3><ol>\n<li>The very first step was to download the data, namely the chat messages. Luckily, Telegram has an open API. However its not a classical REST API, but instead theyre using the <a href=\"https://core.telegram.org/mtproto\">MTProto</a> protocol. I found <a href=\"https://github.com/vysheng/tg\">vysheng/tg</a> as a cool C++-written commandline client on GitHub as well as <a href=\"https://github.com/tvdstaaij/telegram-history-dump\">tvdstaaij/telegram-history-dump</a> as a Ruby script to automate the history download for a set of users / chat partners. I told the script (ran in a Docker container, since I didnt want to install Ruby) to fetch at max 40,000 messages for my top three chat partners (lets call them <em>M</em>, <em>P</em> and <em>J</em>). The outcome were three <a href=\"http://jsonlines.org/\">JSON Lines</a> files.</li>\n<li>To pre-process these files as needed for my learning algorithm, I wrote a Python script that extracted only message text and sender from all incoming messages and dumped these data to a JSON file. Additionally I also extracted the same information for all outgoing messages, i.e. all messages where the sender was me. Consequently, there are four classes: <strong>C = { <em>M</em>, <em>P</em>, <em>J</em>, <em>F</em> }</strong></li>\n<li>Another data-preprocessing step was to convert the JSON objects with class names as keys for message-arrays to one large list of tuples of the form <em>(text, label)</em>, where <em>label</em> is the name of the messages sender and <em>text</em> is the respective message text. In this step I also discarded words with a length of less than 2 characters and converted everything to lower case.</li>\n<li>Next step was to extract the features. In text classification, there is often one binary (<em>contains</em> / <em>contains not</em>) feature for every possible word. So if all messages in total comprise X different words, there will be a X-dimensional feature vector.</li>\n<li>Last step before actually training the classifier is to compute the feature vector for every messages. For examples if the total feature set is <code>[&#39;in&#39;, &#39;case&#39;, &#39;of&#39;, &#39;fire&#39;, &#39;coffee&#39;, &#39;we&#39;, &#39;trust&#39;]</code>, the resultung feature vector for a message <em>in coffee we trust</em> would be <code>(&#39;in&#39;=True, &#39;case&#39;=False, &#39;of&#39;=False, &#39;fire&#39;=False, &#39;coffee&#39;=True, &#39;we&#39;=True, &#39;trust&#39;=True)</code>.</li>\n<li>One more minor thing: shuffle the feature set so that the order of messages and message senders is random. Also divide the feature set into training- and test data, where test data contains about 10 % of the number of messages in the train data.</li>\n<li>Train <a href=\"http://www.nltk.org/api/nltk.classify.html\">nltk.NaiveBayesClassifier</a> classifier. This is really just one line of code.</li>\n<li>Use the returned classifier to predict classes for the test messages, validate them and compute the accuracy.</li>\n</ol>\n<p>Using that basic initial setup on a set of <strong>37257 messages</strong>, (7931 from M, 9795 from P, 9314 from F and 10217 from J), I ended up with an <strong>accuracy of 0.58</strong>. There seemed to be room for optimization.</p>\n<h3 id=\"Optimizations\"><a href=\"#Optimizations\" class=\"headerlink\" title=\"Optimizations\"></a>Optimizations</h3><ul>\n<li>Inspired by <em>fastText</em>, I decided to include n-grams. This seemed resonable to me, because intuitively Id say that single words a way less characteristic for a persons writing style than certain phrases. I extended the feature list from step 4 by all possible <strong>bi- and tri-grams</strong>, which are easy to compute with NLTK. Actually Im not taking ALL bi- and tri-grams and Im not even take all single words as features. Reason for that is that there were approx. 35k different words in the dataset. Plus the n-grams this would make an extremely multi-dimensional feature vector and as it turned out, it was way to complex for my 16 GB MacBook Pro to compute. Consequently, I only took the <strong>top 5000 single words, bigrams and trigrams</strong>, ranked descending by their overall frequency. </li>\n<li>Since NLTK already provides a corpus of <strong>stopwords</strong> (like in, and, of, etc.), which are obviously not characteristic for a persons style of chatting, I decided to remove them (the German ones) from the message set in step 2.</li>\n</ul>\n<p>With these optimizations, I ended up with an <strong>accuracy of 0.61</strong> after a training time of 348 seconds (I didnt log testing time at that point).</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>Certainly 61 % accuracy isnt really a good classifier, but at least significantly better than random guessing (chance of 1/4 in this case). However, I trained a <strong>fastText</strong> classifier on my data as a comparison baseline and it even only reached <strong>60 % accuracy</strong> (but with a much better <strong>training time of only 0.66 seconds</strong>).<br>My intuitive explanation for these rather bad results is the complexity of the problem itself. Given only a set of words without any context and semantics, its not only hard for a machine to predict the messages sender but also for a human.<br>Moreover, given more training data (Id need a longer message history) and more computing power to handle larger feature sets, the accuracy might further improve slightly.<br>Actually, the practical relevance of this project isnt quit high anyway, but it was a good practice for me to get into the basics of ML and its really fun!</p>\n<p>Please leave me feedback if you like to.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h3 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h3><p>First of all, a short disclaimer: Im not an expert in machine learning at all. In fact Im in a rather early stage of the learning process, have basic knowledge and this project is kind of my first practical hands-on ML. Ive done the <a href=\"https://www.youtube.com/user/UWCSE/playlists?sort=dd&shelf_id=16&view=50\">machine learning course</a> by <a href=\"https://homes.cs.washington.edu/~pedrod/\">Pedro Domingos</a> at University of Washington, <a href=\"https://www.udacity.com/course/intro-to-machine-learning--ud120\">Intro to Machine Learning</a> by Udacity and Google and the <a href=\"https://his.anthropomatik.kit.edu/english/28_315.php\">Machine Learning 1 lecture at Karlsruhe Institute Of Technology</a>, all of which I can really recommend.<br>After having gathered all that theoretical knowledge, I wanted to try something practical on my own. I decided to learn a simple classifier for chat messages from my <a href=\"https://telegram.com/\">Telegram</a> messenger history. I wanted to learn a program that can, given a chat message, tell who the sender of that message is. I further got inspired after having read the papers related to Facebooks <a href=\"https://github.com/facebookresearch/fastText\">fastText</a> text classification algorithm. In their examples they classify Wikipedia abstracts / descriptions to <a href=\"https://dbpedia.org/\">DBPedia</a> classes or news article headlines to their respective news categories, only based on plain, natural words. Basically these problems are very similar to mine, so I decided to give it a try. Since I found that many text classifiers are learned using the Naive Bayes algorithm (especially popular in spam detection and part of <a href=\"http://spamassassin.apache.org/\">SpamAssassin</a>) and its really easy to understand, I decided to go for that one, too. Inspired by <a href=\"http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/\">this article</a>, where the sentiment of tweets is analyzed, I chose to also use the <a href=\"http://www.nltk.org/\">natural language toolkit</a> for Python. Another option would have been <a href=\"http://scikit-learn.org/\">sklearn</a>, but NLTK also provided some useful utilities beyond the pure ML scope. </p>\n<p>All of my <strong>code is <a href=\"https://github.com/muety/tg-chat-classification/\">available on GitHub</a></strong>.</p>\n<h3 id=\"Basic-Steps\"><a href=\"#Basic-Steps\" class=\"headerlink\" title=\"Basic Steps\"></a>Basic Steps</h3><ol>\n<li>The very first step was to download the data, namely the chat messages. Luckily, Telegram has an open API. However its not a classical REST API, but instead theyre using the <a href=\"https://core.telegram.org/mtproto\">MTProto</a> protocol. I found <a href=\"https://github.com/vysheng/tg\">vysheng/tg</a> as a cool C++-written commandline client on GitHub as well as <a href=\"https://github.com/tvdstaaij/telegram-history-dump\">tvdstaaij/telegram-history-dump</a> as a Ruby script to automate the history download for a set of users / chat partners. I told the script (ran in a Docker container, since I didnt want to install Ruby) to fetch at max 40,000 messages for my top three chat partners (lets call them <em>M</em>, <em>P</em> and <em>J</em>). The outcome were three <a href=\"http://jsonlines.org/\">JSON Lines</a> files.</li>\n<li>To pre-process these files as needed for my learning algorithm, I wrote a Python script that extracted only message text and sender from all incoming messages and dumped these data to a JSON file. Additionally I also extracted the same information for all outgoing messages, i.e. all messages where the sender was me. Consequently, there are four classes: <strong>C = { <em>M</em>, <em>P</em>, <em>J</em>, <em>F</em> }</strong></li>\n<li>Another data-preprocessing step was to convert the JSON objects with class names as keys for message-arrays to one large list of tuples of the form <em>(text, label)</em>, where <em>label</em> is the name of the messages sender and <em>text</em> is the respective message text. In this step I also discarded words with a length of less than 2 characters and converted everything to lower case.</li>\n<li>Next step was to extract the features. In text classification, there is often one binary (<em>contains</em> / <em>contains not</em>) feature for every possible word. So if all messages in total comprise X different words, there will be a X-dimensional feature vector.</li>\n<li>Last step before actually training the classifier is to compute the feature vector for every messages. For examples if the total feature set is <code>[&#39;in&#39;, &#39;case&#39;, &#39;of&#39;, &#39;fire&#39;, &#39;coffee&#39;, &#39;we&#39;, &#39;trust&#39;]</code>, the resultung feature vector for a message <em>in coffee we trust</em> would be <code>(&#39;in&#39;=True, &#39;case&#39;=False, &#39;of&#39;=False, &#39;fire&#39;=False, &#39;coffee&#39;=True, &#39;we&#39;=True, &#39;trust&#39;=True)</code>.</li>\n<li>One more minor thing: shuffle the feature set so that the order of messages and message senders is random. Also divide the feature set into training- and test data, where test data contains about 10 % of the number of messages in the train data.</li>\n<li>Train <a href=\"http://www.nltk.org/api/nltk.classify.html\">nltk.NaiveBayesClassifier</a> classifier. This is really just one line of code.</li>\n<li>Use the returned classifier to predict classes for the test messages, validate them and compute the accuracy.</li>\n</ol>\n<p>Using that basic initial setup on a set of <strong>37257 messages</strong>, (7931 from M, 9795 from P, 9314 from F and 10217 from J), I ended up with an <strong>accuracy of 0.58</strong>. There seemed to be room for optimization.</p>\n<h3 id=\"Optimizations\"><a href=\"#Optimizations\" class=\"headerlink\" title=\"Optimizations\"></a>Optimizations</h3><ul>\n<li>Inspired by <em>fastText</em>, I decided to include n-grams. This seemed resonable to me, because intuitively Id say that single words a way less characteristic for a persons writing style than certain phrases. I extended the feature list from step 4 by all possible <strong>bi- and tri-grams</strong>, which are easy to compute with NLTK. Actually Im not taking ALL bi- and tri-grams and Im not even take all single words as features. Reason for that is that there were approx. 35k different words in the dataset. Plus the n-grams this would make an extremely multi-dimensional feature vector and as it turned out, it was way to complex for my 16 GB MacBook Pro to compute. Consequently, I only took the <strong>top 5000 single words, bigrams and trigrams</strong>, ranked descending by their overall frequency. </li>\n<li>Since NLTK already provides a corpus of <strong>stopwords</strong> (like in, and, of, etc.), which are obviously not characteristic for a persons style of chatting, I decided to remove them (the German ones) from the message set in step 2.</li>\n</ul>\n<p>With these optimizations, I ended up with an <strong>accuracy of 0.61</strong> after a training time of 348 seconds (I didnt log testing time at that point).</p>\n<h3 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h3><p>Certainly 61 % accuracy isnt really a good classifier, but at least significantly better than random guessing (chance of 1/4 in this case). However, I trained a <strong>fastText</strong> classifier on my data as a comparison baseline and it even only reached <strong>60 % accuracy</strong> (but with a much better <strong>training time of only 0.66 seconds</strong>).<br>My intuitive explanation for these rather bad results is the complexity of the problem itself. Given only a set of words without any context and semantics, its not only hard for a machine to predict the messages sender but also for a human.<br>Moreover, given more training data (Id need a longer message history) and more computing power to handle larger feature sets, the accuracy might further improve slightly.<br>Actually, the practical relevance of this project isnt quit high anyway, but it was a good practice for me to get into the basics of ML and its really fun!</p>\n<p>Please leave me feedback if you like to.</p>\n"},{"title":"Modern, reactive web APIs with GraphQL, Go and Server-Sent Events  Part 1","date":"2020-06-06T09:59:23.000Z","_content":"\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_cover.png)\n\n# Introduction\nIn the course of this two-part article, the interested reader is briefly introduced to the basic of GraphQL and how it compares to traditional approaches. In the second part, an example single-page web application (SPA) is built to demonstrate the use of GraphQL in combination with further modern web technologies. The [final project](https://github.com/muety/go-graphql-sse-example) provides a clean, opinionated code- and project structure for both backend and frontend and constitutes a good starting point for new apps based on the presented tech stack.\n\n\\> **Code**: [muety/go-graphql-sse-example](https://github.com/muety/go-graphql-sse-example)\n\n# What is GraphQL?\n[GraphQL](https://engineering.fb.com/core-data/graphql-a-data-query-language/) is a relatively new (proposed in 2015 by Facebook engineers) approach to designing APIs for (web) backend applications and can be considered an alternative to [REST](https://developer.mozilla.org/en-US/docs/Glossary/REST) or remote-procedure-call (RPC) mechanisms like [JSON-RPC](https://www.jsonrpc.org/). In other words, it's *\"an open-source data query and manipulation language for APIs\"* [[1]](https://en.wikipedia.org/wiki/GraphQL). The specification is open-source and actively evolving on [GitHub](https://github.com/graphql/graphql-spec). While REST APIs are currently the de-facto standard on the web (although not necessarily all of them being fully [mature](https://www.martinfowler.com/articles/richardsonMaturityModel.html))  GraphQL starts to [gain traction](https://trends.google.com/trends/explore?date=2018-05-06%202020-06-06&gprop=youtube&q=graphql).\n\n## Comparison with REST and RPC\n\nIn contrast to REST, which is primarily structured around resources or entities and RPC-based APIs, which focus on actions or methods, GraphQL is all about the underlying data itself. The consumer of an API  usually the frontend / client-side part of a SPA  only has to know the schema and structure of the data provided by the API to [CRUD](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete) it. Compared to REST APIs, where the consumer heavily depends on the fixed data structure delivered by the backend API, this is especially beneficial as it introduced a lot more flexibility and decoupling and can save the developer some time making the client-side application [tolerant](https://martinfowler.com/bliki/TolerantReader.html). Also, you will probably not need [backends for frontends](https://docs.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends) anymore.\n\nEssentially, with GraphQL, **the consumer asks exactly for what it needs and how it needs it**, i.e. your client application tells the backend exactly what to return and in which format. **Consuming a GraphQL API is like querying a database, but with more guidance and control.**\n\n## Example\nLet's look at an example to get a better idea of how GraphQL works, especially in comparison to the REST principles. \n\nImagine you have an e-commerce application with products and orders. Every order consists, among others, of a set of products. As the operator of the web shop, you might want to get a list of all orders. With a more or less RESTful API (we neglect the hypermedia controls in the example though), your request-response pair could look like this:\n\n<details>\n<summary>Click to view</summary>\n```\nRequest\n-------\nGET /api/orders\n\nResponse Body\n-------------\n[\n    {\n        \"id\": 125,\n        \"customerId\": 8977,\n        \"createdAt\": \"2020-06-06T13:40:49.038Z\",\n        \"productIds\": [ 49863176 ]\n    }\n]\n```\n</details>\n\nSo far so good, but potentially you will also want to view the actual products right away. What you got are only ids, for each of which you would have to issue another API call to retrieve it. Alternatively, the API could also return nested objects, like so:\n\n<details>\n<summary>Click to view</summary>\n```\n[\n    {\n        \"id\": 125,\n        \"customerId\": 8977,\n        \"createdAt\": \"2020-06-06T13:40:49.038Z\",\n        \"products\": [\n            {\n                \"id\": 49863176,\n                \"name\": \"Slim T-Shirt navy-blue\",\n                \"price\": 17.90,\n                \"options\": [\n                    {\n                        \"id\": \"size\",\n                        \"name\": \"Size\",\n                        \"description\": \"T-Shirt size\",\n                        \"values\": [\n                            {\n                                \"id\": \"s\",\n                                \"name\": \"Size S\",\n                            },\n                            {\n                                \"id\": \"m\",\n                                \"name\": \"Size M\",\n                            },\n                            {\n                                \"id\": \"l\",\n                                \"name\": \"Size L\",\n                            }\n                        ]\n                    }\n                ]\n            },\n        ]\n    }\n]\n```\n</details>\n\nHowever, that is  to my understanding  not truly RESTful anymore. Also, while the above example is still quite straightforward, things get ugly as nested objects include other nested objects, that include other nested objects, that... Quickly you get JSON responses of several tens or hundreds of kilobytes, although you're potentially only interested in two or three attributes. Moreover, on some pages of your shop you may be interested in all possible options (e.g. \"size\") of a product, but not on others. Should your API define different [view models](https://www.infoq.com/articles/View-Model-Definition/) now and expose different endpoints? Or a single endpoints with query flags like `?expanded=true`? Soon you might be catching yourself **tailoring your API specifically to the needs of your client** while neglecting REST conventions and a straightforward design. \n\nWith GraphQL, things are different. Your API is a bit **dumber and less opinionated** now and does not deliver data in a fixed structure, according to a specified [GQL query](https://graphql.org/learn/queries/), which looks a lot like JSON. The above example might look like this, now:\n\n<details>\n<summary>Click to view</summary>\n```\nRequest\n-------\nPOST /api/graphql/query\n\n{\n    \"query\": \"\\{\n        orders {\n            id\n            customerId\n            products {\n                name\n                price\n                options {\n                    name\n                }\n            }\n        }\n    \\}\"\n}\n\nResponse Body\n-------------\n{\n    \"data\": {\n        \"orders\": [\n            {\n                \"id\": 125,\n                \"customerId\": 8977,\n                \"products\": [\n                    {\n                        \"name\": \"Slim T-Shirt navy-blue\",\n                        \"price\": 17.90,\n                        \"options\": [\n                            {\n                                \"name\": \"Size\",\n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    }\n}\n```\n</details>\n\nThis way, you get only the data you want. All your API has to know is how to fetch every piece of data. All your client has to know is how the data schema itself looks like.\n\n## Try it out\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_github.png)\n\nGitHub's official API offers GraphQL query endpoints. You can try it out using their [GraphQL explorer](https://developer.github.com/v4/explorer/).\n\n# GraphQL Basic\nSince this article does not aim to be another introduction to GraphQL, you can read most of the basics about fields, data types, etc. in the [official docs](https://graphql.org/learn/queries/). However, it is worth mentioning that GraphQL supports three types of queries:\n\n* **`Query`**: \"Standard\" type of queries, used for fetching data (see above). Similar to what you would do with a `GET` in REST.\n* **`Mutation`**: Query type used to modify data. Similar to what you would do with a `PUT`, `POST`, `PATCH` or `DELETE` in REST.\n* **`Subscription`**: Query type to communicate your intent to subscribe to live data updates. \n\n## Subscriptions\n\nWhile a basic GraphQL application will at least use the former two types, the latter is especially interesting in the context of this article. Using subscriptions, you can have your web frontend be notified when new data arrives at the server or existing data changes. For instance, the operator of the above web shop could have a live-updating dashboard, that shows new orders just as they are placed. \n\nFor subscriptions, the GraphQL standard does not define a lot more than their plain existence and purpose. Especially, it is not defined how and which technology to implement them. On the web, any [publish/subscribe](https://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber)-like mechanism that provides bi-directional or uni-directional server-to-client communication is appropriate. For the sake of simplicity, [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events) are used in this article.\n\n# What's next?\nThis part gave a brief introduction to GraphQL. The next part is about actual code. We're going to build an example web app with live-updates using GraphQL, Go, MongoDB and VueJs. Stay tuned!\n\n\\> [**Part 2**](https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.html)","source":"_posts/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.md","raw":"---\ntitle: 'Modern, reactive web APIs with GraphQL, Go and Server-Sent Events  Part 1'\ndate: 2020-06-06 11:59:23\ntags:\n---\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_cover.png)\n\n# Introduction\nIn the course of this two-part article, the interested reader is briefly introduced to the basic of GraphQL and how it compares to traditional approaches. In the second part, an example single-page web application (SPA) is built to demonstrate the use of GraphQL in combination with further modern web technologies. The [final project](https://github.com/muety/go-graphql-sse-example) provides a clean, opinionated code- and project structure for both backend and frontend and constitutes a good starting point for new apps based on the presented tech stack.\n\n\\> **Code**: [muety/go-graphql-sse-example](https://github.com/muety/go-graphql-sse-example)\n\n# What is GraphQL?\n[GraphQL](https://engineering.fb.com/core-data/graphql-a-data-query-language/) is a relatively new (proposed in 2015 by Facebook engineers) approach to designing APIs for (web) backend applications and can be considered an alternative to [REST](https://developer.mozilla.org/en-US/docs/Glossary/REST) or remote-procedure-call (RPC) mechanisms like [JSON-RPC](https://www.jsonrpc.org/). In other words, it's *\"an open-source data query and manipulation language for APIs\"* [[1]](https://en.wikipedia.org/wiki/GraphQL). The specification is open-source and actively evolving on [GitHub](https://github.com/graphql/graphql-spec). While REST APIs are currently the de-facto standard on the web (although not necessarily all of them being fully [mature](https://www.martinfowler.com/articles/richardsonMaturityModel.html))  GraphQL starts to [gain traction](https://trends.google.com/trends/explore?date=2018-05-06%202020-06-06&gprop=youtube&q=graphql).\n\n## Comparison with REST and RPC\n\nIn contrast to REST, which is primarily structured around resources or entities and RPC-based APIs, which focus on actions or methods, GraphQL is all about the underlying data itself. The consumer of an API  usually the frontend / client-side part of a SPA  only has to know the schema and structure of the data provided by the API to [CRUD](https://en.wikipedia.org/wiki/Create,_read,_update_and_delete) it. Compared to REST APIs, where the consumer heavily depends on the fixed data structure delivered by the backend API, this is especially beneficial as it introduced a lot more flexibility and decoupling and can save the developer some time making the client-side application [tolerant](https://martinfowler.com/bliki/TolerantReader.html). Also, you will probably not need [backends for frontends](https://docs.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends) anymore.\n\nEssentially, with GraphQL, **the consumer asks exactly for what it needs and how it needs it**, i.e. your client application tells the backend exactly what to return and in which format. **Consuming a GraphQL API is like querying a database, but with more guidance and control.**\n\n## Example\nLet's look at an example to get a better idea of how GraphQL works, especially in comparison to the REST principles. \n\nImagine you have an e-commerce application with products and orders. Every order consists, among others, of a set of products. As the operator of the web shop, you might want to get a list of all orders. With a more or less RESTful API (we neglect the hypermedia controls in the example though), your request-response pair could look like this:\n\n<details>\n<summary>Click to view</summary>\n```\nRequest\n-------\nGET /api/orders\n\nResponse Body\n-------------\n[\n    {\n        \"id\": 125,\n        \"customerId\": 8977,\n        \"createdAt\": \"2020-06-06T13:40:49.038Z\",\n        \"productIds\": [ 49863176 ]\n    }\n]\n```\n</details>\n\nSo far so good, but potentially you will also want to view the actual products right away. What you got are only ids, for each of which you would have to issue another API call to retrieve it. Alternatively, the API could also return nested objects, like so:\n\n<details>\n<summary>Click to view</summary>\n```\n[\n    {\n        \"id\": 125,\n        \"customerId\": 8977,\n        \"createdAt\": \"2020-06-06T13:40:49.038Z\",\n        \"products\": [\n            {\n                \"id\": 49863176,\n                \"name\": \"Slim T-Shirt navy-blue\",\n                \"price\": 17.90,\n                \"options\": [\n                    {\n                        \"id\": \"size\",\n                        \"name\": \"Size\",\n                        \"description\": \"T-Shirt size\",\n                        \"values\": [\n                            {\n                                \"id\": \"s\",\n                                \"name\": \"Size S\",\n                            },\n                            {\n                                \"id\": \"m\",\n                                \"name\": \"Size M\",\n                            },\n                            {\n                                \"id\": \"l\",\n                                \"name\": \"Size L\",\n                            }\n                        ]\n                    }\n                ]\n            },\n        ]\n    }\n]\n```\n</details>\n\nHowever, that is  to my understanding  not truly RESTful anymore. Also, while the above example is still quite straightforward, things get ugly as nested objects include other nested objects, that include other nested objects, that... Quickly you get JSON responses of several tens or hundreds of kilobytes, although you're potentially only interested in two or three attributes. Moreover, on some pages of your shop you may be interested in all possible options (e.g. \"size\") of a product, but not on others. Should your API define different [view models](https://www.infoq.com/articles/View-Model-Definition/) now and expose different endpoints? Or a single endpoints with query flags like `?expanded=true`? Soon you might be catching yourself **tailoring your API specifically to the needs of your client** while neglecting REST conventions and a straightforward design. \n\nWith GraphQL, things are different. Your API is a bit **dumber and less opinionated** now and does not deliver data in a fixed structure, according to a specified [GQL query](https://graphql.org/learn/queries/), which looks a lot like JSON. The above example might look like this, now:\n\n<details>\n<summary>Click to view</summary>\n```\nRequest\n-------\nPOST /api/graphql/query\n\n{\n    \"query\": \"\\{\n        orders {\n            id\n            customerId\n            products {\n                name\n                price\n                options {\n                    name\n                }\n            }\n        }\n    \\}\"\n}\n\nResponse Body\n-------------\n{\n    \"data\": {\n        \"orders\": [\n            {\n                \"id\": 125,\n                \"customerId\": 8977,\n                \"products\": [\n                    {\n                        \"name\": \"Slim T-Shirt navy-blue\",\n                        \"price\": 17.90,\n                        \"options\": [\n                            {\n                                \"name\": \"Size\",\n                            }\n                        ]\n                    }\n                ]\n            }\n        ]\n    }\n}\n```\n</details>\n\nThis way, you get only the data you want. All your API has to know is how to fetch every piece of data. All your client has to know is how the data schema itself looks like.\n\n## Try it out\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_github.png)\n\nGitHub's official API offers GraphQL query endpoints. You can try it out using their [GraphQL explorer](https://developer.github.com/v4/explorer/).\n\n# GraphQL Basic\nSince this article does not aim to be another introduction to GraphQL, you can read most of the basics about fields, data types, etc. in the [official docs](https://graphql.org/learn/queries/). However, it is worth mentioning that GraphQL supports three types of queries:\n\n* **`Query`**: \"Standard\" type of queries, used for fetching data (see above). Similar to what you would do with a `GET` in REST.\n* **`Mutation`**: Query type used to modify data. Similar to what you would do with a `PUT`, `POST`, `PATCH` or `DELETE` in REST.\n* **`Subscription`**: Query type to communicate your intent to subscribe to live data updates. \n\n## Subscriptions\n\nWhile a basic GraphQL application will at least use the former two types, the latter is especially interesting in the context of this article. Using subscriptions, you can have your web frontend be notified when new data arrives at the server or existing data changes. For instance, the operator of the above web shop could have a live-updating dashboard, that shows new orders just as they are placed. \n\nFor subscriptions, the GraphQL standard does not define a lot more than their plain existence and purpose. Especially, it is not defined how and which technology to implement them. On the web, any [publish/subscribe](https://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber)-like mechanism that provides bi-directional or uni-directional server-to-client communication is appropriate. For the sake of simplicity, [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events) are used in this article.\n\n# What's next?\nThis part gave a brief introduction to GraphQL. The next part is about actual code. We're going to build an example web app with live-updates using GraphQL, Go, MongoDB and VueJs. Stay tuned!\n\n\\> [**Part 2**](https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.html)","slug":"modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1","published":1,"updated":"2020-10-30T20:05:40.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqm6000so2e0fp1p11ha","content":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_cover.png\"></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>In the course of this two-part article, the interested reader is briefly introduced to the basic of GraphQL and how it compares to traditional approaches. In the second part, an example single-page web application (SPA) is built to demonstrate the use of GraphQL in combination with further modern web technologies. The <a href=\"https://github.com/muety/go-graphql-sse-example\">final project</a> provides a clean, opinionated code- and project structure for both backend and frontend and constitutes a good starting point for new apps based on the presented tech stack.</p>\n<p>&gt; <strong>Code</strong>: <a href=\"https://github.com/muety/go-graphql-sse-example\">muety/go-graphql-sse-example</a></p>\n<h1 id=\"What-is-GraphQL\"><a href=\"#What-is-GraphQL\" class=\"headerlink\" title=\"What is GraphQL?\"></a>What is GraphQL?</h1><p><a href=\"https://engineering.fb.com/core-data/graphql-a-data-query-language/\">GraphQL</a> is a relatively new (proposed in 2015 by Facebook engineers) approach to designing APIs for (web) backend applications and can be considered an alternative to <a href=\"https://developer.mozilla.org/en-US/docs/Glossary/REST\">REST</a> or remote-procedure-call (RPC) mechanisms like <a href=\"https://www.jsonrpc.org/\">JSON-RPC</a>. In other words, its <em>an open-source data query and manipulation language for APIs</em> <a href=\"https://en.wikipedia.org/wiki/GraphQL\">[1]</a>. The specification is open-source and actively evolving on <a href=\"https://github.com/graphql/graphql-spec\">GitHub</a>. While REST APIs are currently the de-facto standard on the web (although not necessarily all of them being fully <a href=\"https://www.martinfowler.com/articles/richardsonMaturityModel.html\">mature</a>)  GraphQL starts to <a href=\"https://trends.google.com/trends/explore?date=2018-05-06%202020-06-06&gprop=youtube&q=graphql\">gain traction</a>.</p>\n<h2 id=\"Comparison-with-REST-and-RPC\"><a href=\"#Comparison-with-REST-and-RPC\" class=\"headerlink\" title=\"Comparison with REST and RPC\"></a>Comparison with REST and RPC</h2><p>In contrast to REST, which is primarily structured around resources or entities and RPC-based APIs, which focus on actions or methods, GraphQL is all about the underlying data itself. The consumer of an API  usually the frontend / client-side part of a SPA  only has to know the schema and structure of the data provided by the API to <a href=\"https://en.wikipedia.org/wiki/Create,_read,_update_and_delete\">CRUD</a> it. Compared to REST APIs, where the consumer heavily depends on the fixed data structure delivered by the backend API, this is especially beneficial as it introduced a lot more flexibility and decoupling and can save the developer some time making the client-side application <a href=\"https://martinfowler.com/bliki/TolerantReader.html\">tolerant</a>. Also, you will probably not need <a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends\">backends for frontends</a> anymore.</p>\n<p>Essentially, with GraphQL, <strong>the consumer asks exactly for what it needs and how it needs it</strong>, i.e. your client application tells the backend exactly what to return and in which format. <strong>Consuming a GraphQL API is like querying a database, but with more guidance and control.</strong></p>\n<h2 id=\"Example\"><a href=\"#Example\" class=\"headerlink\" title=\"Example\"></a>Example</h2><p>Lets look at an example to get a better idea of how GraphQL works, especially in comparison to the REST principles. </p>\n<p>Imagine you have an e-commerce application with products and orders. Every order consists, among others, of a set of products. As the operator of the web shop, you might want to get a list of all orders. With a more or less RESTful API (we neglect the hypermedia controls in the example though), your request-response pair could look like this:</p>\n<details>\n<summary>Click to view</summary>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Request</span><br><span class=\"line\">-------</span><br><span class=\"line\">GET &#x2F;api&#x2F;orders</span><br><span class=\"line\"></span><br><span class=\"line\">Response Body</span><br><span class=\"line\">-------------</span><br><span class=\"line\">[</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        &quot;id&quot;: 125,</span><br><span class=\"line\">        &quot;customerId&quot;: 8977,</span><br><span class=\"line\">        &quot;createdAt&quot;: &quot;2020-06-06T13:40:49.038Z&quot;,</span><br><span class=\"line\">        &quot;productIds&quot;: [ 49863176 ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n</details>\n\n<p>So far so good, but potentially you will also want to view the actual products right away. What you got are only ids, for each of which you would have to issue another API call to retrieve it. Alternatively, the API could also return nested objects, like so:</p>\n<details>\n<summary>Click to view</summary>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        &quot;id&quot;: 125,</span><br><span class=\"line\">        &quot;customerId&quot;: 8977,</span><br><span class=\"line\">        &quot;createdAt&quot;: &quot;2020-06-06T13:40:49.038Z&quot;,</span><br><span class=\"line\">        &quot;products&quot;: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                &quot;id&quot;: 49863176,</span><br><span class=\"line\">                &quot;name&quot;: &quot;Slim T-Shirt navy-blue&quot;,</span><br><span class=\"line\">                &quot;price&quot;: 17.90,</span><br><span class=\"line\">                &quot;options&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;id&quot;: &quot;size&quot;,</span><br><span class=\"line\">                        &quot;name&quot;: &quot;Size&quot;,</span><br><span class=\"line\">                        &quot;description&quot;: &quot;T-Shirt size&quot;,</span><br><span class=\"line\">                        &quot;values&quot;: [</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;id&quot;: &quot;s&quot;,</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size S&quot;,</span><br><span class=\"line\">                            &#125;,</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;id&quot;: &quot;m&quot;,</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size M&quot;,</span><br><span class=\"line\">                            &#125;,</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;id&quot;: &quot;l&quot;,</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size L&quot;,</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        ]</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n</details>\n\n<p>However, that is  to my understanding  not truly RESTful anymore. Also, while the above example is still quite straightforward, things get ugly as nested objects include other nested objects, that include other nested objects, that Quickly you get JSON responses of several tens or hundreds of kilobytes, although youre potentially only interested in two or three attributes. Moreover, on some pages of your shop you may be interested in all possible options (e.g. size) of a product, but not on others. Should your API define different <a href=\"https://www.infoq.com/articles/View-Model-Definition/\">view models</a> now and expose different endpoints? Or a single endpoints with query flags like <code>?expanded=true</code>? Soon you might be catching yourself <strong>tailoring your API specifically to the needs of your client</strong> while neglecting REST conventions and a straightforward design. </p>\n<p>With GraphQL, things are different. Your API is a bit <strong>dumber and less opinionated</strong> now and does not deliver data in a fixed structure, according to a specified <a href=\"https://graphql.org/learn/queries/\">GQL query</a>, which looks a lot like JSON. The above example might look like this, now:</p>\n<details>\n<summary>Click to view</summary>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Request</span><br><span class=\"line\">-------</span><br><span class=\"line\">POST &#x2F;api&#x2F;graphql&#x2F;query</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;query&quot;: &quot;\\&#123;</span><br><span class=\"line\">        orders &#123;</span><br><span class=\"line\">            id</span><br><span class=\"line\">            customerId</span><br><span class=\"line\">            products &#123;</span><br><span class=\"line\">                name</span><br><span class=\"line\">                price</span><br><span class=\"line\">                options &#123;</span><br><span class=\"line\">                    name</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    \\&#125;&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">Response Body</span><br><span class=\"line\">-------------</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;data&quot;: &#123;</span><br><span class=\"line\">        &quot;orders&quot;: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                &quot;id&quot;: 125,</span><br><span class=\"line\">                &quot;customerId&quot;: 8977,</span><br><span class=\"line\">                &quot;products&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;name&quot;: &quot;Slim T-Shirt navy-blue&quot;,</span><br><span class=\"line\">                        &quot;price&quot;: 17.90,</span><br><span class=\"line\">                        &quot;options&quot;: [</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size&quot;,</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        ]</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</details>\n\n<p>This way, you get only the data you want. All your API has to know is how to fetch every piece of data. All your client has to know is how the data schema itself looks like.</p>\n<h2 id=\"Try-it-out\"><a href=\"#Try-it-out\" class=\"headerlink\" title=\"Try it out\"></a>Try it out</h2><p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_github.png\"></p>\n<p>GitHubs official API offers GraphQL query endpoints. You can try it out using their <a href=\"https://developer.github.com/v4/explorer/\">GraphQL explorer</a>.</p>\n<h1 id=\"GraphQL-Basic\"><a href=\"#GraphQL-Basic\" class=\"headerlink\" title=\"GraphQL Basic\"></a>GraphQL Basic</h1><p>Since this article does not aim to be another introduction to GraphQL, you can read most of the basics about fields, data types, etc. in the <a href=\"https://graphql.org/learn/queries/\">official docs</a>. However, it is worth mentioning that GraphQL supports three types of queries:</p>\n<ul>\n<li><strong><code>Query</code></strong>: Standard type of queries, used for fetching data (see above). Similar to what you would do with a <code>GET</code> in REST.</li>\n<li><strong><code>Mutation</code></strong>: Query type used to modify data. Similar to what you would do with a <code>PUT</code>, <code>POST</code>, <code>PATCH</code> or <code>DELETE</code> in REST.</li>\n<li><strong><code>Subscription</code></strong>: Query type to communicate your intent to subscribe to live data updates. </li>\n</ul>\n<h2 id=\"Subscriptions\"><a href=\"#Subscriptions\" class=\"headerlink\" title=\"Subscriptions\"></a>Subscriptions</h2><p>While a basic GraphQL application will at least use the former two types, the latter is especially interesting in the context of this article. Using subscriptions, you can have your web frontend be notified when new data arrives at the server or existing data changes. For instance, the operator of the above web shop could have a live-updating dashboard, that shows new orders just as they are placed. </p>\n<p>For subscriptions, the GraphQL standard does not define a lot more than their plain existence and purpose. Especially, it is not defined how and which technology to implement them. On the web, any <a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber\">publish/subscribe</a>-like mechanism that provides bi-directional or uni-directional server-to-client communication is appropriate. For the sake of simplicity, <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\">Server-Sent Events</a> are used in this article.</p>\n<h1 id=\"Whats-next\"><a href=\"#Whats-next\" class=\"headerlink\" title=\"Whats next?\"></a>Whats next?</h1><p>This part gave a brief introduction to GraphQL. The next part is about actual code. Were going to build an example web app with live-updates using GraphQL, Go, MongoDB and VueJs. Stay tuned!</p>\n<p>&gt; <a href=\"https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.html\"><strong>Part 2</strong></a></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_cover.png\"></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>In the course of this two-part article, the interested reader is briefly introduced to the basic of GraphQL and how it compares to traditional approaches. In the second part, an example single-page web application (SPA) is built to demonstrate the use of GraphQL in combination with further modern web technologies. The <a href=\"https://github.com/muety/go-graphql-sse-example\">final project</a> provides a clean, opinionated code- and project structure for both backend and frontend and constitutes a good starting point for new apps based on the presented tech stack.</p>\n<p>&gt; <strong>Code</strong>: <a href=\"https://github.com/muety/go-graphql-sse-example\">muety/go-graphql-sse-example</a></p>\n<h1 id=\"What-is-GraphQL\"><a href=\"#What-is-GraphQL\" class=\"headerlink\" title=\"What is GraphQL?\"></a>What is GraphQL?</h1><p><a href=\"https://engineering.fb.com/core-data/graphql-a-data-query-language/\">GraphQL</a> is a relatively new (proposed in 2015 by Facebook engineers) approach to designing APIs for (web) backend applications and can be considered an alternative to <a href=\"https://developer.mozilla.org/en-US/docs/Glossary/REST\">REST</a> or remote-procedure-call (RPC) mechanisms like <a href=\"https://www.jsonrpc.org/\">JSON-RPC</a>. In other words, its <em>an open-source data query and manipulation language for APIs</em> <a href=\"https://en.wikipedia.org/wiki/GraphQL\">[1]</a>. The specification is open-source and actively evolving on <a href=\"https://github.com/graphql/graphql-spec\">GitHub</a>. While REST APIs are currently the de-facto standard on the web (although not necessarily all of them being fully <a href=\"https://www.martinfowler.com/articles/richardsonMaturityModel.html\">mature</a>)  GraphQL starts to <a href=\"https://trends.google.com/trends/explore?date=2018-05-06%202020-06-06&gprop=youtube&q=graphql\">gain traction</a>.</p>\n<h2 id=\"Comparison-with-REST-and-RPC\"><a href=\"#Comparison-with-REST-and-RPC\" class=\"headerlink\" title=\"Comparison with REST and RPC\"></a>Comparison with REST and RPC</h2><p>In contrast to REST, which is primarily structured around resources or entities and RPC-based APIs, which focus on actions or methods, GraphQL is all about the underlying data itself. The consumer of an API  usually the frontend / client-side part of a SPA  only has to know the schema and structure of the data provided by the API to <a href=\"https://en.wikipedia.org/wiki/Create,_read,_update_and_delete\">CRUD</a> it. Compared to REST APIs, where the consumer heavily depends on the fixed data structure delivered by the backend API, this is especially beneficial as it introduced a lot more flexibility and decoupling and can save the developer some time making the client-side application <a href=\"https://martinfowler.com/bliki/TolerantReader.html\">tolerant</a>. Also, you will probably not need <a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/backends-for-frontends\">backends for frontends</a> anymore.</p>\n<p>Essentially, with GraphQL, <strong>the consumer asks exactly for what it needs and how it needs it</strong>, i.e. your client application tells the backend exactly what to return and in which format. <strong>Consuming a GraphQL API is like querying a database, but with more guidance and control.</strong></p>\n<h2 id=\"Example\"><a href=\"#Example\" class=\"headerlink\" title=\"Example\"></a>Example</h2><p>Lets look at an example to get a better idea of how GraphQL works, especially in comparison to the REST principles. </p>\n<p>Imagine you have an e-commerce application with products and orders. Every order consists, among others, of a set of products. As the operator of the web shop, you might want to get a list of all orders. With a more or less RESTful API (we neglect the hypermedia controls in the example though), your request-response pair could look like this:</p>\n<details>\n<summary>Click to view</summary>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Request</span><br><span class=\"line\">-------</span><br><span class=\"line\">GET &#x2F;api&#x2F;orders</span><br><span class=\"line\"></span><br><span class=\"line\">Response Body</span><br><span class=\"line\">-------------</span><br><span class=\"line\">[</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        &quot;id&quot;: 125,</span><br><span class=\"line\">        &quot;customerId&quot;: 8977,</span><br><span class=\"line\">        &quot;createdAt&quot;: &quot;2020-06-06T13:40:49.038Z&quot;,</span><br><span class=\"line\">        &quot;productIds&quot;: [ 49863176 ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n</details>\n\n<p>So far so good, but potentially you will also want to view the actual products right away. What you got are only ids, for each of which you would have to issue another API call to retrieve it. Alternatively, the API could also return nested objects, like so:</p>\n<details>\n<summary>Click to view</summary>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        &quot;id&quot;: 125,</span><br><span class=\"line\">        &quot;customerId&quot;: 8977,</span><br><span class=\"line\">        &quot;createdAt&quot;: &quot;2020-06-06T13:40:49.038Z&quot;,</span><br><span class=\"line\">        &quot;products&quot;: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                &quot;id&quot;: 49863176,</span><br><span class=\"line\">                &quot;name&quot;: &quot;Slim T-Shirt navy-blue&quot;,</span><br><span class=\"line\">                &quot;price&quot;: 17.90,</span><br><span class=\"line\">                &quot;options&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;id&quot;: &quot;size&quot;,</span><br><span class=\"line\">                        &quot;name&quot;: &quot;Size&quot;,</span><br><span class=\"line\">                        &quot;description&quot;: &quot;T-Shirt size&quot;,</span><br><span class=\"line\">                        &quot;values&quot;: [</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;id&quot;: &quot;s&quot;,</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size S&quot;,</span><br><span class=\"line\">                            &#125;,</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;id&quot;: &quot;m&quot;,</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size M&quot;,</span><br><span class=\"line\">                            &#125;,</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;id&quot;: &quot;l&quot;,</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size L&quot;,</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        ]</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n</details>\n\n<p>However, that is  to my understanding  not truly RESTful anymore. Also, while the above example is still quite straightforward, things get ugly as nested objects include other nested objects, that include other nested objects, that Quickly you get JSON responses of several tens or hundreds of kilobytes, although youre potentially only interested in two or three attributes. Moreover, on some pages of your shop you may be interested in all possible options (e.g. size) of a product, but not on others. Should your API define different <a href=\"https://www.infoq.com/articles/View-Model-Definition/\">view models</a> now and expose different endpoints? Or a single endpoints with query flags like <code>?expanded=true</code>? Soon you might be catching yourself <strong>tailoring your API specifically to the needs of your client</strong> while neglecting REST conventions and a straightforward design. </p>\n<p>With GraphQL, things are different. Your API is a bit <strong>dumber and less opinionated</strong> now and does not deliver data in a fixed structure, according to a specified <a href=\"https://graphql.org/learn/queries/\">GQL query</a>, which looks a lot like JSON. The above example might look like this, now:</p>\n<details>\n<summary>Click to view</summary>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Request</span><br><span class=\"line\">-------</span><br><span class=\"line\">POST &#x2F;api&#x2F;graphql&#x2F;query</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;query&quot;: &quot;\\&#123;</span><br><span class=\"line\">        orders &#123;</span><br><span class=\"line\">            id</span><br><span class=\"line\">            customerId</span><br><span class=\"line\">            products &#123;</span><br><span class=\"line\">                name</span><br><span class=\"line\">                price</span><br><span class=\"line\">                options &#123;</span><br><span class=\"line\">                    name</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    \\&#125;&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">Response Body</span><br><span class=\"line\">-------------</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;data&quot;: &#123;</span><br><span class=\"line\">        &quot;orders&quot;: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                &quot;id&quot;: 125,</span><br><span class=\"line\">                &quot;customerId&quot;: 8977,</span><br><span class=\"line\">                &quot;products&quot;: [</span><br><span class=\"line\">                    &#123;</span><br><span class=\"line\">                        &quot;name&quot;: &quot;Slim T-Shirt navy-blue&quot;,</span><br><span class=\"line\">                        &quot;price&quot;: 17.90,</span><br><span class=\"line\">                        &quot;options&quot;: [</span><br><span class=\"line\">                            &#123;</span><br><span class=\"line\">                                &quot;name&quot;: &quot;Size&quot;,</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        ]</span><br><span class=\"line\">                    &#125;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</details>\n\n<p>This way, you get only the data you want. All your API has to know is how to fetch every piece of data. All your client has to know is how the data schema itself looks like.</p>\n<h2 id=\"Try-it-out\"><a href=\"#Try-it-out\" class=\"headerlink\" title=\"Try it out\"></a>Try it out</h2><p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_github.png\"></p>\n<p>GitHubs official API offers GraphQL query endpoints. You can try it out using their <a href=\"https://developer.github.com/v4/explorer/\">GraphQL explorer</a>.</p>\n<h1 id=\"GraphQL-Basic\"><a href=\"#GraphQL-Basic\" class=\"headerlink\" title=\"GraphQL Basic\"></a>GraphQL Basic</h1><p>Since this article does not aim to be another introduction to GraphQL, you can read most of the basics about fields, data types, etc. in the <a href=\"https://graphql.org/learn/queries/\">official docs</a>. However, it is worth mentioning that GraphQL supports three types of queries:</p>\n<ul>\n<li><strong><code>Query</code></strong>: Standard type of queries, used for fetching data (see above). Similar to what you would do with a <code>GET</code> in REST.</li>\n<li><strong><code>Mutation</code></strong>: Query type used to modify data. Similar to what you would do with a <code>PUT</code>, <code>POST</code>, <code>PATCH</code> or <code>DELETE</code> in REST.</li>\n<li><strong><code>Subscription</code></strong>: Query type to communicate your intent to subscribe to live data updates. </li>\n</ul>\n<h2 id=\"Subscriptions\"><a href=\"#Subscriptions\" class=\"headerlink\" title=\"Subscriptions\"></a>Subscriptions</h2><p>While a basic GraphQL application will at least use the former two types, the latter is especially interesting in the context of this article. Using subscriptions, you can have your web frontend be notified when new data arrives at the server or existing data changes. For instance, the operator of the above web shop could have a live-updating dashboard, that shows new orders just as they are placed. </p>\n<p>For subscriptions, the GraphQL standard does not define a lot more than their plain existence and purpose. Especially, it is not defined how and which technology to implement them. On the web, any <a href=\"https://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber\">publish/subscribe</a>-like mechanism that provides bi-directional or uni-directional server-to-client communication is appropriate. For the sake of simplicity, <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\">Server-Sent Events</a> are used in this article.</p>\n<h1 id=\"Whats-next\"><a href=\"#Whats-next\" class=\"headerlink\" title=\"Whats next?\"></a>Whats next?</h1><p>This part gave a brief introduction to GraphQL. The next part is about actual code. Were going to build an example web app with live-updates using GraphQL, Go, MongoDB and VueJs. Stay tuned!</p>\n<p>&gt; <a href=\"https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.html\"><strong>Part 2</strong></a></p>\n"},{"title":"Modern, reactive web APIs with GraphQL, Go and Server-Sent Events  Part 2","date":"2020-06-06T16:34:02.000Z","_content":"\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_cover.png)\n\nIn the [previous](https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.html) part, concepts and benefits of GraphQL and [GraphQL Subscriptions](https://graphql.org/blog/subscriptions-in-graphql-and-relay/) were presented and proposed as a modern, highly flexible alternative for designing web APIs.\n\nIn this post, we are going to look at actual code. For demonstration purposes, a small single-page application (SPA) is built as an example. It can serve as a cleanly structured starting point for new apps based on the proposed tech stack.\n\n\\> **Code**: [muety/go-graphql-sse-example](https://github.com/muety/go-graphql-sse-example)\n\n# What to build?\nThe demo web app built in the context of this article comprised of a client-side frontend, built with [VueJS](https://vuejs.org) and a server-side component built with Go. The two components interact with each other through a GraphQL API, which additionally offers the option to subscribe to data updates using [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events).\n\nWith this demo app, we aim to mimic the functionality of a very basic, minimalist food ordering system. Customers can choose from a list of food products and place their orders. They are shown a waiting number an estimated processing time for their orders and are notified once the order is ready to be picked up. One the other side there is the kiosk operator, who views a live dashboard of currently pending orders, which appear just as they are being placed. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_screenshots1.png)\n\n## Technology Stack\nIn summary, the following technologies are used:\n* [Go](https://golang.org) as the primary backend-side language \n* [GraphQL](https://graphql.org/) as a \"protocol\" for defining web interfaces\n* [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events) as a simple protocol for live updates, used as an implementation of [GraphQL Subscriptions](https://graphql.org/blog/subscriptions-in-graphql-and-relay/) here\n* [MongoDB](https://www.mongodb.com/) as a flexible document database for storage\n* [VueJS](https://vuejs.org/) as a frontend framework to build single-page web applications\n\nIn addition, [graphql-request](https://www.npmjs.com/package/graphql-request) is used as a little helper library on the frontend to issue GraphQL queries more easily. It constitutes a light-weight wrapper around the plain [Fetch API](https://developer.mozilla.org/de/docs/Web/API/Fetch_API).\n\nOn the backend side, GraphQL development is facilitated by the excellent [graphql-go](github.com/graph-gophers/graphql-go) package, which already provides well-defined guidelines to get started (thanks to the authors!).\n\n# Data Model\nWith GraphQL, you need to specify a [schema](https://graphql.org/learn/schema/) using a GrapQL-specific syntax. It includes all entities, which your API should be able to deal with and is essentially a set of type definitions, split among one or more `.graphql` files.\n\nEvery GraphQL app consists of root types, defining all supported queries, mutations, and subscriptions as well as entity types. \n\nFor our app, the root schema looks like this ([schema/schema.graphql](https://github.com/muety/go-graphql-sse-example/blob/master/schema/schema.graphql)):\n\n```graphql\nschema {\n    query: Query\n    mutation: Mutation\n    subscription: Subscription\n}\n\ntype Query {\n    product(id: ID!): Product\n    products(): [Product]\n    order(id: ID!): Order\n    orders(status: String): [Order]\n}\n\ntype Mutation {\n    createOrder(order: OrderInput!): Order\n    updateOrder(order: OrderUpdateInput!): Order\n}\n\ntype Subscription {\n    orderCreated(): Order\n    orderChanged(id: ID!): Order\n}\n```\n\nThe type definition for the `Product` type referenced in the root schema is given as ([schema/type/product.graphql](https://github.com/muety/go-graphql-sse-example/blob/master/schema/type/product.graphql)):\n\n```graphql\ntype Product {\n    id: ID!\n    name: String\n    description: String\n    price: Float\n}\n```\n\nThe entire schema can be found in the GitHub [repository](https://github.com/muety/go-graphql-sse-example/tree/master/schema/type).\n\nAfter having defined your schema, in the case of [graphql-go](github.com/graph-gophers/graphql-go), it gets transformed into Go code, so it can be compiled into the final Go executable. It can then be loaded on application startup (see [server.go](https://github.com/muety/go-graphql-sse-example/blob/master/server.go#L35)) and served via an HTTP endpoint. Pretty straightforward!\n\n```go\ngraphqlSchema := graphql.MustParseSchema(schema.GetRootSchema(), &resolver.Resolver{})\nhttp.Handle(\"/api/query\", middleware.AddContext(ctx, &middleware.GraphQL{Schema: graphqlSchema}))\n```\n\n# Resolvers\nAfter having defined and loaded the schema, so-called [resolvers](https://graphql.org/learn/execution/#root-fields-resolvers) need to be defined. The concept of resolvers is common among most GraphQL server libraries for various different programming languages. A resolver is responsible for providing the data for every field of an entity, e.g. for the `ID`, `name`, `description`, and `price` fields. Such fields can be literals, as it is the case with all fields of the `Product` type, but also other nested entities, like in the `products` field of the following `Order` type:\n\n```graphql\ntype Order {\n    id: ID!\n    ...\n    products: [Product]\n}\n```\n\nFor literals, the resolver simply fills in the actual string, number, or boolean. For complex types, it delegates their resolution to their respective resolvers recursively. For instance, to resolve `products`, the `orderResolver` will ask a `productResolver` to do its respective duty. \n\nWhen working with `go-graphql`, a simple resolver looks like this ([resolvers/product.go](https://github.com/muety/go-graphql-sse-example/blob/master/resolver/product.go)):\n\n```go\ntype productResolver struct {\n\tp *model.Product\n}\n\nfunc (r *productResolver) Id() graphql.ID {\n\treturn graphql.ID(r.p.Id)\n}\n\nfunc (r *productResolver) Name() *string {\n\treturn &r.p.Name\n}\n\nfunc (r *productResolver) Description() *string {\n\treturn &r.p.Description\n}\n\nfunc (r *productResolver) Price() *float64 {\n\treturn &r.p.Price\n}\n\n```\n\nVery simple. It is instantiated with a reference to a `Product` struct, which was previously loaded from the database and simply maps attributes of the raw data model to the respective GraphQL fields. Please note that it doesn't have to be that trivial. For instance, your MongoDB schema might define `firstName` and `lastName` fields, while your GraphQL type only has `name`. In that case, the resolver would have to do some basic concatenation.\n\nThings get a little more complex when dealing with non-literal fields, like the `Order's` `products` above. In our database schema, an order only holds a list of product IDs as `items`. However, the GraphQL schema declares to return actual product objects. To do so, the respective resolver method first fetches the products for every ID from the database and then passes it on to `productResolver`s (see [resolvers/order.go](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/resolver/order.go#L53)):\n\n```go\n...\nfunc (r *orderResolver) Products(ctx context.Context) (*[]*productResolver, error) {\n\tl := make([]*productResolver, len(r.o.Items))\n\n\tproducts, err := ctx.Value(service.KeyProductService).(*service.ProductService).GetBatchMap(r.o.Items)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i, id := range r.o.Items {\n\t\tl[i] = &productResolver{p: products[id]}\n\t}\n\n\treturn &l, nil\n}\n...\n```\n\nResolvers are very modular, coherent in themselves and can be composed together to build up an entire API. Note that even the very entrypoint of the GraphQL API is just a \"root\" resolver. You can find all resolvers, including those for mutations and subscriptions as well, in the [repo](https://github.com/muety/go-graphql-sse-example/tree/master/resolver).\n\n# Subscriptions\nProbably the most interesting part here is subscriptions, as they provide a nice mechanism to make web applications reactive to backend-side data updates. As mentioned in the previous article, the GraphQL specification does not dictate how to technically implement subscriptions. Therefore, we decided to use Server-Sent Events (SSE) as a server-to-client communication channel. Technically, SSEs are simply a long-running HTTP request, which data is written to in form of a text stream and therefore very light-weight and easy to use. \n\nOn the backend side, we introduce a light-weight [event bus](github.com/leandro-lugaresi/hub) to our [services](https://github.com/muety/go-graphql-sse-example/tree/master/service). For every data change, i.e. updates, creations or deletions, an event is published to the bus (see [services/order.go](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/service/order.go#L66)):\n\n```go\nfunc (s *OrderService) Create(order *model.Order) (*model.Order, error) {\n\t...\n\torder.Id = res.InsertedID.(primitive.ObjectID).Hex()\n\ts.Hub.Publish(eventhub.Message{\n\t\tName:   KeyOrderCreated, // order.create\n\t\tFields: eventhub.Fields{\"id\": order.Id},\n    })\n    ...\n}\n```\n\nInside the resolver responsible for `orderCreated` queries, a subscription to the event bus is made once the user requests that subscription.\n\n```go\nfunc (r *Resolver) OrderCreated(ctx context.Context) (chan *orderResolver, error) {\n\tc := make(chan *orderResolver)\n\tgo subscribeOrder(service.KeyOrderCreated, ctx, c)\n\treturn c, nil\n}\n\nfunc subscribeOrder(key string, ctx context.Context, c chan *orderResolver) {\n\tsrv := ctx.Value(service.KeyOrderService).(*service.OrderService)\n\tsub := srv.Hub.NonBlockingSubscribe(10, key)\n\n\tdefer func() {\n\t\tsrv.Hub.Unsubscribe(sub)\n\t\tclose(c)\n\t}()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase m := <-sub.Receiver:\n\t\t\tif u, err := srv.Get(m.Fields[\"id\"].(string)); err != nil {\n\t\t\t\tlog.Println(err)\n\t\t\t} else {\n\t\t\t\tc <- &orderResolver{u}\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\nWhen a new order is inserted via the respective service, the above method is triggered as a consequence of being subscribed to `order.create` events. It reads the orer's ID from the event, uses the qualified service to fetch it from the database and passes it on to an `orderResolver` to translate it into the schema-conformal format. \n\nThe HTTP handler, which dispatched the user's GraphQL query to the above resolver, in turn, has subscribed to the result Go channel and writes every incoming `Order` instance to always-open HTTP stream (see [middleware/graphql.go](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/middleware/graphql.go#L59)):\n\n```go\n...\nc, err := h.Schema.Subscribe(ctx, params.Query, params.OperationName, params.Variables)\n...\nfor r := range c {\n        response := r.(*graphql.Response)\n        responseJSON, err := json.Marshal(response)\n        ...\n        fmt.Fprintf(w, \"data: %s\\n\\n\", responseJSON)\n        flusher.Flush()\n        ...\n}\n...\n```\n\n# Running Queries\n## Using GraphiQL\nDuring development, queries against a GraphQL API can be issued using the interactive GraphiQL browser, as demonstrated here.\n\n![](images/graphql_screencast2.gif)\n\n# Programatically\nOn the client-side of our application, GraphQL queries are run to consume the API. After all, any client, that is able to speak HTTP, can also consume a GraphQL API, as GraphQL requests are really just `POST` requests with a certain query in the body. \n\nFor instance, the [Vuex](https://vuex.vuejs.org/) store action responsible for loading a list of products in the frontend is this (see [store/products.js](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/web/src/store/products.js#L30)):\n\n```javascript\nasync fetchProducts({commit}) {\n    const q = `{\n        products() {\n        id\n        name\n        description\n        price\n        }\n    }`\n\n    const data = await Vue.$api.graphql.request(q)\n    commit('addProducts', data.products.map(Product.new))\n}\n```\n\nFor subscriptions, we rely on a slightly [modified version](https://github.com/muety/go-graphql-sse-example/blob/master/web/src/vendor/sse.js) of [sse.js](https://github.com/mpetazzoni/sse.js). It acts as a minimal wrapper around the browser's standard [EventSource](https://developer.mozilla.org/en-US/docs/Web/API/EventSource) and allows us to run SSE requests as `POST` (instead of `GET`) queries and pass some data in the body. We do so to fit well with GraphQL, where queries are always `POST` requests with a respective query body. \n\nSubscribing to new orders in the frontend is mostly done like this:\n\n```javascript\nasync subscribeOrderCreated({commit}) {\n    const q = `subscription {\n        orderCreated() {\n            id\n            queueId\n            createdAt\n            updatedAt\n            status\n            eta\n            totalSum\n            products {\n                id\n                name\n            }\n        }\n    }`\n\n    const source = Vue.$api.sse.request(q, null)\n    source.addEventListener('message', e => {\n        const payload = JSON.parse(e.data)\n        ...\n        commit('addOrder', new Order(payload.data.orderCreated))\n    })\n    ...\n    source.stream()\n}\n```\n\n# Demo\nAnd here's a live demo in action!\n\n![](images/graphql_screencast1.gif)\n\n# Outlook\nWhile the current demo implementation serves as a  in our opinion  clean and well-structured starting point for building GraphQL-based web apps, its current state has one major drawback. It lacks authentication and authorization. Usually, you want to control which user can query and modify which data. Therefore, another blog post will follow in the future, which explains how to authorize GraphQL query endpoints.\n\nApart from that, we consider the present technology stack a promising choice for new web applications with the potential to facilitate clean, well-organized code and to make development easier, and more flexible. ","source":"_posts/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2.md","raw":"---\ntitle: 'Modern, reactive web APIs with GraphQL, Go and Server-Sent Events  Part 2'\ndate: 2020-06-06 18:34:02\ntags:\n---\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_cover.png)\n\nIn the [previous](https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.html) part, concepts and benefits of GraphQL and [GraphQL Subscriptions](https://graphql.org/blog/subscriptions-in-graphql-and-relay/) were presented and proposed as a modern, highly flexible alternative for designing web APIs.\n\nIn this post, we are going to look at actual code. For demonstration purposes, a small single-page application (SPA) is built as an example. It can serve as a cleanly structured starting point for new apps based on the proposed tech stack.\n\n\\> **Code**: [muety/go-graphql-sse-example](https://github.com/muety/go-graphql-sse-example)\n\n# What to build?\nThe demo web app built in the context of this article comprised of a client-side frontend, built with [VueJS](https://vuejs.org) and a server-side component built with Go. The two components interact with each other through a GraphQL API, which additionally offers the option to subscribe to data updates using [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events).\n\nWith this demo app, we aim to mimic the functionality of a very basic, minimalist food ordering system. Customers can choose from a list of food products and place their orders. They are shown a waiting number an estimated processing time for their orders and are notified once the order is ready to be picked up. One the other side there is the kiosk operator, who views a live dashboard of currently pending orders, which appear just as they are being placed. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_screenshots1.png)\n\n## Technology Stack\nIn summary, the following technologies are used:\n* [Go](https://golang.org) as the primary backend-side language \n* [GraphQL](https://graphql.org/) as a \"protocol\" for defining web interfaces\n* [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events) as a simple protocol for live updates, used as an implementation of [GraphQL Subscriptions](https://graphql.org/blog/subscriptions-in-graphql-and-relay/) here\n* [MongoDB](https://www.mongodb.com/) as a flexible document database for storage\n* [VueJS](https://vuejs.org/) as a frontend framework to build single-page web applications\n\nIn addition, [graphql-request](https://www.npmjs.com/package/graphql-request) is used as a little helper library on the frontend to issue GraphQL queries more easily. It constitutes a light-weight wrapper around the plain [Fetch API](https://developer.mozilla.org/de/docs/Web/API/Fetch_API).\n\nOn the backend side, GraphQL development is facilitated by the excellent [graphql-go](github.com/graph-gophers/graphql-go) package, which already provides well-defined guidelines to get started (thanks to the authors!).\n\n# Data Model\nWith GraphQL, you need to specify a [schema](https://graphql.org/learn/schema/) using a GrapQL-specific syntax. It includes all entities, which your API should be able to deal with and is essentially a set of type definitions, split among one or more `.graphql` files.\n\nEvery GraphQL app consists of root types, defining all supported queries, mutations, and subscriptions as well as entity types. \n\nFor our app, the root schema looks like this ([schema/schema.graphql](https://github.com/muety/go-graphql-sse-example/blob/master/schema/schema.graphql)):\n\n```graphql\nschema {\n    query: Query\n    mutation: Mutation\n    subscription: Subscription\n}\n\ntype Query {\n    product(id: ID!): Product\n    products(): [Product]\n    order(id: ID!): Order\n    orders(status: String): [Order]\n}\n\ntype Mutation {\n    createOrder(order: OrderInput!): Order\n    updateOrder(order: OrderUpdateInput!): Order\n}\n\ntype Subscription {\n    orderCreated(): Order\n    orderChanged(id: ID!): Order\n}\n```\n\nThe type definition for the `Product` type referenced in the root schema is given as ([schema/type/product.graphql](https://github.com/muety/go-graphql-sse-example/blob/master/schema/type/product.graphql)):\n\n```graphql\ntype Product {\n    id: ID!\n    name: String\n    description: String\n    price: Float\n}\n```\n\nThe entire schema can be found in the GitHub [repository](https://github.com/muety/go-graphql-sse-example/tree/master/schema/type).\n\nAfter having defined your schema, in the case of [graphql-go](github.com/graph-gophers/graphql-go), it gets transformed into Go code, so it can be compiled into the final Go executable. It can then be loaded on application startup (see [server.go](https://github.com/muety/go-graphql-sse-example/blob/master/server.go#L35)) and served via an HTTP endpoint. Pretty straightforward!\n\n```go\ngraphqlSchema := graphql.MustParseSchema(schema.GetRootSchema(), &resolver.Resolver{})\nhttp.Handle(\"/api/query\", middleware.AddContext(ctx, &middleware.GraphQL{Schema: graphqlSchema}))\n```\n\n# Resolvers\nAfter having defined and loaded the schema, so-called [resolvers](https://graphql.org/learn/execution/#root-fields-resolvers) need to be defined. The concept of resolvers is common among most GraphQL server libraries for various different programming languages. A resolver is responsible for providing the data for every field of an entity, e.g. for the `ID`, `name`, `description`, and `price` fields. Such fields can be literals, as it is the case with all fields of the `Product` type, but also other nested entities, like in the `products` field of the following `Order` type:\n\n```graphql\ntype Order {\n    id: ID!\n    ...\n    products: [Product]\n}\n```\n\nFor literals, the resolver simply fills in the actual string, number, or boolean. For complex types, it delegates their resolution to their respective resolvers recursively. For instance, to resolve `products`, the `orderResolver` will ask a `productResolver` to do its respective duty. \n\nWhen working with `go-graphql`, a simple resolver looks like this ([resolvers/product.go](https://github.com/muety/go-graphql-sse-example/blob/master/resolver/product.go)):\n\n```go\ntype productResolver struct {\n\tp *model.Product\n}\n\nfunc (r *productResolver) Id() graphql.ID {\n\treturn graphql.ID(r.p.Id)\n}\n\nfunc (r *productResolver) Name() *string {\n\treturn &r.p.Name\n}\n\nfunc (r *productResolver) Description() *string {\n\treturn &r.p.Description\n}\n\nfunc (r *productResolver) Price() *float64 {\n\treturn &r.p.Price\n}\n\n```\n\nVery simple. It is instantiated with a reference to a `Product` struct, which was previously loaded from the database and simply maps attributes of the raw data model to the respective GraphQL fields. Please note that it doesn't have to be that trivial. For instance, your MongoDB schema might define `firstName` and `lastName` fields, while your GraphQL type only has `name`. In that case, the resolver would have to do some basic concatenation.\n\nThings get a little more complex when dealing with non-literal fields, like the `Order's` `products` above. In our database schema, an order only holds a list of product IDs as `items`. However, the GraphQL schema declares to return actual product objects. To do so, the respective resolver method first fetches the products for every ID from the database and then passes it on to `productResolver`s (see [resolvers/order.go](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/resolver/order.go#L53)):\n\n```go\n...\nfunc (r *orderResolver) Products(ctx context.Context) (*[]*productResolver, error) {\n\tl := make([]*productResolver, len(r.o.Items))\n\n\tproducts, err := ctx.Value(service.KeyProductService).(*service.ProductService).GetBatchMap(r.o.Items)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor i, id := range r.o.Items {\n\t\tl[i] = &productResolver{p: products[id]}\n\t}\n\n\treturn &l, nil\n}\n...\n```\n\nResolvers are very modular, coherent in themselves and can be composed together to build up an entire API. Note that even the very entrypoint of the GraphQL API is just a \"root\" resolver. You can find all resolvers, including those for mutations and subscriptions as well, in the [repo](https://github.com/muety/go-graphql-sse-example/tree/master/resolver).\n\n# Subscriptions\nProbably the most interesting part here is subscriptions, as they provide a nice mechanism to make web applications reactive to backend-side data updates. As mentioned in the previous article, the GraphQL specification does not dictate how to technically implement subscriptions. Therefore, we decided to use Server-Sent Events (SSE) as a server-to-client communication channel. Technically, SSEs are simply a long-running HTTP request, which data is written to in form of a text stream and therefore very light-weight and easy to use. \n\nOn the backend side, we introduce a light-weight [event bus](github.com/leandro-lugaresi/hub) to our [services](https://github.com/muety/go-graphql-sse-example/tree/master/service). For every data change, i.e. updates, creations or deletions, an event is published to the bus (see [services/order.go](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/service/order.go#L66)):\n\n```go\nfunc (s *OrderService) Create(order *model.Order) (*model.Order, error) {\n\t...\n\torder.Id = res.InsertedID.(primitive.ObjectID).Hex()\n\ts.Hub.Publish(eventhub.Message{\n\t\tName:   KeyOrderCreated, // order.create\n\t\tFields: eventhub.Fields{\"id\": order.Id},\n    })\n    ...\n}\n```\n\nInside the resolver responsible for `orderCreated` queries, a subscription to the event bus is made once the user requests that subscription.\n\n```go\nfunc (r *Resolver) OrderCreated(ctx context.Context) (chan *orderResolver, error) {\n\tc := make(chan *orderResolver)\n\tgo subscribeOrder(service.KeyOrderCreated, ctx, c)\n\treturn c, nil\n}\n\nfunc subscribeOrder(key string, ctx context.Context, c chan *orderResolver) {\n\tsrv := ctx.Value(service.KeyOrderService).(*service.OrderService)\n\tsub := srv.Hub.NonBlockingSubscribe(10, key)\n\n\tdefer func() {\n\t\tsrv.Hub.Unsubscribe(sub)\n\t\tclose(c)\n\t}()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase m := <-sub.Receiver:\n\t\t\tif u, err := srv.Get(m.Fields[\"id\"].(string)); err != nil {\n\t\t\t\tlog.Println(err)\n\t\t\t} else {\n\t\t\t\tc <- &orderResolver{u}\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\nWhen a new order is inserted via the respective service, the above method is triggered as a consequence of being subscribed to `order.create` events. It reads the orer's ID from the event, uses the qualified service to fetch it from the database and passes it on to an `orderResolver` to translate it into the schema-conformal format. \n\nThe HTTP handler, which dispatched the user's GraphQL query to the above resolver, in turn, has subscribed to the result Go channel and writes every incoming `Order` instance to always-open HTTP stream (see [middleware/graphql.go](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/middleware/graphql.go#L59)):\n\n```go\n...\nc, err := h.Schema.Subscribe(ctx, params.Query, params.OperationName, params.Variables)\n...\nfor r := range c {\n        response := r.(*graphql.Response)\n        responseJSON, err := json.Marshal(response)\n        ...\n        fmt.Fprintf(w, \"data: %s\\n\\n\", responseJSON)\n        flusher.Flush()\n        ...\n}\n...\n```\n\n# Running Queries\n## Using GraphiQL\nDuring development, queries against a GraphQL API can be issued using the interactive GraphiQL browser, as demonstrated here.\n\n![](images/graphql_screencast2.gif)\n\n# Programatically\nOn the client-side of our application, GraphQL queries are run to consume the API. After all, any client, that is able to speak HTTP, can also consume a GraphQL API, as GraphQL requests are really just `POST` requests with a certain query in the body. \n\nFor instance, the [Vuex](https://vuex.vuejs.org/) store action responsible for loading a list of products in the frontend is this (see [store/products.js](https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/web/src/store/products.js#L30)):\n\n```javascript\nasync fetchProducts({commit}) {\n    const q = `{\n        products() {\n        id\n        name\n        description\n        price\n        }\n    }`\n\n    const data = await Vue.$api.graphql.request(q)\n    commit('addProducts', data.products.map(Product.new))\n}\n```\n\nFor subscriptions, we rely on a slightly [modified version](https://github.com/muety/go-graphql-sse-example/blob/master/web/src/vendor/sse.js) of [sse.js](https://github.com/mpetazzoni/sse.js). It acts as a minimal wrapper around the browser's standard [EventSource](https://developer.mozilla.org/en-US/docs/Web/API/EventSource) and allows us to run SSE requests as `POST` (instead of `GET`) queries and pass some data in the body. We do so to fit well with GraphQL, where queries are always `POST` requests with a respective query body. \n\nSubscribing to new orders in the frontend is mostly done like this:\n\n```javascript\nasync subscribeOrderCreated({commit}) {\n    const q = `subscription {\n        orderCreated() {\n            id\n            queueId\n            createdAt\n            updatedAt\n            status\n            eta\n            totalSum\n            products {\n                id\n                name\n            }\n        }\n    }`\n\n    const source = Vue.$api.sse.request(q, null)\n    source.addEventListener('message', e => {\n        const payload = JSON.parse(e.data)\n        ...\n        commit('addOrder', new Order(payload.data.orderCreated))\n    })\n    ...\n    source.stream()\n}\n```\n\n# Demo\nAnd here's a live demo in action!\n\n![](images/graphql_screencast1.gif)\n\n# Outlook\nWhile the current demo implementation serves as a  in our opinion  clean and well-structured starting point for building GraphQL-based web apps, its current state has one major drawback. It lacks authentication and authorization. Usually, you want to control which user can query and modify which data. Therefore, another blog post will follow in the future, which explains how to authorize GraphQL query endpoints.\n\nApart from that, we consider the present technology stack a promising choice for new web applications with the potential to facilitate clean, well-organized code and to make development easier, and more flexible. ","slug":"modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-2","published":1,"updated":"2020-10-30T20:05:40.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqm6000to2e0ec1j3sj7","content":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_cover.png\"></p>\n<p>In the <a href=\"https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.html\">previous</a> part, concepts and benefits of GraphQL and <a href=\"https://graphql.org/blog/subscriptions-in-graphql-and-relay/\">GraphQL Subscriptions</a> were presented and proposed as a modern, highly flexible alternative for designing web APIs.</p>\n<p>In this post, we are going to look at actual code. For demonstration purposes, a small single-page application (SPA) is built as an example. It can serve as a cleanly structured starting point for new apps based on the proposed tech stack.</p>\n<p>&gt; <strong>Code</strong>: <a href=\"https://github.com/muety/go-graphql-sse-example\">muety/go-graphql-sse-example</a></p>\n<h1 id=\"What-to-build\"><a href=\"#What-to-build\" class=\"headerlink\" title=\"What to build?\"></a>What to build?</h1><p>The demo web app built in the context of this article comprised of a client-side frontend, built with <a href=\"https://vuejs.org/\">VueJS</a> and a server-side component built with Go. The two components interact with each other through a GraphQL API, which additionally offers the option to subscribe to data updates using <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\">Server-Sent Events</a>.</p>\n<p>With this demo app, we aim to mimic the functionality of a very basic, minimalist food ordering system. Customers can choose from a list of food products and place their orders. They are shown a waiting number an estimated processing time for their orders and are notified once the order is ready to be picked up. One the other side there is the kiosk operator, who views a live dashboard of currently pending orders, which appear just as they are being placed. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_screenshots1.png\"></p>\n<h2 id=\"Technology-Stack\"><a href=\"#Technology-Stack\" class=\"headerlink\" title=\"Technology Stack\"></a>Technology Stack</h2><p>In summary, the following technologies are used:</p>\n<ul>\n<li><a href=\"https://golang.org/\">Go</a> as the primary backend-side language </li>\n<li><a href=\"https://graphql.org/\">GraphQL</a> as a protocol for defining web interfaces</li>\n<li><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\">Server-Sent Events</a> as a simple protocol for live updates, used as an implementation of <a href=\"https://graphql.org/blog/subscriptions-in-graphql-and-relay/\">GraphQL Subscriptions</a> here</li>\n<li><a href=\"https://www.mongodb.com/\">MongoDB</a> as a flexible document database for storage</li>\n<li><a href=\"https://vuejs.org/\">VueJS</a> as a frontend framework to build single-page web applications</li>\n</ul>\n<p>In addition, <a href=\"https://www.npmjs.com/package/graphql-request\">graphql-request</a> is used as a little helper library on the frontend to issue GraphQL queries more easily. It constitutes a light-weight wrapper around the plain <a href=\"https://developer.mozilla.org/de/docs/Web/API/Fetch_API\">Fetch API</a>.</p>\n<p>On the backend side, GraphQL development is facilitated by the excellent <a href=\"github.com/graph-gophers/graphql-go\">graphql-go</a> package, which already provides well-defined guidelines to get started (thanks to the authors!).</p>\n<h1 id=\"Data-Model\"><a href=\"#Data-Model\" class=\"headerlink\" title=\"Data Model\"></a>Data Model</h1><p>With GraphQL, you need to specify a <a href=\"https://graphql.org/learn/schema/\">schema</a> using a GrapQL-specific syntax. It includes all entities, which your API should be able to deal with and is essentially a set of type definitions, split among one or more <code>.graphql</code> files.</p>\n<p>Every GraphQL app consists of root types, defining all supported queries, mutations, and subscriptions as well as entity types. </p>\n<p>For our app, the root schema looks like this (<a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/schema/schema.graphql\">schema/schema.graphql</a>):</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">schema &#123;</span><br><span class=\"line\">    query: Query</span><br><span class=\"line\">    mutation: Mutation</span><br><span class=\"line\">    subscription: Subscription</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type Query &#123;</span><br><span class=\"line\">    product(id: ID!): Product</span><br><span class=\"line\">    products(): [Product]</span><br><span class=\"line\">    order(id: ID!): Order</span><br><span class=\"line\">    orders(status: String): [Order]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type Mutation &#123;</span><br><span class=\"line\">    createOrder(order: OrderInput!): Order</span><br><span class=\"line\">    updateOrder(order: OrderUpdateInput!): Order</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type Subscription &#123;</span><br><span class=\"line\">    orderCreated(): Order</span><br><span class=\"line\">    orderChanged(id: ID!): Order</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>The type definition for the <code>Product</code> type referenced in the root schema is given as (<a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/schema/type/product.graphql\">schema/type/product.graphql</a>):</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type Product &#123;</span><br><span class=\"line\">    id: ID!</span><br><span class=\"line\">    name: String</span><br><span class=\"line\">    description: String</span><br><span class=\"line\">    price: Float</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>The entire schema can be found in the GitHub <a href=\"https://github.com/muety/go-graphql-sse-example/tree/master/schema/type\">repository</a>.</p>\n<p>After having defined your schema, in the case of <a href=\"github.com/graph-gophers/graphql-go\">graphql-go</a>, it gets transformed into Go code, so it can be compiled into the final Go executable. It can then be loaded on application startup (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/server.go#L35\">server.go</a>) and served via an HTTP endpoint. Pretty straightforward!</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">graphqlSchema := graphql.MustParseSchema(schema.GetRootSchema(), &amp;resolver.Resolver&#123;&#125;)</span><br><span class=\"line\">http.Handle(<span class=\"string\">&quot;/api/query&quot;</span>, middleware.AddContext(ctx, &amp;middleware.GraphQL&#123;Schema: graphqlSchema&#125;))</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Resolvers\"><a href=\"#Resolvers\" class=\"headerlink\" title=\"Resolvers\"></a>Resolvers</h1><p>After having defined and loaded the schema, so-called <a href=\"https://graphql.org/learn/execution/#root-fields-resolvers\">resolvers</a> need to be defined. The concept of resolvers is common among most GraphQL server libraries for various different programming languages. A resolver is responsible for providing the data for every field of an entity, e.g. for the <code>ID</code>, <code>name</code>, <code>description</code>, and <code>price</code> fields. Such fields can be literals, as it is the case with all fields of the <code>Product</code> type, but also other nested entities, like in the <code>products</code> field of the following <code>Order</code> type:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type Order &#123;</span><br><span class=\"line\">    id: ID!</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    products: [Product]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>For literals, the resolver simply fills in the actual string, number, or boolean. For complex types, it delegates their resolution to their respective resolvers recursively. For instance, to resolve <code>products</code>, the <code>orderResolver</code> will ask a <code>productResolver</code> to do its respective duty. </p>\n<p>When working with <code>go-graphql</code>, a simple resolver looks like this (<a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/resolver/product.go\">resolvers/product.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> productResolver <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">\tp *model.Product</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Id</span><span class=\"params\">()</span> <span class=\"title\">graphql</span>.<span class=\"title\">ID</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> graphql.ID(r.p.Id)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Name</span><span class=\"params\">()</span> *<span class=\"title\">string</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;r.p.Name</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Description</span><span class=\"params\">()</span> *<span class=\"title\">string</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;r.p.Description</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Price</span><span class=\"params\">()</span> *<span class=\"title\">float64</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;r.p.Price</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>Very simple. It is instantiated with a reference to a <code>Product</code> struct, which was previously loaded from the database and simply maps attributes of the raw data model to the respective GraphQL fields. Please note that it doesnt have to be that trivial. For instance, your MongoDB schema might define <code>firstName</code> and <code>lastName</code> fields, while your GraphQL type only has <code>name</code>. In that case, the resolver would have to do some basic concatenation.</p>\n<p>Things get a little more complex when dealing with non-literal fields, like the <code>Order&#39;s</code> <code>products</code> above. In our database schema, an order only holds a list of product IDs as <code>items</code>. However, the GraphQL schema declares to return actual product objects. To do so, the respective resolver method first fetches the products for every ID from the database and then passes it on to <code>productResolver</code>s (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/resolver/order.go#L53\">resolvers/order.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *orderResolver)</span> <span class=\"title\">Products</span><span class=\"params\">(ctx context.Context)</span> <span class=\"params\">(*[]*productResolver, error)</span></span> &#123;</span><br><span class=\"line\">\tl := <span class=\"built_in\">make</span>([]*productResolver, <span class=\"built_in\">len</span>(r.o.Items))</span><br><span class=\"line\"></span><br><span class=\"line\">\tproducts, err := ctx.Value(service.KeyProductService).(*service.ProductService).GetBatchMap(r.o.Items)</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> err != <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"literal\">nil</span>, err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i, id := <span class=\"keyword\">range</span> r.o.Items &#123;</span><br><span class=\"line\">\t\tl[i] = &amp;productResolver&#123;p: products[id]&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;l, <span class=\"literal\">nil</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>Resolvers are very modular, coherent in themselves and can be composed together to build up an entire API. Note that even the very entrypoint of the GraphQL API is just a root resolver. You can find all resolvers, including those for mutations and subscriptions as well, in the <a href=\"https://github.com/muety/go-graphql-sse-example/tree/master/resolver\">repo</a>.</p>\n<h1 id=\"Subscriptions\"><a href=\"#Subscriptions\" class=\"headerlink\" title=\"Subscriptions\"></a>Subscriptions</h1><p>Probably the most interesting part here is subscriptions, as they provide a nice mechanism to make web applications reactive to backend-side data updates. As mentioned in the previous article, the GraphQL specification does not dictate how to technically implement subscriptions. Therefore, we decided to use Server-Sent Events (SSE) as a server-to-client communication channel. Technically, SSEs are simply a long-running HTTP request, which data is written to in form of a text stream and therefore very light-weight and easy to use. </p>\n<p>On the backend side, we introduce a light-weight <a href=\"github.com/leandro-lugaresi/hub\">event bus</a> to our <a href=\"https://github.com/muety/go-graphql-sse-example/tree/master/service\">services</a>. For every data change, i.e. updates, creations or deletions, an event is published to the bus (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/service/order.go#L66\">services/order.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(s *OrderService)</span> <span class=\"title\">Create</span><span class=\"params\">(order *model.Order)</span> <span class=\"params\">(*model.Order, error)</span></span> &#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\torder.Id = res.InsertedID.(primitive.ObjectID).Hex()</span><br><span class=\"line\">\ts.Hub.Publish(eventhub.Message&#123;</span><br><span class=\"line\">\t\tName:   KeyOrderCreated, <span class=\"comment\">// order.create</span></span><br><span class=\"line\">\t\tFields: eventhub.Fields&#123;<span class=\"string\">&quot;id&quot;</span>: order.Id&#125;,</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Inside the resolver responsible for <code>orderCreated</code> queries, a subscription to the event bus is made once the user requests that subscription.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *Resolver)</span> <span class=\"title\">OrderCreated</span><span class=\"params\">(ctx context.Context)</span> <span class=\"params\">(<span class=\"keyword\">chan</span> *orderResolver, error)</span></span> &#123;</span><br><span class=\"line\">\tc := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> *orderResolver)</span><br><span class=\"line\">\t<span class=\"keyword\">go</span> subscribeOrder(service.KeyOrderCreated, ctx, c)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> c, <span class=\"literal\">nil</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">subscribeOrder</span><span class=\"params\">(key <span class=\"keyword\">string</span>, ctx context.Context, c <span class=\"keyword\">chan</span> *orderResolver)</span></span> &#123;</span><br><span class=\"line\">\tsrv := ctx.Value(service.KeyOrderService).(*service.OrderService)</span><br><span class=\"line\">\tsub := srv.Hub.NonBlockingSubscribe(<span class=\"number\">10</span>, key)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">defer</span> <span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t\tsrv.Hub.Unsubscribe(sub)</span><br><span class=\"line\">\t\t<span class=\"built_in\">close</span>(c)</span><br><span class=\"line\">\t&#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">select</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> &lt;-ctx.Done():</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> m := &lt;-sub.Receiver:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> u, err := srv.Get(m.Fields[<span class=\"string\">&quot;id&quot;</span>].(<span class=\"keyword\">string</span>)); err != <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">\t\t\t\tlog.Println(err)</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\tc &lt;- &amp;orderResolver&#123;u&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>When a new order is inserted via the respective service, the above method is triggered as a consequence of being subscribed to <code>order.create</code> events. It reads the orers ID from the event, uses the qualified service to fetch it from the database and passes it on to an <code>orderResolver</code> to translate it into the schema-conformal format. </p>\n<p>The HTTP handler, which dispatched the users GraphQL query to the above resolver, in turn, has subscribed to the result Go channel and writes every incoming <code>Order</code> instance to always-open HTTP stream (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/middleware/graphql.go#L59\">middleware/graphql.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">c, err := h.Schema.Subscribe(ctx, params.Query, params.OperationName, params.Variables)</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"keyword\">for</span> r := <span class=\"keyword\">range</span> c &#123;</span><br><span class=\"line\">        response := r.(*graphql.Response)</span><br><span class=\"line\">        responseJSON, err := json.Marshal(response)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        fmt.Fprintf(w, <span class=\"string\">&quot;data: %s\\n\\n&quot;</span>, responseJSON)</span><br><span class=\"line\">        flusher.Flush()</span><br><span class=\"line\">        ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Running-Queries\"><a href=\"#Running-Queries\" class=\"headerlink\" title=\"Running Queries\"></a>Running Queries</h1><h2 id=\"Using-GraphiQL\"><a href=\"#Using-GraphiQL\" class=\"headerlink\" title=\"Using GraphiQL\"></a>Using GraphiQL</h2><p>During development, queries against a GraphQL API can be issued using the interactive GraphiQL browser, as demonstrated here.</p>\n<p><img src=\"images/graphql_screencast2.gif\"></p>\n<h1 id=\"Programatically\"><a href=\"#Programatically\" class=\"headerlink\" title=\"Programatically\"></a>Programatically</h1><p>On the client-side of our application, GraphQL queries are run to consume the API. After all, any client, that is able to speak HTTP, can also consume a GraphQL API, as GraphQL requests are really just <code>POST</code> requests with a certain query in the body. </p>\n<p>For instance, the <a href=\"https://vuex.vuejs.org/\">Vuex</a> store action responsible for loading a list of products in the frontend is this (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/web/src/store/products.js#L30\">store/products.js</a>):</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">async</span> <span class=\"function\"><span class=\"title\">fetchProducts</span>(<span class=\"params\">&#123;commit&#125;</span>)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> q = <span class=\"string\">`&#123;</span></span><br><span class=\"line\"><span class=\"string\">        products() &#123;</span></span><br><span class=\"line\"><span class=\"string\">        id</span></span><br><span class=\"line\"><span class=\"string\">        name</span></span><br><span class=\"line\"><span class=\"string\">        description</span></span><br><span class=\"line\"><span class=\"string\">        price</span></span><br><span class=\"line\"><span class=\"string\">        &#125;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;`</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">const</span> data = <span class=\"keyword\">await</span> Vue.$api.graphql.request(q)</span><br><span class=\"line\">    commit(<span class=\"string\">&#x27;addProducts&#x27;</span>, data.products.map(Product.new))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>For subscriptions, we rely on a slightly <a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/web/src/vendor/sse.js\">modified version</a> of <a href=\"https://github.com/mpetazzoni/sse.js\">sse.js</a>. It acts as a minimal wrapper around the browsers standard <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/EventSource\">EventSource</a> and allows us to run SSE requests as <code>POST</code> (instead of <code>GET</code>) queries and pass some data in the body. We do so to fit well with GraphQL, where queries are always <code>POST</code> requests with a respective query body. </p>\n<p>Subscribing to new orders in the frontend is mostly done like this:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">async</span> <span class=\"function\"><span class=\"title\">subscribeOrderCreated</span>(<span class=\"params\">&#123;commit&#125;</span>)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> q = <span class=\"string\">`subscription &#123;</span></span><br><span class=\"line\"><span class=\"string\">        orderCreated() &#123;</span></span><br><span class=\"line\"><span class=\"string\">            id</span></span><br><span class=\"line\"><span class=\"string\">            queueId</span></span><br><span class=\"line\"><span class=\"string\">            createdAt</span></span><br><span class=\"line\"><span class=\"string\">            updatedAt</span></span><br><span class=\"line\"><span class=\"string\">            status</span></span><br><span class=\"line\"><span class=\"string\">            eta</span></span><br><span class=\"line\"><span class=\"string\">            totalSum</span></span><br><span class=\"line\"><span class=\"string\">            products &#123;</span></span><br><span class=\"line\"><span class=\"string\">                id</span></span><br><span class=\"line\"><span class=\"string\">                name</span></span><br><span class=\"line\"><span class=\"string\">            &#125;</span></span><br><span class=\"line\"><span class=\"string\">        &#125;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;`</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">const</span> source = Vue.$api.sse.request(q, <span class=\"literal\">null</span>)</span><br><span class=\"line\">    source.addEventListener(<span class=\"string\">&#x27;message&#x27;</span>, <span class=\"function\"><span class=\"params\">e</span> =&gt;</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">const</span> payload = <span class=\"built_in\">JSON</span>.parse(e.data)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        commit(<span class=\"string\">&#x27;addOrder&#x27;</span>, <span class=\"keyword\">new</span> Order(payload.data.orderCreated))</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    source.stream()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Demo\"><a href=\"#Demo\" class=\"headerlink\" title=\"Demo\"></a>Demo</h1><p>And heres a live demo in action!</p>\n<p><img src=\"images/graphql_screencast1.gif\"></p>\n<h1 id=\"Outlook\"><a href=\"#Outlook\" class=\"headerlink\" title=\"Outlook\"></a>Outlook</h1><p>While the current demo implementation serves as a  in our opinion  clean and well-structured starting point for building GraphQL-based web apps, its current state has one major drawback. It lacks authentication and authorization. Usually, you want to control which user can query and modify which data. Therefore, another blog post will follow in the future, which explains how to authorize GraphQL query endpoints.</p>\n<p>Apart from that, we consider the present technology stack a promising choice for new web applications with the potential to facilitate clean, well-organized code and to make development easier, and more flexible. </p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_cover.png\"></p>\n<p>In the <a href=\"https://muetsch.io/modern-reactive-web-apis-with-graphql-go-and-server-sent-events-part-1.html\">previous</a> part, concepts and benefits of GraphQL and <a href=\"https://graphql.org/blog/subscriptions-in-graphql-and-relay/\">GraphQL Subscriptions</a> were presented and proposed as a modern, highly flexible alternative for designing web APIs.</p>\n<p>In this post, we are going to look at actual code. For demonstration purposes, a small single-page application (SPA) is built as an example. It can serve as a cleanly structured starting point for new apps based on the proposed tech stack.</p>\n<p>&gt; <strong>Code</strong>: <a href=\"https://github.com/muety/go-graphql-sse-example\">muety/go-graphql-sse-example</a></p>\n<h1 id=\"What-to-build\"><a href=\"#What-to-build\" class=\"headerlink\" title=\"What to build?\"></a>What to build?</h1><p>The demo web app built in the context of this article comprised of a client-side frontend, built with <a href=\"https://vuejs.org/\">VueJS</a> and a server-side component built with Go. The two components interact with each other through a GraphQL API, which additionally offers the option to subscribe to data updates using <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\">Server-Sent Events</a>.</p>\n<p>With this demo app, we aim to mimic the functionality of a very basic, minimalist food ordering system. Customers can choose from a list of food products and place their orders. They are shown a waiting number an estimated processing time for their orders and are notified once the order is ready to be picked up. One the other side there is the kiosk operator, who views a live dashboard of currently pending orders, which appear just as they are being placed. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/graphql_screenshots1.png\"></p>\n<h2 id=\"Technology-Stack\"><a href=\"#Technology-Stack\" class=\"headerlink\" title=\"Technology Stack\"></a>Technology Stack</h2><p>In summary, the following technologies are used:</p>\n<ul>\n<li><a href=\"https://golang.org/\">Go</a> as the primary backend-side language </li>\n<li><a href=\"https://graphql.org/\">GraphQL</a> as a protocol for defining web interfaces</li>\n<li><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\">Server-Sent Events</a> as a simple protocol for live updates, used as an implementation of <a href=\"https://graphql.org/blog/subscriptions-in-graphql-and-relay/\">GraphQL Subscriptions</a> here</li>\n<li><a href=\"https://www.mongodb.com/\">MongoDB</a> as a flexible document database for storage</li>\n<li><a href=\"https://vuejs.org/\">VueJS</a> as a frontend framework to build single-page web applications</li>\n</ul>\n<p>In addition, <a href=\"https://www.npmjs.com/package/graphql-request\">graphql-request</a> is used as a little helper library on the frontend to issue GraphQL queries more easily. It constitutes a light-weight wrapper around the plain <a href=\"https://developer.mozilla.org/de/docs/Web/API/Fetch_API\">Fetch API</a>.</p>\n<p>On the backend side, GraphQL development is facilitated by the excellent <a href=\"github.com/graph-gophers/graphql-go\">graphql-go</a> package, which already provides well-defined guidelines to get started (thanks to the authors!).</p>\n<h1 id=\"Data-Model\"><a href=\"#Data-Model\" class=\"headerlink\" title=\"Data Model\"></a>Data Model</h1><p>With GraphQL, you need to specify a <a href=\"https://graphql.org/learn/schema/\">schema</a> using a GrapQL-specific syntax. It includes all entities, which your API should be able to deal with and is essentially a set of type definitions, split among one or more <code>.graphql</code> files.</p>\n<p>Every GraphQL app consists of root types, defining all supported queries, mutations, and subscriptions as well as entity types. </p>\n<p>For our app, the root schema looks like this (<a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/schema/schema.graphql\">schema/schema.graphql</a>):</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">schema &#123;</span><br><span class=\"line\">    query: Query</span><br><span class=\"line\">    mutation: Mutation</span><br><span class=\"line\">    subscription: Subscription</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type Query &#123;</span><br><span class=\"line\">    product(id: ID!): Product</span><br><span class=\"line\">    products(): [Product]</span><br><span class=\"line\">    order(id: ID!): Order</span><br><span class=\"line\">    orders(status: String): [Order]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type Mutation &#123;</span><br><span class=\"line\">    createOrder(order: OrderInput!): Order</span><br><span class=\"line\">    updateOrder(order: OrderUpdateInput!): Order</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type Subscription &#123;</span><br><span class=\"line\">    orderCreated(): Order</span><br><span class=\"line\">    orderChanged(id: ID!): Order</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>The type definition for the <code>Product</code> type referenced in the root schema is given as (<a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/schema/type/product.graphql\">schema/type/product.graphql</a>):</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type Product &#123;</span><br><span class=\"line\">    id: ID!</span><br><span class=\"line\">    name: String</span><br><span class=\"line\">    description: String</span><br><span class=\"line\">    price: Float</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>The entire schema can be found in the GitHub <a href=\"https://github.com/muety/go-graphql-sse-example/tree/master/schema/type\">repository</a>.</p>\n<p>After having defined your schema, in the case of <a href=\"github.com/graph-gophers/graphql-go\">graphql-go</a>, it gets transformed into Go code, so it can be compiled into the final Go executable. It can then be loaded on application startup (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/server.go#L35\">server.go</a>) and served via an HTTP endpoint. Pretty straightforward!</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">graphqlSchema := graphql.MustParseSchema(schema.GetRootSchema(), &amp;resolver.Resolver&#123;&#125;)</span><br><span class=\"line\">http.Handle(<span class=\"string\">&quot;/api/query&quot;</span>, middleware.AddContext(ctx, &amp;middleware.GraphQL&#123;Schema: graphqlSchema&#125;))</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Resolvers\"><a href=\"#Resolvers\" class=\"headerlink\" title=\"Resolvers\"></a>Resolvers</h1><p>After having defined and loaded the schema, so-called <a href=\"https://graphql.org/learn/execution/#root-fields-resolvers\">resolvers</a> need to be defined. The concept of resolvers is common among most GraphQL server libraries for various different programming languages. A resolver is responsible for providing the data for every field of an entity, e.g. for the <code>ID</code>, <code>name</code>, <code>description</code>, and <code>price</code> fields. Such fields can be literals, as it is the case with all fields of the <code>Product</code> type, but also other nested entities, like in the <code>products</code> field of the following <code>Order</code> type:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type Order &#123;</span><br><span class=\"line\">    id: ID!</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    products: [Product]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>For literals, the resolver simply fills in the actual string, number, or boolean. For complex types, it delegates their resolution to their respective resolvers recursively. For instance, to resolve <code>products</code>, the <code>orderResolver</code> will ask a <code>productResolver</code> to do its respective duty. </p>\n<p>When working with <code>go-graphql</code>, a simple resolver looks like this (<a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/resolver/product.go\">resolvers/product.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> productResolver <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">\tp *model.Product</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Id</span><span class=\"params\">()</span> <span class=\"title\">graphql</span>.<span class=\"title\">ID</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> graphql.ID(r.p.Id)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Name</span><span class=\"params\">()</span> *<span class=\"title\">string</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;r.p.Name</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Description</span><span class=\"params\">()</span> *<span class=\"title\">string</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;r.p.Description</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *productResolver)</span> <span class=\"title\">Price</span><span class=\"params\">()</span> *<span class=\"title\">float64</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;r.p.Price</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>Very simple. It is instantiated with a reference to a <code>Product</code> struct, which was previously loaded from the database and simply maps attributes of the raw data model to the respective GraphQL fields. Please note that it doesnt have to be that trivial. For instance, your MongoDB schema might define <code>firstName</code> and <code>lastName</code> fields, while your GraphQL type only has <code>name</code>. In that case, the resolver would have to do some basic concatenation.</p>\n<p>Things get a little more complex when dealing with non-literal fields, like the <code>Order&#39;s</code> <code>products</code> above. In our database schema, an order only holds a list of product IDs as <code>items</code>. However, the GraphQL schema declares to return actual product objects. To do so, the respective resolver method first fetches the products for every ID from the database and then passes it on to <code>productResolver</code>s (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/resolver/order.go#L53\">resolvers/order.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *orderResolver)</span> <span class=\"title\">Products</span><span class=\"params\">(ctx context.Context)</span> <span class=\"params\">(*[]*productResolver, error)</span></span> &#123;</span><br><span class=\"line\">\tl := <span class=\"built_in\">make</span>([]*productResolver, <span class=\"built_in\">len</span>(r.o.Items))</span><br><span class=\"line\"></span><br><span class=\"line\">\tproducts, err := ctx.Value(service.KeyProductService).(*service.ProductService).GetBatchMap(r.o.Items)</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> err != <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"literal\">nil</span>, err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i, id := <span class=\"keyword\">range</span> r.o.Items &#123;</span><br><span class=\"line\">\t\tl[i] = &amp;productResolver&#123;p: products[id]&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> &amp;l, <span class=\"literal\">nil</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>Resolvers are very modular, coherent in themselves and can be composed together to build up an entire API. Note that even the very entrypoint of the GraphQL API is just a root resolver. You can find all resolvers, including those for mutations and subscriptions as well, in the <a href=\"https://github.com/muety/go-graphql-sse-example/tree/master/resolver\">repo</a>.</p>\n<h1 id=\"Subscriptions\"><a href=\"#Subscriptions\" class=\"headerlink\" title=\"Subscriptions\"></a>Subscriptions</h1><p>Probably the most interesting part here is subscriptions, as they provide a nice mechanism to make web applications reactive to backend-side data updates. As mentioned in the previous article, the GraphQL specification does not dictate how to technically implement subscriptions. Therefore, we decided to use Server-Sent Events (SSE) as a server-to-client communication channel. Technically, SSEs are simply a long-running HTTP request, which data is written to in form of a text stream and therefore very light-weight and easy to use. </p>\n<p>On the backend side, we introduce a light-weight <a href=\"github.com/leandro-lugaresi/hub\">event bus</a> to our <a href=\"https://github.com/muety/go-graphql-sse-example/tree/master/service\">services</a>. For every data change, i.e. updates, creations or deletions, an event is published to the bus (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/service/order.go#L66\">services/order.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(s *OrderService)</span> <span class=\"title\">Create</span><span class=\"params\">(order *model.Order)</span> <span class=\"params\">(*model.Order, error)</span></span> &#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\torder.Id = res.InsertedID.(primitive.ObjectID).Hex()</span><br><span class=\"line\">\ts.Hub.Publish(eventhub.Message&#123;</span><br><span class=\"line\">\t\tName:   KeyOrderCreated, <span class=\"comment\">// order.create</span></span><br><span class=\"line\">\t\tFields: eventhub.Fields&#123;<span class=\"string\">&quot;id&quot;</span>: order.Id&#125;,</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Inside the resolver responsible for <code>orderCreated</code> queries, a subscription to the event bus is made once the user requests that subscription.</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(r *Resolver)</span> <span class=\"title\">OrderCreated</span><span class=\"params\">(ctx context.Context)</span> <span class=\"params\">(<span class=\"keyword\">chan</span> *orderResolver, error)</span></span> &#123;</span><br><span class=\"line\">\tc := <span class=\"built_in\">make</span>(<span class=\"keyword\">chan</span> *orderResolver)</span><br><span class=\"line\">\t<span class=\"keyword\">go</span> subscribeOrder(service.KeyOrderCreated, ctx, c)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> c, <span class=\"literal\">nil</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">subscribeOrder</span><span class=\"params\">(key <span class=\"keyword\">string</span>, ctx context.Context, c <span class=\"keyword\">chan</span> *orderResolver)</span></span> &#123;</span><br><span class=\"line\">\tsrv := ctx.Value(service.KeyOrderService).(*service.OrderService)</span><br><span class=\"line\">\tsub := srv.Hub.NonBlockingSubscribe(<span class=\"number\">10</span>, key)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">defer</span> <span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t\tsrv.Hub.Unsubscribe(sub)</span><br><span class=\"line\">\t\t<span class=\"built_in\">close</span>(c)</span><br><span class=\"line\">\t&#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">select</span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> &lt;-ctx.Done():</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">case</span> m := &lt;-sub.Receiver:</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> u, err := srv.Get(m.Fields[<span class=\"string\">&quot;id&quot;</span>].(<span class=\"keyword\">string</span>)); err != <span class=\"literal\">nil</span> &#123;</span><br><span class=\"line\">\t\t\t\tlog.Println(err)</span><br><span class=\"line\">\t\t\t&#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">\t\t\t\tc &lt;- &amp;orderResolver&#123;u&#125;</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>When a new order is inserted via the respective service, the above method is triggered as a consequence of being subscribed to <code>order.create</code> events. It reads the orers ID from the event, uses the qualified service to fetch it from the database and passes it on to an <code>orderResolver</code> to translate it into the schema-conformal format. </p>\n<p>The HTTP handler, which dispatched the users GraphQL query to the above resolver, in turn, has subscribed to the result Go channel and writes every incoming <code>Order</code> instance to always-open HTTP stream (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/middleware/graphql.go#L59\">middleware/graphql.go</a>):</p>\n<figure class=\"highlight go\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">c, err := h.Schema.Subscribe(ctx, params.Query, params.OperationName, params.Variables)</span><br><span class=\"line\">...</span><br><span class=\"line\"><span class=\"keyword\">for</span> r := <span class=\"keyword\">range</span> c &#123;</span><br><span class=\"line\">        response := r.(*graphql.Response)</span><br><span class=\"line\">        responseJSON, err := json.Marshal(response)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        fmt.Fprintf(w, <span class=\"string\">&quot;data: %s\\n\\n&quot;</span>, responseJSON)</span><br><span class=\"line\">        flusher.Flush()</span><br><span class=\"line\">        ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Running-Queries\"><a href=\"#Running-Queries\" class=\"headerlink\" title=\"Running Queries\"></a>Running Queries</h1><h2 id=\"Using-GraphiQL\"><a href=\"#Using-GraphiQL\" class=\"headerlink\" title=\"Using GraphiQL\"></a>Using GraphiQL</h2><p>During development, queries against a GraphQL API can be issued using the interactive GraphiQL browser, as demonstrated here.</p>\n<p><img src=\"images/graphql_screencast2.gif\"></p>\n<h1 id=\"Programatically\"><a href=\"#Programatically\" class=\"headerlink\" title=\"Programatically\"></a>Programatically</h1><p>On the client-side of our application, GraphQL queries are run to consume the API. After all, any client, that is able to speak HTTP, can also consume a GraphQL API, as GraphQL requests are really just <code>POST</code> requests with a certain query in the body. </p>\n<p>For instance, the <a href=\"https://vuex.vuejs.org/\">Vuex</a> store action responsible for loading a list of products in the frontend is this (see <a href=\"https://github.com/muety/go-graphql-sse-example/blob/e8e5c24ea413aa078e3c6816a5a85f7337209091/web/src/store/products.js#L30\">store/products.js</a>):</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">async</span> <span class=\"function\"><span class=\"title\">fetchProducts</span>(<span class=\"params\">&#123;commit&#125;</span>)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> q = <span class=\"string\">`&#123;</span></span><br><span class=\"line\"><span class=\"string\">        products() &#123;</span></span><br><span class=\"line\"><span class=\"string\">        id</span></span><br><span class=\"line\"><span class=\"string\">        name</span></span><br><span class=\"line\"><span class=\"string\">        description</span></span><br><span class=\"line\"><span class=\"string\">        price</span></span><br><span class=\"line\"><span class=\"string\">        &#125;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;`</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">const</span> data = <span class=\"keyword\">await</span> Vue.$api.graphql.request(q)</span><br><span class=\"line\">    commit(<span class=\"string\">&#x27;addProducts&#x27;</span>, data.products.map(Product.new))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>For subscriptions, we rely on a slightly <a href=\"https://github.com/muety/go-graphql-sse-example/blob/master/web/src/vendor/sse.js\">modified version</a> of <a href=\"https://github.com/mpetazzoni/sse.js\">sse.js</a>. It acts as a minimal wrapper around the browsers standard <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/EventSource\">EventSource</a> and allows us to run SSE requests as <code>POST</code> (instead of <code>GET</code>) queries and pass some data in the body. We do so to fit well with GraphQL, where queries are always <code>POST</code> requests with a respective query body. </p>\n<p>Subscribing to new orders in the frontend is mostly done like this:</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">async</span> <span class=\"function\"><span class=\"title\">subscribeOrderCreated</span>(<span class=\"params\">&#123;commit&#125;</span>)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> q = <span class=\"string\">`subscription &#123;</span></span><br><span class=\"line\"><span class=\"string\">        orderCreated() &#123;</span></span><br><span class=\"line\"><span class=\"string\">            id</span></span><br><span class=\"line\"><span class=\"string\">            queueId</span></span><br><span class=\"line\"><span class=\"string\">            createdAt</span></span><br><span class=\"line\"><span class=\"string\">            updatedAt</span></span><br><span class=\"line\"><span class=\"string\">            status</span></span><br><span class=\"line\"><span class=\"string\">            eta</span></span><br><span class=\"line\"><span class=\"string\">            totalSum</span></span><br><span class=\"line\"><span class=\"string\">            products &#123;</span></span><br><span class=\"line\"><span class=\"string\">                id</span></span><br><span class=\"line\"><span class=\"string\">                name</span></span><br><span class=\"line\"><span class=\"string\">            &#125;</span></span><br><span class=\"line\"><span class=\"string\">        &#125;</span></span><br><span class=\"line\"><span class=\"string\">    &#125;`</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">const</span> source = Vue.$api.sse.request(q, <span class=\"literal\">null</span>)</span><br><span class=\"line\">    source.addEventListener(<span class=\"string\">&#x27;message&#x27;</span>, <span class=\"function\"><span class=\"params\">e</span> =&gt;</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">const</span> payload = <span class=\"built_in\">JSON</span>.parse(e.data)</span><br><span class=\"line\">        ...</span><br><span class=\"line\">        commit(<span class=\"string\">&#x27;addOrder&#x27;</span>, <span class=\"keyword\">new</span> Order(payload.data.orderCreated))</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    source.stream()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Demo\"><a href=\"#Demo\" class=\"headerlink\" title=\"Demo\"></a>Demo</h1><p>And heres a live demo in action!</p>\n<p><img src=\"images/graphql_screencast1.gif\"></p>\n<h1 id=\"Outlook\"><a href=\"#Outlook\" class=\"headerlink\" title=\"Outlook\"></a>Outlook</h1><p>While the current demo implementation serves as a  in our opinion  clean and well-structured starting point for building GraphQL-based web apps, its current state has one major drawback. It lacks authentication and authorization. Usually, you want to control which user can query and modify which data. Therefore, another blog post will follow in the future, which explains how to authorize GraphQL query endpoints.</p>\n<p>Apart from that, we consider the present technology stack a promising choice for new web applications with the potential to facilitate clean, well-organized code and to make development easier, and more flexible. </p>\n"},{"title":"My teck stack if I had to build an app today","date":"2016-11-11T22:04:56.000Z","_content":"\nWhat technology stack would I choose, if I had to develop a web application completely from scratch? That's the question this article will cover.\n\nFirst of all: by saying web application I'm referring to something between a plain static HTML page and an entire Facebook. Basically, an application that fulfills a certain domain of tasks for the user and that requires the usual features like user management, a backend database, multiple UI views and controls, etc. The size of application I'm thinking of could be a browser-based chat app, password-manager or something similar. Neither too simple, nor too complex.\n\nBack to the topic. Choosing the right technology for a web app feels much like customizing a new PC or even a new car. There are nearly endless options to be weighed to finally pick a bunch of them for a new web application. This super famous article [How it feels to learn JavaScript in 2016](https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f#.m1nodqu6f) complains about the confusing and ever-growing, chaotic jungle of new JavaScript frameworks in an ironical way. Indeed, I hear similar arguments from many developers these days. Many of them claim that code quality was getting worse in the web and that every newbie JavaScript programmer threw out his own new framework on yet another .io domain. Although that might be true to a certain extent, I personally still like the great technological variety and innovation. I love to browse GitHub, Reddit, Hackernews and Co. to discover new cool libraries to try out in a project some day. And here's what I would pick if I had to realize such a project right today and if there weren't any restrictions. \n\nOf course, the technology choice depends on the concrete project requirements to a certain extent, but not completely. Consequently, a new project is always a chance to try something new. ThoughtWorks just published their new [technology radar for 2016](https://www.thoughtworks.com/de/radar), where they separate into categories _adopt_, _trial_, _assess_ and _hold_. Of course, _hold_-techs are not an option for new projects and I actually pretty much agree with their views on what has to be in the _hold_ category. _Adopt_ basically are things that are modern, but also well-established enough to avoid too much risk. _Trial_-techs are more experimental and _assess_ are the latest fancy s***, so to say. Since I'm extremely eager to try out new things, my stack would probably mostly consist of technologies from the last category. So what would my stack now look like? Actually, I couldn't decide on one stack, but set up two: _the fancy one_ and _the super fancy one_. Additionally, I define their intersection as _the base stack_, which consists of fundamental tools etc. that both have in common.\n\n### The base stack\nFirst of all, I'd use __Git__ for version control, __Visual Studio Code__ as code editor and __GitLab__ for repository hosting and as build server. If I didn't had to implement user management myself, I'd pick __Auth0__ for that. For deployment, I'd use containers with __Docker__ on __DigitalOcean__ machines and if I needed multiple instances, __Rancher__ would help me to manage them. As reverse proxy in front of the backend I'd choose __nginx__ since it's extremely efficient, performant and has __HTTP/2.0__ support. For bundling, __Webpack__ would be my choice and task automation would be done using plain __npm scripts__. For styling the UI, I'd simply use __Bootstrap 4__ and __SCSS__.  \n\n### The super fancy stack\nThe key point here is that I'd want to abandon a traditional REST API in my project and use __GraphQL__ instead. The backend would be written in __NodeJS__ with [__Graffiti__](https://github.com/RisingStack/graffiti) as GraphQL implementation. I don't know much about the latter one, yet, except for that it's the de-facto GraphQL solution for Node. Why Node? Because it's simply the best choice for the web (my view...). It's performant, comfortable to develop and especially brings consistency by having JS in front- and backend. By always being quite up-to-date with the latest ES* features, Node doesn't get boring. Since GraphQL is told to work best with other Facebook technology, I'd not be that experimental here and build the frontend on __React__ plus __Relay__ (which is still completely new for me). Database would probably be a __MongoDB__ (JSON everywhere!) with [__Waterline__](https://github.com/balderdashy/waterline) ORM. To put a cherry on the cake, I'd also introduce __Redux__ in addition. I haven't worked with it much, yet, and I heard that it's kind of mind-blowing in the beginning. However, I consider its concept to cover a large potential to manage consistency in my app. The last thing here is that I desperately want is ES6 syntax. It isn't supported by the React compiler afaik (please correct me, if I'm wrong), so I'd use __Babel__ to have latest JavaScript features. If having to go mobile, __React Native__ would be the rational choice.\n\n### The fancy stack\nThis stack differs from the _super fancy stack_ in a few points. A key point is that it would not use GraphQL, but a good old REST API. This API would be written in __Go__, since I like the language - especially its efficiency and its good suitability for web development. More precisely, I'd use the [__Iris__](http://iris-go.com/) framework. I've read the documentation and it looked incredibly powerful to me (in terms of both functionality and perfomance). _(__EDIT:__ It's not what it seems! Please see my comment below!)._ For the frontend I'm balancing between __Angular 2__ and [__Aurelia__](http://aurelia.io/). Angular 2 is guaranteed to work for any potential case, is extremely powerful and has great community- and library support. However, Aurelia look promising, too, and probably is even more clear and less boilerplate code. Consequently, I'd give it a try. But if having to go mobile, I'd still favor Angular 2, since it perfectly aligns with __Ionic 2__.\n\nTwo other options, which look really interesting to me are __Meteor__ and __HorizonJS__. However, I'm not sure, if it's a good idea to commit to only one comprehensive framework through the full stack.\n\nSo these are my two alternative ways through the webdev jungle - btw, [this good article](https://medium.freecodecamp.com/a-study-plan-to-cure-javascript-fatigue-8ad3a54f2eb1#.3ri3a1fdv) describes another one, especially for newcomer web developers. Sorry, that I haven't justified all choices. Actually, as you probably know, if you're a developer, subjective views like these often aren't even based on pure rational considerations, but are rather emotional and spontaneous.\n\nPlease feel free to give me feedback on my tech stack of choice! \n\n__EDIT:__ Another framework Id really like to try out is [InfernoJS](https://infernojs.org/), because it claims to be extremely lightweight and performant. However, before using Inferno, one should probably be familiar with React, since it uses very similar concepts and syntaxes.\n\n__EDIT 2:__ After having read [this article](http://www.florinpatan.ro/2016/10/why-you-should-not-use-iris-for-your-go.html) and having done some further research on the Iris framework I really have to retract my above statement that Id use it as a web backend. While it looks nice on paper, after diving a little deeper I really have to admit that itd be morally tenable to support the authors of that project. So please forget about Iris and take a look at [Beego](https://beego.me/) instead.","source":"_posts/my-teck-stack-if-i-had-to-build-an-app-today.md","raw":"---\ntitle: My teck stack if I had to build an app today\ndate: 2016-11-11 23:04:56\ntags:\n---\n\nWhat technology stack would I choose, if I had to develop a web application completely from scratch? That's the question this article will cover.\n\nFirst of all: by saying web application I'm referring to something between a plain static HTML page and an entire Facebook. Basically, an application that fulfills a certain domain of tasks for the user and that requires the usual features like user management, a backend database, multiple UI views and controls, etc. The size of application I'm thinking of could be a browser-based chat app, password-manager or something similar. Neither too simple, nor too complex.\n\nBack to the topic. Choosing the right technology for a web app feels much like customizing a new PC or even a new car. There are nearly endless options to be weighed to finally pick a bunch of them for a new web application. This super famous article [How it feels to learn JavaScript in 2016](https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f#.m1nodqu6f) complains about the confusing and ever-growing, chaotic jungle of new JavaScript frameworks in an ironical way. Indeed, I hear similar arguments from many developers these days. Many of them claim that code quality was getting worse in the web and that every newbie JavaScript programmer threw out his own new framework on yet another .io domain. Although that might be true to a certain extent, I personally still like the great technological variety and innovation. I love to browse GitHub, Reddit, Hackernews and Co. to discover new cool libraries to try out in a project some day. And here's what I would pick if I had to realize such a project right today and if there weren't any restrictions. \n\nOf course, the technology choice depends on the concrete project requirements to a certain extent, but not completely. Consequently, a new project is always a chance to try something new. ThoughtWorks just published their new [technology radar for 2016](https://www.thoughtworks.com/de/radar), where they separate into categories _adopt_, _trial_, _assess_ and _hold_. Of course, _hold_-techs are not an option for new projects and I actually pretty much agree with their views on what has to be in the _hold_ category. _Adopt_ basically are things that are modern, but also well-established enough to avoid too much risk. _Trial_-techs are more experimental and _assess_ are the latest fancy s***, so to say. Since I'm extremely eager to try out new things, my stack would probably mostly consist of technologies from the last category. So what would my stack now look like? Actually, I couldn't decide on one stack, but set up two: _the fancy one_ and _the super fancy one_. Additionally, I define their intersection as _the base stack_, which consists of fundamental tools etc. that both have in common.\n\n### The base stack\nFirst of all, I'd use __Git__ for version control, __Visual Studio Code__ as code editor and __GitLab__ for repository hosting and as build server. If I didn't had to implement user management myself, I'd pick __Auth0__ for that. For deployment, I'd use containers with __Docker__ on __DigitalOcean__ machines and if I needed multiple instances, __Rancher__ would help me to manage them. As reverse proxy in front of the backend I'd choose __nginx__ since it's extremely efficient, performant and has __HTTP/2.0__ support. For bundling, __Webpack__ would be my choice and task automation would be done using plain __npm scripts__. For styling the UI, I'd simply use __Bootstrap 4__ and __SCSS__.  \n\n### The super fancy stack\nThe key point here is that I'd want to abandon a traditional REST API in my project and use __GraphQL__ instead. The backend would be written in __NodeJS__ with [__Graffiti__](https://github.com/RisingStack/graffiti) as GraphQL implementation. I don't know much about the latter one, yet, except for that it's the de-facto GraphQL solution for Node. Why Node? Because it's simply the best choice for the web (my view...). It's performant, comfortable to develop and especially brings consistency by having JS in front- and backend. By always being quite up-to-date with the latest ES* features, Node doesn't get boring. Since GraphQL is told to work best with other Facebook technology, I'd not be that experimental here and build the frontend on __React__ plus __Relay__ (which is still completely new for me). Database would probably be a __MongoDB__ (JSON everywhere!) with [__Waterline__](https://github.com/balderdashy/waterline) ORM. To put a cherry on the cake, I'd also introduce __Redux__ in addition. I haven't worked with it much, yet, and I heard that it's kind of mind-blowing in the beginning. However, I consider its concept to cover a large potential to manage consistency in my app. The last thing here is that I desperately want is ES6 syntax. It isn't supported by the React compiler afaik (please correct me, if I'm wrong), so I'd use __Babel__ to have latest JavaScript features. If having to go mobile, __React Native__ would be the rational choice.\n\n### The fancy stack\nThis stack differs from the _super fancy stack_ in a few points. A key point is that it would not use GraphQL, but a good old REST API. This API would be written in __Go__, since I like the language - especially its efficiency and its good suitability for web development. More precisely, I'd use the [__Iris__](http://iris-go.com/) framework. I've read the documentation and it looked incredibly powerful to me (in terms of both functionality and perfomance). _(__EDIT:__ It's not what it seems! Please see my comment below!)._ For the frontend I'm balancing between __Angular 2__ and [__Aurelia__](http://aurelia.io/). Angular 2 is guaranteed to work for any potential case, is extremely powerful and has great community- and library support. However, Aurelia look promising, too, and probably is even more clear and less boilerplate code. Consequently, I'd give it a try. But if having to go mobile, I'd still favor Angular 2, since it perfectly aligns with __Ionic 2__.\n\nTwo other options, which look really interesting to me are __Meteor__ and __HorizonJS__. However, I'm not sure, if it's a good idea to commit to only one comprehensive framework through the full stack.\n\nSo these are my two alternative ways through the webdev jungle - btw, [this good article](https://medium.freecodecamp.com/a-study-plan-to-cure-javascript-fatigue-8ad3a54f2eb1#.3ri3a1fdv) describes another one, especially for newcomer web developers. Sorry, that I haven't justified all choices. Actually, as you probably know, if you're a developer, subjective views like these often aren't even based on pure rational considerations, but are rather emotional and spontaneous.\n\nPlease feel free to give me feedback on my tech stack of choice! \n\n__EDIT:__ Another framework Id really like to try out is [InfernoJS](https://infernojs.org/), because it claims to be extremely lightweight and performant. However, before using Inferno, one should probably be familiar with React, since it uses very similar concepts and syntaxes.\n\n__EDIT 2:__ After having read [this article](http://www.florinpatan.ro/2016/10/why-you-should-not-use-iris-for-your-go.html) and having done some further research on the Iris framework I really have to retract my above statement that Id use it as a web backend. While it looks nice on paper, after diving a little deeper I really have to admit that itd be morally tenable to support the authors of that project. So please forget about Iris and take a look at [Beego](https://beego.me/) instead.","slug":"my-teck-stack-if-i-had-to-build-an-app-today","published":1,"updated":"2020-10-30T20:05:40.288Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqm9000uo2e01xw62b50","content":"<p>What technology stack would I choose, if I had to develop a web application completely from scratch? Thats the question this article will cover.</p>\n<p>First of all: by saying web application Im referring to something between a plain static HTML page and an entire Facebook. Basically, an application that fulfills a certain domain of tasks for the user and that requires the usual features like user management, a backend database, multiple UI views and controls, etc. The size of application Im thinking of could be a browser-based chat app, password-manager or something similar. Neither too simple, nor too complex.</p>\n<p>Back to the topic. Choosing the right technology for a web app feels much like customizing a new PC or even a new car. There are nearly endless options to be weighed to finally pick a bunch of them for a new web application. This super famous article <a href=\"https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f#.m1nodqu6f\">How it feels to learn JavaScript in 2016</a> complains about the confusing and ever-growing, chaotic jungle of new JavaScript frameworks in an ironical way. Indeed, I hear similar arguments from many developers these days. Many of them claim that code quality was getting worse in the web and that every newbie JavaScript programmer threw out his own new framework on yet another .io domain. Although that might be true to a certain extent, I personally still like the great technological variety and innovation. I love to browse GitHub, Reddit, Hackernews and Co. to discover new cool libraries to try out in a project some day. And heres what I would pick if I had to realize such a project right today and if there werent any restrictions. </p>\n<p>Of course, the technology choice depends on the concrete project requirements to a certain extent, but not completely. Consequently, a new project is always a chance to try something new. ThoughtWorks just published their new <a href=\"https://www.thoughtworks.com/de/radar\">technology radar for 2016</a>, where they separate into categories <em>adopt</em>, <em>trial</em>, <em>assess</em> and <em>hold</em>. Of course, <em>hold</em>-techs are not an option for new projects and I actually pretty much agree with their views on what has to be in the <em>hold</em> category. <em>Adopt</em> basically are things that are modern, but also well-established enough to avoid too much risk. <em>Trial</em>-techs are more experimental and <em>assess</em> are the latest fancy s***, so to say. Since Im extremely eager to try out new things, my stack would probably mostly consist of technologies from the last category. So what would my stack now look like? Actually, I couldnt decide on one stack, but set up two: <em>the fancy one</em> and <em>the super fancy one</em>. Additionally, I define their intersection as <em>the base stack</em>, which consists of fundamental tools etc. that both have in common.</p>\n<h3 id=\"The-base-stack\"><a href=\"#The-base-stack\" class=\"headerlink\" title=\"The base stack\"></a>The base stack</h3><p>First of all, Id use <strong>Git</strong> for version control, <strong>Visual Studio Code</strong> as code editor and <strong>GitLab</strong> for repository hosting and as build server. If I didnt had to implement user management myself, Id pick <strong>Auth0</strong> for that. For deployment, Id use containers with <strong>Docker</strong> on <strong>DigitalOcean</strong> machines and if I needed multiple instances, <strong>Rancher</strong> would help me to manage them. As reverse proxy in front of the backend Id choose <strong>nginx</strong> since its extremely efficient, performant and has <strong>HTTP/2.0</strong> support. For bundling, <strong>Webpack</strong> would be my choice and task automation would be done using plain <strong>npm scripts</strong>. For styling the UI, Id simply use <strong>Bootstrap 4</strong> and <strong>SCSS</strong>.  </p>\n<h3 id=\"The-super-fancy-stack\"><a href=\"#The-super-fancy-stack\" class=\"headerlink\" title=\"The super fancy stack\"></a>The super fancy stack</h3><p>The key point here is that Id want to abandon a traditional REST API in my project and use <strong>GraphQL</strong> instead. The backend would be written in <strong>NodeJS</strong> with <a href=\"https://github.com/RisingStack/graffiti\"><strong>Graffiti</strong></a> as GraphQL implementation. I dont know much about the latter one, yet, except for that its the de-facto GraphQL solution for Node. Why Node? Because its simply the best choice for the web (my view). Its performant, comfortable to develop and especially brings consistency by having JS in front- and backend. By always being quite up-to-date with the latest ES* features, Node doesnt get boring. Since GraphQL is told to work best with other Facebook technology, Id not be that experimental here and build the frontend on <strong>React</strong> plus <strong>Relay</strong> (which is still completely new for me). Database would probably be a <strong>MongoDB</strong> (JSON everywhere!) with <a href=\"https://github.com/balderdashy/waterline\"><strong>Waterline</strong></a> ORM. To put a cherry on the cake, Id also introduce <strong>Redux</strong> in addition. I havent worked with it much, yet, and I heard that its kind of mind-blowing in the beginning. However, I consider its concept to cover a large potential to manage consistency in my app. The last thing here is that I desperately want is ES6 syntax. It isnt supported by the React compiler afaik (please correct me, if Im wrong), so Id use <strong>Babel</strong> to have latest JavaScript features. If having to go mobile, <strong>React Native</strong> would be the rational choice.</p>\n<h3 id=\"The-fancy-stack\"><a href=\"#The-fancy-stack\" class=\"headerlink\" title=\"The fancy stack\"></a>The fancy stack</h3><p>This stack differs from the <em>super fancy stack</em> in a few points. A key point is that it would not use GraphQL, but a good old REST API. This API would be written in <strong>Go</strong>, since I like the language - especially its efficiency and its good suitability for web development. More precisely, Id use the <a href=\"http://iris-go.com/\"><strong>Iris</strong></a> framework. Ive read the documentation and it looked incredibly powerful to me (in terms of both functionality and perfomance). <em>(<strong>EDIT:</strong> Its not what it seems! Please see my comment below!).</em> For the frontend Im balancing between <strong>Angular 2</strong> and <a href=\"http://aurelia.io/\"><strong>Aurelia</strong></a>. Angular 2 is guaranteed to work for any potential case, is extremely powerful and has great community- and library support. However, Aurelia look promising, too, and probably is even more clear and less boilerplate code. Consequently, Id give it a try. But if having to go mobile, Id still favor Angular 2, since it perfectly aligns with <strong>Ionic 2</strong>.</p>\n<p>Two other options, which look really interesting to me are <strong>Meteor</strong> and <strong>HorizonJS</strong>. However, Im not sure, if its a good idea to commit to only one comprehensive framework through the full stack.</p>\n<p>So these are my two alternative ways through the webdev jungle - btw, <a href=\"https://medium.freecodecamp.com/a-study-plan-to-cure-javascript-fatigue-8ad3a54f2eb1#.3ri3a1fdv\">this good article</a> describes another one, especially for newcomer web developers. Sorry, that I havent justified all choices. Actually, as you probably know, if youre a developer, subjective views like these often arent even based on pure rational considerations, but are rather emotional and spontaneous.</p>\n<p>Please feel free to give me feedback on my tech stack of choice! </p>\n<p><strong>EDIT:</strong> Another framework Id really like to try out is <a href=\"https://infernojs.org/\">InfernoJS</a>, because it claims to be extremely lightweight and performant. However, before using Inferno, one should probably be familiar with React, since it uses very similar concepts and syntaxes.</p>\n<p><strong>EDIT 2:</strong> After having read <a href=\"http://www.florinpatan.ro/2016/10/why-you-should-not-use-iris-for-your-go.html\">this article</a> and having done some further research on the Iris framework I really have to retract my above statement that Id use it as a web backend. While it looks nice on paper, after diving a little deeper I really have to admit that itd be morally tenable to support the authors of that project. So please forget about Iris and take a look at <a href=\"https://beego.me/\">Beego</a> instead.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>What technology stack would I choose, if I had to develop a web application completely from scratch? Thats the question this article will cover.</p>\n<p>First of all: by saying web application Im referring to something between a plain static HTML page and an entire Facebook. Basically, an application that fulfills a certain domain of tasks for the user and that requires the usual features like user management, a backend database, multiple UI views and controls, etc. The size of application Im thinking of could be a browser-based chat app, password-manager or something similar. Neither too simple, nor too complex.</p>\n<p>Back to the topic. Choosing the right technology for a web app feels much like customizing a new PC or even a new car. There are nearly endless options to be weighed to finally pick a bunch of them for a new web application. This super famous article <a href=\"https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f#.m1nodqu6f\">How it feels to learn JavaScript in 2016</a> complains about the confusing and ever-growing, chaotic jungle of new JavaScript frameworks in an ironical way. Indeed, I hear similar arguments from many developers these days. Many of them claim that code quality was getting worse in the web and that every newbie JavaScript programmer threw out his own new framework on yet another .io domain. Although that might be true to a certain extent, I personally still like the great technological variety and innovation. I love to browse GitHub, Reddit, Hackernews and Co. to discover new cool libraries to try out in a project some day. And heres what I would pick if I had to realize such a project right today and if there werent any restrictions. </p>\n<p>Of course, the technology choice depends on the concrete project requirements to a certain extent, but not completely. Consequently, a new project is always a chance to try something new. ThoughtWorks just published their new <a href=\"https://www.thoughtworks.com/de/radar\">technology radar for 2016</a>, where they separate into categories <em>adopt</em>, <em>trial</em>, <em>assess</em> and <em>hold</em>. Of course, <em>hold</em>-techs are not an option for new projects and I actually pretty much agree with their views on what has to be in the <em>hold</em> category. <em>Adopt</em> basically are things that are modern, but also well-established enough to avoid too much risk. <em>Trial</em>-techs are more experimental and <em>assess</em> are the latest fancy s***, so to say. Since Im extremely eager to try out new things, my stack would probably mostly consist of technologies from the last category. So what would my stack now look like? Actually, I couldnt decide on one stack, but set up two: <em>the fancy one</em> and <em>the super fancy one</em>. Additionally, I define their intersection as <em>the base stack</em>, which consists of fundamental tools etc. that both have in common.</p>\n<h3 id=\"The-base-stack\"><a href=\"#The-base-stack\" class=\"headerlink\" title=\"The base stack\"></a>The base stack</h3><p>First of all, Id use <strong>Git</strong> for version control, <strong>Visual Studio Code</strong> as code editor and <strong>GitLab</strong> for repository hosting and as build server. If I didnt had to implement user management myself, Id pick <strong>Auth0</strong> for that. For deployment, Id use containers with <strong>Docker</strong> on <strong>DigitalOcean</strong> machines and if I needed multiple instances, <strong>Rancher</strong> would help me to manage them. As reverse proxy in front of the backend Id choose <strong>nginx</strong> since its extremely efficient, performant and has <strong>HTTP/2.0</strong> support. For bundling, <strong>Webpack</strong> would be my choice and task automation would be done using plain <strong>npm scripts</strong>. For styling the UI, Id simply use <strong>Bootstrap 4</strong> and <strong>SCSS</strong>.  </p>\n<h3 id=\"The-super-fancy-stack\"><a href=\"#The-super-fancy-stack\" class=\"headerlink\" title=\"The super fancy stack\"></a>The super fancy stack</h3><p>The key point here is that Id want to abandon a traditional REST API in my project and use <strong>GraphQL</strong> instead. The backend would be written in <strong>NodeJS</strong> with <a href=\"https://github.com/RisingStack/graffiti\"><strong>Graffiti</strong></a> as GraphQL implementation. I dont know much about the latter one, yet, except for that its the de-facto GraphQL solution for Node. Why Node? Because its simply the best choice for the web (my view). Its performant, comfortable to develop and especially brings consistency by having JS in front- and backend. By always being quite up-to-date with the latest ES* features, Node doesnt get boring. Since GraphQL is told to work best with other Facebook technology, Id not be that experimental here and build the frontend on <strong>React</strong> plus <strong>Relay</strong> (which is still completely new for me). Database would probably be a <strong>MongoDB</strong> (JSON everywhere!) with <a href=\"https://github.com/balderdashy/waterline\"><strong>Waterline</strong></a> ORM. To put a cherry on the cake, Id also introduce <strong>Redux</strong> in addition. I havent worked with it much, yet, and I heard that its kind of mind-blowing in the beginning. However, I consider its concept to cover a large potential to manage consistency in my app. The last thing here is that I desperately want is ES6 syntax. It isnt supported by the React compiler afaik (please correct me, if Im wrong), so Id use <strong>Babel</strong> to have latest JavaScript features. If having to go mobile, <strong>React Native</strong> would be the rational choice.</p>\n<h3 id=\"The-fancy-stack\"><a href=\"#The-fancy-stack\" class=\"headerlink\" title=\"The fancy stack\"></a>The fancy stack</h3><p>This stack differs from the <em>super fancy stack</em> in a few points. A key point is that it would not use GraphQL, but a good old REST API. This API would be written in <strong>Go</strong>, since I like the language - especially its efficiency and its good suitability for web development. More precisely, Id use the <a href=\"http://iris-go.com/\"><strong>Iris</strong></a> framework. Ive read the documentation and it looked incredibly powerful to me (in terms of both functionality and perfomance). <em>(<strong>EDIT:</strong> Its not what it seems! Please see my comment below!).</em> For the frontend Im balancing between <strong>Angular 2</strong> and <a href=\"http://aurelia.io/\"><strong>Aurelia</strong></a>. Angular 2 is guaranteed to work for any potential case, is extremely powerful and has great community- and library support. However, Aurelia look promising, too, and probably is even more clear and less boilerplate code. Consequently, Id give it a try. But if having to go mobile, Id still favor Angular 2, since it perfectly aligns with <strong>Ionic 2</strong>.</p>\n<p>Two other options, which look really interesting to me are <strong>Meteor</strong> and <strong>HorizonJS</strong>. However, Im not sure, if its a good idea to commit to only one comprehensive framework through the full stack.</p>\n<p>So these are my two alternative ways through the webdev jungle - btw, <a href=\"https://medium.freecodecamp.com/a-study-plan-to-cure-javascript-fatigue-8ad3a54f2eb1#.3ri3a1fdv\">this good article</a> describes another one, especially for newcomer web developers. Sorry, that I havent justified all choices. Actually, as you probably know, if youre a developer, subjective views like these often arent even based on pure rational considerations, but are rather emotional and spontaneous.</p>\n<p>Please feel free to give me feedback on my tech stack of choice! </p>\n<p><strong>EDIT:</strong> Another framework Id really like to try out is <a href=\"https://infernojs.org/\">InfernoJS</a>, because it claims to be extremely lightweight and performant. However, before using Inferno, one should probably be familiar with React, since it uses very similar concepts and syntaxes.</p>\n<p><strong>EDIT 2:</strong> After having read <a href=\"http://www.florinpatan.ro/2016/10/why-you-should-not-use-iris-for-your-go.html\">this article</a> and having done some further research on the Iris framework I really have to retract my above statement that Id use it as a web backend. While it looks nice on paper, after diving a little deeper I really have to admit that itd be morally tenable to support the authors of that project. So please forget about Iris and take a look at <a href=\"https://beego.me/\">Beego</a> instead.</p>\n"},{"title":"OverGrive not starting on Ubuntu 18.04","date":"2018-06-18T05:37:13.000Z","_content":"# Problem\nFirst of all: I am really happy that it seems like I finally found a working Google Drive client for Linux, [overGrive](https://www.thefanclub.co.za/overgrive). It seems to be written in Python and, unfortunately is not open source, but rather costs $4.99. But that is fine as long as it actually works.\nHowever, I had some issues when trying to install overGrive for the first time. I am using Ubuntu 18.04, so I followed the [official installation instructions](https://www.thefanclub.co.za/overgrive/installation-instructions-ubuntu), including downloaing the `.deb` package and installing it with _dpkg_. I could see overGrive in Gnome's app launcher afterwards, but when I clicked it, nothing happened. It did not start without any error message.\n\n# Solution\nThe solution was the following. I figured out that overGrive is by default installed to `/opt/thefanclub/overgrive` and the binary can be run with Python (2.7), like `python /opt/thefanclub/overgrive/overgrive`. This gave me the error message that `No module named oauth2client.client`. After googling I found that I manually had to install these Python modules in addition.\n\n* `sudo pip install google-api-python-client`\n* `sudo pip install oauth2client`\n\nIf pip complains about further missing dependencies while installing one of the above packages, simply `pip install` them, too.\n\nGood luck! And again, thanks to [thefanclub](https://www.thefanclub.co.za/) for this Google Drive sync client.\n\n**EDIT:** After a few days I found that OverGrive did not work well at all, unfortunately. Syncs produced inconsistent states and the client \"logged out\" randomly after rebooting. Eventually, I set up a [NextCloud](https://nextcloud.com/) and migrated from Google Drive. I was already pleased with NextCloud a few months ago and only switched to GDrive because of Google Docs, but that's not worth the cr****y clients (the official Windows client is not quite better). NextCloud has a great web interface, sync clients for all major platform, a decent Android app and is open-source :-)","source":"_posts/overgrive-not-starting-on-ubuntu-18-04.md","raw":"---\ntitle: OverGrive not starting on Ubuntu 18.04\ndate: 2018-06-18 07:37:13\ntags:\n---\n# Problem\nFirst of all: I am really happy that it seems like I finally found a working Google Drive client for Linux, [overGrive](https://www.thefanclub.co.za/overgrive). It seems to be written in Python and, unfortunately is not open source, but rather costs $4.99. But that is fine as long as it actually works.\nHowever, I had some issues when trying to install overGrive for the first time. I am using Ubuntu 18.04, so I followed the [official installation instructions](https://www.thefanclub.co.za/overgrive/installation-instructions-ubuntu), including downloaing the `.deb` package and installing it with _dpkg_. I could see overGrive in Gnome's app launcher afterwards, but when I clicked it, nothing happened. It did not start without any error message.\n\n# Solution\nThe solution was the following. I figured out that overGrive is by default installed to `/opt/thefanclub/overgrive` and the binary can be run with Python (2.7), like `python /opt/thefanclub/overgrive/overgrive`. This gave me the error message that `No module named oauth2client.client`. After googling I found that I manually had to install these Python modules in addition.\n\n* `sudo pip install google-api-python-client`\n* `sudo pip install oauth2client`\n\nIf pip complains about further missing dependencies while installing one of the above packages, simply `pip install` them, too.\n\nGood luck! And again, thanks to [thefanclub](https://www.thefanclub.co.za/) for this Google Drive sync client.\n\n**EDIT:** After a few days I found that OverGrive did not work well at all, unfortunately. Syncs produced inconsistent states and the client \"logged out\" randomly after rebooting. Eventually, I set up a [NextCloud](https://nextcloud.com/) and migrated from Google Drive. I was already pleased with NextCloud a few months ago and only switched to GDrive because of Google Docs, but that's not worth the cr****y clients (the official Windows client is not quite better). NextCloud has a great web interface, sync clients for all major platform, a decent Android app and is open-source :-)","slug":"overgrive-not-starting-on-ubuntu-18-04","published":1,"updated":"2020-10-30T20:05:40.291Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqma000vo2e058760pw6","content":"<h1 id=\"Problem\"><a href=\"#Problem\" class=\"headerlink\" title=\"Problem\"></a>Problem</h1><p>First of all: I am really happy that it seems like I finally found a working Google Drive client for Linux, <a href=\"https://www.thefanclub.co.za/overgrive\">overGrive</a>. It seems to be written in Python and, unfortunately is not open source, but rather costs $4.99. But that is fine as long as it actually works.<br>However, I had some issues when trying to install overGrive for the first time. I am using Ubuntu 18.04, so I followed the <a href=\"https://www.thefanclub.co.za/overgrive/installation-instructions-ubuntu\">official installation instructions</a>, including downloaing the <code>.deb</code> package and installing it with <em>dpkg</em>. I could see overGrive in Gnomes app launcher afterwards, but when I clicked it, nothing happened. It did not start without any error message.</p>\n<h1 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h1><p>The solution was the following. I figured out that overGrive is by default installed to <code>/opt/thefanclub/overgrive</code> and the binary can be run with Python (2.7), like <code>python /opt/thefanclub/overgrive/overgrive</code>. This gave me the error message that <code>No module named oauth2client.client</code>. After googling I found that I manually had to install these Python modules in addition.</p>\n<ul>\n<li><code>sudo pip install google-api-python-client</code></li>\n<li><code>sudo pip install oauth2client</code></li>\n</ul>\n<p>If pip complains about further missing dependencies while installing one of the above packages, simply <code>pip install</code> them, too.</p>\n<p>Good luck! And again, thanks to <a href=\"https://www.thefanclub.co.za/\">thefanclub</a> for this Google Drive sync client.</p>\n<p><strong>EDIT:</strong> After a few days I found that OverGrive did not work well at all, unfortunately. Syncs produced inconsistent states and the client logged out randomly after rebooting. Eventually, I set up a <a href=\"https://nextcloud.com/\">NextCloud</a> and migrated from Google Drive. I was already pleased with NextCloud a few months ago and only switched to GDrive because of Google Docs, but thats not worth the cr****y clients (the official Windows client is not quite better). NextCloud has a great web interface, sync clients for all major platform, a decent Android app and is open-source :-)</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1 id=\"Problem\"><a href=\"#Problem\" class=\"headerlink\" title=\"Problem\"></a>Problem</h1><p>First of all: I am really happy that it seems like I finally found a working Google Drive client for Linux, <a href=\"https://www.thefanclub.co.za/overgrive\">overGrive</a>. It seems to be written in Python and, unfortunately is not open source, but rather costs $4.99. But that is fine as long as it actually works.<br>However, I had some issues when trying to install overGrive for the first time. I am using Ubuntu 18.04, so I followed the <a href=\"https://www.thefanclub.co.za/overgrive/installation-instructions-ubuntu\">official installation instructions</a>, including downloaing the <code>.deb</code> package and installing it with <em>dpkg</em>. I could see overGrive in Gnomes app launcher afterwards, but when I clicked it, nothing happened. It did not start without any error message.</p>\n<h1 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h1><p>The solution was the following. I figured out that overGrive is by default installed to <code>/opt/thefanclub/overgrive</code> and the binary can be run with Python (2.7), like <code>python /opt/thefanclub/overgrive/overgrive</code>. This gave me the error message that <code>No module named oauth2client.client</code>. After googling I found that I manually had to install these Python modules in addition.</p>\n<ul>\n<li><code>sudo pip install google-api-python-client</code></li>\n<li><code>sudo pip install oauth2client</code></li>\n</ul>\n<p>If pip complains about further missing dependencies while installing one of the above packages, simply <code>pip install</code> them, too.</p>\n<p>Good luck! And again, thanks to <a href=\"https://www.thefanclub.co.za/\">thefanclub</a> for this Google Drive sync client.</p>\n<p><strong>EDIT:</strong> After a few days I found that OverGrive did not work well at all, unfortunately. Syncs produced inconsistent states and the client logged out randomly after rebooting. Eventually, I set up a <a href=\"https://nextcloud.com/\">NextCloud</a> and migrated from Google Drive. I was already pleased with NextCloud a few months ago and only switched to GDrive because of Google Docs, but thats not worth the cr****y clients (the official Windows client is not quite better). NextCloud has a great web interface, sync clients for all major platform, a decent Android app and is open-source :-)</p>\n"},{"title":"PGP-encrypted personal e-mail inbox","date":"2020-09-29T17:18:33.000Z","_content":"\n![](https://apps.muetsch.io/images/o:auto/rs,s:640/?image=https://muetsch.io/images/lock.jpg)\n\n# Introduction\nMost instant messengers today, including WhatsApp, Signal, Telegram, Threema, Wire and others, support end-to-end encryption (E2EE) of text messages. In fact, messengers that don't support E2E encryption are usually considered dubious. However, when it comes to e-mails, things are mostly still quite insecure  even in 2020. \n\nThe majority of personal- and business-related e-mails are still sent as plain text (however, at least encrypted in transit [[1]](https://transparencyreport.google.com/safer-email/overview?hl=en&encrypt_region_table=encryption_level:RED,YELLOW,GREEN;region:001&lu=encrypt_region_table) mostly). A major reason is the fact that PGP (e.g. with [OpenPGP](https://www.openpgp.org/)) is still quite cumbersome to set up and use for non-technical people. Morever, [PGP has some issues](https://saltpack.org/pgp-message-format-problems), however, some kind of encryption is usually better than none at all. \n\nIf you want to learn more about E2EE for e-mails, I recommend [this article](https://protonmail.com/blog/what-is-end-to-end-encryption/) by ProtonMail. However, E2EE is not particularly the topic of this post.\n\n# Inbox encryption\nWhile true end-to-end-encrypted communication requires both parties to have a working PGP setup and is therefore impractical to realize without cooperation, you can at least boost your e-mail security to a certain extent by encrypting your own inbox. That is, mails are stored on the server encryptedly and can, at least, not be read by a malicious attacker and  depending on the implementation  potentially not even by your mail provider. In the following, I want to present the setup with which I realized my personal, encrypted inbox.\n\n## My setup\nFirst of all, I do not host my own mail server (I used to, but maintining such turned out to be almost a full-time job!), but rely on an external provider. In my case, this provider is **[mailbox.org](https://mailbox.org)**, which I can absolutely recommend to anyone. It offers an intuitive webmail, a ton of expert-level configuration options, the ability to bring your own domain, German storage locations and a lot more. Also, they claim to care a lot about [privacy](https://kb.mailbox.org/display/MBOKBEN/Can+I+trust+the+staff+at+mailbox.org) and the founder, [Peer Heinlein](https://de.wikipedia.org/wiki/Peer_Heinlein), is both the author of a collection of e-mail technology-related literature and an active supporter of open-source and / or creative commons projects like the Wikimedia foundation. Other PGP-capable e-mail providers include [ProtonMail](https://protonmail.com/) and [Posteo](https://posteo.de/en). \n\n[![](https://apps.muetsch.io/images/o:auto/rs,s:150/?image=https://muetsch.io/images/mborg_logo.jpg)](https://mailbox.org)\n\n\nMailbox.org supports different kinds of PGP-based encryption, one of them being the \"[mailbox.org Guard](https://kb.mailbox.org/m/mobile.action#page/1181454)\". In essence, this feature provides server-side encryption of everything inside your inbox using customer-provided PGP keys. While server-side encryption still requires you to trust your provider, I consider it a good compromise between security and still having an intuitive, almost seemless user experience. \n\n[![](https://apps.muetsch.io/images/o:auto/rs,s:100/?image=https://muetsch.io/images/tb_logo.png)](https://thunderbird.net)\n\nApart from the webmail client I use open-source **[Thunderbird](https://thunderbird.net)** as my desktop e-mail client. In combination with the Enigmail extension, integration with mailbox.org's PGP encryption is set up almost trivially. More recent version of Thunderbird even have built-in PGP support. Each time I want to access my inbox from Thunderbird, I am prompted to enter my PGP key's passphrase. You can also save the key to your operating system's key store, so you don't have to enter it repeatedly.\n\n<div style=\"display: flex; justify-content: space-evenly\">\n    <a href=\"https://k9mail.app\" style=\"background: none\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:100/?image=https://muetsch.io/images/k9_logo.png\"/></a>\n    <a href=\"https://openkeychain.org\" style=\"background: none\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:100/?image=https://muetsch.io/images/okc_logo.png\"/></a>\n</div>\n\nThe last missing piece is the ability to read mails from my Android phone as well. Setting that up was a little more complicated as most e-mail clients on Android do not include support for encryption  except for the excellent, fully open-source [K-9 Mail](https://k9mail.app/) client. It integrates with [OpenKeychain](https://www.openkeychain.org/)  a general-purpose, open-source PGP provider for Android. The setup involves to import your PGP keys to OpenKeychain and link it to K-9. Afterwards, it works completely seemless as well. Only when your phone was restarted you are requrired to enter your key's passphrase.\n\n# Conclusion\nMy intension with this article is to present a working, easy-to-use tech stack for inbox encryption, solely based on open-source software to eventually encourage more people to think about using encryption more extensively. Give it a try!","source":"_posts/pgp-encrypted-personal-e-mail-inbox.md","raw":"---\ntitle: PGP-encrypted personal e-mail inbox\ndate: 2020-09-29 19:18:33\ntags:\n---\n\n![](https://apps.muetsch.io/images/o:auto/rs,s:640/?image=https://muetsch.io/images/lock.jpg)\n\n# Introduction\nMost instant messengers today, including WhatsApp, Signal, Telegram, Threema, Wire and others, support end-to-end encryption (E2EE) of text messages. In fact, messengers that don't support E2E encryption are usually considered dubious. However, when it comes to e-mails, things are mostly still quite insecure  even in 2020. \n\nThe majority of personal- and business-related e-mails are still sent as plain text (however, at least encrypted in transit [[1]](https://transparencyreport.google.com/safer-email/overview?hl=en&encrypt_region_table=encryption_level:RED,YELLOW,GREEN;region:001&lu=encrypt_region_table) mostly). A major reason is the fact that PGP (e.g. with [OpenPGP](https://www.openpgp.org/)) is still quite cumbersome to set up and use for non-technical people. Morever, [PGP has some issues](https://saltpack.org/pgp-message-format-problems), however, some kind of encryption is usually better than none at all. \n\nIf you want to learn more about E2EE for e-mails, I recommend [this article](https://protonmail.com/blog/what-is-end-to-end-encryption/) by ProtonMail. However, E2EE is not particularly the topic of this post.\n\n# Inbox encryption\nWhile true end-to-end-encrypted communication requires both parties to have a working PGP setup and is therefore impractical to realize without cooperation, you can at least boost your e-mail security to a certain extent by encrypting your own inbox. That is, mails are stored on the server encryptedly and can, at least, not be read by a malicious attacker and  depending on the implementation  potentially not even by your mail provider. In the following, I want to present the setup with which I realized my personal, encrypted inbox.\n\n## My setup\nFirst of all, I do not host my own mail server (I used to, but maintining such turned out to be almost a full-time job!), but rely on an external provider. In my case, this provider is **[mailbox.org](https://mailbox.org)**, which I can absolutely recommend to anyone. It offers an intuitive webmail, a ton of expert-level configuration options, the ability to bring your own domain, German storage locations and a lot more. Also, they claim to care a lot about [privacy](https://kb.mailbox.org/display/MBOKBEN/Can+I+trust+the+staff+at+mailbox.org) and the founder, [Peer Heinlein](https://de.wikipedia.org/wiki/Peer_Heinlein), is both the author of a collection of e-mail technology-related literature and an active supporter of open-source and / or creative commons projects like the Wikimedia foundation. Other PGP-capable e-mail providers include [ProtonMail](https://protonmail.com/) and [Posteo](https://posteo.de/en). \n\n[![](https://apps.muetsch.io/images/o:auto/rs,s:150/?image=https://muetsch.io/images/mborg_logo.jpg)](https://mailbox.org)\n\n\nMailbox.org supports different kinds of PGP-based encryption, one of them being the \"[mailbox.org Guard](https://kb.mailbox.org/m/mobile.action#page/1181454)\". In essence, this feature provides server-side encryption of everything inside your inbox using customer-provided PGP keys. While server-side encryption still requires you to trust your provider, I consider it a good compromise between security and still having an intuitive, almost seemless user experience. \n\n[![](https://apps.muetsch.io/images/o:auto/rs,s:100/?image=https://muetsch.io/images/tb_logo.png)](https://thunderbird.net)\n\nApart from the webmail client I use open-source **[Thunderbird](https://thunderbird.net)** as my desktop e-mail client. In combination with the Enigmail extension, integration with mailbox.org's PGP encryption is set up almost trivially. More recent version of Thunderbird even have built-in PGP support. Each time I want to access my inbox from Thunderbird, I am prompted to enter my PGP key's passphrase. You can also save the key to your operating system's key store, so you don't have to enter it repeatedly.\n\n<div style=\"display: flex; justify-content: space-evenly\">\n    <a href=\"https://k9mail.app\" style=\"background: none\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:100/?image=https://muetsch.io/images/k9_logo.png\"/></a>\n    <a href=\"https://openkeychain.org\" style=\"background: none\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:100/?image=https://muetsch.io/images/okc_logo.png\"/></a>\n</div>\n\nThe last missing piece is the ability to read mails from my Android phone as well. Setting that up was a little more complicated as most e-mail clients on Android do not include support for encryption  except for the excellent, fully open-source [K-9 Mail](https://k9mail.app/) client. It integrates with [OpenKeychain](https://www.openkeychain.org/)  a general-purpose, open-source PGP provider for Android. The setup involves to import your PGP keys to OpenKeychain and link it to K-9. Afterwards, it works completely seemless as well. Only when your phone was restarted you are requrired to enter your key's passphrase.\n\n# Conclusion\nMy intension with this article is to present a working, easy-to-use tech stack for inbox encryption, solely based on open-source software to eventually encourage more people to think about using encryption more extensively. Give it a try!","slug":"pgp-encrypted-personal-e-mail-inbox","published":1,"updated":"2020-10-30T20:05:40.286Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqmb000wo2e03zcy0ayj","content":"<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640/?image=https://muetsch.io/images/lock.jpg\"></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>Most instant messengers today, including WhatsApp, Signal, Telegram, Threema, Wire and others, support end-to-end encryption (E2EE) of text messages. In fact, messengers that dont support E2E encryption are usually considered dubious. However, when it comes to e-mails, things are mostly still quite insecure  even in 2020. </p>\n<p>The majority of personal- and business-related e-mails are still sent as plain text (however, at least encrypted in transit <a href=\"https://transparencyreport.google.com/safer-email/overview?hl=en&encrypt_region_table=encryption_level:RED,YELLOW,GREEN;region:001&lu=encrypt_region_table\">[1]</a> mostly). A major reason is the fact that PGP (e.g. with <a href=\"https://www.openpgp.org/\">OpenPGP</a>) is still quite cumbersome to set up and use for non-technical people. Morever, <a href=\"https://saltpack.org/pgp-message-format-problems\">PGP has some issues</a>, however, some kind of encryption is usually better than none at all. </p>\n<p>If you want to learn more about E2EE for e-mails, I recommend <a href=\"https://protonmail.com/blog/what-is-end-to-end-encryption/\">this article</a> by ProtonMail. However, E2EE is not particularly the topic of this post.</p>\n<h1 id=\"Inbox-encryption\"><a href=\"#Inbox-encryption\" class=\"headerlink\" title=\"Inbox encryption\"></a>Inbox encryption</h1><p>While true end-to-end-encrypted communication requires both parties to have a working PGP setup and is therefore impractical to realize without cooperation, you can at least boost your e-mail security to a certain extent by encrypting your own inbox. That is, mails are stored on the server encryptedly and can, at least, not be read by a malicious attacker and  depending on the implementation  potentially not even by your mail provider. In the following, I want to present the setup with which I realized my personal, encrypted inbox.</p>\n<h2 id=\"My-setup\"><a href=\"#My-setup\" class=\"headerlink\" title=\"My setup\"></a>My setup</h2><p>First of all, I do not host my own mail server (I used to, but maintining such turned out to be almost a full-time job!), but rely on an external provider. In my case, this provider is <strong><a href=\"https://mailbox.org/\">mailbox.org</a></strong>, which I can absolutely recommend to anyone. It offers an intuitive webmail, a ton of expert-level configuration options, the ability to bring your own domain, German storage locations and a lot more. Also, they claim to care a lot about <a href=\"https://kb.mailbox.org/display/MBOKBEN/Can+I+trust+the+staff+at+mailbox.org\">privacy</a> and the founder, <a href=\"https://de.wikipedia.org/wiki/Peer_Heinlein\">Peer Heinlein</a>, is both the author of a collection of e-mail technology-related literature and an active supporter of open-source and / or creative commons projects like the Wikimedia foundation. Other PGP-capable e-mail providers include <a href=\"https://protonmail.com/\">ProtonMail</a> and <a href=\"https://posteo.de/en\">Posteo</a>. </p>\n<p><a href=\"https://mailbox.org/\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:150/?image=https://muetsch.io/images/mborg_logo.jpg\"></a></p>\n<p>Mailbox.org supports different kinds of PGP-based encryption, one of them being the <a href=\"https://kb.mailbox.org/m/mobile.action#page/1181454\">mailbox.org Guard</a>. In essence, this feature provides server-side encryption of everything inside your inbox using customer-provided PGP keys. While server-side encryption still requires you to trust your provider, I consider it a good compromise between security and still having an intuitive, almost seemless user experience. </p>\n<p><a href=\"https://thunderbird.net/\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:100/?image=https://muetsch.io/images/tb_logo.png\"></a></p>\n<p>Apart from the webmail client I use open-source <strong><a href=\"https://thunderbird.net/\">Thunderbird</a></strong> as my desktop e-mail client. In combination with the Enigmail extension, integration with mailbox.orgs PGP encryption is set up almost trivially. More recent version of Thunderbird even have built-in PGP support. Each time I want to access my inbox from Thunderbird, I am prompted to enter my PGP keys passphrase. You can also save the key to your operating systems key store, so you dont have to enter it repeatedly.</p>\n<div style=\"display: flex; justify-content: space-evenly\">\n    <a href=\"https://k9mail.app\" style=\"background: none\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:100/?image=https://muetsch.io/images/k9_logo.png\"/></a>\n    <a href=\"https://openkeychain.org\" style=\"background: none\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:100/?image=https://muetsch.io/images/okc_logo.png\"/></a>\n</div>\n\n<p>The last missing piece is the ability to read mails from my Android phone as well. Setting that up was a little more complicated as most e-mail clients on Android do not include support for encryption  except for the excellent, fully open-source <a href=\"https://k9mail.app/\">K-9 Mail</a> client. It integrates with <a href=\"https://www.openkeychain.org/\">OpenKeychain</a>  a general-purpose, open-source PGP provider for Android. The setup involves to import your PGP keys to OpenKeychain and link it to K-9. Afterwards, it works completely seemless as well. Only when your phone was restarted you are requrired to enter your keys passphrase.</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>My intension with this article is to present a working, easy-to-use tech stack for inbox encryption, solely based on open-source software to eventually encourage more people to think about using encryption more extensively. Give it a try!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640/?image=https://muetsch.io/images/lock.jpg\"></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>Most instant messengers today, including WhatsApp, Signal, Telegram, Threema, Wire and others, support end-to-end encryption (E2EE) of text messages. In fact, messengers that dont support E2E encryption are usually considered dubious. However, when it comes to e-mails, things are mostly still quite insecure  even in 2020. </p>\n<p>The majority of personal- and business-related e-mails are still sent as plain text (however, at least encrypted in transit <a href=\"https://transparencyreport.google.com/safer-email/overview?hl=en&encrypt_region_table=encryption_level:RED,YELLOW,GREEN;region:001&lu=encrypt_region_table\">[1]</a> mostly). A major reason is the fact that PGP (e.g. with <a href=\"https://www.openpgp.org/\">OpenPGP</a>) is still quite cumbersome to set up and use for non-technical people. Morever, <a href=\"https://saltpack.org/pgp-message-format-problems\">PGP has some issues</a>, however, some kind of encryption is usually better than none at all. </p>\n<p>If you want to learn more about E2EE for e-mails, I recommend <a href=\"https://protonmail.com/blog/what-is-end-to-end-encryption/\">this article</a> by ProtonMail. However, E2EE is not particularly the topic of this post.</p>\n<h1 id=\"Inbox-encryption\"><a href=\"#Inbox-encryption\" class=\"headerlink\" title=\"Inbox encryption\"></a>Inbox encryption</h1><p>While true end-to-end-encrypted communication requires both parties to have a working PGP setup and is therefore impractical to realize without cooperation, you can at least boost your e-mail security to a certain extent by encrypting your own inbox. That is, mails are stored on the server encryptedly and can, at least, not be read by a malicious attacker and  depending on the implementation  potentially not even by your mail provider. In the following, I want to present the setup with which I realized my personal, encrypted inbox.</p>\n<h2 id=\"My-setup\"><a href=\"#My-setup\" class=\"headerlink\" title=\"My setup\"></a>My setup</h2><p>First of all, I do not host my own mail server (I used to, but maintining such turned out to be almost a full-time job!), but rely on an external provider. In my case, this provider is <strong><a href=\"https://mailbox.org/\">mailbox.org</a></strong>, which I can absolutely recommend to anyone. It offers an intuitive webmail, a ton of expert-level configuration options, the ability to bring your own domain, German storage locations and a lot more. Also, they claim to care a lot about <a href=\"https://kb.mailbox.org/display/MBOKBEN/Can+I+trust+the+staff+at+mailbox.org\">privacy</a> and the founder, <a href=\"https://de.wikipedia.org/wiki/Peer_Heinlein\">Peer Heinlein</a>, is both the author of a collection of e-mail technology-related literature and an active supporter of open-source and / or creative commons projects like the Wikimedia foundation. Other PGP-capable e-mail providers include <a href=\"https://protonmail.com/\">ProtonMail</a> and <a href=\"https://posteo.de/en\">Posteo</a>. </p>\n<p><a href=\"https://mailbox.org/\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:150/?image=https://muetsch.io/images/mborg_logo.jpg\"></a></p>\n<p>Mailbox.org supports different kinds of PGP-based encryption, one of them being the <a href=\"https://kb.mailbox.org/m/mobile.action#page/1181454\">mailbox.org Guard</a>. In essence, this feature provides server-side encryption of everything inside your inbox using customer-provided PGP keys. While server-side encryption still requires you to trust your provider, I consider it a good compromise between security and still having an intuitive, almost seemless user experience. </p>\n<p><a href=\"https://thunderbird.net/\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:100/?image=https://muetsch.io/images/tb_logo.png\"></a></p>\n<p>Apart from the webmail client I use open-source <strong><a href=\"https://thunderbird.net/\">Thunderbird</a></strong> as my desktop e-mail client. In combination with the Enigmail extension, integration with mailbox.orgs PGP encryption is set up almost trivially. More recent version of Thunderbird even have built-in PGP support. Each time I want to access my inbox from Thunderbird, I am prompted to enter my PGP keys passphrase. You can also save the key to your operating systems key store, so you dont have to enter it repeatedly.</p>\n<div style=\"display: flex; justify-content: space-evenly\">\n    <a href=\"https://k9mail.app\" style=\"background: none\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:100/?image=https://muetsch.io/images/k9_logo.png\"/></a>\n    <a href=\"https://openkeychain.org\" style=\"background: none\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:100/?image=https://muetsch.io/images/okc_logo.png\"/></a>\n</div>\n\n<p>The last missing piece is the ability to read mails from my Android phone as well. Setting that up was a little more complicated as most e-mail clients on Android do not include support for encryption  except for the excellent, fully open-source <a href=\"https://k9mail.app/\">K-9 Mail</a> client. It integrates with <a href=\"https://www.openkeychain.org/\">OpenKeychain</a>  a general-purpose, open-source PGP provider for Android. The setup involves to import your PGP keys to OpenKeychain and link it to K-9. Afterwards, it works completely seemless as well. Only when your phone was restarted you are requrired to enter your keys passphrase.</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>My intension with this article is to present a working, easy-to-use tech stack for inbox encryption, solely based on open-source software to eventually encourage more people to think about using encryption more extensively. Give it a try!</p>\n"},{"title":"My experiences with the Android Developer Nanodegree","date":"2019-01-26T08:01:50.000Z","_content":"\nLast year from March to August I participated in Udacity's [Android Developer Nanodegree](https://www.udacity.com/course/android-developer-nanodegree-by-google--nd801) program and here I want to share my experiences. \n\nOriginally, I got an ad on Facebook to apply for a scholarship offered by Google and Udacity for the Nanodegree program, which normally costs ~ 900 . I had done very few Android development before and although I did not particularly want to prepare for a career as an Android developer, I was definitely interested in learning Android more thoroughly. However, as we all know, it is very hard to find motivation to learn a new technology without having a real, serious project to build. Consequently, I applied for the scholarship and I was lucky (thanks to Google and Udacity for this opportunity!). \n\n# The Nanodegree\n## Time Management\nWhen participating in a Nanodegree program, you usually have six months to complete it. Due to classes at university, I only had time to start in my, so essentially I had three months to complete it. But since I was not a complete beginner, that was totally manageable.\n\nUdacity suggests a schedule for when to do which module of the course as well as soft deadlines for the finals projects at the end of every learning section. However, that is only a suggestion. The only hard deadline is the submission of the very final project. Personally, I decided to take two days in a week to fully focus on learning and coding for the Nanodegree. \n\n## Structure \nThis specific Nanodegree program consisted of five major sections.\n\n1. Developing Android Apps\n2. Advanced Android App Development\n3. Gradle for Android and Java\n4. Material Design for Android Developers\n6. Capstone Project\n\nEvery section consists of several **(1) video lessons**, in which Udacity developers explain concepts and do live coding. Between lessons, there are **(2) quizzes** to test your gained knowledge, however, the quizzes were usually quite easy and obvious. In addition to that, there are several **(3) coding tasks**, which require you to practically apply the newly learned concepts. Every coding task starts with an unfinished, small toy app and a list of TODOs you have to fulfill in order to finish it. The TODOs tell you quite precisely what to do, so sometimes it was not really a challenge. Also, you do not have to do the coding tasks at all, if you do not want to, because nobody checks your results. But obviously it makes sense to do them and it helps a lot for the **(4) project app** at the end of each section (sometimes there are more than one). For this project, you are told to implement an app with a certain functionality (e.g. a cooking recipe manager, a movie collection manager, an RSS reader, etc.). Usually it starts off with a raw scaffolded app skeleton which you have to finish - this time without specific TODOs or instructions. At the end, you submit your code either via a GitHub repository or as a ZIP file and a Udacity mentor will review your code and give you helpful feedback regarding functionality, design and code style. \n\nHere are three of my section-end project apps:\n\n1. **[popular-movies-android](https://github.com/muety/popular-movies-android):** App for displaying movie information fetched from an online movie database. Focus was on interacting with an external, third-party web API.\n2. **[baking-time-android](https://github.com/muety/baking-time-android):** App for showing baking recipes and instructions. Focus was on widgets, responsive design and integrating a video player.\n3. **[xyz-reader-android](https://github.com/muety/xyz-reader-android):** Basic reading app for text articles. Focus was on properly implementing Material Design, animations and UX.\n\n## Community\nProbably the best thing about the whole course was the community. There is an official Slack channel and a forum full of like-minded developers from literally all around the world who are going through the same experience like you. People ask questions, discuss about certain tasks or technology in general and you immediately feel extremely welcome. Whether you are unsure about a task or cannot get a certain error fixed, there are people who will help you. Also, a lot of Udacity mentors hang out on Slack and in the forum and provide support as well, e.g. in form of weekly AMA sessions. In addition to that, every participant of the course is assigned a personal mentor, who is a Udacity mentor that you can contact directly of you have questions. Actually, I did never contact mine, but I am sure they are willing to help.\n\n## Career Boost\nDespite the fact that having a Nanodegree looks quite good on your resum anyway, Udacity also provides a lot of support to help you building a successful career with your newly gained Android knowledge. They provide information and support for your application, review your CV and more.\n\n## The Capstone project\nAt the very end of the course, there is the so called **Capstone project** and this was the most fun part during the whole Nanodegree (and also the greatest effort). You are given the task to freely realize any app you like, given some requirements, e.g. that you use at least three third-party libraries, follow Material Design guidelines, provide a homescreen widget and a few more. \n\nThe final project consists of two parts. First, you have to submit a design proposal, which includes your app idea, some mock-ups and details on how you plan to implement it. After your design has been approved by Udacity mentors, you can start with phase 2, the actual implementation.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cert.png)\n\n# QuizNerd\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/qn_feature.png)\n\nAt that time I had a few coding interviews, so I came to the idea to implement a multi-player coding quiz game as a final project. Although that was probably a more comprehensive project than most of the others, I still wanted to do it, especially because it was an app I really wanted to have for myself, not only for the Nanodegree. \n\nI spent approximately two weeks of nearly full-time coding on that final project and finally came up with my app called QuizNerd. It is implemented in pure Android (using Java) without any structural frameworks (e.g. like [Dagger](http://square.github.io/dagger/)) and uses Google's [Firebase](https://firebase.google.com/) as a backend. More precisely I used Firebase Authentication for user management, [Firestore](https://firebase.google.com/docs/firestore/) as a real-time document database, FCM for notifications and and Firebase [Cloud Functions](https://firebase.google.com/docs/functions/) as a Serverless framework for backend-side logic. \n\nIf you are a developer who likes games like [QuizClash](https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite), I would love to have you try out QuizNerd! You can find it on the [Play Store](https://play.google.com/store/apps/details?id=com.github.n1try.quiznerd) and it has several hundred questions for Android, C++, C#, HTML, Java, JavaScript, PHP, Python and Swift. Feel free to share your feedback with me .\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/qn2.png)\n\n# Conclusion\nIt was fun! I learned a lot during the Nanodegree and I am kind of confident calling myself an Android developer now. Most of the concepts were explained very detailed. Fore instance one of my favorite chapters was the one about Gradle, where they precisely explained how Gradle works, how to write own Gradle tasks and how to apply that to Android.\n\nIf you keep on motivating yourself to work through the lessons and especially through the final project, it is going to pay off. Moreover, in addition to precious programming knowledge, you also get to know a lot of interesting people from the community all over the world. \n\nHowever, there is two things I would want to criticize. Firstly, nowadays Java is rapidly becoming less popular for Android development, while Kotlin is considered the future. Many professional developers who I have talked to claim that it does not make a lot of sense today to still start a new Android project with Java instead of Kotlin, so I wish the Nandogree was based on Kotlin in order to be even more future-proof. Also, de-facto standard frameworks like Dagger were not mentioned throughout the course, while (in my opinion) less useful things like homescreen widget had been pushed by Udacity. Maybe that is going to change in newer versions of the course.\n\nThe second thing is that, as I mentioned earlier, the TODO-tasks in the final projects of every section were too specific and too fine-grained. Sometimes I found myself just stupidly doing exactly what the TODOs wanted me to do instead of trying to capture a more high-level picture and solve design problems by myself. \n\nThat being said, I would recommend the Udacity Nanodegree to everyone who is interested in becoming an Android developer. Have fun!\n\n# Links\n* [QuizNerd on Google Play](https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite)\n* [Collection of Nanodegree 2018 final projects](https://google-udacity-scholars18.github.io/and/)","source":"_posts/quiznerd-my-experiences-with-the-android-developer-nanodegree.md","raw":"---\ntitle: My experiences with the Android Developer Nanodegree\ndate: 2019-01-26 09:01:50\ntags:\n---\n\nLast year from March to August I participated in Udacity's [Android Developer Nanodegree](https://www.udacity.com/course/android-developer-nanodegree-by-google--nd801) program and here I want to share my experiences. \n\nOriginally, I got an ad on Facebook to apply for a scholarship offered by Google and Udacity for the Nanodegree program, which normally costs ~ 900 . I had done very few Android development before and although I did not particularly want to prepare for a career as an Android developer, I was definitely interested in learning Android more thoroughly. However, as we all know, it is very hard to find motivation to learn a new technology without having a real, serious project to build. Consequently, I applied for the scholarship and I was lucky (thanks to Google and Udacity for this opportunity!). \n\n# The Nanodegree\n## Time Management\nWhen participating in a Nanodegree program, you usually have six months to complete it. Due to classes at university, I only had time to start in my, so essentially I had three months to complete it. But since I was not a complete beginner, that was totally manageable.\n\nUdacity suggests a schedule for when to do which module of the course as well as soft deadlines for the finals projects at the end of every learning section. However, that is only a suggestion. The only hard deadline is the submission of the very final project. Personally, I decided to take two days in a week to fully focus on learning and coding for the Nanodegree. \n\n## Structure \nThis specific Nanodegree program consisted of five major sections.\n\n1. Developing Android Apps\n2. Advanced Android App Development\n3. Gradle for Android and Java\n4. Material Design for Android Developers\n6. Capstone Project\n\nEvery section consists of several **(1) video lessons**, in which Udacity developers explain concepts and do live coding. Between lessons, there are **(2) quizzes** to test your gained knowledge, however, the quizzes were usually quite easy and obvious. In addition to that, there are several **(3) coding tasks**, which require you to practically apply the newly learned concepts. Every coding task starts with an unfinished, small toy app and a list of TODOs you have to fulfill in order to finish it. The TODOs tell you quite precisely what to do, so sometimes it was not really a challenge. Also, you do not have to do the coding tasks at all, if you do not want to, because nobody checks your results. But obviously it makes sense to do them and it helps a lot for the **(4) project app** at the end of each section (sometimes there are more than one). For this project, you are told to implement an app with a certain functionality (e.g. a cooking recipe manager, a movie collection manager, an RSS reader, etc.). Usually it starts off with a raw scaffolded app skeleton which you have to finish - this time without specific TODOs or instructions. At the end, you submit your code either via a GitHub repository or as a ZIP file and a Udacity mentor will review your code and give you helpful feedback regarding functionality, design and code style. \n\nHere are three of my section-end project apps:\n\n1. **[popular-movies-android](https://github.com/muety/popular-movies-android):** App for displaying movie information fetched from an online movie database. Focus was on interacting with an external, third-party web API.\n2. **[baking-time-android](https://github.com/muety/baking-time-android):** App for showing baking recipes and instructions. Focus was on widgets, responsive design and integrating a video player.\n3. **[xyz-reader-android](https://github.com/muety/xyz-reader-android):** Basic reading app for text articles. Focus was on properly implementing Material Design, animations and UX.\n\n## Community\nProbably the best thing about the whole course was the community. There is an official Slack channel and a forum full of like-minded developers from literally all around the world who are going through the same experience like you. People ask questions, discuss about certain tasks or technology in general and you immediately feel extremely welcome. Whether you are unsure about a task or cannot get a certain error fixed, there are people who will help you. Also, a lot of Udacity mentors hang out on Slack and in the forum and provide support as well, e.g. in form of weekly AMA sessions. In addition to that, every participant of the course is assigned a personal mentor, who is a Udacity mentor that you can contact directly of you have questions. Actually, I did never contact mine, but I am sure they are willing to help.\n\n## Career Boost\nDespite the fact that having a Nanodegree looks quite good on your resum anyway, Udacity also provides a lot of support to help you building a successful career with your newly gained Android knowledge. They provide information and support for your application, review your CV and more.\n\n## The Capstone project\nAt the very end of the course, there is the so called **Capstone project** and this was the most fun part during the whole Nanodegree (and also the greatest effort). You are given the task to freely realize any app you like, given some requirements, e.g. that you use at least three third-party libraries, follow Material Design guidelines, provide a homescreen widget and a few more. \n\nThe final project consists of two parts. First, you have to submit a design proposal, which includes your app idea, some mock-ups and details on how you plan to implement it. After your design has been approved by Udacity mentors, you can start with phase 2, the actual implementation.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cert.png)\n\n# QuizNerd\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/qn_feature.png)\n\nAt that time I had a few coding interviews, so I came to the idea to implement a multi-player coding quiz game as a final project. Although that was probably a more comprehensive project than most of the others, I still wanted to do it, especially because it was an app I really wanted to have for myself, not only for the Nanodegree. \n\nI spent approximately two weeks of nearly full-time coding on that final project and finally came up with my app called QuizNerd. It is implemented in pure Android (using Java) without any structural frameworks (e.g. like [Dagger](http://square.github.io/dagger/)) and uses Google's [Firebase](https://firebase.google.com/) as a backend. More precisely I used Firebase Authentication for user management, [Firestore](https://firebase.google.com/docs/firestore/) as a real-time document database, FCM for notifications and and Firebase [Cloud Functions](https://firebase.google.com/docs/functions/) as a Serverless framework for backend-side logic. \n\nIf you are a developer who likes games like [QuizClash](https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite), I would love to have you try out QuizNerd! You can find it on the [Play Store](https://play.google.com/store/apps/details?id=com.github.n1try.quiznerd) and it has several hundred questions for Android, C++, C#, HTML, Java, JavaScript, PHP, Python and Swift. Feel free to share your feedback with me .\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/qn2.png)\n\n# Conclusion\nIt was fun! I learned a lot during the Nanodegree and I am kind of confident calling myself an Android developer now. Most of the concepts were explained very detailed. Fore instance one of my favorite chapters was the one about Gradle, where they precisely explained how Gradle works, how to write own Gradle tasks and how to apply that to Android.\n\nIf you keep on motivating yourself to work through the lessons and especially through the final project, it is going to pay off. Moreover, in addition to precious programming knowledge, you also get to know a lot of interesting people from the community all over the world. \n\nHowever, there is two things I would want to criticize. Firstly, nowadays Java is rapidly becoming less popular for Android development, while Kotlin is considered the future. Many professional developers who I have talked to claim that it does not make a lot of sense today to still start a new Android project with Java instead of Kotlin, so I wish the Nandogree was based on Kotlin in order to be even more future-proof. Also, de-facto standard frameworks like Dagger were not mentioned throughout the course, while (in my opinion) less useful things like homescreen widget had been pushed by Udacity. Maybe that is going to change in newer versions of the course.\n\nThe second thing is that, as I mentioned earlier, the TODO-tasks in the final projects of every section were too specific and too fine-grained. Sometimes I found myself just stupidly doing exactly what the TODOs wanted me to do instead of trying to capture a more high-level picture and solve design problems by myself. \n\nThat being said, I would recommend the Udacity Nanodegree to everyone who is interested in becoming an Android developer. Have fun!\n\n# Links\n* [QuizNerd on Google Play](https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite)\n* [Collection of Nanodegree 2018 final projects](https://google-udacity-scholars18.github.io/and/)","slug":"quiznerd-my-experiences-with-the-android-developer-nanodegree","published":1,"updated":"2020-10-30T20:05:40.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqmb000xo2e0ag2sd6kz","content":"<p>Last year from March to August I participated in Udacitys <a href=\"https://www.udacity.com/course/android-developer-nanodegree-by-google--nd801\">Android Developer Nanodegree</a> program and here I want to share my experiences. </p>\n<p>Originally, I got an ad on Facebook to apply for a scholarship offered by Google and Udacity for the Nanodegree program, which normally costs ~ 900 . I had done very few Android development before and although I did not particularly want to prepare for a career as an Android developer, I was definitely interested in learning Android more thoroughly. However, as we all know, it is very hard to find motivation to learn a new technology without having a real, serious project to build. Consequently, I applied for the scholarship and I was lucky (thanks to Google and Udacity for this opportunity!). </p>\n<h1 id=\"The-Nanodegree\"><a href=\"#The-Nanodegree\" class=\"headerlink\" title=\"The Nanodegree\"></a>The Nanodegree</h1><h2 id=\"Time-Management\"><a href=\"#Time-Management\" class=\"headerlink\" title=\"Time Management\"></a>Time Management</h2><p>When participating in a Nanodegree program, you usually have six months to complete it. Due to classes at university, I only had time to start in my, so essentially I had three months to complete it. But since I was not a complete beginner, that was totally manageable.</p>\n<p>Udacity suggests a schedule for when to do which module of the course as well as soft deadlines for the finals projects at the end of every learning section. However, that is only a suggestion. The only hard deadline is the submission of the very final project. Personally, I decided to take two days in a week to fully focus on learning and coding for the Nanodegree. </p>\n<h2 id=\"Structure\"><a href=\"#Structure\" class=\"headerlink\" title=\"Structure\"></a>Structure</h2><p>This specific Nanodegree program consisted of five major sections.</p>\n<ol>\n<li>Developing Android Apps</li>\n<li>Advanced Android App Development</li>\n<li>Gradle for Android and Java</li>\n<li>Material Design for Android Developers</li>\n<li>Capstone Project</li>\n</ol>\n<p>Every section consists of several <strong>(1) video lessons</strong>, in which Udacity developers explain concepts and do live coding. Between lessons, there are <strong>(2) quizzes</strong> to test your gained knowledge, however, the quizzes were usually quite easy and obvious. In addition to that, there are several <strong>(3) coding tasks</strong>, which require you to practically apply the newly learned concepts. Every coding task starts with an unfinished, small toy app and a list of TODOs you have to fulfill in order to finish it. The TODOs tell you quite precisely what to do, so sometimes it was not really a challenge. Also, you do not have to do the coding tasks at all, if you do not want to, because nobody checks your results. But obviously it makes sense to do them and it helps a lot for the <strong>(4) project app</strong> at the end of each section (sometimes there are more than one). For this project, you are told to implement an app with a certain functionality (e.g. a cooking recipe manager, a movie collection manager, an RSS reader, etc.). Usually it starts off with a raw scaffolded app skeleton which you have to finish - this time without specific TODOs or instructions. At the end, you submit your code either via a GitHub repository or as a ZIP file and a Udacity mentor will review your code and give you helpful feedback regarding functionality, design and code style. </p>\n<p>Here are three of my section-end project apps:</p>\n<ol>\n<li><strong><a href=\"https://github.com/muety/popular-movies-android\">popular-movies-android</a>:</strong> App for displaying movie information fetched from an online movie database. Focus was on interacting with an external, third-party web API.</li>\n<li><strong><a href=\"https://github.com/muety/baking-time-android\">baking-time-android</a>:</strong> App for showing baking recipes and instructions. Focus was on widgets, responsive design and integrating a video player.</li>\n<li><strong><a href=\"https://github.com/muety/xyz-reader-android\">xyz-reader-android</a>:</strong> Basic reading app for text articles. Focus was on properly implementing Material Design, animations and UX.</li>\n</ol>\n<h2 id=\"Community\"><a href=\"#Community\" class=\"headerlink\" title=\"Community\"></a>Community</h2><p>Probably the best thing about the whole course was the community. There is an official Slack channel and a forum full of like-minded developers from literally all around the world who are going through the same experience like you. People ask questions, discuss about certain tasks or technology in general and you immediately feel extremely welcome. Whether you are unsure about a task or cannot get a certain error fixed, there are people who will help you. Also, a lot of Udacity mentors hang out on Slack and in the forum and provide support as well, e.g. in form of weekly AMA sessions. In addition to that, every participant of the course is assigned a personal mentor, who is a Udacity mentor that you can contact directly of you have questions. Actually, I did never contact mine, but I am sure they are willing to help.</p>\n<h2 id=\"Career-Boost\"><a href=\"#Career-Boost\" class=\"headerlink\" title=\"Career Boost\"></a>Career Boost</h2><p>Despite the fact that having a Nanodegree looks quite good on your resum anyway, Udacity also provides a lot of support to help you building a successful career with your newly gained Android knowledge. They provide information and support for your application, review your CV and more.</p>\n<h2 id=\"The-Capstone-project\"><a href=\"#The-Capstone-project\" class=\"headerlink\" title=\"The Capstone project\"></a>The Capstone project</h2><p>At the very end of the course, there is the so called <strong>Capstone project</strong> and this was the most fun part during the whole Nanodegree (and also the greatest effort). You are given the task to freely realize any app you like, given some requirements, e.g. that you use at least three third-party libraries, follow Material Design guidelines, provide a homescreen widget and a few more. </p>\n<p>The final project consists of two parts. First, you have to submit a design proposal, which includes your app idea, some mock-ups and details on how you plan to implement it. After your design has been approved by Udacity mentors, you can start with phase 2, the actual implementation.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cert.png\"></p>\n<h1 id=\"QuizNerd\"><a href=\"#QuizNerd\" class=\"headerlink\" title=\"QuizNerd\"></a>QuizNerd</h1><p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/qn_feature.png\"></p>\n<p>At that time I had a few coding interviews, so I came to the idea to implement a multi-player coding quiz game as a final project. Although that was probably a more comprehensive project than most of the others, I still wanted to do it, especially because it was an app I really wanted to have for myself, not only for the Nanodegree. </p>\n<p>I spent approximately two weeks of nearly full-time coding on that final project and finally came up with my app called QuizNerd. It is implemented in pure Android (using Java) without any structural frameworks (e.g. like <a href=\"http://square.github.io/dagger/\">Dagger</a>) and uses Googles <a href=\"https://firebase.google.com/\">Firebase</a> as a backend. More precisely I used Firebase Authentication for user management, <a href=\"https://firebase.google.com/docs/firestore/\">Firestore</a> as a real-time document database, FCM for notifications and and Firebase <a href=\"https://firebase.google.com/docs/functions/\">Cloud Functions</a> as a Serverless framework for backend-side logic. </p>\n<p>If you are a developer who likes games like <a href=\"https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite\">QuizClash</a>, I would love to have you try out QuizNerd! You can find it on the <a href=\"https://play.google.com/store/apps/details?id=com.github.n1try.quiznerd\">Play Store</a> and it has several hundred questions for Android, C++, C#, HTML, Java, JavaScript, PHP, Python and Swift. Feel free to share your feedback with me .</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/qn2.png\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>It was fun! I learned a lot during the Nanodegree and I am kind of confident calling myself an Android developer now. Most of the concepts were explained very detailed. Fore instance one of my favorite chapters was the one about Gradle, where they precisely explained how Gradle works, how to write own Gradle tasks and how to apply that to Android.</p>\n<p>If you keep on motivating yourself to work through the lessons and especially through the final project, it is going to pay off. Moreover, in addition to precious programming knowledge, you also get to know a lot of interesting people from the community all over the world. </p>\n<p>However, there is two things I would want to criticize. Firstly, nowadays Java is rapidly becoming less popular for Android development, while Kotlin is considered the future. Many professional developers who I have talked to claim that it does not make a lot of sense today to still start a new Android project with Java instead of Kotlin, so I wish the Nandogree was based on Kotlin in order to be even more future-proof. Also, de-facto standard frameworks like Dagger were not mentioned throughout the course, while (in my opinion) less useful things like homescreen widget had been pushed by Udacity. Maybe that is going to change in newer versions of the course.</p>\n<p>The second thing is that, as I mentioned earlier, the TODO-tasks in the final projects of every section were too specific and too fine-grained. Sometimes I found myself just stupidly doing exactly what the TODOs wanted me to do instead of trying to capture a more high-level picture and solve design problems by myself. </p>\n<p>That being said, I would recommend the Udacity Nanodegree to everyone who is interested in becoming an Android developer. Have fun!</p>\n<h1 id=\"Links\"><a href=\"#Links\" class=\"headerlink\" title=\"Links\"></a>Links</h1><ul>\n<li><a href=\"https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite\">QuizNerd on Google Play</a></li>\n<li><a href=\"https://google-udacity-scholars18.github.io/and/\">Collection of Nanodegree 2018 final projects</a></li>\n</ul>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Last year from March to August I participated in Udacitys <a href=\"https://www.udacity.com/course/android-developer-nanodegree-by-google--nd801\">Android Developer Nanodegree</a> program and here I want to share my experiences. </p>\n<p>Originally, I got an ad on Facebook to apply for a scholarship offered by Google and Udacity for the Nanodegree program, which normally costs ~ 900 . I had done very few Android development before and although I did not particularly want to prepare for a career as an Android developer, I was definitely interested in learning Android more thoroughly. However, as we all know, it is very hard to find motivation to learn a new technology without having a real, serious project to build. Consequently, I applied for the scholarship and I was lucky (thanks to Google and Udacity for this opportunity!). </p>\n<h1 id=\"The-Nanodegree\"><a href=\"#The-Nanodegree\" class=\"headerlink\" title=\"The Nanodegree\"></a>The Nanodegree</h1><h2 id=\"Time-Management\"><a href=\"#Time-Management\" class=\"headerlink\" title=\"Time Management\"></a>Time Management</h2><p>When participating in a Nanodegree program, you usually have six months to complete it. Due to classes at university, I only had time to start in my, so essentially I had three months to complete it. But since I was not a complete beginner, that was totally manageable.</p>\n<p>Udacity suggests a schedule for when to do which module of the course as well as soft deadlines for the finals projects at the end of every learning section. However, that is only a suggestion. The only hard deadline is the submission of the very final project. Personally, I decided to take two days in a week to fully focus on learning and coding for the Nanodegree. </p>\n<h2 id=\"Structure\"><a href=\"#Structure\" class=\"headerlink\" title=\"Structure\"></a>Structure</h2><p>This specific Nanodegree program consisted of five major sections.</p>\n<ol>\n<li>Developing Android Apps</li>\n<li>Advanced Android App Development</li>\n<li>Gradle for Android and Java</li>\n<li>Material Design for Android Developers</li>\n<li>Capstone Project</li>\n</ol>\n<p>Every section consists of several <strong>(1) video lessons</strong>, in which Udacity developers explain concepts and do live coding. Between lessons, there are <strong>(2) quizzes</strong> to test your gained knowledge, however, the quizzes were usually quite easy and obvious. In addition to that, there are several <strong>(3) coding tasks</strong>, which require you to practically apply the newly learned concepts. Every coding task starts with an unfinished, small toy app and a list of TODOs you have to fulfill in order to finish it. The TODOs tell you quite precisely what to do, so sometimes it was not really a challenge. Also, you do not have to do the coding tasks at all, if you do not want to, because nobody checks your results. But obviously it makes sense to do them and it helps a lot for the <strong>(4) project app</strong> at the end of each section (sometimes there are more than one). For this project, you are told to implement an app with a certain functionality (e.g. a cooking recipe manager, a movie collection manager, an RSS reader, etc.). Usually it starts off with a raw scaffolded app skeleton which you have to finish - this time without specific TODOs or instructions. At the end, you submit your code either via a GitHub repository or as a ZIP file and a Udacity mentor will review your code and give you helpful feedback regarding functionality, design and code style. </p>\n<p>Here are three of my section-end project apps:</p>\n<ol>\n<li><strong><a href=\"https://github.com/muety/popular-movies-android\">popular-movies-android</a>:</strong> App for displaying movie information fetched from an online movie database. Focus was on interacting with an external, third-party web API.</li>\n<li><strong><a href=\"https://github.com/muety/baking-time-android\">baking-time-android</a>:</strong> App for showing baking recipes and instructions. Focus was on widgets, responsive design and integrating a video player.</li>\n<li><strong><a href=\"https://github.com/muety/xyz-reader-android\">xyz-reader-android</a>:</strong> Basic reading app for text articles. Focus was on properly implementing Material Design, animations and UX.</li>\n</ol>\n<h2 id=\"Community\"><a href=\"#Community\" class=\"headerlink\" title=\"Community\"></a>Community</h2><p>Probably the best thing about the whole course was the community. There is an official Slack channel and a forum full of like-minded developers from literally all around the world who are going through the same experience like you. People ask questions, discuss about certain tasks or technology in general and you immediately feel extremely welcome. Whether you are unsure about a task or cannot get a certain error fixed, there are people who will help you. Also, a lot of Udacity mentors hang out on Slack and in the forum and provide support as well, e.g. in form of weekly AMA sessions. In addition to that, every participant of the course is assigned a personal mentor, who is a Udacity mentor that you can contact directly of you have questions. Actually, I did never contact mine, but I am sure they are willing to help.</p>\n<h2 id=\"Career-Boost\"><a href=\"#Career-Boost\" class=\"headerlink\" title=\"Career Boost\"></a>Career Boost</h2><p>Despite the fact that having a Nanodegree looks quite good on your resum anyway, Udacity also provides a lot of support to help you building a successful career with your newly gained Android knowledge. They provide information and support for your application, review your CV and more.</p>\n<h2 id=\"The-Capstone-project\"><a href=\"#The-Capstone-project\" class=\"headerlink\" title=\"The Capstone project\"></a>The Capstone project</h2><p>At the very end of the course, there is the so called <strong>Capstone project</strong> and this was the most fun part during the whole Nanodegree (and also the greatest effort). You are given the task to freely realize any app you like, given some requirements, e.g. that you use at least three third-party libraries, follow Material Design guidelines, provide a homescreen widget and a few more. </p>\n<p>The final project consists of two parts. First, you have to submit a design proposal, which includes your app idea, some mock-ups and details on how you plan to implement it. After your design has been approved by Udacity mentors, you can start with phase 2, the actual implementation.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/cert.png\"></p>\n<h1 id=\"QuizNerd\"><a href=\"#QuizNerd\" class=\"headerlink\" title=\"QuizNerd\"></a>QuizNerd</h1><p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/qn_feature.png\"></p>\n<p>At that time I had a few coding interviews, so I came to the idea to implement a multi-player coding quiz game as a final project. Although that was probably a more comprehensive project than most of the others, I still wanted to do it, especially because it was an app I really wanted to have for myself, not only for the Nanodegree. </p>\n<p>I spent approximately two weeks of nearly full-time coding on that final project and finally came up with my app called QuizNerd. It is implemented in pure Android (using Java) without any structural frameworks (e.g. like <a href=\"http://square.github.io/dagger/\">Dagger</a>) and uses Googles <a href=\"https://firebase.google.com/\">Firebase</a> as a backend. More precisely I used Firebase Authentication for user management, <a href=\"https://firebase.google.com/docs/firestore/\">Firestore</a> as a real-time document database, FCM for notifications and and Firebase <a href=\"https://firebase.google.com/docs/functions/\">Cloud Functions</a> as a Serverless framework for backend-side logic. </p>\n<p>If you are a developer who likes games like <a href=\"https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite\">QuizClash</a>, I would love to have you try out QuizNerd! You can find it on the <a href=\"https://play.google.com/store/apps/details?id=com.github.n1try.quiznerd\">Play Store</a> and it has several hundred questions for Android, C++, C#, HTML, Java, JavaScript, PHP, Python and Swift. Feel free to share your feedback with me .</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/qn2.png\"></p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>It was fun! I learned a lot during the Nanodegree and I am kind of confident calling myself an Android developer now. Most of the concepts were explained very detailed. Fore instance one of my favorite chapters was the one about Gradle, where they precisely explained how Gradle works, how to write own Gradle tasks and how to apply that to Android.</p>\n<p>If you keep on motivating yourself to work through the lessons and especially through the final project, it is going to pay off. Moreover, in addition to precious programming knowledge, you also get to know a lot of interesting people from the community all over the world. </p>\n<p>However, there is two things I would want to criticize. Firstly, nowadays Java is rapidly becoming less popular for Android development, while Kotlin is considered the future. Many professional developers who I have talked to claim that it does not make a lot of sense today to still start a new Android project with Java instead of Kotlin, so I wish the Nandogree was based on Kotlin in order to be even more future-proof. Also, de-facto standard frameworks like Dagger were not mentioned throughout the course, while (in my opinion) less useful things like homescreen widget had been pushed by Udacity. Maybe that is going to change in newer versions of the course.</p>\n<p>The second thing is that, as I mentioned earlier, the TODO-tasks in the final projects of every section were too specific and too fine-grained. Sometimes I found myself just stupidly doing exactly what the TODOs wanted me to do instead of trying to capture a more high-level picture and solve design problems by myself. </p>\n<p>That being said, I would recommend the Udacity Nanodegree to everyone who is interested in becoming an Android developer. Have fun!</p>\n<h1 id=\"Links\"><a href=\"#Links\" class=\"headerlink\" title=\"Links\"></a>Links</h1><ul>\n<li><a href=\"https://play.google.com/store/apps/details?id=se.feomedia.quizkampen.de.lite\">QuizNerd on Google Play</a></li>\n<li><a href=\"https://google-udacity-scholars18.github.io/and/\">Collection of Nanodegree 2018 final projects</a></li>\n</ul>\n"},{"title":"TalkyCars: A Distributed Software Platform for Cooperative Perception","date":"2020-09-11T07:24:31.000Z","_content":"\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars9.png)\n\n# Introduction\nThis is a short, summarizing write-up about the topics of my Master's thesis, published in February 2020 in cooperation with [Prof. Dr. Sax](https://www.itiv.kit.edu/21_3940.php), [M.Sc. Martin Sommer](https://www.itiv.kit.edu/21_6289.php) and [M.Sc. Marco Stang](https://www.itiv.kit.edu/21_5341.php) at the [KIT Institute for Information Processing Technologies](https://itiv.kit.edu).\n\n# Citation (BibTeX)\n```bibtex\n@mastersthesis{Mutsch2020,\n    author = {Mtsch, Ferdinand},\n    school = {Karlsruhe Institute of Technology},\n    title = {{TalkyCars: A Distributed Software Platform for Cooperative Perception among Connected Autonomous Vehicles based on Cellular-V2X Communication}},\n    year = {2020},\n    doi = {10.5445/IR/1000118118},\n    URL = {http://dx.doi.org/10.5445/IR/1000118118},\n}\n```\n\n# Motivation\nOur work is in the field of autonomous driving, more specifically: cooperative perception. While the topic itself is very diverse, we attempted to approach cooperative perception mainly from a software architecture point of view.\n\nLet us first have a quick outlook. Nowadays, automated driving is still quite rare. However, although forecasts on its future development vary greatly, many researchers predict that level 4 automated vehicles may be around quite soon and level 5 cars may be production-ready already by 2030 [[1]](#refs), [[2]](#refs), [[3]](#refs). However, in the beginning, self-driving cars will find themselves having to operate among highly mixed traffic. That is, traffic with still a high percentage of manully controlled cars. Therefore, a tremendously high level of safety is required.\n\n# Cooperative Perception\n\nOne approach to increase reliability and safety of automated vehicles is the concept of cooperative perception. In essence, the idea is to have the cars not only rely on their own sensory, but communicate with surrounding vehicles to mutually exchange information about their environment. In that sense, cooperative perception is a use case of vehicle-to-vehicle  or more broadly  vehicle-to-everything communication. It enables cars to virtually extend their field of view and, for instance, be able to look around corners.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nloss_2.png)\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nloss_3.png)\n(Source: [[4]](#refs))\n\nCooperative perception is a topic of research for years already. Most present approaches rely on so called vehicular ad-hoc networks among participants using dedicated short-range communication. That is, cars within range of each other form pair-wise temporary connections using a technology similar to WiFi. Not uncommonly, they exchange low-level sensor data, for instance LiDAR point cloud, camera images, etc.\n\nThere is research which shows that these approaches face some limitations, mainly in terms of latency and data throughput. Consequently, our work assumes a different technological basis. We require that information is exchanged not in form of ad-hoc mesh networks, but through on or more central nodes using cellular communication. Moreover, we do not want to share raw senor data, which can become quite large, but higher-level information that corresponds to a common, shared environment model.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars2.png)\n(Source: [[4]](#refs))\n\n# Approach \nEssentially, there are two major areas to cover in order to develop an end-to-end cooperative perception system prototype. First, we need to define a common language or **model** that all traffic participants use to describe themselves and their environment. Second, a software- and communications **architecture** needs to be designed, as we need a system that enables efficient, scalable and low-latency communications among network participants.\n\n## Environment Modeling\nLet us focus on the modeling aspect first. There already exist elaborate approaches for modeling and state representation for cooperative perception. Most famously, there are cooperative awareness- and cooperative perception messages [[5]](#refs), [[6]](#refs). In addition, Kohlhaas et al. [[7]](#refs) suggested to combine low-level, primitive attributes  like position and velocity  with higher-level, semantic knowledge. More specifically, they propose a modeling approach that also incorporates relationships among different entities and traffic participants within a traffic scene. In 2018, another research group first proposed the use of so called probabilistic entity relationship models for traffic scene representation [[8]](#refs). \n\nFor our environment model, we combine these aapproches with the concept of geo tiling using so called QuadKeys ([[10]](#refs)). In essence, geo tiling means to recursively divide the world map into four squares up to almost arbitrarily high precision. Each square is then uniquely identifiable through a key. Using these QuadKeys, we have a simple, commonly-understood and unique way of referencing geographical locations. Especially, it prevents from having to translate between different local- and global coordinate systems.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/quadkeys.jpg)\n(Source: [[9]](#refs))\n\nMoreover, we can utilize geo tiles as a basis for our environment model. We propose a model that represents an occupancy grid at its core, whereas each cell corresponds to a tile of a certain, fixed precision level. For instance, with a QuadKey precision level of 25, we would require our intelligent vehicles to represent and share their perceived environment state as an occupancy grid with cells of approx. 1.1 by 1.1 meters in size. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars3.png)\n\nNaturally, a cell's occupancy state alone, that is \"occupied\" or \"not occupied\", is not sufficient to comprehensively model a traffic scene. Therefore, we allow each cell to contain additional state information following a comprehensive meta model. For instance, the type of traffic participant, that currently occupies a cell, alongside all of its respective static and dynamic properties might be included as well.\n\nMoreover, there is an other interesting aspect to our proposed modeling approach. Because all facts of a vehicle's environment are derived from imperfect sensory, they're inevitably accompanied by uncertainty. Moreover, the fact that the description of a traffic scene is only valid for a very short temporal horizon even adds to this uncertainty. This leads to the requirement to use a modeling approach capable of coping with such uncertainty. As a consequence, we decided to pick up on the concept of probabilistic entity-relationship model and represent every fact as a quadruple of subject, predicate, object and confidence. The confidence component corresponds to the likelihood of this fact being true.\n\n```\nGraph consisting of 4-tuples:\n------\n(subject, predicate, object, confidence)\n\nExample:\n------\n(obstacle_5, isOfType, pedestrian, 0.921)\n(obstacle_5, hasPosition, 310112030021333, 0.448)\n(obstacle_5, isStoppedAt, traffic_light_189, 0.995)\n```\n\n## System Architecture\nLet us now focus on the second major aspect: the architecture of a software system that enables for cooperative perception.\n\nNaturally, one essential requirement is good performance. That is, information from different vehicles shall be transmitted and aggregated as fast as possible. As mentioned earlier, many approaches relying on decentralized DSRC communication are potentially limited in latency and throughput. This is why we want to take the approach to rely on cellular communication, preferably using 5G networks. Also, to reduce communication complexity, our approach relies on centralized compute nodes that receive, transform, fuse and re-distribute information from many network participants. Of course, this introduces new challenges. First and foremost there is the requirement of high scalability, as these compute nodes have to handle large amount of data.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars4.png)\n\nTo address this challenge, we once again utilize geo tiling with QuadKeys. In our approach, the environment is split into QuadTiles of a certain size, for instance 5 by 5 km, and a data fusion node is not deployed globally, but once per every tile. Vehicles within such a tile exchange their data through the tile's responsible node. This way, the node only has to cope with a certain, limited amount of senders and receivers.\n\nIn summary, the proposed system consists of three core components. On the client-side, that is, the vehicle side, an application is responsible for generating, sending and receiving instances of the previously mentioned environment model. Second, a message broker is deployed once per tile to consolidate incoming data messages. Third, the previously mentioned fusion node is responsible for aggregating these information packets to a single, globally valid model instance and sending it back to the cars.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars5.png)\n\n# Evaluation\nFor evaluation purposes, we implemented all parts of our system and integrated it with the CARLA simulation environment. We conducted two different experiments to evaluate the proposed system with respect to performance and quality.\n\n## Software Performance\nIn the first part of the evaluation, we attempted to answer the question of how many concurrent vehicles the central data broker or fusion node is able to handle at a time. In our experiment setup, we deployed one instance of the fusion node as well as a distributed data generator that simulates an increasing amount of vehicles sending their environment models. We required that observations must be fused at a fixed rate, for instance, 10 times per second. Then we measured the actual rate at which the fusion node is able to process incoming observations as the number of sender vehicles increased. Eventually, we found that the possible fusion rate heavily depends on the size of the underlying occupancy grid and that our very little optimized implementation is able to maintain a fusion rate of 10 Hz with up to 220 concurrent vehicles at a grid size of approx. 50 meters and 100 concurrent vehicles at a grid size of approx. 100 meters. \n\n![](https://apps.muetsch.io/images/o:auto/rs,s:400?image=https://muetsch.io/images/talkycars6.png)\n\n## End-to-End Perception Quality\nThe second part of the evaluation aimed to assess whether a vehicle's overall perception quality can be improved through the use of our system in general.\n\nTherefore, we utilized the CARLA simulation environment and repeatedly generated different urban traffic scenes with a fixed amount of connected, intelligent vehicles, each of which runs our proposed software system. Every vehicle was tasked to traverse the environment from a random start point to a random destination. While driving, every vehicle recorded its own local perception, derived from its sensors, including all perceived obstacles. Later, these perceived environment states are compared to the true state of the environment provided by the simulator. Accordingly, an error measure can be derived. To asses whether or not our proposed system is beneficial to a self-driving car's perception quality, every instance of the experiment is run twice: once with and once without cooperative perception. Eventually the respective error measures can be compared.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars7.png)\n\nDoing so, we found that the average perception quality, that is, the accuracy of detecting obstacles in the simulation, could be improved significantly in our test scenarios.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars8.png)\n(Image: Recall (\"hit rate\") for occupied cells with / without CP)\n\n# Conclusion\nWith our work, we contributed a proof-of-concept for a novel approach to cooperative perception, utilizing centralized, cellular communication and involving a comprehensive meta model for traffic scene representation. Both our reference implementation as well as the concept itself still demands for optimizations in order to be advanced to a real-world applicable system. However, it can serve as a basis for future research. \n\nThe entire project is [open-source](https://github.com/muety/talkycars-thesis).\n\n# References\n<a name=\"refs\"></a>\n\n1. McKinsey Center for Future Mobility. (2019). Autonomous Driving | MCFM | McKinsey. Retrieved November 25, 2019, from [Link](https://www.mckinsey.com/features/mckinsey-center-for-future-mobility/overview/autonomous-driving)\n2. Thomson, C. (2017). Elon Musk has a stunning prediction for what cars will be like 10 years from now. Retrieved from [Link](https://www.businessinsider.com/elon-musk-predicts-most-cars-will-be-driverless-in-10-years-2017-2?r=DE&IR=T)\n3. BMW Group. (2019). Contract signed: BMW Group and Daimler AG launch long-term development cooperation for automated driving. Retrieved from [Link](https://www.press.bmwgroup.com/global/article/detail/T0298266EN/)contract-signed:-bmw-group-and-daimler-ag-launch-long-term-development-cooperation-for-automated-driving\n4. [Link](https://www.qualcomm.com/media/documents/files/accelerating-c-v2x-commercialization.pdf)\n5. European Telecommunications Standards Institute (ETSI), ETSI TR 103 562, Sophia Antipolis, 2019.\n6. A. Rauch, F. Klanner, and K. Dietmayer, Analysis of V2X communication parameters for the development of a fusion architecture for cooperative perception systems, in 2011 IEEE Intelligent Vehicles Symposium (IV), 2011, pp. 685690.\n7. R. Kohlhaas, T. Bittner, T. Schamm, and J. M. Zollner, Semantic state space for high-level maneuver planning in structured traffic scenes, in 17th International IEEE Conference on Intelligent Transportation Systems (ITSC), 2014, pp. 10601065.\n8. D. Petrich, D. Azarfar, F. Kuhnt, and J. M. Zollner, The Fingerprint of a Traffic Situation: A Semantic Relationship Tensor for Situation Description and Awareness, in 2018 21st International Conference on Intelligent Transportation Systems (ITSC), 2018, pp. 429435.\n9. [Link](https://docs.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system)\n10. Schwartz, J. (2018). Bing Maps Tile System - Bing Maps | Microsoft Docs. Retrieved November 26, 2019, from [Link](https://docs.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system)","source":"_posts/talkycars-a-distributed-software-platform-for-cooperative-perception.md","raw":"---\ntitle: 'TalkyCars: A Distributed Software Platform for Cooperative Perception'\ndate: 2020-09-11 09:24:31\ntags:\n---\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars9.png)\n\n# Introduction\nThis is a short, summarizing write-up about the topics of my Master's thesis, published in February 2020 in cooperation with [Prof. Dr. Sax](https://www.itiv.kit.edu/21_3940.php), [M.Sc. Martin Sommer](https://www.itiv.kit.edu/21_6289.php) and [M.Sc. Marco Stang](https://www.itiv.kit.edu/21_5341.php) at the [KIT Institute for Information Processing Technologies](https://itiv.kit.edu).\n\n# Citation (BibTeX)\n```bibtex\n@mastersthesis{Mutsch2020,\n    author = {Mtsch, Ferdinand},\n    school = {Karlsruhe Institute of Technology},\n    title = {{TalkyCars: A Distributed Software Platform for Cooperative Perception among Connected Autonomous Vehicles based on Cellular-V2X Communication}},\n    year = {2020},\n    doi = {10.5445/IR/1000118118},\n    URL = {http://dx.doi.org/10.5445/IR/1000118118},\n}\n```\n\n# Motivation\nOur work is in the field of autonomous driving, more specifically: cooperative perception. While the topic itself is very diverse, we attempted to approach cooperative perception mainly from a software architecture point of view.\n\nLet us first have a quick outlook. Nowadays, automated driving is still quite rare. However, although forecasts on its future development vary greatly, many researchers predict that level 4 automated vehicles may be around quite soon and level 5 cars may be production-ready already by 2030 [[1]](#refs), [[2]](#refs), [[3]](#refs). However, in the beginning, self-driving cars will find themselves having to operate among highly mixed traffic. That is, traffic with still a high percentage of manully controlled cars. Therefore, a tremendously high level of safety is required.\n\n# Cooperative Perception\n\nOne approach to increase reliability and safety of automated vehicles is the concept of cooperative perception. In essence, the idea is to have the cars not only rely on their own sensory, but communicate with surrounding vehicles to mutually exchange information about their environment. In that sense, cooperative perception is a use case of vehicle-to-vehicle  or more broadly  vehicle-to-everything communication. It enables cars to virtually extend their field of view and, for instance, be able to look around corners.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nloss_2.png)\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nloss_3.png)\n(Source: [[4]](#refs))\n\nCooperative perception is a topic of research for years already. Most present approaches rely on so called vehicular ad-hoc networks among participants using dedicated short-range communication. That is, cars within range of each other form pair-wise temporary connections using a technology similar to WiFi. Not uncommonly, they exchange low-level sensor data, for instance LiDAR point cloud, camera images, etc.\n\nThere is research which shows that these approaches face some limitations, mainly in terms of latency and data throughput. Consequently, our work assumes a different technological basis. We require that information is exchanged not in form of ad-hoc mesh networks, but through on or more central nodes using cellular communication. Moreover, we do not want to share raw senor data, which can become quite large, but higher-level information that corresponds to a common, shared environment model.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars2.png)\n(Source: [[4]](#refs))\n\n# Approach \nEssentially, there are two major areas to cover in order to develop an end-to-end cooperative perception system prototype. First, we need to define a common language or **model** that all traffic participants use to describe themselves and their environment. Second, a software- and communications **architecture** needs to be designed, as we need a system that enables efficient, scalable and low-latency communications among network participants.\n\n## Environment Modeling\nLet us focus on the modeling aspect first. There already exist elaborate approaches for modeling and state representation for cooperative perception. Most famously, there are cooperative awareness- and cooperative perception messages [[5]](#refs), [[6]](#refs). In addition, Kohlhaas et al. [[7]](#refs) suggested to combine low-level, primitive attributes  like position and velocity  with higher-level, semantic knowledge. More specifically, they propose a modeling approach that also incorporates relationships among different entities and traffic participants within a traffic scene. In 2018, another research group first proposed the use of so called probabilistic entity relationship models for traffic scene representation [[8]](#refs). \n\nFor our environment model, we combine these aapproches with the concept of geo tiling using so called QuadKeys ([[10]](#refs)). In essence, geo tiling means to recursively divide the world map into four squares up to almost arbitrarily high precision. Each square is then uniquely identifiable through a key. Using these QuadKeys, we have a simple, commonly-understood and unique way of referencing geographical locations. Especially, it prevents from having to translate between different local- and global coordinate systems.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/quadkeys.jpg)\n(Source: [[9]](#refs))\n\nMoreover, we can utilize geo tiles as a basis for our environment model. We propose a model that represents an occupancy grid at its core, whereas each cell corresponds to a tile of a certain, fixed precision level. For instance, with a QuadKey precision level of 25, we would require our intelligent vehicles to represent and share their perceived environment state as an occupancy grid with cells of approx. 1.1 by 1.1 meters in size. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars3.png)\n\nNaturally, a cell's occupancy state alone, that is \"occupied\" or \"not occupied\", is not sufficient to comprehensively model a traffic scene. Therefore, we allow each cell to contain additional state information following a comprehensive meta model. For instance, the type of traffic participant, that currently occupies a cell, alongside all of its respective static and dynamic properties might be included as well.\n\nMoreover, there is an other interesting aspect to our proposed modeling approach. Because all facts of a vehicle's environment are derived from imperfect sensory, they're inevitably accompanied by uncertainty. Moreover, the fact that the description of a traffic scene is only valid for a very short temporal horizon even adds to this uncertainty. This leads to the requirement to use a modeling approach capable of coping with such uncertainty. As a consequence, we decided to pick up on the concept of probabilistic entity-relationship model and represent every fact as a quadruple of subject, predicate, object and confidence. The confidence component corresponds to the likelihood of this fact being true.\n\n```\nGraph consisting of 4-tuples:\n------\n(subject, predicate, object, confidence)\n\nExample:\n------\n(obstacle_5, isOfType, pedestrian, 0.921)\n(obstacle_5, hasPosition, 310112030021333, 0.448)\n(obstacle_5, isStoppedAt, traffic_light_189, 0.995)\n```\n\n## System Architecture\nLet us now focus on the second major aspect: the architecture of a software system that enables for cooperative perception.\n\nNaturally, one essential requirement is good performance. That is, information from different vehicles shall be transmitted and aggregated as fast as possible. As mentioned earlier, many approaches relying on decentralized DSRC communication are potentially limited in latency and throughput. This is why we want to take the approach to rely on cellular communication, preferably using 5G networks. Also, to reduce communication complexity, our approach relies on centralized compute nodes that receive, transform, fuse and re-distribute information from many network participants. Of course, this introduces new challenges. First and foremost there is the requirement of high scalability, as these compute nodes have to handle large amount of data.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars4.png)\n\nTo address this challenge, we once again utilize geo tiling with QuadKeys. In our approach, the environment is split into QuadTiles of a certain size, for instance 5 by 5 km, and a data fusion node is not deployed globally, but once per every tile. Vehicles within such a tile exchange their data through the tile's responsible node. This way, the node only has to cope with a certain, limited amount of senders and receivers.\n\nIn summary, the proposed system consists of three core components. On the client-side, that is, the vehicle side, an application is responsible for generating, sending and receiving instances of the previously mentioned environment model. Second, a message broker is deployed once per tile to consolidate incoming data messages. Third, the previously mentioned fusion node is responsible for aggregating these information packets to a single, globally valid model instance and sending it back to the cars.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars5.png)\n\n# Evaluation\nFor evaluation purposes, we implemented all parts of our system and integrated it with the CARLA simulation environment. We conducted two different experiments to evaluate the proposed system with respect to performance and quality.\n\n## Software Performance\nIn the first part of the evaluation, we attempted to answer the question of how many concurrent vehicles the central data broker or fusion node is able to handle at a time. In our experiment setup, we deployed one instance of the fusion node as well as a distributed data generator that simulates an increasing amount of vehicles sending their environment models. We required that observations must be fused at a fixed rate, for instance, 10 times per second. Then we measured the actual rate at which the fusion node is able to process incoming observations as the number of sender vehicles increased. Eventually, we found that the possible fusion rate heavily depends on the size of the underlying occupancy grid and that our very little optimized implementation is able to maintain a fusion rate of 10 Hz with up to 220 concurrent vehicles at a grid size of approx. 50 meters and 100 concurrent vehicles at a grid size of approx. 100 meters. \n\n![](https://apps.muetsch.io/images/o:auto/rs,s:400?image=https://muetsch.io/images/talkycars6.png)\n\n## End-to-End Perception Quality\nThe second part of the evaluation aimed to assess whether a vehicle's overall perception quality can be improved through the use of our system in general.\n\nTherefore, we utilized the CARLA simulation environment and repeatedly generated different urban traffic scenes with a fixed amount of connected, intelligent vehicles, each of which runs our proposed software system. Every vehicle was tasked to traverse the environment from a random start point to a random destination. While driving, every vehicle recorded its own local perception, derived from its sensors, including all perceived obstacles. Later, these perceived environment states are compared to the true state of the environment provided by the simulator. Accordingly, an error measure can be derived. To asses whether or not our proposed system is beneficial to a self-driving car's perception quality, every instance of the experiment is run twice: once with and once without cooperative perception. Eventually the respective error measures can be compared.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars7.png)\n\nDoing so, we found that the average perception quality, that is, the accuracy of detecting obstacles in the simulation, could be improved significantly in our test scenarios.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars8.png)\n(Image: Recall (\"hit rate\") for occupied cells with / without CP)\n\n# Conclusion\nWith our work, we contributed a proof-of-concept for a novel approach to cooperative perception, utilizing centralized, cellular communication and involving a comprehensive meta model for traffic scene representation. Both our reference implementation as well as the concept itself still demands for optimizations in order to be advanced to a real-world applicable system. However, it can serve as a basis for future research. \n\nThe entire project is [open-source](https://github.com/muety/talkycars-thesis).\n\n# References\n<a name=\"refs\"></a>\n\n1. McKinsey Center for Future Mobility. (2019). Autonomous Driving | MCFM | McKinsey. Retrieved November 25, 2019, from [Link](https://www.mckinsey.com/features/mckinsey-center-for-future-mobility/overview/autonomous-driving)\n2. Thomson, C. (2017). Elon Musk has a stunning prediction for what cars will be like 10 years from now. Retrieved from [Link](https://www.businessinsider.com/elon-musk-predicts-most-cars-will-be-driverless-in-10-years-2017-2?r=DE&IR=T)\n3. BMW Group. (2019). Contract signed: BMW Group and Daimler AG launch long-term development cooperation for automated driving. Retrieved from [Link](https://www.press.bmwgroup.com/global/article/detail/T0298266EN/)contract-signed:-bmw-group-and-daimler-ag-launch-long-term-development-cooperation-for-automated-driving\n4. [Link](https://www.qualcomm.com/media/documents/files/accelerating-c-v2x-commercialization.pdf)\n5. European Telecommunications Standards Institute (ETSI), ETSI TR 103 562, Sophia Antipolis, 2019.\n6. A. Rauch, F. Klanner, and K. Dietmayer, Analysis of V2X communication parameters for the development of a fusion architecture for cooperative perception systems, in 2011 IEEE Intelligent Vehicles Symposium (IV), 2011, pp. 685690.\n7. R. Kohlhaas, T. Bittner, T. Schamm, and J. M. Zollner, Semantic state space for high-level maneuver planning in structured traffic scenes, in 17th International IEEE Conference on Intelligent Transportation Systems (ITSC), 2014, pp. 10601065.\n8. D. Petrich, D. Azarfar, F. Kuhnt, and J. M. Zollner, The Fingerprint of a Traffic Situation: A Semantic Relationship Tensor for Situation Description and Awareness, in 2018 21st International Conference on Intelligent Transportation Systems (ITSC), 2018, pp. 429435.\n9. [Link](https://docs.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system)\n10. Schwartz, J. (2018). Bing Maps Tile System - Bing Maps | Microsoft Docs. Retrieved November 26, 2019, from [Link](https://docs.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system)","slug":"talkycars-a-distributed-software-platform-for-cooperative-perception","published":1,"updated":"2020-10-30T20:05:40.285Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqmc000yo2e02idu4rz6","content":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars9.png\"></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>This is a short, summarizing write-up about the topics of my Masters thesis, published in February 2020 in cooperation with <a href=\"https://www.itiv.kit.edu/21_3940.php\">Prof. Dr. Sax</a>, <a href=\"https://www.itiv.kit.edu/21_6289.php\">M.Sc. Martin Sommer</a> and <a href=\"https://www.itiv.kit.edu/21_5341.php\">M.Sc. Marco Stang</a> at the <a href=\"https://itiv.kit.edu/\">KIT Institute for Information Processing Technologies</a>.</p>\n<h1 id=\"Citation-BibTeX\"><a href=\"#Citation-BibTeX\" class=\"headerlink\" title=\"Citation (BibTeX)\"></a>Citation (BibTeX)</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@mastersthesis&#123;Mutsch2020,</span><br><span class=\"line\">    author &#x3D; &#123;Mtsch, Ferdinand&#125;,</span><br><span class=\"line\">    school &#x3D; &#123;Karlsruhe Institute of Technology&#125;,</span><br><span class=\"line\">    title &#x3D; &#123;&#123;TalkyCars: A Distributed Software Platform for Cooperative Perception among Connected Autonomous Vehicles based on Cellular-V2X Communication&#125;&#125;,</span><br><span class=\"line\">    year &#x3D; &#123;2020&#125;,</span><br><span class=\"line\">    doi &#x3D; &#123;10.5445&#x2F;IR&#x2F;1000118118&#125;,</span><br><span class=\"line\">    URL &#x3D; &#123;http:&#x2F;&#x2F;dx.doi.org&#x2F;10.5445&#x2F;IR&#x2F;1000118118&#125;,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h1><p>Our work is in the field of autonomous driving, more specifically: cooperative perception. While the topic itself is very diverse, we attempted to approach cooperative perception mainly from a software architecture point of view.</p>\n<p>Let us first have a quick outlook. Nowadays, automated driving is still quite rare. However, although forecasts on its future development vary greatly, many researchers predict that level 4 automated vehicles may be around quite soon and level 5 cars may be production-ready already by 2030 <a href=\"#refs\">[1]</a>, <a href=\"#refs\">[2]</a>, <a href=\"#refs\">[3]</a>. However, in the beginning, self-driving cars will find themselves having to operate among highly mixed traffic. That is, traffic with still a high percentage of manully controlled cars. Therefore, a tremendously high level of safety is required.</p>\n<h1 id=\"Cooperative-Perception\"><a href=\"#Cooperative-Perception\" class=\"headerlink\" title=\"Cooperative Perception\"></a>Cooperative Perception</h1><p>One approach to increase reliability and safety of automated vehicles is the concept of cooperative perception. In essence, the idea is to have the cars not only rely on their own sensory, but communicate with surrounding vehicles to mutually exchange information about their environment. In that sense, cooperative perception is a use case of vehicle-to-vehicle  or more broadly  vehicle-to-everything communication. It enables cars to virtually extend their field of view and, for instance, be able to look around corners.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nloss_2.png\"><br><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nloss_3.png\"><br>(Source: <a href=\"#refs\">[4]</a>)</p>\n<p>Cooperative perception is a topic of research for years already. Most present approaches rely on so called vehicular ad-hoc networks among participants using dedicated short-range communication. That is, cars within range of each other form pair-wise temporary connections using a technology similar to WiFi. Not uncommonly, they exchange low-level sensor data, for instance LiDAR point cloud, camera images, etc.</p>\n<p>There is research which shows that these approaches face some limitations, mainly in terms of latency and data throughput. Consequently, our work assumes a different technological basis. We require that information is exchanged not in form of ad-hoc mesh networks, but through on or more central nodes using cellular communication. Moreover, we do not want to share raw senor data, which can become quite large, but higher-level information that corresponds to a common, shared environment model.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars2.png\"><br>(Source: <a href=\"#refs\">[4]</a>)</p>\n<h1 id=\"Approach\"><a href=\"#Approach\" class=\"headerlink\" title=\"Approach\"></a>Approach</h1><p>Essentially, there are two major areas to cover in order to develop an end-to-end cooperative perception system prototype. First, we need to define a common language or <strong>model</strong> that all traffic participants use to describe themselves and their environment. Second, a software- and communications <strong>architecture</strong> needs to be designed, as we need a system that enables efficient, scalable and low-latency communications among network participants.</p>\n<h2 id=\"Environment-Modeling\"><a href=\"#Environment-Modeling\" class=\"headerlink\" title=\"Environment Modeling\"></a>Environment Modeling</h2><p>Let us focus on the modeling aspect first. There already exist elaborate approaches for modeling and state representation for cooperative perception. Most famously, there are cooperative awareness- and cooperative perception messages <a href=\"#refs\">[5]</a>, <a href=\"#refs\">[6]</a>. In addition, Kohlhaas et al. <a href=\"#refs\">[7]</a> suggested to combine low-level, primitive attributes  like position and velocity  with higher-level, semantic knowledge. More specifically, they propose a modeling approach that also incorporates relationships among different entities and traffic participants within a traffic scene. In 2018, another research group first proposed the use of so called probabilistic entity relationship models for traffic scene representation <a href=\"#refs\">[8]</a>. </p>\n<p>For our environment model, we combine these aapproches with the concept of geo tiling using so called QuadKeys (<a href=\"#refs\">[10]</a>). In essence, geo tiling means to recursively divide the world map into four squares up to almost arbitrarily high precision. Each square is then uniquely identifiable through a key. Using these QuadKeys, we have a simple, commonly-understood and unique way of referencing geographical locations. Especially, it prevents from having to translate between different local- and global coordinate systems.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/quadkeys.jpg\"><br>(Source: <a href=\"#refs\">[9]</a>)</p>\n<p>Moreover, we can utilize geo tiles as a basis for our environment model. We propose a model that represents an occupancy grid at its core, whereas each cell corresponds to a tile of a certain, fixed precision level. For instance, with a QuadKey precision level of 25, we would require our intelligent vehicles to represent and share their perceived environment state as an occupancy grid with cells of approx. 1.1 by 1.1 meters in size. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars3.png\"></p>\n<p>Naturally, a cells occupancy state alone, that is occupied or not occupied, is not sufficient to comprehensively model a traffic scene. Therefore, we allow each cell to contain additional state information following a comprehensive meta model. For instance, the type of traffic participant, that currently occupies a cell, alongside all of its respective static and dynamic properties might be included as well.</p>\n<p>Moreover, there is an other interesting aspect to our proposed modeling approach. Because all facts of a vehicles environment are derived from imperfect sensory, theyre inevitably accompanied by uncertainty. Moreover, the fact that the description of a traffic scene is only valid for a very short temporal horizon even adds to this uncertainty. This leads to the requirement to use a modeling approach capable of coping with such uncertainty. As a consequence, we decided to pick up on the concept of probabilistic entity-relationship model and represent every fact as a quadruple of subject, predicate, object and confidence. The confidence component corresponds to the likelihood of this fact being true.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Graph consisting of 4-tuples:</span><br><span class=\"line\">------</span><br><span class=\"line\">(subject, predicate, object, confidence)</span><br><span class=\"line\"></span><br><span class=\"line\">Example:</span><br><span class=\"line\">------</span><br><span class=\"line\">(obstacle_5, isOfType, pedestrian, 0.921)</span><br><span class=\"line\">(obstacle_5, hasPosition, 310112030021333, 0.448)</span><br><span class=\"line\">(obstacle_5, isStoppedAt, traffic_light_189, 0.995)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"System-Architecture\"><a href=\"#System-Architecture\" class=\"headerlink\" title=\"System Architecture\"></a>System Architecture</h2><p>Let us now focus on the second major aspect: the architecture of a software system that enables for cooperative perception.</p>\n<p>Naturally, one essential requirement is good performance. That is, information from different vehicles shall be transmitted and aggregated as fast as possible. As mentioned earlier, many approaches relying on decentralized DSRC communication are potentially limited in latency and throughput. This is why we want to take the approach to rely on cellular communication, preferably using 5G networks. Also, to reduce communication complexity, our approach relies on centralized compute nodes that receive, transform, fuse and re-distribute information from many network participants. Of course, this introduces new challenges. First and foremost there is the requirement of high scalability, as these compute nodes have to handle large amount of data.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars4.png\"></p>\n<p>To address this challenge, we once again utilize geo tiling with QuadKeys. In our approach, the environment is split into QuadTiles of a certain size, for instance 5 by 5 km, and a data fusion node is not deployed globally, but once per every tile. Vehicles within such a tile exchange their data through the tiles responsible node. This way, the node only has to cope with a certain, limited amount of senders and receivers.</p>\n<p>In summary, the proposed system consists of three core components. On the client-side, that is, the vehicle side, an application is responsible for generating, sending and receiving instances of the previously mentioned environment model. Second, a message broker is deployed once per tile to consolidate incoming data messages. Third, the previously mentioned fusion node is responsible for aggregating these information packets to a single, globally valid model instance and sending it back to the cars.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars5.png\"></p>\n<h1 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h1><p>For evaluation purposes, we implemented all parts of our system and integrated it with the CARLA simulation environment. We conducted two different experiments to evaluate the proposed system with respect to performance and quality.</p>\n<h2 id=\"Software-Performance\"><a href=\"#Software-Performance\" class=\"headerlink\" title=\"Software Performance\"></a>Software Performance</h2><p>In the first part of the evaluation, we attempted to answer the question of how many concurrent vehicles the central data broker or fusion node is able to handle at a time. In our experiment setup, we deployed one instance of the fusion node as well as a distributed data generator that simulates an increasing amount of vehicles sending their environment models. We required that observations must be fused at a fixed rate, for instance, 10 times per second. Then we measured the actual rate at which the fusion node is able to process incoming observations as the number of sender vehicles increased. Eventually, we found that the possible fusion rate heavily depends on the size of the underlying occupancy grid and that our very little optimized implementation is able to maintain a fusion rate of 10 Hz with up to 220 concurrent vehicles at a grid size of approx. 50 meters and 100 concurrent vehicles at a grid size of approx. 100 meters. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:400?image=https://muetsch.io/images/talkycars6.png\"></p>\n<h2 id=\"End-to-End-Perception-Quality\"><a href=\"#End-to-End-Perception-Quality\" class=\"headerlink\" title=\"End-to-End Perception Quality\"></a>End-to-End Perception Quality</h2><p>The second part of the evaluation aimed to assess whether a vehicles overall perception quality can be improved through the use of our system in general.</p>\n<p>Therefore, we utilized the CARLA simulation environment and repeatedly generated different urban traffic scenes with a fixed amount of connected, intelligent vehicles, each of which runs our proposed software system. Every vehicle was tasked to traverse the environment from a random start point to a random destination. While driving, every vehicle recorded its own local perception, derived from its sensors, including all perceived obstacles. Later, these perceived environment states are compared to the true state of the environment provided by the simulator. Accordingly, an error measure can be derived. To asses whether or not our proposed system is beneficial to a self-driving cars perception quality, every instance of the experiment is run twice: once with and once without cooperative perception. Eventually the respective error measures can be compared.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars7.png\"></p>\n<p>Doing so, we found that the average perception quality, that is, the accuracy of detecting obstacles in the simulation, could be improved significantly in our test scenarios.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars8.png\"><br>(Image: Recall (hit rate) for occupied cells with / without CP)</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>With our work, we contributed a proof-of-concept for a novel approach to cooperative perception, utilizing centralized, cellular communication and involving a comprehensive meta model for traffic scene representation. Both our reference implementation as well as the concept itself still demands for optimizations in order to be advanced to a real-world applicable system. However, it can serve as a basis for future research. </p>\n<p>The entire project is <a href=\"https://github.com/muety/talkycars-thesis\">open-source</a>.</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><p><a name=\"refs\"></a></p>\n<ol>\n<li>McKinsey Center for Future Mobility. (2019). Autonomous Driving | MCFM | McKinsey. Retrieved November 25, 2019, from <a href=\"https://www.mckinsey.com/features/mckinsey-center-for-future-mobility/overview/autonomous-driving\">Link</a></li>\n<li>Thomson, C. (2017). Elon Musk has a stunning prediction for what cars will be like 10 years from now. Retrieved from <a href=\"https://www.businessinsider.com/elon-musk-predicts-most-cars-will-be-driverless-in-10-years-2017-2?r=DE&IR=T\">Link</a></li>\n<li>BMW Group. (2019). Contract signed: BMW Group and Daimler AG launch long-term development cooperation for automated driving. Retrieved from <a href=\"https://www.press.bmwgroup.com/global/article/detail/T0298266EN/\">Link</a>contract-signed:-bmw-group-and-daimler-ag-launch-long-term-development-cooperation-for-automated-driving</li>\n<li><a href=\"https://www.qualcomm.com/media/documents/files/accelerating-c-v2x-commercialization.pdf\">Link</a></li>\n<li>European Telecommunications Standards Institute (ETSI), ETSI TR 103 562, Sophia Antipolis, 2019.</li>\n<li>A. Rauch, F. Klanner, and K. Dietmayer, Analysis of V2X communication parameters for the development of a fusion architecture for cooperative perception systems, in 2011 IEEE Intelligent Vehicles Symposium (IV), 2011, pp. 685690.</li>\n<li>R. Kohlhaas, T. Bittner, T. Schamm, and J. M. Zollner, Semantic state space for high-level maneuver planning in structured traffic scenes, in 17th International IEEE Conference on Intelligent Transportation Systems (ITSC), 2014, pp. 10601065.</li>\n<li>D. Petrich, D. Azarfar, F. Kuhnt, and J. M. Zollner, The Fingerprint of a Traffic Situation: A Semantic Relationship Tensor for Situation Description and Awareness, in 2018 21st International Conference on Intelligent Transportation Systems (ITSC), 2018, pp. 429435.</li>\n<li><a href=\"https://docs.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system\">Link</a></li>\n<li>Schwartz, J. (2018). Bing Maps Tile System - Bing Maps | Microsoft Docs. Retrieved November 26, 2019, from <a href=\"https://docs.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system\">Link</a></li>\n</ol>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars9.png\"></p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>This is a short, summarizing write-up about the topics of my Masters thesis, published in February 2020 in cooperation with <a href=\"https://www.itiv.kit.edu/21_3940.php\">Prof. Dr. Sax</a>, <a href=\"https://www.itiv.kit.edu/21_6289.php\">M.Sc. Martin Sommer</a> and <a href=\"https://www.itiv.kit.edu/21_5341.php\">M.Sc. Marco Stang</a> at the <a href=\"https://itiv.kit.edu/\">KIT Institute for Information Processing Technologies</a>.</p>\n<h1 id=\"Citation-BibTeX\"><a href=\"#Citation-BibTeX\" class=\"headerlink\" title=\"Citation (BibTeX)\"></a>Citation (BibTeX)</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@mastersthesis&#123;Mutsch2020,</span><br><span class=\"line\">    author &#x3D; &#123;Mtsch, Ferdinand&#125;,</span><br><span class=\"line\">    school &#x3D; &#123;Karlsruhe Institute of Technology&#125;,</span><br><span class=\"line\">    title &#x3D; &#123;&#123;TalkyCars: A Distributed Software Platform for Cooperative Perception among Connected Autonomous Vehicles based on Cellular-V2X Communication&#125;&#125;,</span><br><span class=\"line\">    year &#x3D; &#123;2020&#125;,</span><br><span class=\"line\">    doi &#x3D; &#123;10.5445&#x2F;IR&#x2F;1000118118&#125;,</span><br><span class=\"line\">    URL &#x3D; &#123;http:&#x2F;&#x2F;dx.doi.org&#x2F;10.5445&#x2F;IR&#x2F;1000118118&#125;,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h1><p>Our work is in the field of autonomous driving, more specifically: cooperative perception. While the topic itself is very diverse, we attempted to approach cooperative perception mainly from a software architecture point of view.</p>\n<p>Let us first have a quick outlook. Nowadays, automated driving is still quite rare. However, although forecasts on its future development vary greatly, many researchers predict that level 4 automated vehicles may be around quite soon and level 5 cars may be production-ready already by 2030 <a href=\"#refs\">[1]</a>, <a href=\"#refs\">[2]</a>, <a href=\"#refs\">[3]</a>. However, in the beginning, self-driving cars will find themselves having to operate among highly mixed traffic. That is, traffic with still a high percentage of manully controlled cars. Therefore, a tremendously high level of safety is required.</p>\n<h1 id=\"Cooperative-Perception\"><a href=\"#Cooperative-Perception\" class=\"headerlink\" title=\"Cooperative Perception\"></a>Cooperative Perception</h1><p>One approach to increase reliability and safety of automated vehicles is the concept of cooperative perception. In essence, the idea is to have the cars not only rely on their own sensory, but communicate with surrounding vehicles to mutually exchange information about their environment. In that sense, cooperative perception is a use case of vehicle-to-vehicle  or more broadly  vehicle-to-everything communication. It enables cars to virtually extend their field of view and, for instance, be able to look around corners.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nloss_2.png\"><br><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nloss_3.png\"><br>(Source: <a href=\"#refs\">[4]</a>)</p>\n<p>Cooperative perception is a topic of research for years already. Most present approaches rely on so called vehicular ad-hoc networks among participants using dedicated short-range communication. That is, cars within range of each other form pair-wise temporary connections using a technology similar to WiFi. Not uncommonly, they exchange low-level sensor data, for instance LiDAR point cloud, camera images, etc.</p>\n<p>There is research which shows that these approaches face some limitations, mainly in terms of latency and data throughput. Consequently, our work assumes a different technological basis. We require that information is exchanged not in form of ad-hoc mesh networks, but through on or more central nodes using cellular communication. Moreover, we do not want to share raw senor data, which can become quite large, but higher-level information that corresponds to a common, shared environment model.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars2.png\"><br>(Source: <a href=\"#refs\">[4]</a>)</p>\n<h1 id=\"Approach\"><a href=\"#Approach\" class=\"headerlink\" title=\"Approach\"></a>Approach</h1><p>Essentially, there are two major areas to cover in order to develop an end-to-end cooperative perception system prototype. First, we need to define a common language or <strong>model</strong> that all traffic participants use to describe themselves and their environment. Second, a software- and communications <strong>architecture</strong> needs to be designed, as we need a system that enables efficient, scalable and low-latency communications among network participants.</p>\n<h2 id=\"Environment-Modeling\"><a href=\"#Environment-Modeling\" class=\"headerlink\" title=\"Environment Modeling\"></a>Environment Modeling</h2><p>Let us focus on the modeling aspect first. There already exist elaborate approaches for modeling and state representation for cooperative perception. Most famously, there are cooperative awareness- and cooperative perception messages <a href=\"#refs\">[5]</a>, <a href=\"#refs\">[6]</a>. In addition, Kohlhaas et al. <a href=\"#refs\">[7]</a> suggested to combine low-level, primitive attributes  like position and velocity  with higher-level, semantic knowledge. More specifically, they propose a modeling approach that also incorporates relationships among different entities and traffic participants within a traffic scene. In 2018, another research group first proposed the use of so called probabilistic entity relationship models for traffic scene representation <a href=\"#refs\">[8]</a>. </p>\n<p>For our environment model, we combine these aapproches with the concept of geo tiling using so called QuadKeys (<a href=\"#refs\">[10]</a>). In essence, geo tiling means to recursively divide the world map into four squares up to almost arbitrarily high precision. Each square is then uniquely identifiable through a key. Using these QuadKeys, we have a simple, commonly-understood and unique way of referencing geographical locations. Especially, it prevents from having to translate between different local- and global coordinate systems.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/quadkeys.jpg\"><br>(Source: <a href=\"#refs\">[9]</a>)</p>\n<p>Moreover, we can utilize geo tiles as a basis for our environment model. We propose a model that represents an occupancy grid at its core, whereas each cell corresponds to a tile of a certain, fixed precision level. For instance, with a QuadKey precision level of 25, we would require our intelligent vehicles to represent and share their perceived environment state as an occupancy grid with cells of approx. 1.1 by 1.1 meters in size. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars3.png\"></p>\n<p>Naturally, a cells occupancy state alone, that is occupied or not occupied, is not sufficient to comprehensively model a traffic scene. Therefore, we allow each cell to contain additional state information following a comprehensive meta model. For instance, the type of traffic participant, that currently occupies a cell, alongside all of its respective static and dynamic properties might be included as well.</p>\n<p>Moreover, there is an other interesting aspect to our proposed modeling approach. Because all facts of a vehicles environment are derived from imperfect sensory, theyre inevitably accompanied by uncertainty. Moreover, the fact that the description of a traffic scene is only valid for a very short temporal horizon even adds to this uncertainty. This leads to the requirement to use a modeling approach capable of coping with such uncertainty. As a consequence, we decided to pick up on the concept of probabilistic entity-relationship model and represent every fact as a quadruple of subject, predicate, object and confidence. The confidence component corresponds to the likelihood of this fact being true.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Graph consisting of 4-tuples:</span><br><span class=\"line\">------</span><br><span class=\"line\">(subject, predicate, object, confidence)</span><br><span class=\"line\"></span><br><span class=\"line\">Example:</span><br><span class=\"line\">------</span><br><span class=\"line\">(obstacle_5, isOfType, pedestrian, 0.921)</span><br><span class=\"line\">(obstacle_5, hasPosition, 310112030021333, 0.448)</span><br><span class=\"line\">(obstacle_5, isStoppedAt, traffic_light_189, 0.995)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"System-Architecture\"><a href=\"#System-Architecture\" class=\"headerlink\" title=\"System Architecture\"></a>System Architecture</h2><p>Let us now focus on the second major aspect: the architecture of a software system that enables for cooperative perception.</p>\n<p>Naturally, one essential requirement is good performance. That is, information from different vehicles shall be transmitted and aggregated as fast as possible. As mentioned earlier, many approaches relying on decentralized DSRC communication are potentially limited in latency and throughput. This is why we want to take the approach to rely on cellular communication, preferably using 5G networks. Also, to reduce communication complexity, our approach relies on centralized compute nodes that receive, transform, fuse and re-distribute information from many network participants. Of course, this introduces new challenges. First and foremost there is the requirement of high scalability, as these compute nodes have to handle large amount of data.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars4.png\"></p>\n<p>To address this challenge, we once again utilize geo tiling with QuadKeys. In our approach, the environment is split into QuadTiles of a certain size, for instance 5 by 5 km, and a data fusion node is not deployed globally, but once per every tile. Vehicles within such a tile exchange their data through the tiles responsible node. This way, the node only has to cope with a certain, limited amount of senders and receivers.</p>\n<p>In summary, the proposed system consists of three core components. On the client-side, that is, the vehicle side, an application is responsible for generating, sending and receiving instances of the previously mentioned environment model. Second, a message broker is deployed once per tile to consolidate incoming data messages. Third, the previously mentioned fusion node is responsible for aggregating these information packets to a single, globally valid model instance and sending it back to the cars.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars5.png\"></p>\n<h1 id=\"Evaluation\"><a href=\"#Evaluation\" class=\"headerlink\" title=\"Evaluation\"></a>Evaluation</h1><p>For evaluation purposes, we implemented all parts of our system and integrated it with the CARLA simulation environment. We conducted two different experiments to evaluate the proposed system with respect to performance and quality.</p>\n<h2 id=\"Software-Performance\"><a href=\"#Software-Performance\" class=\"headerlink\" title=\"Software Performance\"></a>Software Performance</h2><p>In the first part of the evaluation, we attempted to answer the question of how many concurrent vehicles the central data broker or fusion node is able to handle at a time. In our experiment setup, we deployed one instance of the fusion node as well as a distributed data generator that simulates an increasing amount of vehicles sending their environment models. We required that observations must be fused at a fixed rate, for instance, 10 times per second. Then we measured the actual rate at which the fusion node is able to process incoming observations as the number of sender vehicles increased. Eventually, we found that the possible fusion rate heavily depends on the size of the underlying occupancy grid and that our very little optimized implementation is able to maintain a fusion rate of 10 Hz with up to 220 concurrent vehicles at a grid size of approx. 50 meters and 100 concurrent vehicles at a grid size of approx. 100 meters. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:400?image=https://muetsch.io/images/talkycars6.png\"></p>\n<h2 id=\"End-to-End-Perception-Quality\"><a href=\"#End-to-End-Perception-Quality\" class=\"headerlink\" title=\"End-to-End Perception Quality\"></a>End-to-End Perception Quality</h2><p>The second part of the evaluation aimed to assess whether a vehicles overall perception quality can be improved through the use of our system in general.</p>\n<p>Therefore, we utilized the CARLA simulation environment and repeatedly generated different urban traffic scenes with a fixed amount of connected, intelligent vehicles, each of which runs our proposed software system. Every vehicle was tasked to traverse the environment from a random start point to a random destination. While driving, every vehicle recorded its own local perception, derived from its sensors, including all perceived obstacles. Later, these perceived environment states are compared to the true state of the environment provided by the simulator. Accordingly, an error measure can be derived. To asses whether or not our proposed system is beneficial to a self-driving cars perception quality, every instance of the experiment is run twice: once with and once without cooperative perception. Eventually the respective error measures can be compared.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars7.png\"></p>\n<p>Doing so, we found that the average perception quality, that is, the accuracy of detecting obstacles in the simulation, could be improved significantly in our test scenarios.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/talkycars8.png\"><br>(Image: Recall (hit rate) for occupied cells with / without CP)</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>With our work, we contributed a proof-of-concept for a novel approach to cooperative perception, utilizing centralized, cellular communication and involving a comprehensive meta model for traffic scene representation. Both our reference implementation as well as the concept itself still demands for optimizations in order to be advanced to a real-world applicable system. However, it can serve as a basis for future research. </p>\n<p>The entire project is <a href=\"https://github.com/muety/talkycars-thesis\">open-source</a>.</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><p><a name=\"refs\"></a></p>\n<ol>\n<li>McKinsey Center for Future Mobility. (2019). Autonomous Driving | MCFM | McKinsey. Retrieved November 25, 2019, from <a href=\"https://www.mckinsey.com/features/mckinsey-center-for-future-mobility/overview/autonomous-driving\">Link</a></li>\n<li>Thomson, C. (2017). Elon Musk has a stunning prediction for what cars will be like 10 years from now. Retrieved from <a href=\"https://www.businessinsider.com/elon-musk-predicts-most-cars-will-be-driverless-in-10-years-2017-2?r=DE&IR=T\">Link</a></li>\n<li>BMW Group. (2019). Contract signed: BMW Group and Daimler AG launch long-term development cooperation for automated driving. Retrieved from <a href=\"https://www.press.bmwgroup.com/global/article/detail/T0298266EN/\">Link</a>contract-signed:-bmw-group-and-daimler-ag-launch-long-term-development-cooperation-for-automated-driving</li>\n<li><a href=\"https://www.qualcomm.com/media/documents/files/accelerating-c-v2x-commercialization.pdf\">Link</a></li>\n<li>European Telecommunications Standards Institute (ETSI), ETSI TR 103 562, Sophia Antipolis, 2019.</li>\n<li>A. Rauch, F. Klanner, and K. Dietmayer, Analysis of V2X communication parameters for the development of a fusion architecture for cooperative perception systems, in 2011 IEEE Intelligent Vehicles Symposium (IV), 2011, pp. 685690.</li>\n<li>R. Kohlhaas, T. Bittner, T. Schamm, and J. M. Zollner, Semantic state space for high-level maneuver planning in structured traffic scenes, in 17th International IEEE Conference on Intelligent Transportation Systems (ITSC), 2014, pp. 10601065.</li>\n<li>D. Petrich, D. Azarfar, F. Kuhnt, and J. M. Zollner, The Fingerprint of a Traffic Situation: A Semantic Relationship Tensor for Situation Description and Awareness, in 2018 21st International Conference on Intelligent Transportation Systems (ITSC), 2018, pp. 429435.</li>\n<li><a href=\"https://docs.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system\">Link</a></li>\n<li>Schwartz, J. (2018). Bing Maps Tile System - Bing Maps | Microsoft Docs. Retrieved November 26, 2019, from <a href=\"https://docs.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system\">Link</a></li>\n</ol>\n"},{"title":"Telegram bot example code in Node.js","date":"2015-12-01T21:42:05.000Z","_content":"\nAs a response to my [latest article](how-to-make-telegram-bots.html) on how to make Telegram bots, I got many requests from guys who had problems with actually getting such a bot set up with my tutorial and they asked for a complete, working code example. So I made one. It is limited to the very basics, but it works and may serve as inspiration for you. What most of you would probably like to have further in your bots, but what is not covered in my example, is a database connection. Because in most cases your bot will probably need to persist data. To realize this you should have a little rather advanced skills in Node as well as basic knowledge of the database system of your choice, which is MongoDB with Mongoose Node module for me.\n\nAlright, heres my sample bot: [http://github.com/muety/telegram-bot-node-sample](http://github.com/muety/telegram-bot-node-sample/)\n\nAlso check out my _@FavoriteBot_.  \nHave fun and good luck...","source":"_posts/telegram-bot-example-code-in-nodejs.md","raw":"---\ntitle: Telegram bot example code in Node.js\ndate: 2015-12-01 22:42:05\ntags:\n---\n\nAs a response to my [latest article](how-to-make-telegram-bots.html) on how to make Telegram bots, I got many requests from guys who had problems with actually getting such a bot set up with my tutorial and they asked for a complete, working code example. So I made one. It is limited to the very basics, but it works and may serve as inspiration for you. What most of you would probably like to have further in your bots, but what is not covered in my example, is a database connection. Because in most cases your bot will probably need to persist data. To realize this you should have a little rather advanced skills in Node as well as basic knowledge of the database system of your choice, which is MongoDB with Mongoose Node module for me.\n\nAlright, heres my sample bot: [http://github.com/muety/telegram-bot-node-sample](http://github.com/muety/telegram-bot-node-sample/)\n\nAlso check out my _@FavoriteBot_.  \nHave fun and good luck...","slug":"telegram-bot-example-code-in-nodejs","published":1,"updated":"2020-10-30T20:05:40.289Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqmc000zo2e066yk2u4d","content":"<p>As a response to my <a href=\"how-to-make-telegram-bots.html\">latest article</a> on how to make Telegram bots, I got many requests from guys who had problems with actually getting such a bot set up with my tutorial and they asked for a complete, working code example. So I made one. It is limited to the very basics, but it works and may serve as inspiration for you. What most of you would probably like to have further in your bots, but what is not covered in my example, is a database connection. Because in most cases your bot will probably need to persist data. To realize this you should have a little rather advanced skills in Node as well as basic knowledge of the database system of your choice, which is MongoDB with Mongoose Node module for me.</p>\n<p>Alright, heres my sample bot: <a href=\"http://github.com/muety/telegram-bot-node-sample/\">http://github.com/muety/telegram-bot-node-sample</a></p>\n<p>Also check out my <em>@FavoriteBot</em>.<br>Have fun and good luck</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>As a response to my <a href=\"how-to-make-telegram-bots.html\">latest article</a> on how to make Telegram bots, I got many requests from guys who had problems with actually getting such a bot set up with my tutorial and they asked for a complete, working code example. So I made one. It is limited to the very basics, but it works and may serve as inspiration for you. What most of you would probably like to have further in your bots, but what is not covered in my example, is a database connection. Because in most cases your bot will probably need to persist data. To realize this you should have a little rather advanced skills in Node as well as basic knowledge of the database system of your choice, which is MongoDB with Mongoose Node module for me.</p>\n<p>Alright, heres my sample bot: <a href=\"http://github.com/muety/telegram-bot-node-sample/\">http://github.com/muety/telegram-bot-node-sample</a></p>\n<p>Also check out my <em>@FavoriteBot</em>.<br>Have fun and good luck</p>\n"},{"title":"Telegram: ExpenseBot & DoodlerBot","date":"2016-05-08T20:59:33.000Z","_content":"\n\nIn 2015, the [Telegram](https://telegram.org) messenger announced their [Bots](https://core.telegram.org/bots). Basically they are pieces of software that act like a normal chat user in many ways. They could have any functionality, from being helpful at daily tasks to even simple games or trivias  all within an ordinary Telegram chat. You send them message, they give answers  some more and some less intelligent. Recently, also other companies  like [Facebook](http://techcrunch.com/2016/04/07/facebook-chatbots/) or [Microsoft](https://dev.botframework.com/)  announced such bots for their messaging apps. Sometimes bots are even considered kind of the next step after native (web-) applications in the future.\n\nFrom a developers perspective, making a bot is fun, because there are almost no restrictions on how to develop your bot. All communication with Telegram works by requesting a single REST API provided by them. Choices like which programming language and -framework to use and how to structure the code are completely up to the developer. A Telegram bot can theoretically be built in any programming language. The only requirements are to be able to make HTTP request from the application and to have a server to host the bot on.\n\nIve recently created two bots for Telegram that should each help with a daily task.\n\n### ExpenseBot  Keep track of your finances\n\n![1461614801_Money-Increase](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/expensebot_icon.png)\n\nThis bots purpose is to help people manage their daily expenses and keep track of their financial situation. Users can add expenses from wherever they are using a few simple commands from within the chat and have an eye on how much they have spent in a month or a day. This obviates the need for confusing Excel spreadsheets or paper notes. You can reach the bot by sending a Message to *[@ExpenseBot](https://telegram.me/ExpenseBot)* in Telegram.\n\n### DoodlerBot  Coordinate group appointments\n\n![1462726473_calendar](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/doodlerbot_icon.png)\n\nMy second bot helps users coordinate a group of people and find the right date for a common appointment, just like you might know from [doodle.com](http://doodle.com) (even though it doesnt have anything to do with that commercial service, except for fulfilling the same need). Open a new doodle and let your mates in the group chat vote for their preferred date to finally make the best decision for everyone. You can reach the bot by sending a Message to *[@DoodlerBot](https://telegram.me/DoodlerBot)* in Telegram.\n\nBoth projects are completely independent, non-commercial and privately operated. If you have any questions our found a bug (both bots are still in beta phase and therefore might show some unexpected behavior), please contact me at @n1try or via e-mail. In both cases, you should first read a basic introduction on how to use the respective bot, by sending a message with */help* to it.\n\nIn case you like my bots, Id be really happy if you rated them at [https://storebot.me/bot/expensebot](https://storebot.me/bot/expensebot) and [https://storebot.me/bot/doodlerbot](https://storebot.me/bot/doodlerbot). Have fun!","source":"_posts/telegram-expensebot-doodlerbot.md","raw":"---\ntitle: 'Telegram: ExpenseBot & DoodlerBot'\ndate: 2016-05-08 22:59:33\ntags:\n---\n\n\nIn 2015, the [Telegram](https://telegram.org) messenger announced their [Bots](https://core.telegram.org/bots). Basically they are pieces of software that act like a normal chat user in many ways. They could have any functionality, from being helpful at daily tasks to even simple games or trivias  all within an ordinary Telegram chat. You send them message, they give answers  some more and some less intelligent. Recently, also other companies  like [Facebook](http://techcrunch.com/2016/04/07/facebook-chatbots/) or [Microsoft](https://dev.botframework.com/)  announced such bots for their messaging apps. Sometimes bots are even considered kind of the next step after native (web-) applications in the future.\n\nFrom a developers perspective, making a bot is fun, because there are almost no restrictions on how to develop your bot. All communication with Telegram works by requesting a single REST API provided by them. Choices like which programming language and -framework to use and how to structure the code are completely up to the developer. A Telegram bot can theoretically be built in any programming language. The only requirements are to be able to make HTTP request from the application and to have a server to host the bot on.\n\nIve recently created two bots for Telegram that should each help with a daily task.\n\n### ExpenseBot  Keep track of your finances\n\n![1461614801_Money-Increase](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/expensebot_icon.png)\n\nThis bots purpose is to help people manage their daily expenses and keep track of their financial situation. Users can add expenses from wherever they are using a few simple commands from within the chat and have an eye on how much they have spent in a month or a day. This obviates the need for confusing Excel spreadsheets or paper notes. You can reach the bot by sending a Message to *[@ExpenseBot](https://telegram.me/ExpenseBot)* in Telegram.\n\n### DoodlerBot  Coordinate group appointments\n\n![1462726473_calendar](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/doodlerbot_icon.png)\n\nMy second bot helps users coordinate a group of people and find the right date for a common appointment, just like you might know from [doodle.com](http://doodle.com) (even though it doesnt have anything to do with that commercial service, except for fulfilling the same need). Open a new doodle and let your mates in the group chat vote for their preferred date to finally make the best decision for everyone. You can reach the bot by sending a Message to *[@DoodlerBot](https://telegram.me/DoodlerBot)* in Telegram.\n\nBoth projects are completely independent, non-commercial and privately operated. If you have any questions our found a bug (both bots are still in beta phase and therefore might show some unexpected behavior), please contact me at @n1try or via e-mail. In both cases, you should first read a basic introduction on how to use the respective bot, by sending a message with */help* to it.\n\nIn case you like my bots, Id be really happy if you rated them at [https://storebot.me/bot/expensebot](https://storebot.me/bot/expensebot) and [https://storebot.me/bot/doodlerbot](https://storebot.me/bot/doodlerbot). Have fun!","slug":"telegram-expensebot-doodlerbot","published":1,"updated":"2020-10-30T20:05:40.285Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqmd0010o2e0d4d44vrx","content":"<p>In 2015, the <a href=\"https://telegram.org/\">Telegram</a> messenger announced their <a href=\"https://core.telegram.org/bots\">Bots</a>. Basically they are pieces of software that act like a normal chat user in many ways. They could have any functionality, from being helpful at daily tasks to even simple games or trivias  all within an ordinary Telegram chat. You send them message, they give answers  some more and some less intelligent. Recently, also other companies  like <a href=\"http://techcrunch.com/2016/04/07/facebook-chatbots/\">Facebook</a> or <a href=\"https://dev.botframework.com/\">Microsoft</a>  announced such bots for their messaging apps. Sometimes bots are even considered kind of the next step after native (web-) applications in the future.</p>\n<p>From a developers perspective, making a bot is fun, because there are almost no restrictions on how to develop your bot. All communication with Telegram works by requesting a single REST API provided by them. Choices like which programming language and -framework to use and how to structure the code are completely up to the developer. A Telegram bot can theoretically be built in any programming language. The only requirements are to be able to make HTTP request from the application and to have a server to host the bot on.</p>\n<p>Ive recently created two bots for Telegram that should each help with a daily task.</p>\n<h3 id=\"ExpenseBot--Keep-track-of-your-finances\"><a href=\"#ExpenseBot--Keep-track-of-your-finances\" class=\"headerlink\" title=\"ExpenseBot  Keep track of your finances\"></a>ExpenseBot  Keep track of your finances</h3><p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/expensebot_icon.png\" alt=\"1461614801_Money-Increase\"></p>\n<p>This bots purpose is to help people manage their daily expenses and keep track of their financial situation. Users can add expenses from wherever they are using a few simple commands from within the chat and have an eye on how much they have spent in a month or a day. This obviates the need for confusing Excel spreadsheets or paper notes. You can reach the bot by sending a Message to <em><a href=\"https://telegram.me/ExpenseBot\">@ExpenseBot</a></em> in Telegram.</p>\n<h3 id=\"DoodlerBot--Coordinate-group-appointments\"><a href=\"#DoodlerBot--Coordinate-group-appointments\" class=\"headerlink\" title=\"DoodlerBot  Coordinate group appointments\"></a>DoodlerBot  Coordinate group appointments</h3><p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/doodlerbot_icon.png\" alt=\"1462726473_calendar\"></p>\n<p>My second bot helps users coordinate a group of people and find the right date for a common appointment, just like you might know from <a href=\"http://doodle.com/\">doodle.com</a> (even though it doesnt have anything to do with that commercial service, except for fulfilling the same need). Open a new doodle and let your mates in the group chat vote for their preferred date to finally make the best decision for everyone. You can reach the bot by sending a Message to <em><a href=\"https://telegram.me/DoodlerBot\">@DoodlerBot</a></em> in Telegram.</p>\n<p>Both projects are completely independent, non-commercial and privately operated. If you have any questions our found a bug (both bots are still in beta phase and therefore might show some unexpected behavior), please contact me at @n1try or via e-mail. In both cases, you should first read a basic introduction on how to use the respective bot, by sending a message with <em>/help</em> to it.</p>\n<p>In case you like my bots, Id be really happy if you rated them at <a href=\"https://storebot.me/bot/expensebot\">https://storebot.me/bot/expensebot</a> and <a href=\"https://storebot.me/bot/doodlerbot\">https://storebot.me/bot/doodlerbot</a>. Have fun!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>In 2015, the <a href=\"https://telegram.org/\">Telegram</a> messenger announced their <a href=\"https://core.telegram.org/bots\">Bots</a>. Basically they are pieces of software that act like a normal chat user in many ways. They could have any functionality, from being helpful at daily tasks to even simple games or trivias  all within an ordinary Telegram chat. You send them message, they give answers  some more and some less intelligent. Recently, also other companies  like <a href=\"http://techcrunch.com/2016/04/07/facebook-chatbots/\">Facebook</a> or <a href=\"https://dev.botframework.com/\">Microsoft</a>  announced such bots for their messaging apps. Sometimes bots are even considered kind of the next step after native (web-) applications in the future.</p>\n<p>From a developers perspective, making a bot is fun, because there are almost no restrictions on how to develop your bot. All communication with Telegram works by requesting a single REST API provided by them. Choices like which programming language and -framework to use and how to structure the code are completely up to the developer. A Telegram bot can theoretically be built in any programming language. The only requirements are to be able to make HTTP request from the application and to have a server to host the bot on.</p>\n<p>Ive recently created two bots for Telegram that should each help with a daily task.</p>\n<h3 id=\"ExpenseBot--Keep-track-of-your-finances\"><a href=\"#ExpenseBot--Keep-track-of-your-finances\" class=\"headerlink\" title=\"ExpenseBot  Keep track of your finances\"></a>ExpenseBot  Keep track of your finances</h3><p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/expensebot_icon.png\" alt=\"1461614801_Money-Increase\"></p>\n<p>This bots purpose is to help people manage their daily expenses and keep track of their financial situation. Users can add expenses from wherever they are using a few simple commands from within the chat and have an eye on how much they have spent in a month or a day. This obviates the need for confusing Excel spreadsheets or paper notes. You can reach the bot by sending a Message to <em><a href=\"https://telegram.me/ExpenseBot\">@ExpenseBot</a></em> in Telegram.</p>\n<h3 id=\"DoodlerBot--Coordinate-group-appointments\"><a href=\"#DoodlerBot--Coordinate-group-appointments\" class=\"headerlink\" title=\"DoodlerBot  Coordinate group appointments\"></a>DoodlerBot  Coordinate group appointments</h3><p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/doodlerbot_icon.png\" alt=\"1462726473_calendar\"></p>\n<p>My second bot helps users coordinate a group of people and find the right date for a common appointment, just like you might know from <a href=\"http://doodle.com/\">doodle.com</a> (even though it doesnt have anything to do with that commercial service, except for fulfilling the same need). Open a new doodle and let your mates in the group chat vote for their preferred date to finally make the best decision for everyone. You can reach the bot by sending a Message to <em><a href=\"https://telegram.me/DoodlerBot\">@DoodlerBot</a></em> in Telegram.</p>\n<p>Both projects are completely independent, non-commercial and privately operated. If you have any questions our found a bug (both bots are still in beta phase and therefore might show some unexpected behavior), please contact me at @n1try or via e-mail. In both cases, you should first read a basic introduction on how to use the respective bot, by sending a message with <em>/help</em> to it.</p>\n<p>In case you like my bots, Id be really happy if you rated them at <a href=\"https://storebot.me/bot/expensebot\">https://storebot.me/bot/expensebot</a> and <a href=\"https://storebot.me/bot/doodlerbot\">https://storebot.me/bot/doodlerbot</a>. Have fun!</p>\n"},{"title":"Middleman Bot - Push notifications as easy as POST","date":"2017-07-04T20:13:57.000Z","_content":"\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/middleman.png)\n\n## E-Mails are so 2010\nE-Mails are everywhere. Literally anybody who gets in contact with a computer in some way will use the electronic mail system, which dates back to 1971. And certainly the E-Mail still has its right to exist, even in 2017. Especially in a business context it simply makes sense for many use cases - however not for all of those it is nowadays used for. The smaller the piece of information you want to communicate is, the less efficient e-mails are. Due to tons of header rows and other heavy load, a mail has a large overhead. Just look at a small random sample of mail from your inbox, find out the core information you get from it and look at the file size. Often you will find an overhead of more than 70 %. Great examples are subscription mails from online forum posts. Their essential information is basically only one bit in size, namely that something new has happened. However, they usually contain lots of HTML code, CSS, attachment pictures and some boilerplate text. Another example is those chatty-like conversations between two or more people, where each of them responds with only a few words to his conversation partner's last question or the like. Although only few words are added, the whole previous mail is replicated again, enriched with some headers and then sent to the server, where it is stored and delivered to the recipients. I'm glad that for those cases apps like Slack are gradually establishing and I hope they will replace the e-mail some day. Anyway, I get off the topic. This shouldn't be a hate letter against the e-mail. Rather I want to present another bot for the __Telegram__ instant messenger.\n\n## The pain I had\nIt mainly aims at developers and sysadmins, but is not limited to these addressees. Once I had the problem that I wanted to code a little watcher that regularly visits my university's website to check whether an announcement about the exam results has been made and notify me, if that was the case. The only way for realizing that notification that came to my mind was to send an e-mail. However, I had to integrate an SMTP library into my script then, which authenticates against my mail server using my account credentials - comparatively high effort. Then I wanted to write a little script that runs on my server in regular intervals to gather some basic statistics from log files and databases and send them to me so I could passively consume them (instead of me having to actively go to some webpage or so). Again, same problem. \n\n## The simple solution I created\nFor these reasons I created the Middleman Bot. It takes a simple, small JSON fragment as an input via HTTP call and forwards it to my Telegram chat. There's no need for an additional library (nearly every programming language has a built-in API for HTTP calls or you could use cURL) or authentication process. Simply register at the bot once and fire lightweight HTTP calls afterwards. \n\n## Example\nTo make my webserver send me a notification when its load is above a certain threshold, the only thing it would need to do is a `POST https://apps.muetsch.io/middleman/api/messages` with this body: \n\n```json\n{\n\t\"recipient_token\": \"3edf633a-eab0-45ea-9721-16c07bb8f245\",\n\t\"text\": \"Watch out! Average load in the last 10 minutes is >= 10000 requests per second.\",\n\t\"origin\": \"Caddy webserver @ muetsch.io\"\n}\n```\n\nThe token is your unique identifier the bot uses internally to decide to which chat / recipient to route an incoming message to. (For the trolls among you: I've invalidated the token from the example, so nobody could spam me.)\n\nAnd there you go. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/middleman2.png)\n\n\n## Give it a try\nI've pushed the code as well as some introductions on how to run and use that bot to GitHub at  [muety/telegram-middleman-bot](https://github.com/muety/telegram-middleman-bot). You can either run your own instance of the bot or use mine, which is running at _https://apps.muetsch.io_. Let me know what you think!","source":"_posts/telegram-middleman-bot-push-notifications-as-easy-as-post.md","raw":"---\ntitle: Middleman Bot - Push notifications as easy as POST\ndate: 2017-07-04 22:13:57\ntags:\n---\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/middleman.png)\n\n## E-Mails are so 2010\nE-Mails are everywhere. Literally anybody who gets in contact with a computer in some way will use the electronic mail system, which dates back to 1971. And certainly the E-Mail still has its right to exist, even in 2017. Especially in a business context it simply makes sense for many use cases - however not for all of those it is nowadays used for. The smaller the piece of information you want to communicate is, the less efficient e-mails are. Due to tons of header rows and other heavy load, a mail has a large overhead. Just look at a small random sample of mail from your inbox, find out the core information you get from it and look at the file size. Often you will find an overhead of more than 70 %. Great examples are subscription mails from online forum posts. Their essential information is basically only one bit in size, namely that something new has happened. However, they usually contain lots of HTML code, CSS, attachment pictures and some boilerplate text. Another example is those chatty-like conversations between two or more people, where each of them responds with only a few words to his conversation partner's last question or the like. Although only few words are added, the whole previous mail is replicated again, enriched with some headers and then sent to the server, where it is stored and delivered to the recipients. I'm glad that for those cases apps like Slack are gradually establishing and I hope they will replace the e-mail some day. Anyway, I get off the topic. This shouldn't be a hate letter against the e-mail. Rather I want to present another bot for the __Telegram__ instant messenger.\n\n## The pain I had\nIt mainly aims at developers and sysadmins, but is not limited to these addressees. Once I had the problem that I wanted to code a little watcher that regularly visits my university's website to check whether an announcement about the exam results has been made and notify me, if that was the case. The only way for realizing that notification that came to my mind was to send an e-mail. However, I had to integrate an SMTP library into my script then, which authenticates against my mail server using my account credentials - comparatively high effort. Then I wanted to write a little script that runs on my server in regular intervals to gather some basic statistics from log files and databases and send them to me so I could passively consume them (instead of me having to actively go to some webpage or so). Again, same problem. \n\n## The simple solution I created\nFor these reasons I created the Middleman Bot. It takes a simple, small JSON fragment as an input via HTTP call and forwards it to my Telegram chat. There's no need for an additional library (nearly every programming language has a built-in API for HTTP calls or you could use cURL) or authentication process. Simply register at the bot once and fire lightweight HTTP calls afterwards. \n\n## Example\nTo make my webserver send me a notification when its load is above a certain threshold, the only thing it would need to do is a `POST https://apps.muetsch.io/middleman/api/messages` with this body: \n\n```json\n{\n\t\"recipient_token\": \"3edf633a-eab0-45ea-9721-16c07bb8f245\",\n\t\"text\": \"Watch out! Average load in the last 10 minutes is >= 10000 requests per second.\",\n\t\"origin\": \"Caddy webserver @ muetsch.io\"\n}\n```\n\nThe token is your unique identifier the bot uses internally to decide to which chat / recipient to route an incoming message to. (For the trolls among you: I've invalidated the token from the example, so nobody could spam me.)\n\nAnd there you go. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/middleman2.png)\n\n\n## Give it a try\nI've pushed the code as well as some introductions on how to run and use that bot to GitHub at  [muety/telegram-middleman-bot](https://github.com/muety/telegram-middleman-bot). You can either run your own instance of the bot or use mine, which is running at _https://apps.muetsch.io_. Let me know what you think!","slug":"telegram-middleman-bot-push-notifications-as-easy-as-post","published":1,"updated":"2020-10-30T20:05:40.285Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqme0011o2e0d42d62e5","content":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/middleman.png\"></p>\n<h2 id=\"E-Mails-are-so-2010\"><a href=\"#E-Mails-are-so-2010\" class=\"headerlink\" title=\"E-Mails are so 2010\"></a>E-Mails are so 2010</h2><p>E-Mails are everywhere. Literally anybody who gets in contact with a computer in some way will use the electronic mail system, which dates back to 1971. And certainly the E-Mail still has its right to exist, even in 2017. Especially in a business context it simply makes sense for many use cases - however not for all of those it is nowadays used for. The smaller the piece of information you want to communicate is, the less efficient e-mails are. Due to tons of header rows and other heavy load, a mail has a large overhead. Just look at a small random sample of mail from your inbox, find out the core information you get from it and look at the file size. Often you will find an overhead of more than 70 %. Great examples are subscription mails from online forum posts. Their essential information is basically only one bit in size, namely that something new has happened. However, they usually contain lots of HTML code, CSS, attachment pictures and some boilerplate text. Another example is those chatty-like conversations between two or more people, where each of them responds with only a few words to his conversation partners last question or the like. Although only few words are added, the whole previous mail is replicated again, enriched with some headers and then sent to the server, where it is stored and delivered to the recipients. Im glad that for those cases apps like Slack are gradually establishing and I hope they will replace the e-mail some day. Anyway, I get off the topic. This shouldnt be a hate letter against the e-mail. Rather I want to present another bot for the <strong>Telegram</strong> instant messenger.</p>\n<h2 id=\"The-pain-I-had\"><a href=\"#The-pain-I-had\" class=\"headerlink\" title=\"The pain I had\"></a>The pain I had</h2><p>It mainly aims at developers and sysadmins, but is not limited to these addressees. Once I had the problem that I wanted to code a little watcher that regularly visits my universitys website to check whether an announcement about the exam results has been made and notify me, if that was the case. The only way for realizing that notification that came to my mind was to send an e-mail. However, I had to integrate an SMTP library into my script then, which authenticates against my mail server using my account credentials - comparatively high effort. Then I wanted to write a little script that runs on my server in regular intervals to gather some basic statistics from log files and databases and send them to me so I could passively consume them (instead of me having to actively go to some webpage or so). Again, same problem. </p>\n<h2 id=\"The-simple-solution-I-created\"><a href=\"#The-simple-solution-I-created\" class=\"headerlink\" title=\"The simple solution I created\"></a>The simple solution I created</h2><p>For these reasons I created the Middleman Bot. It takes a simple, small JSON fragment as an input via HTTP call and forwards it to my Telegram chat. Theres no need for an additional library (nearly every programming language has a built-in API for HTTP calls or you could use cURL) or authentication process. Simply register at the bot once and fire lightweight HTTP calls afterwards. </p>\n<h2 id=\"Example\"><a href=\"#Example\" class=\"headerlink\" title=\"Example\"></a>Example</h2><p>To make my webserver send me a notification when its load is above a certain threshold, the only thing it would need to do is a <code>POST https://apps.muetsch.io/middleman/api/messages</code> with this body: </p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"attr\">&quot;recipient_token&quot;</span>: <span class=\"string\">&quot;3edf633a-eab0-45ea-9721-16c07bb8f245&quot;</span>,</span><br><span class=\"line\">\t<span class=\"attr\">&quot;text&quot;</span>: <span class=\"string\">&quot;Watch out! Average load in the last 10 minutes is &gt;= 10000 requests per second.&quot;</span>,</span><br><span class=\"line\">\t<span class=\"attr\">&quot;origin&quot;</span>: <span class=\"string\">&quot;Caddy webserver @ muetsch.io&quot;</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>The token is your unique identifier the bot uses internally to decide to which chat / recipient to route an incoming message to. (For the trolls among you: Ive invalidated the token from the example, so nobody could spam me.)</p>\n<p>And there you go. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/middleman2.png\"></p>\n<h2 id=\"Give-it-a-try\"><a href=\"#Give-it-a-try\" class=\"headerlink\" title=\"Give it a try\"></a>Give it a try</h2><p>Ive pushed the code as well as some introductions on how to run and use that bot to GitHub at  <a href=\"https://github.com/muety/telegram-middleman-bot\">muety/telegram-middleman-bot</a>. You can either run your own instance of the bot or use mine, which is running at <em><a href=\"https://apps.muetsch.io/\">https://apps.muetsch.io</a></em>. Let me know what you think!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/middleman.png\"></p>\n<h2 id=\"E-Mails-are-so-2010\"><a href=\"#E-Mails-are-so-2010\" class=\"headerlink\" title=\"E-Mails are so 2010\"></a>E-Mails are so 2010</h2><p>E-Mails are everywhere. Literally anybody who gets in contact with a computer in some way will use the electronic mail system, which dates back to 1971. And certainly the E-Mail still has its right to exist, even in 2017. Especially in a business context it simply makes sense for many use cases - however not for all of those it is nowadays used for. The smaller the piece of information you want to communicate is, the less efficient e-mails are. Due to tons of header rows and other heavy load, a mail has a large overhead. Just look at a small random sample of mail from your inbox, find out the core information you get from it and look at the file size. Often you will find an overhead of more than 70 %. Great examples are subscription mails from online forum posts. Their essential information is basically only one bit in size, namely that something new has happened. However, they usually contain lots of HTML code, CSS, attachment pictures and some boilerplate text. Another example is those chatty-like conversations between two or more people, where each of them responds with only a few words to his conversation partners last question or the like. Although only few words are added, the whole previous mail is replicated again, enriched with some headers and then sent to the server, where it is stored and delivered to the recipients. Im glad that for those cases apps like Slack are gradually establishing and I hope they will replace the e-mail some day. Anyway, I get off the topic. This shouldnt be a hate letter against the e-mail. Rather I want to present another bot for the <strong>Telegram</strong> instant messenger.</p>\n<h2 id=\"The-pain-I-had\"><a href=\"#The-pain-I-had\" class=\"headerlink\" title=\"The pain I had\"></a>The pain I had</h2><p>It mainly aims at developers and sysadmins, but is not limited to these addressees. Once I had the problem that I wanted to code a little watcher that regularly visits my universitys website to check whether an announcement about the exam results has been made and notify me, if that was the case. The only way for realizing that notification that came to my mind was to send an e-mail. However, I had to integrate an SMTP library into my script then, which authenticates against my mail server using my account credentials - comparatively high effort. Then I wanted to write a little script that runs on my server in regular intervals to gather some basic statistics from log files and databases and send them to me so I could passively consume them (instead of me having to actively go to some webpage or so). Again, same problem. </p>\n<h2 id=\"The-simple-solution-I-created\"><a href=\"#The-simple-solution-I-created\" class=\"headerlink\" title=\"The simple solution I created\"></a>The simple solution I created</h2><p>For these reasons I created the Middleman Bot. It takes a simple, small JSON fragment as an input via HTTP call and forwards it to my Telegram chat. Theres no need for an additional library (nearly every programming language has a built-in API for HTTP calls or you could use cURL) or authentication process. Simply register at the bot once and fire lightweight HTTP calls afterwards. </p>\n<h2 id=\"Example\"><a href=\"#Example\" class=\"headerlink\" title=\"Example\"></a>Example</h2><p>To make my webserver send me a notification when its load is above a certain threshold, the only thing it would need to do is a <code>POST https://apps.muetsch.io/middleman/api/messages</code> with this body: </p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"attr\">&quot;recipient_token&quot;</span>: <span class=\"string\">&quot;3edf633a-eab0-45ea-9721-16c07bb8f245&quot;</span>,</span><br><span class=\"line\">\t<span class=\"attr\">&quot;text&quot;</span>: <span class=\"string\">&quot;Watch out! Average load in the last 10 minutes is &gt;= 10000 requests per second.&quot;</span>,</span><br><span class=\"line\">\t<span class=\"attr\">&quot;origin&quot;</span>: <span class=\"string\">&quot;Caddy webserver @ muetsch.io&quot;</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>The token is your unique identifier the bot uses internally to decide to which chat / recipient to route an incoming message to. (For the trolls among you: Ive invalidated the token from the example, so nobody could spam me.)</p>\n<p>And there you go. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/middleman2.png\"></p>\n<h2 id=\"Give-it-a-try\"><a href=\"#Give-it-a-try\" class=\"headerlink\" title=\"Give it a try\"></a>Give it a try</h2><p>Ive pushed the code as well as some introductions on how to run and use that bot to GitHub at  <a href=\"https://github.com/muety/telegram-middleman-bot\">muety/telegram-middleman-bot</a>. You can either run your own instance of the bot or use mine, which is running at <em><a href=\"https://apps.muetsch.io/\">https://apps.muetsch.io</a></em>. Let me know what you think!</p>\n"},{"title":"Transfer Learning for Multi-Digit Recognition using TensorFlow Object Detection and SVHN Classifier","date":"2019-09-05T13:41:49.000Z","description":"This article presents how to use convolutional neural networks and TensorFlow object detection to localize and recognize multi-digit labels from pictures of runners at sports events.","_content":"![Dublin City Marathon 2016](https://miro.medium.com/max/1024/1*ZrlYBlKXiADNJC6OsOKTMg.jpeg)\n(Dublin City Marathon 2016, {% link Source https://commons.wikimedia.org/wiki/File:Dublin_City_Marathon_2006_(283653500).jpg Wikimedia Commons %})\n\n# Introduction\nThis summer I attended a run in my home town, where each of the 6,000 runners was assigned a certain bib number to wear on their shirt for time tracking. During the run, several photographers took pictures of each runner, which were made available online afterward. To find oneself among tens of thousands of pictures, the web portal offered an option to search by one's bib number. However, the images are tagged manually by volunteer users, so only a very small fraction of all photos was actually searchable by number.\n\nI wondered if this might not be a perfectly suitable task for machine learning-based image processing and so I took it as a challenge to build a system that automatically tags pictures with the bib numbers they contain.\n\n# Problem Statement\nGiven high-resolution RGB images like the above, which contain one to N persons, each of them wearing 0 to 1 numbers at the front of their bodies, I want to output each of these numbers in text form and associate them to the picture. I figured the numbers to be between 1 and 5 digits long.\n\nBefore starting, I conducted some research and found that, surprisingly, the problem seems to be less trivial than it seemed. First, classical OCR methods didn't seem to work at all, even if the image is precisely cropped only to the number. Second, while one-digit recognition with machine learning is trivial (MNIST, etc.), multi-digit is a much harder problem. Usually, it can't just be solved as a simple classification, because there are not 10 possible output classes anymore, but several thousand. Some other solution was required.\n\n# Approach\n## Multi-Digit Recognition \nAs a starting point, I discovered a paper called [\"Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks\"](http://arxiv.org/pdf/1312.6082.pdf), which presents a multi-digit classifier for house numbers  using convolutional neural nets  that was trained on [Stanford's SVHN dataset](http://ufldl.stanford.edu/housenumbers/). Recognizing house numbers is a quite similar problem to recognizing bib numbers, so I decided to take this approach as a basis.\nLuckily, I found an [open-source PyTorch implementation](https://github.com/potterhsu/SVHNClassifier-PyTorch) of the neural net on GitHub. I needed to do several tweaks and change some code to make it fit my needs, but it was a good start.\n\nEventually, I hoped that I could take the pre-trained SVHN model and use transfer-learning to fit it to my problem. \n\nHowever, before I could get started with the actual classification, there was another problem to solve. The input for the digit classifier is, as one could expect, not a high-res image with a lot of noise and distraction in it, but rather only a very precise excerpt from that image, that exactly contains one single number and not much more. \n\n\n## Object Detection for Localization\nI needed to find a way to localize the 2D-coordinates of all number signs within a picture. To solve that, I decided to utilize TensorFlow's [object detection framework](https://github.com/tensorflow/models/tree/master/research/object_detection), whose purpose is exactly that; recognizing certain objects in an image and outputting a 2-dimensional bounding box for it. \n\n## Outline\n\nIn summary, I planned to build a two-step classification system.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_steps.png)\n\n**Step 1:** Recognize bib numbers and crop them out\n**Step 2:** Use first step's output as input for a fine-tuned SVHN classifier\n\n# Data Preparation\n## Data Acquisition and Labeling for Step 1\nFirst, I started collecting 1,000 images from the web portal mentioned above. I manually labeled them for step 1 by drawing bounding boxes around each number, using [labelImg](https://github.com/tzutalin/labelImg) and wrote a short script to separate them into training, test and validation sets.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_labelimg.png)\n\nThe output of this step is an XML file for every image, containing information about the respective labels and their bounding boxes. Using a script called [`generate_tfrecord.py`](https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py), those XML files can be combined together with their corresponding images into one big _TFRecord_ file for each set (training, test, validation), that is the format required as input for TensorFlow object detection.\n\n## Training the Object Detector\nFor training the object detection model to recognize bib numbers, I decided not to train it completely from scratch, but fine-tune the pre-trained `ssd_mobilenet_v1_pets` set to my needs. The TensorFlow object detection framework provides a quite convenient way to do so by simply adjusting a few config files. If you're interested in more details about training a custom object detector, there's a very interesting [article on pythonprogramming.net](https://pythonprogramming.net/introduction-use-tensorflow-object-detection-api-tutorial/) on this. \n\nAfter training for ~ 100,000 episodes, I ended up with a model  represented as a so-called `frozen_inference_graph.pb` binary file - that was able to find bib number signs in sports imagery. \n\nLetting the model run on my data yields quite reliable results of bounding boxes of bib numbers, which I could then use to crop the original images to smaller ones with another small script. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_cropped_images.png)\n\n## Labeling for Step 2\nTo produce training data for the second step  digit recognition  I needed to do another round of labeling. This time, the little cropped images of numbers had to be assigned their actual numbers in text form. I did this manually and using a simple CSV table. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_labels.png) \n\n## Adding Data Augmentation\nTo (1.) overcome my lack of training data and (2.) hopefully make the model generalize better, I considered it a good idea to introduce some image augmentation. I extended the given [`DataLoader`](https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/dataset.py) in a way that a specified fraction of the number of raw training images is artificially added to the data set in a slightly transformed form. To be more precise, I used [PyTorch's TorchVision Transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) to introduce (a) color jitter (variance in brightness, contrast, saturation, and hue), (b) [affine transformations](https://en.wikipedia.org/wiki/Affine_transformation) and (c) rotation.\n\nAnother thing I changed from the original implementation is the way input images are transformed. The net's 64 x 64 x 3 input layer expects square RGB images. However, obviously, barely any of the training images are actually square. Whereas the original implementation essentially squeezes or tugs the images to match the required dimensions, I considered this unfavorable, especially for wide numbers, e.g. 5-digit numbers. Instead, I changed the input transformation in a way, that images are \"thumbnailed\". \n\nHowever, so far I didn't evaluate which way of pre-processing yields better performance.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_cropping.png)\n\n \n# Fine-Tuning using Transfer Learning\nEventually, after all data massaging and pre-processing was done, I could start with the interesting part: the actual digit recognition.\nDue to my lack of large amounts of high-variance, representative training data, I decided that it might not be a good idea to train the CNN model-based classifier completely from scratch. Instead, I used a [pre-trained model](https://github.com/potterhsu/SVHNClassifier-PyTorch#results) (trained on SVHN dataset) with an accuracy of 95 % for house numbers as a feature extractor and fine-tune it to work with bib numbers. To do so, I conveniently used the given [`train.py`](https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/train.py) script. However, in my understanding, it does not train only the net's very last classification layer while keeping all previous convolutional- and normalization layers frozen, but re-trains every layer. This is not exactly what I wanted, but it turned out to work quite well.\n\n# Results\nAfter training for 72,000 episodes with a batch size of 256, a learning rate of 10^-3 and an augmentation factor of 1.5, I eventually evaluated my two-step classification system on a set of 120 test images and reached an accuracy of **~ 76 %**. That is, about 3/4 of all numbers among all images were detected and classified correctly.\n\nHowever, there is still room for improvements. First, using a lot more training data would probably boost accuracy. Second, I didn't do any hyper-parameter tuning, which would probably also improve performance by a few percentage points.","source":"_posts/transfer-learning-for-multi-digit-recognition-using-tensorflow-object-detection-and-svhn-classifier.md","raw":"---\ntitle: Transfer Learning for Multi-Digit Recognition using TensorFlow Object Detection and SVHN Classifier\ndate: 2019-09-05 15:41:49\ntags:\ndescription: This article presents how to use convolutional neural networks and TensorFlow object detection to localize and recognize multi-digit labels from pictures of runners at sports events.\n---\n![Dublin City Marathon 2016](https://miro.medium.com/max/1024/1*ZrlYBlKXiADNJC6OsOKTMg.jpeg)\n(Dublin City Marathon 2016, {% link Source https://commons.wikimedia.org/wiki/File:Dublin_City_Marathon_2006_(283653500).jpg Wikimedia Commons %})\n\n# Introduction\nThis summer I attended a run in my home town, where each of the 6,000 runners was assigned a certain bib number to wear on their shirt for time tracking. During the run, several photographers took pictures of each runner, which were made available online afterward. To find oneself among tens of thousands of pictures, the web portal offered an option to search by one's bib number. However, the images are tagged manually by volunteer users, so only a very small fraction of all photos was actually searchable by number.\n\nI wondered if this might not be a perfectly suitable task for machine learning-based image processing and so I took it as a challenge to build a system that automatically tags pictures with the bib numbers they contain.\n\n# Problem Statement\nGiven high-resolution RGB images like the above, which contain one to N persons, each of them wearing 0 to 1 numbers at the front of their bodies, I want to output each of these numbers in text form and associate them to the picture. I figured the numbers to be between 1 and 5 digits long.\n\nBefore starting, I conducted some research and found that, surprisingly, the problem seems to be less trivial than it seemed. First, classical OCR methods didn't seem to work at all, even if the image is precisely cropped only to the number. Second, while one-digit recognition with machine learning is trivial (MNIST, etc.), multi-digit is a much harder problem. Usually, it can't just be solved as a simple classification, because there are not 10 possible output classes anymore, but several thousand. Some other solution was required.\n\n# Approach\n## Multi-Digit Recognition \nAs a starting point, I discovered a paper called [\"Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks\"](http://arxiv.org/pdf/1312.6082.pdf), which presents a multi-digit classifier for house numbers  using convolutional neural nets  that was trained on [Stanford's SVHN dataset](http://ufldl.stanford.edu/housenumbers/). Recognizing house numbers is a quite similar problem to recognizing bib numbers, so I decided to take this approach as a basis.\nLuckily, I found an [open-source PyTorch implementation](https://github.com/potterhsu/SVHNClassifier-PyTorch) of the neural net on GitHub. I needed to do several tweaks and change some code to make it fit my needs, but it was a good start.\n\nEventually, I hoped that I could take the pre-trained SVHN model and use transfer-learning to fit it to my problem. \n\nHowever, before I could get started with the actual classification, there was another problem to solve. The input for the digit classifier is, as one could expect, not a high-res image with a lot of noise and distraction in it, but rather only a very precise excerpt from that image, that exactly contains one single number and not much more. \n\n\n## Object Detection for Localization\nI needed to find a way to localize the 2D-coordinates of all number signs within a picture. To solve that, I decided to utilize TensorFlow's [object detection framework](https://github.com/tensorflow/models/tree/master/research/object_detection), whose purpose is exactly that; recognizing certain objects in an image and outputting a 2-dimensional bounding box for it. \n\n## Outline\n\nIn summary, I planned to build a two-step classification system.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_steps.png)\n\n**Step 1:** Recognize bib numbers and crop them out\n**Step 2:** Use first step's output as input for a fine-tuned SVHN classifier\n\n# Data Preparation\n## Data Acquisition and Labeling for Step 1\nFirst, I started collecting 1,000 images from the web portal mentioned above. I manually labeled them for step 1 by drawing bounding boxes around each number, using [labelImg](https://github.com/tzutalin/labelImg) and wrote a short script to separate them into training, test and validation sets.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_labelimg.png)\n\nThe output of this step is an XML file for every image, containing information about the respective labels and their bounding boxes. Using a script called [`generate_tfrecord.py`](https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py), those XML files can be combined together with their corresponding images into one big _TFRecord_ file for each set (training, test, validation), that is the format required as input for TensorFlow object detection.\n\n## Training the Object Detector\nFor training the object detection model to recognize bib numbers, I decided not to train it completely from scratch, but fine-tune the pre-trained `ssd_mobilenet_v1_pets` set to my needs. The TensorFlow object detection framework provides a quite convenient way to do so by simply adjusting a few config files. If you're interested in more details about training a custom object detector, there's a very interesting [article on pythonprogramming.net](https://pythonprogramming.net/introduction-use-tensorflow-object-detection-api-tutorial/) on this. \n\nAfter training for ~ 100,000 episodes, I ended up with a model  represented as a so-called `frozen_inference_graph.pb` binary file - that was able to find bib number signs in sports imagery. \n\nLetting the model run on my data yields quite reliable results of bounding boxes of bib numbers, which I could then use to crop the original images to smaller ones with another small script. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_cropped_images.png)\n\n## Labeling for Step 2\nTo produce training data for the second step  digit recognition  I needed to do another round of labeling. This time, the little cropped images of numbers had to be assigned their actual numbers in text form. I did this manually and using a simple CSV table. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_labels.png) \n\n## Adding Data Augmentation\nTo (1.) overcome my lack of training data and (2.) hopefully make the model generalize better, I considered it a good idea to introduce some image augmentation. I extended the given [`DataLoader`](https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/dataset.py) in a way that a specified fraction of the number of raw training images is artificially added to the data set in a slightly transformed form. To be more precise, I used [PyTorch's TorchVision Transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) to introduce (a) color jitter (variance in brightness, contrast, saturation, and hue), (b) [affine transformations](https://en.wikipedia.org/wiki/Affine_transformation) and (c) rotation.\n\nAnother thing I changed from the original implementation is the way input images are transformed. The net's 64 x 64 x 3 input layer expects square RGB images. However, obviously, barely any of the training images are actually square. Whereas the original implementation essentially squeezes or tugs the images to match the required dimensions, I considered this unfavorable, especially for wide numbers, e.g. 5-digit numbers. Instead, I changed the input transformation in a way, that images are \"thumbnailed\". \n\nHowever, so far I didn't evaluate which way of pre-processing yields better performance.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_cropping.png)\n\n \n# Fine-Tuning using Transfer Learning\nEventually, after all data massaging and pre-processing was done, I could start with the interesting part: the actual digit recognition.\nDue to my lack of large amounts of high-variance, representative training data, I decided that it might not be a good idea to train the CNN model-based classifier completely from scratch. Instead, I used a [pre-trained model](https://github.com/potterhsu/SVHNClassifier-PyTorch#results) (trained on SVHN dataset) with an accuracy of 95 % for house numbers as a feature extractor and fine-tune it to work with bib numbers. To do so, I conveniently used the given [`train.py`](https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/train.py) script. However, in my understanding, it does not train only the net's very last classification layer while keeping all previous convolutional- and normalization layers frozen, but re-trains every layer. This is not exactly what I wanted, but it turned out to work quite well.\n\n# Results\nAfter training for 72,000 episodes with a batch size of 256, a learning rate of 10^-3 and an augmentation factor of 1.5, I eventually evaluated my two-step classification system on a set of 120 test images and reached an accuracy of **~ 76 %**. That is, about 3/4 of all numbers among all images were detected and classified correctly.\n\nHowever, there is still room for improvements. First, using a lot more training data would probably boost accuracy. Second, I didn't do any hyper-parameter tuning, which would probably also improve performance by a few percentage points.","slug":"transfer-learning-for-multi-digit-recognition-using-tensorflow-object-detection-and-svhn-classifier","published":1,"updated":"2020-10-30T20:05:40.285Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqme0012o2e03eoce3v7","content":"<p><img src=\"https://miro.medium.com/max/1024/1*ZrlYBlKXiADNJC6OsOKTMg.jpeg\" alt=\"Dublin City Marathon 2016\"><br>(Dublin City Marathon 2016, <a href=\"https://commons.wikimedia.org/wiki/File:Dublin_City_Marathon_2006_(283653500).jpg\" title=\"Wikimedia Commons\" target=\"\">Source</a>)</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>This summer I attended a run in my home town, where each of the 6,000 runners was assigned a certain bib number to wear on their shirt for time tracking. During the run, several photographers took pictures of each runner, which were made available online afterward. To find oneself among tens of thousands of pictures, the web portal offered an option to search by ones bib number. However, the images are tagged manually by volunteer users, so only a very small fraction of all photos was actually searchable by number.</p>\n<p>I wondered if this might not be a perfectly suitable task for machine learning-based image processing and so I took it as a challenge to build a system that automatically tags pictures with the bib numbers they contain.</p>\n<h1 id=\"Problem-Statement\"><a href=\"#Problem-Statement\" class=\"headerlink\" title=\"Problem Statement\"></a>Problem Statement</h1><p>Given high-resolution RGB images like the above, which contain one to N persons, each of them wearing 0 to 1 numbers at the front of their bodies, I want to output each of these numbers in text form and associate them to the picture. I figured the numbers to be between 1 and 5 digits long.</p>\n<p>Before starting, I conducted some research and found that, surprisingly, the problem seems to be less trivial than it seemed. First, classical OCR methods didnt seem to work at all, even if the image is precisely cropped only to the number. Second, while one-digit recognition with machine learning is trivial (MNIST, etc.), multi-digit is a much harder problem. Usually, it cant just be solved as a simple classification, because there are not 10 possible output classes anymore, but several thousand. Some other solution was required.</p>\n<h1 id=\"Approach\"><a href=\"#Approach\" class=\"headerlink\" title=\"Approach\"></a>Approach</h1><h2 id=\"Multi-Digit-Recognition\"><a href=\"#Multi-Digit-Recognition\" class=\"headerlink\" title=\"Multi-Digit Recognition\"></a>Multi-Digit Recognition</h2><p>As a starting point, I discovered a paper called <a href=\"http://arxiv.org/pdf/1312.6082.pdf\">Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks</a>, which presents a multi-digit classifier for house numbers  using convolutional neural nets  that was trained on <a href=\"http://ufldl.stanford.edu/housenumbers/\">Stanfords SVHN dataset</a>. Recognizing house numbers is a quite similar problem to recognizing bib numbers, so I decided to take this approach as a basis.<br>Luckily, I found an <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch\">open-source PyTorch implementation</a> of the neural net on GitHub. I needed to do several tweaks and change some code to make it fit my needs, but it was a good start.</p>\n<p>Eventually, I hoped that I could take the pre-trained SVHN model and use transfer-learning to fit it to my problem. </p>\n<p>However, before I could get started with the actual classification, there was another problem to solve. The input for the digit classifier is, as one could expect, not a high-res image with a lot of noise and distraction in it, but rather only a very precise excerpt from that image, that exactly contains one single number and not much more. </p>\n<h2 id=\"Object-Detection-for-Localization\"><a href=\"#Object-Detection-for-Localization\" class=\"headerlink\" title=\"Object Detection for Localization\"></a>Object Detection for Localization</h2><p>I needed to find a way to localize the 2D-coordinates of all number signs within a picture. To solve that, I decided to utilize TensorFlows <a href=\"https://github.com/tensorflow/models/tree/master/research/object_detection\">object detection framework</a>, whose purpose is exactly that; recognizing certain objects in an image and outputting a 2-dimensional bounding box for it. </p>\n<h2 id=\"Outline\"><a href=\"#Outline\" class=\"headerlink\" title=\"Outline\"></a>Outline</h2><p>In summary, I planned to build a two-step classification system.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_steps.png\"></p>\n<p><strong>Step 1:</strong> Recognize bib numbers and crop them out<br><strong>Step 2:</strong> Use first steps output as input for a fine-tuned SVHN classifier</p>\n<h1 id=\"Data-Preparation\"><a href=\"#Data-Preparation\" class=\"headerlink\" title=\"Data Preparation\"></a>Data Preparation</h1><h2 id=\"Data-Acquisition-and-Labeling-for-Step-1\"><a href=\"#Data-Acquisition-and-Labeling-for-Step-1\" class=\"headerlink\" title=\"Data Acquisition and Labeling for Step 1\"></a>Data Acquisition and Labeling for Step 1</h2><p>First, I started collecting 1,000 images from the web portal mentioned above. I manually labeled them for step 1 by drawing bounding boxes around each number, using <a href=\"https://github.com/tzutalin/labelImg\">labelImg</a> and wrote a short script to separate them into training, test and validation sets.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_labelimg.png\"></p>\n<p>The output of this step is an XML file for every image, containing information about the respective labels and their bounding boxes. Using a script called <a href=\"https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py\"><code>generate_tfrecord.py</code></a>, those XML files can be combined together with their corresponding images into one big <em>TFRecord</em> file for each set (training, test, validation), that is the format required as input for TensorFlow object detection.</p>\n<h2 id=\"Training-the-Object-Detector\"><a href=\"#Training-the-Object-Detector\" class=\"headerlink\" title=\"Training the Object Detector\"></a>Training the Object Detector</h2><p>For training the object detection model to recognize bib numbers, I decided not to train it completely from scratch, but fine-tune the pre-trained <code>ssd_mobilenet_v1_pets</code> set to my needs. The TensorFlow object detection framework provides a quite convenient way to do so by simply adjusting a few config files. If youre interested in more details about training a custom object detector, theres a very interesting <a href=\"https://pythonprogramming.net/introduction-use-tensorflow-object-detection-api-tutorial/\">article on pythonprogramming.net</a> on this. </p>\n<p>After training for ~ 100,000 episodes, I ended up with a model  represented as a so-called <code>frozen_inference_graph.pb</code> binary file - that was able to find bib number signs in sports imagery. </p>\n<p>Letting the model run on my data yields quite reliable results of bounding boxes of bib numbers, which I could then use to crop the original images to smaller ones with another small script. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_cropped_images.png\"></p>\n<h2 id=\"Labeling-for-Step-2\"><a href=\"#Labeling-for-Step-2\" class=\"headerlink\" title=\"Labeling for Step 2\"></a>Labeling for Step 2</h2><p>To produce training data for the second step  digit recognition  I needed to do another round of labeling. This time, the little cropped images of numbers had to be assigned their actual numbers in text form. I did this manually and using a simple CSV table. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_labels.png\"> </p>\n<h2 id=\"Adding-Data-Augmentation\"><a href=\"#Adding-Data-Augmentation\" class=\"headerlink\" title=\"Adding Data Augmentation\"></a>Adding Data Augmentation</h2><p>To (1.) overcome my lack of training data and (2.) hopefully make the model generalize better, I considered it a good idea to introduce some image augmentation. I extended the given <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/dataset.py\"><code>DataLoader</code></a> in a way that a specified fraction of the number of raw training images is artificially added to the data set in a slightly transformed form. To be more precise, I used <a href=\"https://pytorch.org/docs/stable/torchvision/transforms.html\">PyTorchs TorchVision Transforms</a> to introduce (a) color jitter (variance in brightness, contrast, saturation, and hue), (b) <a href=\"https://en.wikipedia.org/wiki/Affine_transformation\">affine transformations</a> and (c) rotation.</p>\n<p>Another thing I changed from the original implementation is the way input images are transformed. The nets 64 x 64 x 3 input layer expects square RGB images. However, obviously, barely any of the training images are actually square. Whereas the original implementation essentially squeezes or tugs the images to match the required dimensions, I considered this unfavorable, especially for wide numbers, e.g. 5-digit numbers. Instead, I changed the input transformation in a way, that images are thumbnailed. </p>\n<p>However, so far I didnt evaluate which way of pre-processing yields better performance.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_cropping.png\"></p>\n<h1 id=\"Fine-Tuning-using-Transfer-Learning\"><a href=\"#Fine-Tuning-using-Transfer-Learning\" class=\"headerlink\" title=\"Fine-Tuning using Transfer Learning\"></a>Fine-Tuning using Transfer Learning</h1><p>Eventually, after all data massaging and pre-processing was done, I could start with the interesting part: the actual digit recognition.<br>Due to my lack of large amounts of high-variance, representative training data, I decided that it might not be a good idea to train the CNN model-based classifier completely from scratch. Instead, I used a <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch#results\">pre-trained model</a> (trained on SVHN dataset) with an accuracy of 95 % for house numbers as a feature extractor and fine-tune it to work with bib numbers. To do so, I conveniently used the given <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/train.py\"><code>train.py</code></a> script. However, in my understanding, it does not train only the nets very last classification layer while keeping all previous convolutional- and normalization layers frozen, but re-trains every layer. This is not exactly what I wanted, but it turned out to work quite well.</p>\n<h1 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h1><p>After training for 72,000 episodes with a batch size of 256, a learning rate of 10^-3 and an augmentation factor of 1.5, I eventually evaluated my two-step classification system on a set of 120 test images and reached an accuracy of <strong>~ 76 %</strong>. That is, about 3/4 of all numbers among all images were detected and classified correctly.</p>\n<p>However, there is still room for improvements. First, using a lot more training data would probably boost accuracy. Second, I didnt do any hyper-parameter tuning, which would probably also improve performance by a few percentage points.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"https://miro.medium.com/max/1024/1*ZrlYBlKXiADNJC6OsOKTMg.jpeg\" alt=\"Dublin City Marathon 2016\"><br>(Dublin City Marathon 2016, <a href=\"https://commons.wikimedia.org/wiki/File:Dublin_City_Marathon_2006_(283653500).jpg\" title=\"Wikimedia Commons\" target=\"\">Source</a>)</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>This summer I attended a run in my home town, where each of the 6,000 runners was assigned a certain bib number to wear on their shirt for time tracking. During the run, several photographers took pictures of each runner, which were made available online afterward. To find oneself among tens of thousands of pictures, the web portal offered an option to search by ones bib number. However, the images are tagged manually by volunteer users, so only a very small fraction of all photos was actually searchable by number.</p>\n<p>I wondered if this might not be a perfectly suitable task for machine learning-based image processing and so I took it as a challenge to build a system that automatically tags pictures with the bib numbers they contain.</p>\n<h1 id=\"Problem-Statement\"><a href=\"#Problem-Statement\" class=\"headerlink\" title=\"Problem Statement\"></a>Problem Statement</h1><p>Given high-resolution RGB images like the above, which contain one to N persons, each of them wearing 0 to 1 numbers at the front of their bodies, I want to output each of these numbers in text form and associate them to the picture. I figured the numbers to be between 1 and 5 digits long.</p>\n<p>Before starting, I conducted some research and found that, surprisingly, the problem seems to be less trivial than it seemed. First, classical OCR methods didnt seem to work at all, even if the image is precisely cropped only to the number. Second, while one-digit recognition with machine learning is trivial (MNIST, etc.), multi-digit is a much harder problem. Usually, it cant just be solved as a simple classification, because there are not 10 possible output classes anymore, but several thousand. Some other solution was required.</p>\n<h1 id=\"Approach\"><a href=\"#Approach\" class=\"headerlink\" title=\"Approach\"></a>Approach</h1><h2 id=\"Multi-Digit-Recognition\"><a href=\"#Multi-Digit-Recognition\" class=\"headerlink\" title=\"Multi-Digit Recognition\"></a>Multi-Digit Recognition</h2><p>As a starting point, I discovered a paper called <a href=\"http://arxiv.org/pdf/1312.6082.pdf\">Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks</a>, which presents a multi-digit classifier for house numbers  using convolutional neural nets  that was trained on <a href=\"http://ufldl.stanford.edu/housenumbers/\">Stanfords SVHN dataset</a>. Recognizing house numbers is a quite similar problem to recognizing bib numbers, so I decided to take this approach as a basis.<br>Luckily, I found an <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch\">open-source PyTorch implementation</a> of the neural net on GitHub. I needed to do several tweaks and change some code to make it fit my needs, but it was a good start.</p>\n<p>Eventually, I hoped that I could take the pre-trained SVHN model and use transfer-learning to fit it to my problem. </p>\n<p>However, before I could get started with the actual classification, there was another problem to solve. The input for the digit classifier is, as one could expect, not a high-res image with a lot of noise and distraction in it, but rather only a very precise excerpt from that image, that exactly contains one single number and not much more. </p>\n<h2 id=\"Object-Detection-for-Localization\"><a href=\"#Object-Detection-for-Localization\" class=\"headerlink\" title=\"Object Detection for Localization\"></a>Object Detection for Localization</h2><p>I needed to find a way to localize the 2D-coordinates of all number signs within a picture. To solve that, I decided to utilize TensorFlows <a href=\"https://github.com/tensorflow/models/tree/master/research/object_detection\">object detection framework</a>, whose purpose is exactly that; recognizing certain objects in an image and outputting a 2-dimensional bounding box for it. </p>\n<h2 id=\"Outline\"><a href=\"#Outline\" class=\"headerlink\" title=\"Outline\"></a>Outline</h2><p>In summary, I planned to build a two-step classification system.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_steps.png\"></p>\n<p><strong>Step 1:</strong> Recognize bib numbers and crop them out<br><strong>Step 2:</strong> Use first steps output as input for a fine-tuned SVHN classifier</p>\n<h1 id=\"Data-Preparation\"><a href=\"#Data-Preparation\" class=\"headerlink\" title=\"Data Preparation\"></a>Data Preparation</h1><h2 id=\"Data-Acquisition-and-Labeling-for-Step-1\"><a href=\"#Data-Acquisition-and-Labeling-for-Step-1\" class=\"headerlink\" title=\"Data Acquisition and Labeling for Step 1\"></a>Data Acquisition and Labeling for Step 1</h2><p>First, I started collecting 1,000 images from the web portal mentioned above. I manually labeled them for step 1 by drawing bounding boxes around each number, using <a href=\"https://github.com/tzutalin/labelImg\">labelImg</a> and wrote a short script to separate them into training, test and validation sets.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_labelimg.png\"></p>\n<p>The output of this step is an XML file for every image, containing information about the respective labels and their bounding boxes. Using a script called <a href=\"https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py\"><code>generate_tfrecord.py</code></a>, those XML files can be combined together with their corresponding images into one big <em>TFRecord</em> file for each set (training, test, validation), that is the format required as input for TensorFlow object detection.</p>\n<h2 id=\"Training-the-Object-Detector\"><a href=\"#Training-the-Object-Detector\" class=\"headerlink\" title=\"Training the Object Detector\"></a>Training the Object Detector</h2><p>For training the object detection model to recognize bib numbers, I decided not to train it completely from scratch, but fine-tune the pre-trained <code>ssd_mobilenet_v1_pets</code> set to my needs. The TensorFlow object detection framework provides a quite convenient way to do so by simply adjusting a few config files. If youre interested in more details about training a custom object detector, theres a very interesting <a href=\"https://pythonprogramming.net/introduction-use-tensorflow-object-detection-api-tutorial/\">article on pythonprogramming.net</a> on this. </p>\n<p>After training for ~ 100,000 episodes, I ended up with a model  represented as a so-called <code>frozen_inference_graph.pb</code> binary file - that was able to find bib number signs in sports imagery. </p>\n<p>Letting the model run on my data yields quite reliable results of bounding boxes of bib numbers, which I could then use to crop the original images to smaller ones with another small script. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_cropped_images.png\"></p>\n<h2 id=\"Labeling-for-Step-2\"><a href=\"#Labeling-for-Step-2\" class=\"headerlink\" title=\"Labeling for Step 2\"></a>Labeling for Step 2</h2><p>To produce training data for the second step  digit recognition  I needed to do another round of labeling. This time, the little cropped images of numbers had to be assigned their actual numbers in text form. I did this manually and using a simple CSV table. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_labels.png\"> </p>\n<h2 id=\"Adding-Data-Augmentation\"><a href=\"#Adding-Data-Augmentation\" class=\"headerlink\" title=\"Adding Data Augmentation\"></a>Adding Data Augmentation</h2><p>To (1.) overcome my lack of training data and (2.) hopefully make the model generalize better, I considered it a good idea to introduce some image augmentation. I extended the given <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/dataset.py\"><code>DataLoader</code></a> in a way that a specified fraction of the number of raw training images is artificially added to the data set in a slightly transformed form. To be more precise, I used <a href=\"https://pytorch.org/docs/stable/torchvision/transforms.html\">PyTorchs TorchVision Transforms</a> to introduce (a) color jitter (variance in brightness, contrast, saturation, and hue), (b) <a href=\"https://en.wikipedia.org/wiki/Affine_transformation\">affine transformations</a> and (c) rotation.</p>\n<p>Another thing I changed from the original implementation is the way input images are transformed. The nets 64 x 64 x 3 input layer expects square RGB images. However, obviously, barely any of the training images are actually square. Whereas the original implementation essentially squeezes or tugs the images to match the required dimensions, I considered this unfavorable, especially for wide numbers, e.g. 5-digit numbers. Instead, I changed the input transformation in a way, that images are thumbnailed. </p>\n<p>However, so far I didnt evaluate which way of pre-processing yields better performance.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/svhn_cropping.png\"></p>\n<h1 id=\"Fine-Tuning-using-Transfer-Learning\"><a href=\"#Fine-Tuning-using-Transfer-Learning\" class=\"headerlink\" title=\"Fine-Tuning using Transfer Learning\"></a>Fine-Tuning using Transfer Learning</h1><p>Eventually, after all data massaging and pre-processing was done, I could start with the interesting part: the actual digit recognition.<br>Due to my lack of large amounts of high-variance, representative training data, I decided that it might not be a good idea to train the CNN model-based classifier completely from scratch. Instead, I used a <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch#results\">pre-trained model</a> (trained on SVHN dataset) with an accuracy of 95 % for house numbers as a feature extractor and fine-tune it to work with bib numbers. To do so, I conveniently used the given <a href=\"https://github.com/potterhsu/SVHNClassifier-PyTorch/blob/master/train.py\"><code>train.py</code></a> script. However, in my understanding, it does not train only the nets very last classification layer while keeping all previous convolutional- and normalization layers frozen, but re-trains every layer. This is not exactly what I wanted, but it turned out to work quite well.</p>\n<h1 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h1><p>After training for 72,000 episodes with a batch size of 256, a learning rate of 10^-3 and an augmentation factor of 1.5, I eventually evaluated my two-step classification system on a set of 120 test images and reached an accuracy of <strong>~ 76 %</strong>. That is, about 3/4 of all numbers among all images were detected and classified correctly.</p>\n<p>However, there is still room for improvements. First, using a lot more training data would probably boost accuracy. Second, I didnt do any hyper-parameter tuning, which would probably also improve performance by a few percentage points.</p>\n"},{"title":"Unhosted.org applications with remoteStorage.io and WebFinger.net","date":"2016-04-12T20:57:43.000Z","_content":"\nLately you as an interested web developer might have heard or read about a thing called **unhosted applications**, mostly with a reference to [unhosted.org](http://unhosted.org/). This basically means web-apps running in your browser which do not rely on any kind of backend. Most apps youre using on the web store data to a backend service at the providers host server, regardless of them being Google Docs, Evernote, Wunderlist or also [Anchr.io](https://anchr.io). Obviously this makes you dependent on the provider in terms of availability and security. Also these apps are usually online-only apps, meaning that you can only use them with an internet connection. Another type of apps are those without a specific backend but still with a central, cloud-based data store, e.g. Firebase. In this case your data is still anywhere out there in the cloud at a provider you potentially could not trust (even though not on any kind of dubious private-hosted server but on a certified platform of one of the big players  deciding whether that is better or worse its up to you, actually). These apps are usually offline-capable, so you can use them without internet connection and if the connection comes back again, the data is synced to the data platform. And then theres this third kind of apps, which they call unhosted ones. These are intended to be completely static, without needing any kind of backend. Theoretically you could download a zip of their HTML, JavaScript and CSS files to your computer and perfectly run the app without a webserver and a database. Well, ok, probably youll need a webserver, but it only needs to server static files (like an Apache2, nginx or [http-server](https://www.npmjs.com/package/http-server)), nothing else  no PHP, node Node.js Those apps (e.g. a simple todo-list) store all their data to your browsers localStorage or IndexedDB. As a result it obviously is only available on your local computer and only until you clear you browser data. After all you might still want to sync your data to other devices now  but without giving your data away to a untrusted provider.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/unhosted.jpg)\n\nThis is where quite a new thing called [RemoteStorage](https://remotestorage.io) comes in. Its a protocol for storing per-user data on the web in an unhosted fasion. Anything implementing the remoteStorage protocol can be your data-repository, which basically works as a simple key-value store. As a result you can implement your own remoteStorage or e.g. take the official PHP library to host one on your own server. There are also a few providers (like [5apps](http://5apps.com)) out there, yet, which you can use, but dont have to, if you dont trust them. An unhosted-app that is remoteStorage-capable can now connect to your remoteStorage, e.g. _https://rs.yourserver.com/user1/appXyz_, and sync its data there. RemoteStorage works together with [WebFinger](https://webfinger.net). What is WebFinger now? WebFinger is kind of a registry on the web, where unique keys (usually in email-address style, like user1@yourserver.com), are mapped to URLs for a certain type of relation. In this case you would tell your unhosted app such an email-address-like identifier, which maps to your remoteStorage-endpoint for the relation-type remotestorage. The app queries WebFinger for that key and follows the returned registered URL to the datastore. In [this example](https://client.webfinger.net/lookup?resource=tony%405apps.com) the identifier *tony@5apps.com* maps to a *remotestorage* located at _https://storage.5apps.com/tony_. This makes the entire thing as decentralized as possible. Note that you could change your remoteStorage anytime by simply registering a new URL at WebFinger (you usually wouldnt have to do this registration on your own, but the remoteStorage server implementation handles that for you). The authorization  like which app may access which subkeys on the remoteStorage  is handled by OAuth at your remoteStorage server implementation, where you can grant or revoke access for certain apps to certain store keys.\n\nTwo apps you can try are [litewrite.net](https://litewrite.net/) and [Grouptabs](http://grouptabs.5apps.com). If you just want to play around with remoteStorage it might be the easiest way to use [5apps](http://5apps.com) remoteStorage for this.","source":"_posts/unhostedorg-applications-with-remotestorageio-and-webfingernet.md","raw":"---\ntitle: Unhosted.org applications with remoteStorage.io and WebFinger.net\ndate: 2016-04-12 22:57:43\ntags:\n---\n\nLately you as an interested web developer might have heard or read about a thing called **unhosted applications**, mostly with a reference to [unhosted.org](http://unhosted.org/). This basically means web-apps running in your browser which do not rely on any kind of backend. Most apps youre using on the web store data to a backend service at the providers host server, regardless of them being Google Docs, Evernote, Wunderlist or also [Anchr.io](https://anchr.io). Obviously this makes you dependent on the provider in terms of availability and security. Also these apps are usually online-only apps, meaning that you can only use them with an internet connection. Another type of apps are those without a specific backend but still with a central, cloud-based data store, e.g. Firebase. In this case your data is still anywhere out there in the cloud at a provider you potentially could not trust (even though not on any kind of dubious private-hosted server but on a certified platform of one of the big players  deciding whether that is better or worse its up to you, actually). These apps are usually offline-capable, so you can use them without internet connection and if the connection comes back again, the data is synced to the data platform. And then theres this third kind of apps, which they call unhosted ones. These are intended to be completely static, without needing any kind of backend. Theoretically you could download a zip of their HTML, JavaScript and CSS files to your computer and perfectly run the app without a webserver and a database. Well, ok, probably youll need a webserver, but it only needs to server static files (like an Apache2, nginx or [http-server](https://www.npmjs.com/package/http-server)), nothing else  no PHP, node Node.js Those apps (e.g. a simple todo-list) store all their data to your browsers localStorage or IndexedDB. As a result it obviously is only available on your local computer and only until you clear you browser data. After all you might still want to sync your data to other devices now  but without giving your data away to a untrusted provider.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/unhosted.jpg)\n\nThis is where quite a new thing called [RemoteStorage](https://remotestorage.io) comes in. Its a protocol for storing per-user data on the web in an unhosted fasion. Anything implementing the remoteStorage protocol can be your data-repository, which basically works as a simple key-value store. As a result you can implement your own remoteStorage or e.g. take the official PHP library to host one on your own server. There are also a few providers (like [5apps](http://5apps.com)) out there, yet, which you can use, but dont have to, if you dont trust them. An unhosted-app that is remoteStorage-capable can now connect to your remoteStorage, e.g. _https://rs.yourserver.com/user1/appXyz_, and sync its data there. RemoteStorage works together with [WebFinger](https://webfinger.net). What is WebFinger now? WebFinger is kind of a registry on the web, where unique keys (usually in email-address style, like user1@yourserver.com), are mapped to URLs for a certain type of relation. In this case you would tell your unhosted app such an email-address-like identifier, which maps to your remoteStorage-endpoint for the relation-type remotestorage. The app queries WebFinger for that key and follows the returned registered URL to the datastore. In [this example](https://client.webfinger.net/lookup?resource=tony%405apps.com) the identifier *tony@5apps.com* maps to a *remotestorage* located at _https://storage.5apps.com/tony_. This makes the entire thing as decentralized as possible. Note that you could change your remoteStorage anytime by simply registering a new URL at WebFinger (you usually wouldnt have to do this registration on your own, but the remoteStorage server implementation handles that for you). The authorization  like which app may access which subkeys on the remoteStorage  is handled by OAuth at your remoteStorage server implementation, where you can grant or revoke access for certain apps to certain store keys.\n\nTwo apps you can try are [litewrite.net](https://litewrite.net/) and [Grouptabs](http://grouptabs.5apps.com). If you just want to play around with remoteStorage it might be the easiest way to use [5apps](http://5apps.com) remoteStorage for this.","slug":"unhostedorg-applications-with-remotestorageio-and-webfingernet","published":1,"updated":"2020-10-30T20:05:40.286Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqmf0013o2e0d2809dic","content":"<p>Lately you as an interested web developer might have heard or read about a thing called <strong>unhosted applications</strong>, mostly with a reference to <a href=\"http://unhosted.org/\">unhosted.org</a>. This basically means web-apps running in your browser which do not rely on any kind of backend. Most apps youre using on the web store data to a backend service at the providers host server, regardless of them being Google Docs, Evernote, Wunderlist or also <a href=\"https://anchr.io/\">Anchr.io</a>. Obviously this makes you dependent on the provider in terms of availability and security. Also these apps are usually online-only apps, meaning that you can only use them with an internet connection. Another type of apps are those without a specific backend but still with a central, cloud-based data store, e.g. Firebase. In this case your data is still anywhere out there in the cloud at a provider you potentially could not trust (even though not on any kind of dubious private-hosted server but on a certified platform of one of the big players  deciding whether that is better or worse its up to you, actually). These apps are usually offline-capable, so you can use them without internet connection and if the connection comes back again, the data is synced to the data platform. And then theres this third kind of apps, which they call unhosted ones. These are intended to be completely static, without needing any kind of backend. Theoretically you could download a zip of their HTML, JavaScript and CSS files to your computer and perfectly run the app without a webserver and a database. Well, ok, probably youll need a webserver, but it only needs to server static files (like an Apache2, nginx or <a href=\"https://www.npmjs.com/package/http-server\">http-server</a>), nothing else  no PHP, node Node.js Those apps (e.g. a simple todo-list) store all their data to your browsers localStorage or IndexedDB. As a result it obviously is only available on your local computer and only until you clear you browser data. After all you might still want to sync your data to other devices now  but without giving your data away to a untrusted provider.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/unhosted.jpg\"></p>\n<p>This is where quite a new thing called <a href=\"https://remotestorage.io/\">RemoteStorage</a> comes in. Its a protocol for storing per-user data on the web in an unhosted fasion. Anything implementing the remoteStorage protocol can be your data-repository, which basically works as a simple key-value store. As a result you can implement your own remoteStorage or e.g. take the official PHP library to host one on your own server. There are also a few providers (like <a href=\"http://5apps.com/\">5apps</a>) out there, yet, which you can use, but dont have to, if you dont trust them. An unhosted-app that is remoteStorage-capable can now connect to your remoteStorage, e.g. <em><a href=\"https://rs.yourserver.com/user1/appXyz\">https://rs.yourserver.com/user1/appXyz</a></em>, and sync its data there. RemoteStorage works together with <a href=\"https://webfinger.net/\">WebFinger</a>. What is WebFinger now? WebFinger is kind of a registry on the web, where unique keys (usually in email-address style, like <a href=\"mailto:&#117;&#x73;&#x65;&#x72;&#x31;&#64;&#121;&#x6f;&#x75;&#x72;&#115;&#x65;&#x72;&#118;&#x65;&#x72;&#x2e;&#99;&#x6f;&#x6d;\">&#117;&#x73;&#x65;&#x72;&#x31;&#64;&#121;&#x6f;&#x75;&#x72;&#115;&#x65;&#x72;&#118;&#x65;&#x72;&#x2e;&#99;&#x6f;&#x6d;</a>), are mapped to URLs for a certain type of relation. In this case you would tell your unhosted app such an email-address-like identifier, which maps to your remoteStorage-endpoint for the relation-type remotestorage. The app queries WebFinger for that key and follows the returned registered URL to the datastore. In <a href=\"https://client.webfinger.net/lookup?resource=tony@5apps.com\">this example</a> the identifier <em><a href=\"mailto:&#x74;&#x6f;&#110;&#121;&#x40;&#x35;&#97;&#112;&#112;&#x73;&#x2e;&#99;&#x6f;&#x6d;\">&#x74;&#x6f;&#110;&#121;&#x40;&#x35;&#97;&#112;&#112;&#x73;&#x2e;&#99;&#x6f;&#x6d;</a></em> maps to a <em>remotestorage</em> located at <em><a href=\"https://storage.5apps.com/tony\">https://storage.5apps.com/tony</a></em>. This makes the entire thing as decentralized as possible. Note that you could change your remoteStorage anytime by simply registering a new URL at WebFinger (you usually wouldnt have to do this registration on your own, but the remoteStorage server implementation handles that for you). The authorization  like which app may access which subkeys on the remoteStorage  is handled by OAuth at your remoteStorage server implementation, where you can grant or revoke access for certain apps to certain store keys.</p>\n<p>Two apps you can try are <a href=\"https://litewrite.net/\">litewrite.net</a> and <a href=\"http://grouptabs.5apps.com/\">Grouptabs</a>. If you just want to play around with remoteStorage it might be the easiest way to use <a href=\"http://5apps.com/\">5apps</a> remoteStorage for this.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Lately you as an interested web developer might have heard or read about a thing called <strong>unhosted applications</strong>, mostly with a reference to <a href=\"http://unhosted.org/\">unhosted.org</a>. This basically means web-apps running in your browser which do not rely on any kind of backend. Most apps youre using on the web store data to a backend service at the providers host server, regardless of them being Google Docs, Evernote, Wunderlist or also <a href=\"https://anchr.io/\">Anchr.io</a>. Obviously this makes you dependent on the provider in terms of availability and security. Also these apps are usually online-only apps, meaning that you can only use them with an internet connection. Another type of apps are those without a specific backend but still with a central, cloud-based data store, e.g. Firebase. In this case your data is still anywhere out there in the cloud at a provider you potentially could not trust (even though not on any kind of dubious private-hosted server but on a certified platform of one of the big players  deciding whether that is better or worse its up to you, actually). These apps are usually offline-capable, so you can use them without internet connection and if the connection comes back again, the data is synced to the data platform. And then theres this third kind of apps, which they call unhosted ones. These are intended to be completely static, without needing any kind of backend. Theoretically you could download a zip of their HTML, JavaScript and CSS files to your computer and perfectly run the app without a webserver and a database. Well, ok, probably youll need a webserver, but it only needs to server static files (like an Apache2, nginx or <a href=\"https://www.npmjs.com/package/http-server\">http-server</a>), nothing else  no PHP, node Node.js Those apps (e.g. a simple todo-list) store all their data to your browsers localStorage or IndexedDB. As a result it obviously is only available on your local computer and only until you clear you browser data. After all you might still want to sync your data to other devices now  but without giving your data away to a untrusted provider.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/unhosted.jpg\"></p>\n<p>This is where quite a new thing called <a href=\"https://remotestorage.io/\">RemoteStorage</a> comes in. Its a protocol for storing per-user data on the web in an unhosted fasion. Anything implementing the remoteStorage protocol can be your data-repository, which basically works as a simple key-value store. As a result you can implement your own remoteStorage or e.g. take the official PHP library to host one on your own server. There are also a few providers (like <a href=\"http://5apps.com/\">5apps</a>) out there, yet, which you can use, but dont have to, if you dont trust them. An unhosted-app that is remoteStorage-capable can now connect to your remoteStorage, e.g. <em><a href=\"https://rs.yourserver.com/user1/appXyz\">https://rs.yourserver.com/user1/appXyz</a></em>, and sync its data there. RemoteStorage works together with <a href=\"https://webfinger.net/\">WebFinger</a>. What is WebFinger now? WebFinger is kind of a registry on the web, where unique keys (usually in email-address style, like <a href=\"mailto:&#117;&#x73;&#x65;&#x72;&#x31;&#64;&#121;&#x6f;&#x75;&#x72;&#115;&#x65;&#x72;&#118;&#x65;&#x72;&#x2e;&#99;&#x6f;&#x6d;\">&#117;&#x73;&#x65;&#x72;&#x31;&#64;&#121;&#x6f;&#x75;&#x72;&#115;&#x65;&#x72;&#118;&#x65;&#x72;&#x2e;&#99;&#x6f;&#x6d;</a>), are mapped to URLs for a certain type of relation. In this case you would tell your unhosted app such an email-address-like identifier, which maps to your remoteStorage-endpoint for the relation-type remotestorage. The app queries WebFinger for that key and follows the returned registered URL to the datastore. In <a href=\"https://client.webfinger.net/lookup?resource=tony@5apps.com\">this example</a> the identifier <em><a href=\"mailto:&#x74;&#x6f;&#110;&#121;&#x40;&#x35;&#97;&#112;&#112;&#x73;&#x2e;&#99;&#x6f;&#x6d;\">&#x74;&#x6f;&#110;&#121;&#x40;&#x35;&#97;&#112;&#112;&#x73;&#x2e;&#99;&#x6f;&#x6d;</a></em> maps to a <em>remotestorage</em> located at <em><a href=\"https://storage.5apps.com/tony\">https://storage.5apps.com/tony</a></em>. This makes the entire thing as decentralized as possible. Note that you could change your remoteStorage anytime by simply registering a new URL at WebFinger (you usually wouldnt have to do this registration on your own, but the remoteStorage server implementation handles that for you). The authorization  like which app may access which subkeys on the remoteStorage  is handled by OAuth at your remoteStorage server implementation, where you can grant or revoke access for certain apps to certain store keys.</p>\n<p>Two apps you can try are <a href=\"https://litewrite.net/\">litewrite.net</a> and <a href=\"http://grouptabs.5apps.com/\">Grouptabs</a>. If you just want to play around with remoteStorage it might be the easiest way to use <a href=\"http://5apps.com/\">5apps</a> remoteStorage for this.</p>\n"},{"title":"Web Development Technology Stack","date":"2016-03-15T21:54:04.000Z","_content":"\nI tried to give a comprehensive overview of all important and fashionable technologies concerning web- and cross-platform development, so I created kind of a mind map outlining relevant technologies in terms of languages, frameworks, libraries, webservices and tools. I consider them kind of must-haves for JavaScript fullstack web-developers. You might have already heard of most of them and probably you have even worked with the majority.\n\nFeel free to give your opinion on this collection and maybe add something youre missing (in the comments after having logged in, via mail or via [telegram.me/n1try](http://telegram.me/n1try)).\n\n[![Web Tech Stack](/imgages/webdev_techstack.png)](/imgages/webdev_techstack_large.png)","source":"_posts/web-development-technology-stack.md","raw":"---\ntitle: Web Development Technology Stack\ndate: 2016-03-15 22:54:04\ntags:\n---\n\nI tried to give a comprehensive overview of all important and fashionable technologies concerning web- and cross-platform development, so I created kind of a mind map outlining relevant technologies in terms of languages, frameworks, libraries, webservices and tools. I consider them kind of must-haves for JavaScript fullstack web-developers. You might have already heard of most of them and probably you have even worked with the majority.\n\nFeel free to give your opinion on this collection and maybe add something youre missing (in the comments after having logged in, via mail or via [telegram.me/n1try](http://telegram.me/n1try)).\n\n[![Web Tech Stack](/imgages/webdev_techstack.png)](/imgages/webdev_techstack_large.png)","slug":"web-development-technology-stack","published":1,"updated":"2020-10-30T20:05:40.289Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqmg0014o2e0ada42hqb","content":"<p>I tried to give a comprehensive overview of all important and fashionable technologies concerning web- and cross-platform development, so I created kind of a mind map outlining relevant technologies in terms of languages, frameworks, libraries, webservices and tools. I consider them kind of must-haves for JavaScript fullstack web-developers. You might have already heard of most of them and probably you have even worked with the majority.</p>\n<p>Feel free to give your opinion on this collection and maybe add something youre missing (in the comments after having logged in, via mail or via <a href=\"http://telegram.me/n1try\">telegram.me/n1try</a>).</p>\n<p><a href=\"/imgages/webdev_techstack_large.png\"><img src=\"/imgages/webdev_techstack.png\" alt=\"Web Tech Stack\"></a></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>I tried to give a comprehensive overview of all important and fashionable technologies concerning web- and cross-platform development, so I created kind of a mind map outlining relevant technologies in terms of languages, frameworks, libraries, webservices and tools. I consider them kind of must-haves for JavaScript fullstack web-developers. You might have already heard of most of them and probably you have even worked with the majority.</p>\n<p>Feel free to give your opinion on this collection and maybe add something youre missing (in the comments after having logged in, via mail or via <a href=\"http://telegram.me/n1try\">telegram.me/n1try</a>).</p>\n<p><a href=\"/imgages/webdev_techstack_large.png\"><img src=\"/imgages/webdev_techstack.png\" alt=\"Web Tech Stack\"></a></p>\n"},{"title":"Webdevlist.net - The Developer's Resource Collection","date":"2016-09-21T21:02:25.000Z","_content":"\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/webdevlist.jpg)\n\nI just put up a new project of mine, which is called Webdevlist.\n\n### So what is Webdevlist?\n\nIm pretty sure most developers know it all too well: you browse Twitter, Reddit or Stackoverflow on a late Sunday evening and come across some cool programming framework or webservice. You think like _Wow thats cool! Could be helpful some time. I need to remember it._ Then you put the link anywhere in a messy text file or the like and never visit it again. Webdevlists aim is to become a comprehensive, community-maintained collection of useful resources concerning software development. Such could include frameworks and libraries for any language, devtools, software applications, apps, webservices like PaaS, SaaS or IaaS and also learning resources like guides and tutorials. Everytime you find a cool tool on the web, just post it to the list. Everytime youre looking for a tool to help you with your development problems come visit the list. Easy enough.\n\n### Tech facts\n\nWebdevlists frontend is built with [Angular2](https://angular.io/ \"Angular2\"), which just has had its first final release. The backend makes use of [LoopbackJS](http://loopback.io/ \"LoopbackJS\"), which is a mighty framework to build REST APIs with a minimum of boilerplate code. Additionally the site is powered by HTTP/2.0 to get some more speed.\n\n### Is it finished, yet?\n\nNo, it isnt. Actually, it probably never will be. Im continuously going to add new technology to Webdevlists stack and change and refactor things. Currently Im considering to switch to [GraphQL](http://graphql.org/learn/ \"GraphQL\"). Right now, Webdevlist is in some kind of beta state, meaning that it still might have bugs.\n\nId really appreciate to get feedback on this project!\n\n[>> Webdevlist.net](https://webdevlist.net)  \n[>> Webdevlist on GitHub](https://github.com/muety/webdevlist.net) ","source":"_posts/webdevlistnet-the-developers-resource-collection.md","raw":"---\ntitle: Webdevlist.net - The Developer's Resource Collection\ndate: 2016-09-21 23:02:25\ntags:\n---\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/webdevlist.jpg)\n\nI just put up a new project of mine, which is called Webdevlist.\n\n### So what is Webdevlist?\n\nIm pretty sure most developers know it all too well: you browse Twitter, Reddit or Stackoverflow on a late Sunday evening and come across some cool programming framework or webservice. You think like _Wow thats cool! Could be helpful some time. I need to remember it._ Then you put the link anywhere in a messy text file or the like and never visit it again. Webdevlists aim is to become a comprehensive, community-maintained collection of useful resources concerning software development. Such could include frameworks and libraries for any language, devtools, software applications, apps, webservices like PaaS, SaaS or IaaS and also learning resources like guides and tutorials. Everytime you find a cool tool on the web, just post it to the list. Everytime youre looking for a tool to help you with your development problems come visit the list. Easy enough.\n\n### Tech facts\n\nWebdevlists frontend is built with [Angular2](https://angular.io/ \"Angular2\"), which just has had its first final release. The backend makes use of [LoopbackJS](http://loopback.io/ \"LoopbackJS\"), which is a mighty framework to build REST APIs with a minimum of boilerplate code. Additionally the site is powered by HTTP/2.0 to get some more speed.\n\n### Is it finished, yet?\n\nNo, it isnt. Actually, it probably never will be. Im continuously going to add new technology to Webdevlists stack and change and refactor things. Currently Im considering to switch to [GraphQL](http://graphql.org/learn/ \"GraphQL\"). Right now, Webdevlist is in some kind of beta state, meaning that it still might have bugs.\n\nId really appreciate to get feedback on this project!\n\n[>> Webdevlist.net](https://webdevlist.net)  \n[>> Webdevlist on GitHub](https://github.com/muety/webdevlist.net) ","slug":"webdevlistnet-the-developers-resource-collection","published":1,"updated":"2020-10-30T20:05:40.286Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqmm0015o2e0aobwa0go","content":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/webdevlist.jpg\"></p>\n<p>I just put up a new project of mine, which is called Webdevlist.</p>\n<h3 id=\"So-what-is-Webdevlist\"><a href=\"#So-what-is-Webdevlist\" class=\"headerlink\" title=\"So what is Webdevlist?\"></a>So what is Webdevlist?</h3><p>Im pretty sure most developers know it all too well: you browse Twitter, Reddit or Stackoverflow on a late Sunday evening and come across some cool programming framework or webservice. You think like <em>Wow thats cool! Could be helpful some time. I need to remember it.</em> Then you put the link anywhere in a messy text file or the like and never visit it again. Webdevlists aim is to become a comprehensive, community-maintained collection of useful resources concerning software development. Such could include frameworks and libraries for any language, devtools, software applications, apps, webservices like PaaS, SaaS or IaaS and also learning resources like guides and tutorials. Everytime you find a cool tool on the web, just post it to the list. Everytime youre looking for a tool to help you with your development problems come visit the list. Easy enough.</p>\n<h3 id=\"Tech-facts\"><a href=\"#Tech-facts\" class=\"headerlink\" title=\"Tech facts\"></a>Tech facts</h3><p>Webdevlists frontend is built with <a href=\"https://angular.io/\" title=\"Angular2\">Angular2</a>, which just has had its first final release. The backend makes use of <a href=\"http://loopback.io/\" title=\"LoopbackJS\">LoopbackJS</a>, which is a mighty framework to build REST APIs with a minimum of boilerplate code. Additionally the site is powered by HTTP/2.0 to get some more speed.</p>\n<h3 id=\"Is-it-finished-yet\"><a href=\"#Is-it-finished-yet\" class=\"headerlink\" title=\"Is it finished, yet?\"></a>Is it finished, yet?</h3><p>No, it isnt. Actually, it probably never will be. Im continuously going to add new technology to Webdevlists stack and change and refactor things. Currently Im considering to switch to <a href=\"http://graphql.org/learn/\" title=\"GraphQL\">GraphQL</a>. Right now, Webdevlist is in some kind of beta state, meaning that it still might have bugs.</p>\n<p>Id really appreciate to get feedback on this project!</p>\n<p><a href=\"https://webdevlist.net/\">&gt;&gt; Webdevlist.net</a><br><a href=\"https://github.com/muety/webdevlist.net\">&gt;&gt; Webdevlist on GitHub</a> </p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/webdevlist.jpg\"></p>\n<p>I just put up a new project of mine, which is called Webdevlist.</p>\n<h3 id=\"So-what-is-Webdevlist\"><a href=\"#So-what-is-Webdevlist\" class=\"headerlink\" title=\"So what is Webdevlist?\"></a>So what is Webdevlist?</h3><p>Im pretty sure most developers know it all too well: you browse Twitter, Reddit or Stackoverflow on a late Sunday evening and come across some cool programming framework or webservice. You think like <em>Wow thats cool! Could be helpful some time. I need to remember it.</em> Then you put the link anywhere in a messy text file or the like and never visit it again. Webdevlists aim is to become a comprehensive, community-maintained collection of useful resources concerning software development. Such could include frameworks and libraries for any language, devtools, software applications, apps, webservices like PaaS, SaaS or IaaS and also learning resources like guides and tutorials. Everytime you find a cool tool on the web, just post it to the list. Everytime youre looking for a tool to help you with your development problems come visit the list. Easy enough.</p>\n<h3 id=\"Tech-facts\"><a href=\"#Tech-facts\" class=\"headerlink\" title=\"Tech facts\"></a>Tech facts</h3><p>Webdevlists frontend is built with <a href=\"https://angular.io/\" title=\"Angular2\">Angular2</a>, which just has had its first final release. The backend makes use of <a href=\"http://loopback.io/\" title=\"LoopbackJS\">LoopbackJS</a>, which is a mighty framework to build REST APIs with a minimum of boilerplate code. Additionally the site is powered by HTTP/2.0 to get some more speed.</p>\n<h3 id=\"Is-it-finished-yet\"><a href=\"#Is-it-finished-yet\" class=\"headerlink\" title=\"Is it finished, yet?\"></a>Is it finished, yet?</h3><p>No, it isnt. Actually, it probably never will be. Im continuously going to add new technology to Webdevlists stack and change and refactor things. Currently Im considering to switch to <a href=\"http://graphql.org/learn/\" title=\"GraphQL\">GraphQL</a>. Right now, Webdevlist is in some kind of beta state, meaning that it still might have bugs.</p>\n<p>Id really appreciate to get feedback on this project!</p>\n<p><a href=\"https://webdevlist.net/\">&gt;&gt; Webdevlist.net</a><br><a href=\"https://github.com/muety/webdevlist.net\">&gt;&gt; Webdevlist on GitHub</a> </p>\n"},{"title":"What I like about developing apps with Flutter","date":"2019-03-11T07:45:20.000Z","_content":"\nAfter hearing about [Flutter](https://flutter.dev) by [Matt Carroll](https://twitter.com/@flttry) and [Abraham Williams](https://twitter.com/abraham) at the [SFHTML5 Meetup](https://www.meetup.com/de-DE/sfhtml5/events/256523273/) hosted by Google in San Francisco a few weeks ago, I decided to give it a try. I developed a small [bookmark manager](https://github.com/muety/anchr-android) app for Android while attempting to learn Flutter.\n\n# What is Flutter?\nFlutter is an open-source framework for developing cross-platform mobile apps. Or in their own words it ...\n> [...] allows you to build beautiful native apps on iOS and Android from a single codebase. \n\n\"Cross-platform\" means that you develop an app that can run on multiple platforms, in this case, Android, iOS and (in the future) also [Fuchsia](https://en.wikipedia.org/wiki/Google_Fuchsia). Traditionally, you would have to write the same app multiple times, e.g. once in Java for Android and a second time in Swift for iOS. This is a pain for developers, obviously, and Flutter tries to overcome it. \n\nThere are already several approaches out there, trying to solve cross-platform mobile development. They include **hybrid** app frameworks like [Ionic](https://ionicframework.com/) and [Cordova](https://cordova.apache.org/) as well as **native cross-platform** frameworks like [React Native](https://facebook.github.io/react-native/), [NativeScript](https://www.nativescript.org/) and [Xamarin](https://visualstudio.microsoft.com/xamarin/).\n\nBy the way, there is is a very interesting comparison of [Ionic vs. React Native](https://www.codementor.io/fmcorz/react-native-vs-ionic-du1087rsw). \n\nWhile hybrid frameworks basically render a website to the device's screen and native frameworks translate TypeScript (or C#) code into native iOS / Android components, Flutter [\"works more like a game engine\"](https://buildflutter.com/how-flutter-works/). It has an extremely efficient engine that layouts widgets and renders them to a canvas. Therefore it usually a lot faster and less laggy than hybrid frameworks. \n\nUnlike most other cross-platform frameworks, where you usually program in JavaScript (or C# in the case of Xamarin), Flutter relies on [Dart](https://dartlang.org), created by Google. Consequently, in order to learn Flutter, you would usually not only have to learn the framework itself, but also a new programming language. This may seem like a high entry barrier, but it actually is not.\n\n# Pros\n## What I **DO** like about Dart\n1. **It looks familiar.** Dart really just feels like Java and JavaScript having a baby. It does not follow any exotic paradigms. Instead, if you know Java and JavaScript, learning Dart is quite easy since there are only very few new concepts and syntaxes. \nFor instance, it is **object-oriented** and has a proper inheritance system, similar to Java. In addition, it has a **static typing system**, including generics, but with **optional type declarations**, similar to Scala.\nJavaScript people will find well-known concepts as well, such as **futures, generators and `async / await`**.\nSimilar to Scala's _traits_, Dart even supports a kind of **multiple class inheritance** using _mixins_, which is quite handy once you get used to it. Having a Java background another cool new thing in Dart is **factory constructors**.\n2. **It feels consistent.** What I did not like about TypeScript is that sometimes it just feels a bit hacky, e.g. when third-party typings for a certain library are buggy or even missing. In my opinion, this originates in TypeScript's approach of trying to extend an existing programming language with whole new functionality. While TypeScripts is still a great language, Dart just feels more consistent in its whole. It was designed completely from scratch with all the above concepts in mind and since it is quite young, it still feels clean and minimalistic. However, since it is not that widely used, yet, the ecosystem (including tooling, libraries, ...) is way smaller compared to JavaScript or native Android / iOS.\n3. **It is multi-purpose.** Dart can not only be used for mobile development but is quite generic. You can use it to write your backend and there are transpilers to use it in the browser as well. Since frameworks like Angular start to have Dart support (see [AngularDart](https://webdev.dartlang.org/angular/)), I feel like it could [gain popularity](https://medium.com/@mswehli/why-dart-is-the-language-to-learn-of-2018-e5fa12adb6c1) in the next years. \n4. **It has named parameters .** They exist in many programming languages, including Python and Scala and I really got used to them. In my opinion, code gets a lot cleaner with named parameters. \n\n## What I **DO** like about Flutter\n1. **Easy layouting.** Dart follows the paradigm that _\"everything is a widget\"_. So instead of defining layouts with XML (like in Android) or with HTML + CSS (like in the web), everything is done programmatically. The way you build components is by recursively nesting widgets into other widgets. Such might include UI components like a `TextView` and `FloatingActionButton` or more abstract things like a `GestureDetector` or `Padding`. The structure is always the same and that makes UI composition easy to me.\n2. **Tons of built-in components.** Flutter comes with a giant [catalog of widgets](https://flutter.dev/docs/development/ui/widgets) for layouting, styling, animations and UI components (all of which perfectly follow Material design). So far I could find everything I needed in the widget catalog without having to use any poorly-maintained third-party libs.\n3. **It feels so real.** Often times, hybrid apps still do not completely feel like a real app, even if they are using Material design libraries etc. Maybe the navigation drawer's sliding animation is too rough, a page transition is slightly laggy or the text style just does not look quite right. At least this is what I experienced with Ionic. With Flutter, however, I could not tell the difference compared to a truly native app. Everything is super fast, smooth and pixel-perfect. \n4. **It is future-proof.** Google is pushing Flutter really hard as they keep posting blog articles and developer videos about it. I feel like they really want people to adopt it, which might relate to their development on Fuchsia. No matter whether or not they are planning to [replace Android with Fuchsia](https://www.reddit.com/r/androiddev/comments/6aga8e/in_your_opinion_will_google_fuchsia_replace/) some day, Flutter is definitely a good thing to know.\n5. **Good tooling.** Although the ecosystem around Flutter is not that big, yet, the built-in tooling is great. Flutter comes with an intuitive **CLI** and is well integrated with my favorite editors IntelliJ (**Android Studio**) and **Visual Studio Code**. \n6. **Open-Source.** I love open-source, so I am glad that Flutter is completely open to the community and available on [GitHub](https://github.com/flutter/flutter). \n\n# Cons\n\n## What I **DO NOT** like about Dart\n1. **No functional APIs.** I do not expect a functional programming language like Scala, but since I really got used to Java 8's stream API, I am a bit sad that Dart has almost no functional APIs. In my opinion, code gets a lot cleaner with such.\n\n## What I **DO NOT** like about Flutter\n1. **Is it truly cross-platform?** This is rather a question than something I strictly do not like. So far, I only developed an Android app with Flutter and I wonder whether it is actually possible to use the exact same code for iOS. Sometimes you will still need to access native APIs, e.g. when attempting to [receive a sharing intent](https://muetsch.io/how-to-receive-sharing-intents-in-flutter.html), or have varying design elements. So although I am not totally sure about this, I would assume that for large projects you would still need to write two separate Flutter apps for Android and iOS, but have the ability to reuse large parts of the code in form of a shared library. \n2. **Apps are large * .** Maybe this will be improved in the future, but right now Flutter apps are pretty large. My [simple bookmark manager](https://github.com/muety/anchr-android) is 46 MB in size when installed.\n\n# Where to get started?\nIf you want to start learning Flutter, I would recommend the following. \n1. Take the [Tour of Dart](https://www.dartlang.org/guides/language/language-tour) and get familiar with Dart's syntax and concepts.\n2. Walk through [A month of Flutter](https://bendyworks.com/blog/a-month-of-flutter) to start building a real-world Flutter app step-by-step.\n\nHave fun and happy coding! \n\n***** As it turned out, this is actually not a valid point of criticism, since I had generated the app in debug mode. When building a release, the total size is only 6.4 MB.","source":"_posts/what-i-like-about-developing-apps-with-flutter.md","raw":"---\ntitle: What I like about developing apps with Flutter\ndate: 2019-03-11 08:45:20\ntags:\n---\n\nAfter hearing about [Flutter](https://flutter.dev) by [Matt Carroll](https://twitter.com/@flttry) and [Abraham Williams](https://twitter.com/abraham) at the [SFHTML5 Meetup](https://www.meetup.com/de-DE/sfhtml5/events/256523273/) hosted by Google in San Francisco a few weeks ago, I decided to give it a try. I developed a small [bookmark manager](https://github.com/muety/anchr-android) app for Android while attempting to learn Flutter.\n\n# What is Flutter?\nFlutter is an open-source framework for developing cross-platform mobile apps. Or in their own words it ...\n> [...] allows you to build beautiful native apps on iOS and Android from a single codebase. \n\n\"Cross-platform\" means that you develop an app that can run on multiple platforms, in this case, Android, iOS and (in the future) also [Fuchsia](https://en.wikipedia.org/wiki/Google_Fuchsia). Traditionally, you would have to write the same app multiple times, e.g. once in Java for Android and a second time in Swift for iOS. This is a pain for developers, obviously, and Flutter tries to overcome it. \n\nThere are already several approaches out there, trying to solve cross-platform mobile development. They include **hybrid** app frameworks like [Ionic](https://ionicframework.com/) and [Cordova](https://cordova.apache.org/) as well as **native cross-platform** frameworks like [React Native](https://facebook.github.io/react-native/), [NativeScript](https://www.nativescript.org/) and [Xamarin](https://visualstudio.microsoft.com/xamarin/).\n\nBy the way, there is is a very interesting comparison of [Ionic vs. React Native](https://www.codementor.io/fmcorz/react-native-vs-ionic-du1087rsw). \n\nWhile hybrid frameworks basically render a website to the device's screen and native frameworks translate TypeScript (or C#) code into native iOS / Android components, Flutter [\"works more like a game engine\"](https://buildflutter.com/how-flutter-works/). It has an extremely efficient engine that layouts widgets and renders them to a canvas. Therefore it usually a lot faster and less laggy than hybrid frameworks. \n\nUnlike most other cross-platform frameworks, where you usually program in JavaScript (or C# in the case of Xamarin), Flutter relies on [Dart](https://dartlang.org), created by Google. Consequently, in order to learn Flutter, you would usually not only have to learn the framework itself, but also a new programming language. This may seem like a high entry barrier, but it actually is not.\n\n# Pros\n## What I **DO** like about Dart\n1. **It looks familiar.** Dart really just feels like Java and JavaScript having a baby. It does not follow any exotic paradigms. Instead, if you know Java and JavaScript, learning Dart is quite easy since there are only very few new concepts and syntaxes. \nFor instance, it is **object-oriented** and has a proper inheritance system, similar to Java. In addition, it has a **static typing system**, including generics, but with **optional type declarations**, similar to Scala.\nJavaScript people will find well-known concepts as well, such as **futures, generators and `async / await`**.\nSimilar to Scala's _traits_, Dart even supports a kind of **multiple class inheritance** using _mixins_, which is quite handy once you get used to it. Having a Java background another cool new thing in Dart is **factory constructors**.\n2. **It feels consistent.** What I did not like about TypeScript is that sometimes it just feels a bit hacky, e.g. when third-party typings for a certain library are buggy or even missing. In my opinion, this originates in TypeScript's approach of trying to extend an existing programming language with whole new functionality. While TypeScripts is still a great language, Dart just feels more consistent in its whole. It was designed completely from scratch with all the above concepts in mind and since it is quite young, it still feels clean and minimalistic. However, since it is not that widely used, yet, the ecosystem (including tooling, libraries, ...) is way smaller compared to JavaScript or native Android / iOS.\n3. **It is multi-purpose.** Dart can not only be used for mobile development but is quite generic. You can use it to write your backend and there are transpilers to use it in the browser as well. Since frameworks like Angular start to have Dart support (see [AngularDart](https://webdev.dartlang.org/angular/)), I feel like it could [gain popularity](https://medium.com/@mswehli/why-dart-is-the-language-to-learn-of-2018-e5fa12adb6c1) in the next years. \n4. **It has named parameters .** They exist in many programming languages, including Python and Scala and I really got used to them. In my opinion, code gets a lot cleaner with named parameters. \n\n## What I **DO** like about Flutter\n1. **Easy layouting.** Dart follows the paradigm that _\"everything is a widget\"_. So instead of defining layouts with XML (like in Android) or with HTML + CSS (like in the web), everything is done programmatically. The way you build components is by recursively nesting widgets into other widgets. Such might include UI components like a `TextView` and `FloatingActionButton` or more abstract things like a `GestureDetector` or `Padding`. The structure is always the same and that makes UI composition easy to me.\n2. **Tons of built-in components.** Flutter comes with a giant [catalog of widgets](https://flutter.dev/docs/development/ui/widgets) for layouting, styling, animations and UI components (all of which perfectly follow Material design). So far I could find everything I needed in the widget catalog without having to use any poorly-maintained third-party libs.\n3. **It feels so real.** Often times, hybrid apps still do not completely feel like a real app, even if they are using Material design libraries etc. Maybe the navigation drawer's sliding animation is too rough, a page transition is slightly laggy or the text style just does not look quite right. At least this is what I experienced with Ionic. With Flutter, however, I could not tell the difference compared to a truly native app. Everything is super fast, smooth and pixel-perfect. \n4. **It is future-proof.** Google is pushing Flutter really hard as they keep posting blog articles and developer videos about it. I feel like they really want people to adopt it, which might relate to their development on Fuchsia. No matter whether or not they are planning to [replace Android with Fuchsia](https://www.reddit.com/r/androiddev/comments/6aga8e/in_your_opinion_will_google_fuchsia_replace/) some day, Flutter is definitely a good thing to know.\n5. **Good tooling.** Although the ecosystem around Flutter is not that big, yet, the built-in tooling is great. Flutter comes with an intuitive **CLI** and is well integrated with my favorite editors IntelliJ (**Android Studio**) and **Visual Studio Code**. \n6. **Open-Source.** I love open-source, so I am glad that Flutter is completely open to the community and available on [GitHub](https://github.com/flutter/flutter). \n\n# Cons\n\n## What I **DO NOT** like about Dart\n1. **No functional APIs.** I do not expect a functional programming language like Scala, but since I really got used to Java 8's stream API, I am a bit sad that Dart has almost no functional APIs. In my opinion, code gets a lot cleaner with such.\n\n## What I **DO NOT** like about Flutter\n1. **Is it truly cross-platform?** This is rather a question than something I strictly do not like. So far, I only developed an Android app with Flutter and I wonder whether it is actually possible to use the exact same code for iOS. Sometimes you will still need to access native APIs, e.g. when attempting to [receive a sharing intent](https://muetsch.io/how-to-receive-sharing-intents-in-flutter.html), or have varying design elements. So although I am not totally sure about this, I would assume that for large projects you would still need to write two separate Flutter apps for Android and iOS, but have the ability to reuse large parts of the code in form of a shared library. \n2. **Apps are large * .** Maybe this will be improved in the future, but right now Flutter apps are pretty large. My [simple bookmark manager](https://github.com/muety/anchr-android) is 46 MB in size when installed.\n\n# Where to get started?\nIf you want to start learning Flutter, I would recommend the following. \n1. Take the [Tour of Dart](https://www.dartlang.org/guides/language/language-tour) and get familiar with Dart's syntax and concepts.\n2. Walk through [A month of Flutter](https://bendyworks.com/blog/a-month-of-flutter) to start building a real-world Flutter app step-by-step.\n\nHave fun and happy coding! \n\n***** As it turned out, this is actually not a valid point of criticism, since I had generated the app in debug mode. When building a release, the total size is only 6.4 MB.","slug":"what-i-like-about-developing-apps-with-flutter","published":1,"updated":"2020-10-30T20:05:40.290Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqmn0016o2e01rmc0gyi","content":"<p>After hearing about <a href=\"https://flutter.dev/\">Flutter</a> by <a href=\"https://twitter.com/@flttry\">Matt Carroll</a> and <a href=\"https://twitter.com/abraham\">Abraham Williams</a> at the <a href=\"https://www.meetup.com/de-DE/sfhtml5/events/256523273/\">SFHTML5 Meetup</a> hosted by Google in San Francisco a few weeks ago, I decided to give it a try. I developed a small <a href=\"https://github.com/muety/anchr-android\">bookmark manager</a> app for Android while attempting to learn Flutter.</p>\n<h1 id=\"What-is-Flutter\"><a href=\"#What-is-Flutter\" class=\"headerlink\" title=\"What is Flutter?\"></a>What is Flutter?</h1><p>Flutter is an open-source framework for developing cross-platform mobile apps. Or in their own words it </p>\n<blockquote>\n<p>[] allows you to build beautiful native apps on iOS and Android from a single codebase. </p>\n</blockquote>\n<p>Cross-platform means that you develop an app that can run on multiple platforms, in this case, Android, iOS and (in the future) also <a href=\"https://en.wikipedia.org/wiki/Google_Fuchsia\">Fuchsia</a>. Traditionally, you would have to write the same app multiple times, e.g. once in Java for Android and a second time in Swift for iOS. This is a pain for developers, obviously, and Flutter tries to overcome it. </p>\n<p>There are already several approaches out there, trying to solve cross-platform mobile development. They include <strong>hybrid</strong> app frameworks like <a href=\"https://ionicframework.com/\">Ionic</a> and <a href=\"https://cordova.apache.org/\">Cordova</a> as well as <strong>native cross-platform</strong> frameworks like <a href=\"https://facebook.github.io/react-native/\">React Native</a>, <a href=\"https://www.nativescript.org/\">NativeScript</a> and <a href=\"https://visualstudio.microsoft.com/xamarin/\">Xamarin</a>.</p>\n<p>By the way, there is is a very interesting comparison of <a href=\"https://www.codementor.io/fmcorz/react-native-vs-ionic-du1087rsw\">Ionic vs. React Native</a>. </p>\n<p>While hybrid frameworks basically render a website to the devices screen and native frameworks translate TypeScript (or C#) code into native iOS / Android components, Flutter <a href=\"https://buildflutter.com/how-flutter-works/\">works more like a game engine</a>. It has an extremely efficient engine that layouts widgets and renders them to a canvas. Therefore it usually a lot faster and less laggy than hybrid frameworks. </p>\n<p>Unlike most other cross-platform frameworks, where you usually program in JavaScript (or C# in the case of Xamarin), Flutter relies on <a href=\"https://dartlang.org/\">Dart</a>, created by Google. Consequently, in order to learn Flutter, you would usually not only have to learn the framework itself, but also a new programming language. This may seem like a high entry barrier, but it actually is not.</p>\n<h1 id=\"Pros\"><a href=\"#Pros\" class=\"headerlink\" title=\"Pros\"></a>Pros</h1><h2 id=\"What-I-DO-like-about-Dart\"><a href=\"#What-I-DO-like-about-Dart\" class=\"headerlink\" title=\"What I DO like about Dart\"></a>What I <strong>DO</strong> like about Dart</h2><ol>\n<li><strong>It looks familiar.</strong> Dart really just feels like Java and JavaScript having a baby. It does not follow any exotic paradigms. Instead, if you know Java and JavaScript, learning Dart is quite easy since there are only very few new concepts and syntaxes.<br>For instance, it is <strong>object-oriented</strong> and has a proper inheritance system, similar to Java. In addition, it has a <strong>static typing system</strong>, including generics, but with <strong>optional type declarations</strong>, similar to Scala.<br>JavaScript people will find well-known concepts as well, such as <strong>futures, generators and <code>async / await</code></strong>.<br>Similar to Scalas <em>traits</em>, Dart even supports a kind of <strong>multiple class inheritance</strong> using <em>mixins</em>, which is quite handy once you get used to it. Having a Java background another cool new thing in Dart is <strong>factory constructors</strong>.</li>\n<li><strong>It feels consistent.</strong> What I did not like about TypeScript is that sometimes it just feels a bit hacky, e.g. when third-party typings for a certain library are buggy or even missing. In my opinion, this originates in TypeScripts approach of trying to extend an existing programming language with whole new functionality. While TypeScripts is still a great language, Dart just feels more consistent in its whole. It was designed completely from scratch with all the above concepts in mind and since it is quite young, it still feels clean and minimalistic. However, since it is not that widely used, yet, the ecosystem (including tooling, libraries, ) is way smaller compared to JavaScript or native Android / iOS.</li>\n<li><strong>It is multi-purpose.</strong> Dart can not only be used for mobile development but is quite generic. You can use it to write your backend and there are transpilers to use it in the browser as well. Since frameworks like Angular start to have Dart support (see <a href=\"https://webdev.dartlang.org/angular/\">AngularDart</a>), I feel like it could <a href=\"https://medium.com/@mswehli/why-dart-is-the-language-to-learn-of-2018-e5fa12adb6c1\">gain popularity</a> in the next years. </li>\n<li><strong>It has named parameters .</strong> They exist in many programming languages, including Python and Scala and I really got used to them. In my opinion, code gets a lot cleaner with named parameters. </li>\n</ol>\n<h2 id=\"What-I-DO-like-about-Flutter\"><a href=\"#What-I-DO-like-about-Flutter\" class=\"headerlink\" title=\"What I DO like about Flutter\"></a>What I <strong>DO</strong> like about Flutter</h2><ol>\n<li><strong>Easy layouting.</strong> Dart follows the paradigm that <em>everything is a widget</em>. So instead of defining layouts with XML (like in Android) or with HTML + CSS (like in the web), everything is done programmatically. The way you build components is by recursively nesting widgets into other widgets. Such might include UI components like a <code>TextView</code> and <code>FloatingActionButton</code> or more abstract things like a <code>GestureDetector</code> or <code>Padding</code>. The structure is always the same and that makes UI composition easy to me.</li>\n<li><strong>Tons of built-in components.</strong> Flutter comes with a giant <a href=\"https://flutter.dev/docs/development/ui/widgets\">catalog of widgets</a> for layouting, styling, animations and UI components (all of which perfectly follow Material design). So far I could find everything I needed in the widget catalog without having to use any poorly-maintained third-party libs.</li>\n<li><strong>It feels so real.</strong> Often times, hybrid apps still do not completely feel like a real app, even if they are using Material design libraries etc. Maybe the navigation drawers sliding animation is too rough, a page transition is slightly laggy or the text style just does not look quite right. At least this is what I experienced with Ionic. With Flutter, however, I could not tell the difference compared to a truly native app. Everything is super fast, smooth and pixel-perfect. </li>\n<li><strong>It is future-proof.</strong> Google is pushing Flutter really hard as they keep posting blog articles and developer videos about it. I feel like they really want people to adopt it, which might relate to their development on Fuchsia. No matter whether or not they are planning to <a href=\"https://www.reddit.com/r/androiddev/comments/6aga8e/in_your_opinion_will_google_fuchsia_replace/\">replace Android with Fuchsia</a> some day, Flutter is definitely a good thing to know.</li>\n<li><strong>Good tooling.</strong> Although the ecosystem around Flutter is not that big, yet, the built-in tooling is great. Flutter comes with an intuitive <strong>CLI</strong> and is well integrated with my favorite editors IntelliJ (<strong>Android Studio</strong>) and <strong>Visual Studio Code</strong>. </li>\n<li><strong>Open-Source.</strong> I love open-source, so I am glad that Flutter is completely open to the community and available on <a href=\"https://github.com/flutter/flutter\">GitHub</a>. </li>\n</ol>\n<h1 id=\"Cons\"><a href=\"#Cons\" class=\"headerlink\" title=\"Cons\"></a>Cons</h1><h2 id=\"What-I-DO-NOT-like-about-Dart\"><a href=\"#What-I-DO-NOT-like-about-Dart\" class=\"headerlink\" title=\"What I DO NOT like about Dart\"></a>What I <strong>DO NOT</strong> like about Dart</h2><ol>\n<li><strong>No functional APIs.</strong> I do not expect a functional programming language like Scala, but since I really got used to Java 8s stream API, I am a bit sad that Dart has almost no functional APIs. In my opinion, code gets a lot cleaner with such.</li>\n</ol>\n<h2 id=\"What-I-DO-NOT-like-about-Flutter\"><a href=\"#What-I-DO-NOT-like-about-Flutter\" class=\"headerlink\" title=\"What I DO NOT like about Flutter\"></a>What I <strong>DO NOT</strong> like about Flutter</h2><ol>\n<li><strong>Is it truly cross-platform?</strong> This is rather a question than something I strictly do not like. So far, I only developed an Android app with Flutter and I wonder whether it is actually possible to use the exact same code for iOS. Sometimes you will still need to access native APIs, e.g. when attempting to <a href=\"https://muetsch.io/how-to-receive-sharing-intents-in-flutter.html\">receive a sharing intent</a>, or have varying design elements. So although I am not totally sure about this, I would assume that for large projects you would still need to write two separate Flutter apps for Android and iOS, but have the ability to reuse large parts of the code in form of a shared library. </li>\n<li>*<em>Apps are large * .*</em> Maybe this will be improved in the future, but right now Flutter apps are pretty large. My <a href=\"https://github.com/muety/anchr-android\">simple bookmark manager</a> is 46 MB in size when installed.</li>\n</ol>\n<h1 id=\"Where-to-get-started\"><a href=\"#Where-to-get-started\" class=\"headerlink\" title=\"Where to get started?\"></a>Where to get started?</h1><p>If you want to start learning Flutter, I would recommend the following. </p>\n<ol>\n<li>Take the <a href=\"https://www.dartlang.org/guides/language/language-tour\">Tour of Dart</a> and get familiar with Darts syntax and concepts.</li>\n<li>Walk through <a href=\"https://bendyworks.com/blog/a-month-of-flutter\">A month of Flutter</a> to start building a real-world Flutter app step-by-step.</li>\n</ol>\n<p>Have fun and happy coding! </p>\n<p>***** As it turned out, this is actually not a valid point of criticism, since I had generated the app in debug mode. When building a release, the total size is only 6.4 MB.</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>After hearing about <a href=\"https://flutter.dev/\">Flutter</a> by <a href=\"https://twitter.com/@flttry\">Matt Carroll</a> and <a href=\"https://twitter.com/abraham\">Abraham Williams</a> at the <a href=\"https://www.meetup.com/de-DE/sfhtml5/events/256523273/\">SFHTML5 Meetup</a> hosted by Google in San Francisco a few weeks ago, I decided to give it a try. I developed a small <a href=\"https://github.com/muety/anchr-android\">bookmark manager</a> app for Android while attempting to learn Flutter.</p>\n<h1 id=\"What-is-Flutter\"><a href=\"#What-is-Flutter\" class=\"headerlink\" title=\"What is Flutter?\"></a>What is Flutter?</h1><p>Flutter is an open-source framework for developing cross-platform mobile apps. Or in their own words it </p>\n<blockquote>\n<p>[] allows you to build beautiful native apps on iOS and Android from a single codebase. </p>\n</blockquote>\n<p>Cross-platform means that you develop an app that can run on multiple platforms, in this case, Android, iOS and (in the future) also <a href=\"https://en.wikipedia.org/wiki/Google_Fuchsia\">Fuchsia</a>. Traditionally, you would have to write the same app multiple times, e.g. once in Java for Android and a second time in Swift for iOS. This is a pain for developers, obviously, and Flutter tries to overcome it. </p>\n<p>There are already several approaches out there, trying to solve cross-platform mobile development. They include <strong>hybrid</strong> app frameworks like <a href=\"https://ionicframework.com/\">Ionic</a> and <a href=\"https://cordova.apache.org/\">Cordova</a> as well as <strong>native cross-platform</strong> frameworks like <a href=\"https://facebook.github.io/react-native/\">React Native</a>, <a href=\"https://www.nativescript.org/\">NativeScript</a> and <a href=\"https://visualstudio.microsoft.com/xamarin/\">Xamarin</a>.</p>\n<p>By the way, there is is a very interesting comparison of <a href=\"https://www.codementor.io/fmcorz/react-native-vs-ionic-du1087rsw\">Ionic vs. React Native</a>. </p>\n<p>While hybrid frameworks basically render a website to the devices screen and native frameworks translate TypeScript (or C#) code into native iOS / Android components, Flutter <a href=\"https://buildflutter.com/how-flutter-works/\">works more like a game engine</a>. It has an extremely efficient engine that layouts widgets and renders them to a canvas. Therefore it usually a lot faster and less laggy than hybrid frameworks. </p>\n<p>Unlike most other cross-platform frameworks, where you usually program in JavaScript (or C# in the case of Xamarin), Flutter relies on <a href=\"https://dartlang.org/\">Dart</a>, created by Google. Consequently, in order to learn Flutter, you would usually not only have to learn the framework itself, but also a new programming language. This may seem like a high entry barrier, but it actually is not.</p>\n<h1 id=\"Pros\"><a href=\"#Pros\" class=\"headerlink\" title=\"Pros\"></a>Pros</h1><h2 id=\"What-I-DO-like-about-Dart\"><a href=\"#What-I-DO-like-about-Dart\" class=\"headerlink\" title=\"What I DO like about Dart\"></a>What I <strong>DO</strong> like about Dart</h2><ol>\n<li><strong>It looks familiar.</strong> Dart really just feels like Java and JavaScript having a baby. It does not follow any exotic paradigms. Instead, if you know Java and JavaScript, learning Dart is quite easy since there are only very few new concepts and syntaxes.<br>For instance, it is <strong>object-oriented</strong> and has a proper inheritance system, similar to Java. In addition, it has a <strong>static typing system</strong>, including generics, but with <strong>optional type declarations</strong>, similar to Scala.<br>JavaScript people will find well-known concepts as well, such as <strong>futures, generators and <code>async / await</code></strong>.<br>Similar to Scalas <em>traits</em>, Dart even supports a kind of <strong>multiple class inheritance</strong> using <em>mixins</em>, which is quite handy once you get used to it. Having a Java background another cool new thing in Dart is <strong>factory constructors</strong>.</li>\n<li><strong>It feels consistent.</strong> What I did not like about TypeScript is that sometimes it just feels a bit hacky, e.g. when third-party typings for a certain library are buggy or even missing. In my opinion, this originates in TypeScripts approach of trying to extend an existing programming language with whole new functionality. While TypeScripts is still a great language, Dart just feels more consistent in its whole. It was designed completely from scratch with all the above concepts in mind and since it is quite young, it still feels clean and minimalistic. However, since it is not that widely used, yet, the ecosystem (including tooling, libraries, ) is way smaller compared to JavaScript or native Android / iOS.</li>\n<li><strong>It is multi-purpose.</strong> Dart can not only be used for mobile development but is quite generic. You can use it to write your backend and there are transpilers to use it in the browser as well. Since frameworks like Angular start to have Dart support (see <a href=\"https://webdev.dartlang.org/angular/\">AngularDart</a>), I feel like it could <a href=\"https://medium.com/@mswehli/why-dart-is-the-language-to-learn-of-2018-e5fa12adb6c1\">gain popularity</a> in the next years. </li>\n<li><strong>It has named parameters .</strong> They exist in many programming languages, including Python and Scala and I really got used to them. In my opinion, code gets a lot cleaner with named parameters. </li>\n</ol>\n<h2 id=\"What-I-DO-like-about-Flutter\"><a href=\"#What-I-DO-like-about-Flutter\" class=\"headerlink\" title=\"What I DO like about Flutter\"></a>What I <strong>DO</strong> like about Flutter</h2><ol>\n<li><strong>Easy layouting.</strong> Dart follows the paradigm that <em>everything is a widget</em>. So instead of defining layouts with XML (like in Android) or with HTML + CSS (like in the web), everything is done programmatically. The way you build components is by recursively nesting widgets into other widgets. Such might include UI components like a <code>TextView</code> and <code>FloatingActionButton</code> or more abstract things like a <code>GestureDetector</code> or <code>Padding</code>. The structure is always the same and that makes UI composition easy to me.</li>\n<li><strong>Tons of built-in components.</strong> Flutter comes with a giant <a href=\"https://flutter.dev/docs/development/ui/widgets\">catalog of widgets</a> for layouting, styling, animations and UI components (all of which perfectly follow Material design). So far I could find everything I needed in the widget catalog without having to use any poorly-maintained third-party libs.</li>\n<li><strong>It feels so real.</strong> Often times, hybrid apps still do not completely feel like a real app, even if they are using Material design libraries etc. Maybe the navigation drawers sliding animation is too rough, a page transition is slightly laggy or the text style just does not look quite right. At least this is what I experienced with Ionic. With Flutter, however, I could not tell the difference compared to a truly native app. Everything is super fast, smooth and pixel-perfect. </li>\n<li><strong>It is future-proof.</strong> Google is pushing Flutter really hard as they keep posting blog articles and developer videos about it. I feel like they really want people to adopt it, which might relate to their development on Fuchsia. No matter whether or not they are planning to <a href=\"https://www.reddit.com/r/androiddev/comments/6aga8e/in_your_opinion_will_google_fuchsia_replace/\">replace Android with Fuchsia</a> some day, Flutter is definitely a good thing to know.</li>\n<li><strong>Good tooling.</strong> Although the ecosystem around Flutter is not that big, yet, the built-in tooling is great. Flutter comes with an intuitive <strong>CLI</strong> and is well integrated with my favorite editors IntelliJ (<strong>Android Studio</strong>) and <strong>Visual Studio Code</strong>. </li>\n<li><strong>Open-Source.</strong> I love open-source, so I am glad that Flutter is completely open to the community and available on <a href=\"https://github.com/flutter/flutter\">GitHub</a>. </li>\n</ol>\n<h1 id=\"Cons\"><a href=\"#Cons\" class=\"headerlink\" title=\"Cons\"></a>Cons</h1><h2 id=\"What-I-DO-NOT-like-about-Dart\"><a href=\"#What-I-DO-NOT-like-about-Dart\" class=\"headerlink\" title=\"What I DO NOT like about Dart\"></a>What I <strong>DO NOT</strong> like about Dart</h2><ol>\n<li><strong>No functional APIs.</strong> I do not expect a functional programming language like Scala, but since I really got used to Java 8s stream API, I am a bit sad that Dart has almost no functional APIs. In my opinion, code gets a lot cleaner with such.</li>\n</ol>\n<h2 id=\"What-I-DO-NOT-like-about-Flutter\"><a href=\"#What-I-DO-NOT-like-about-Flutter\" class=\"headerlink\" title=\"What I DO NOT like about Flutter\"></a>What I <strong>DO NOT</strong> like about Flutter</h2><ol>\n<li><strong>Is it truly cross-platform?</strong> This is rather a question than something I strictly do not like. So far, I only developed an Android app with Flutter and I wonder whether it is actually possible to use the exact same code for iOS. Sometimes you will still need to access native APIs, e.g. when attempting to <a href=\"https://muetsch.io/how-to-receive-sharing-intents-in-flutter.html\">receive a sharing intent</a>, or have varying design elements. So although I am not totally sure about this, I would assume that for large projects you would still need to write two separate Flutter apps for Android and iOS, but have the ability to reuse large parts of the code in form of a shared library. </li>\n<li>*<em>Apps are large * .*</em> Maybe this will be improved in the future, but right now Flutter apps are pretty large. My <a href=\"https://github.com/muety/anchr-android\">simple bookmark manager</a> is 46 MB in size when installed.</li>\n</ol>\n<h1 id=\"Where-to-get-started\"><a href=\"#Where-to-get-started\" class=\"headerlink\" title=\"Where to get started?\"></a>Where to get started?</h1><p>If you want to start learning Flutter, I would recommend the following. </p>\n<ol>\n<li>Take the <a href=\"https://www.dartlang.org/guides/language/language-tour\">Tour of Dart</a> and get familiar with Darts syntax and concepts.</li>\n<li>Walk through <a href=\"https://bendyworks.com/blog/a-month-of-flutter\">A month of Flutter</a> to start building a real-world Flutter app step-by-step.</li>\n</ol>\n<p>Have fun and happy coding! </p>\n<p>***** As it turned out, this is actually not a valid point of criticism, since I had generated the app in debug mode. When building a release, the total size is only 6.4 MB.</p>\n"},{"title":"Why RAID 10 is better than RAID 01","date":"2015-11-19T21:43:31.000Z","_content":"\nSince it took me a while to completely understand why a RAID 1+0 configuration should be better than a RAID 0+1 one in terms of failure tolerance I want to put my insights down.  \nFirst you should have a basic understanding of what the first two RAID levels are and what it means to nest them. Very basically at level 0 data gets striped, meaning a datum gets split up into two or more blocks that get stored on a different hard drive each. Goal is to improve read and write performance linearly to the number of disks used, because the fragments dont have to be read sequentially anymore but in parallel. Level 1 is about mirroring a datum on two disks with the goal to improve security. Of course, both levels can be combined  you could either mirror striped data or stripe mirrored data which finally gives both: security and performance. In each case at least four disks are needed, while the half of the disks usually is a mirror. So if you took six disks you would do 3-striping. With eight disks you would do 4-striping and so on. Technically you could have more than one mirror (like doing 2-striping and having a 3-mirror or even more) but its very unusual.  \nThe following diagrams shall illustrate these two ways and are useful for further explanations. In both cases we have a RAID with six disks.\n\n![raid01](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/raid01.png)  \n\n*Disks 4, 5 and 6 are the mirrors of 1, 2 and 3.*\n\n![raid10](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/raid10.png)  \n\n*Disk 2 mirrors 1, 4 mirrors 3 and 6 mirrors 5.*\n\nWe assert that RAID 10 is better in terms of fault tolerance because a total failure (= loss of data) is less likely. In other words if some drives crash in a RAID 01 configuration the chance that those are the right drives for suffering a data loss is higher.  \nFirst of all, both configurations can easily survive the crash of one drive. No matter which drive (see figures above) crashes, we have a second one with the exact same data on it in any case. Potentially both configurations can handle the simultaneous crash of two or even more drives (up to N/2), if theyre the right ones, but in the worst case, the second crash could end up in a total failure. What you need to make clear before understanding how RAID 01 is worse than RAID 10 is that a RAID 0 (sub)system immediately gets unusable if one of its disks goes down. This is apparent: In the upper figure (figure 1 from now on) data is divided up into three strips in both RAID 0 subsystem. So if one of their disks fails (assume a crash of Disc1), the subsystem is broken since the first two parts of a date wont make sense without the third. You would still be in posession of all data, nothing is lost yet, but nevertheless the left RAID 0 subsystem is down. If a second drive fails this should only be 2 or 3 (since the left system in inacessible anyway) to keep the entire system up. Disc 4, 5 or 6 failing would cause a total crash. So the chance of the second failing disk resulting in a total crash is 3/5\\. Now take a look at figure 2\\. The crash of one disk in a RAID 1 (sub)system wont make this subsystem go down because the RAID controller will seamlessly switch to the mirror drive which has exactly identical data. In figure 2 all RAID 0 disks (which actually are stanalone RAID 1 systems again) need to keep running for the entire system to stay up. So theoretically there wouldnt be a problem with disks 1, 3 and 5 (or 2, 4 and 6) could crashing simultaneously. After one disk having failed (assume a crash of Disc1 again) the second one failing could be 3, 4, 5 or 6  all but NOT 2\\. The probability of a total crash is 1/5 (namely Disc2 of the remaining five) now and therefore lower than 3/5 with RAID 01\\. Hope you got it","source":"_posts/why-raid-10-is-better-than-raid-01.md","raw":"---\ntitle: Why RAID 10 is better than RAID 01\ndate: 2015-11-19 22:43:31\ntags:\n---\n\nSince it took me a while to completely understand why a RAID 1+0 configuration should be better than a RAID 0+1 one in terms of failure tolerance I want to put my insights down.  \nFirst you should have a basic understanding of what the first two RAID levels are and what it means to nest them. Very basically at level 0 data gets striped, meaning a datum gets split up into two or more blocks that get stored on a different hard drive each. Goal is to improve read and write performance linearly to the number of disks used, because the fragments dont have to be read sequentially anymore but in parallel. Level 1 is about mirroring a datum on two disks with the goal to improve security. Of course, both levels can be combined  you could either mirror striped data or stripe mirrored data which finally gives both: security and performance. In each case at least four disks are needed, while the half of the disks usually is a mirror. So if you took six disks you would do 3-striping. With eight disks you would do 4-striping and so on. Technically you could have more than one mirror (like doing 2-striping and having a 3-mirror or even more) but its very unusual.  \nThe following diagrams shall illustrate these two ways and are useful for further explanations. In both cases we have a RAID with six disks.\n\n![raid01](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/raid01.png)  \n\n*Disks 4, 5 and 6 are the mirrors of 1, 2 and 3.*\n\n![raid10](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/raid10.png)  \n\n*Disk 2 mirrors 1, 4 mirrors 3 and 6 mirrors 5.*\n\nWe assert that RAID 10 is better in terms of fault tolerance because a total failure (= loss of data) is less likely. In other words if some drives crash in a RAID 01 configuration the chance that those are the right drives for suffering a data loss is higher.  \nFirst of all, both configurations can easily survive the crash of one drive. No matter which drive (see figures above) crashes, we have a second one with the exact same data on it in any case. Potentially both configurations can handle the simultaneous crash of two or even more drives (up to N/2), if theyre the right ones, but in the worst case, the second crash could end up in a total failure. What you need to make clear before understanding how RAID 01 is worse than RAID 10 is that a RAID 0 (sub)system immediately gets unusable if one of its disks goes down. This is apparent: In the upper figure (figure 1 from now on) data is divided up into three strips in both RAID 0 subsystem. So if one of their disks fails (assume a crash of Disc1), the subsystem is broken since the first two parts of a date wont make sense without the third. You would still be in posession of all data, nothing is lost yet, but nevertheless the left RAID 0 subsystem is down. If a second drive fails this should only be 2 or 3 (since the left system in inacessible anyway) to keep the entire system up. Disc 4, 5 or 6 failing would cause a total crash. So the chance of the second failing disk resulting in a total crash is 3/5\\. Now take a look at figure 2\\. The crash of one disk in a RAID 1 (sub)system wont make this subsystem go down because the RAID controller will seamlessly switch to the mirror drive which has exactly identical data. In figure 2 all RAID 0 disks (which actually are stanalone RAID 1 systems again) need to keep running for the entire system to stay up. So theoretically there wouldnt be a problem with disks 1, 3 and 5 (or 2, 4 and 6) could crashing simultaneously. After one disk having failed (assume a crash of Disc1 again) the second one failing could be 3, 4, 5 or 6  all but NOT 2\\. The probability of a total crash is 1/5 (namely Disc2 of the remaining five) now and therefore lower than 3/5 with RAID 01\\. Hope you got it","slug":"why-raid-10-is-better-than-raid-01","published":1,"updated":"2020-10-30T20:05:40.286Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqmn0017o2e0es37341s","content":"<p>Since it took me a while to completely understand why a RAID 1+0 configuration should be better than a RAID 0+1 one in terms of failure tolerance I want to put my insights down.<br>First you should have a basic understanding of what the first two RAID levels are and what it means to nest them. Very basically at level 0 data gets striped, meaning a datum gets split up into two or more blocks that get stored on a different hard drive each. Goal is to improve read and write performance linearly to the number of disks used, because the fragments dont have to be read sequentially anymore but in parallel. Level 1 is about mirroring a datum on two disks with the goal to improve security. Of course, both levels can be combined  you could either mirror striped data or stripe mirrored data which finally gives both: security and performance. In each case at least four disks are needed, while the half of the disks usually is a mirror. So if you took six disks you would do 3-striping. With eight disks you would do 4-striping and so on. Technically you could have more than one mirror (like doing 2-striping and having a 3-mirror or even more) but its very unusual.<br>The following diagrams shall illustrate these two ways and are useful for further explanations. In both cases we have a RAID with six disks.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/raid01.png\" alt=\"raid01\">  </p>\n<p><em>Disks 4, 5 and 6 are the mirrors of 1, 2 and 3.</em></p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/raid10.png\" alt=\"raid10\">  </p>\n<p><em>Disk 2 mirrors 1, 4 mirrors 3 and 6 mirrors 5.</em></p>\n<p>We assert that RAID 10 is better in terms of fault tolerance because a total failure (= loss of data) is less likely. In other words if some drives crash in a RAID 01 configuration the chance that those are the right drives for suffering a data loss is higher.<br>First of all, both configurations can easily survive the crash of one drive. No matter which drive (see figures above) crashes, we have a second one with the exact same data on it in any case. Potentially both configurations can handle the simultaneous crash of two or even more drives (up to N/2), if theyre the right ones, but in the worst case, the second crash could end up in a total failure. What you need to make clear before understanding how RAID 01 is worse than RAID 10 is that a RAID 0 (sub)system immediately gets unusable if one of its disks goes down. This is apparent: In the upper figure (figure 1 from now on) data is divided up into three strips in both RAID 0 subsystem. So if one of their disks fails (assume a crash of Disc1), the subsystem is broken since the first two parts of a date wont make sense without the third. You would still be in posession of all data, nothing is lost yet, but nevertheless the left RAID 0 subsystem is down. If a second drive fails this should only be 2 or 3 (since the left system in inacessible anyway) to keep the entire system up. Disc 4, 5 or 6 failing would cause a total crash. So the chance of the second failing disk resulting in a total crash is 3/5. Now take a look at figure 2. The crash of one disk in a RAID 1 (sub)system wont make this subsystem go down because the RAID controller will seamlessly switch to the mirror drive which has exactly identical data. In figure 2 all RAID 0 disks (which actually are stanalone RAID 1 systems again) need to keep running for the entire system to stay up. So theoretically there wouldnt be a problem with disks 1, 3 and 5 (or 2, 4 and 6) could crashing simultaneously. After one disk having failed (assume a crash of Disc1 again) the second one failing could be 3, 4, 5 or 6  all but NOT 2. The probability of a total crash is 1/5 (namely Disc2 of the remaining five) now and therefore lower than 3/5 with RAID 01. Hope you got it</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Since it took me a while to completely understand why a RAID 1+0 configuration should be better than a RAID 0+1 one in terms of failure tolerance I want to put my insights down.<br>First you should have a basic understanding of what the first two RAID levels are and what it means to nest them. Very basically at level 0 data gets striped, meaning a datum gets split up into two or more blocks that get stored on a different hard drive each. Goal is to improve read and write performance linearly to the number of disks used, because the fragments dont have to be read sequentially anymore but in parallel. Level 1 is about mirroring a datum on two disks with the goal to improve security. Of course, both levels can be combined  you could either mirror striped data or stripe mirrored data which finally gives both: security and performance. In each case at least four disks are needed, while the half of the disks usually is a mirror. So if you took six disks you would do 3-striping. With eight disks you would do 4-striping and so on. Technically you could have more than one mirror (like doing 2-striping and having a 3-mirror or even more) but its very unusual.<br>The following diagrams shall illustrate these two ways and are useful for further explanations. In both cases we have a RAID with six disks.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/raid01.png\" alt=\"raid01\">  </p>\n<p><em>Disks 4, 5 and 6 are the mirrors of 1, 2 and 3.</em></p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/raid10.png\" alt=\"raid10\">  </p>\n<p><em>Disk 2 mirrors 1, 4 mirrors 3 and 6 mirrors 5.</em></p>\n<p>We assert that RAID 10 is better in terms of fault tolerance because a total failure (= loss of data) is less likely. In other words if some drives crash in a RAID 01 configuration the chance that those are the right drives for suffering a data loss is higher.<br>First of all, both configurations can easily survive the crash of one drive. No matter which drive (see figures above) crashes, we have a second one with the exact same data on it in any case. Potentially both configurations can handle the simultaneous crash of two or even more drives (up to N/2), if theyre the right ones, but in the worst case, the second crash could end up in a total failure. What you need to make clear before understanding how RAID 01 is worse than RAID 10 is that a RAID 0 (sub)system immediately gets unusable if one of its disks goes down. This is apparent: In the upper figure (figure 1 from now on) data is divided up into three strips in both RAID 0 subsystem. So if one of their disks fails (assume a crash of Disc1), the subsystem is broken since the first two parts of a date wont make sense without the third. You would still be in posession of all data, nothing is lost yet, but nevertheless the left RAID 0 subsystem is down. If a second drive fails this should only be 2 or 3 (since the left system in inacessible anyway) to keep the entire system up. Disc 4, 5 or 6 failing would cause a total crash. So the chance of the second failing disk resulting in a total crash is 3/5. Now take a look at figure 2. The crash of one disk in a RAID 1 (sub)system wont make this subsystem go down because the RAID controller will seamlessly switch to the mirror drive which has exactly identical data. In figure 2 all RAID 0 disks (which actually are stanalone RAID 1 systems again) need to keep running for the entire system to stay up. So theoretically there wouldnt be a problem with disks 1, 3 and 5 (or 2, 4 and 6) could crashing simultaneously. After one disk having failed (assume a crash of Disc1 again) the second one failing could be 3, 4, 5 or 6  all but NOT 2. The probability of a total crash is 1/5 (namely Disc2 of the remaining five) now and therefore lower than 3/5 with RAID 01. Hope you got it</p>\n"},{"title":"Detecting academics' major from facial images","date":"2019-01-02T10:02:21.000Z","_content":"\n# The Idea\nA few months ago I read a paper with the title [\"Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images\"](https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual), which caused a lot of [controversy](https://news.ycombinator.com/item?id=15198997). While I don't want to comment on the methodology and quality of the paper (that was already done, e.g. in [an article by Jeremy Howard](https://www.fast.ai/2017/09/13/kosinski/)), I found it very interesting and inspiring. In a nutshell, the researchers collected face pictures from dating websites and built a machine learning model to classify people's sexual orientation and reached quite an impressive accuracy with their approach.\n\n[This guest post](https://scatter.wordpress.com/2017/09/10/guest-post-artificial-intelligence-discovers-gayface-sigh/) summarizes the results as: \n\n> AI Cant Tell if Youre Gay But it Can Tell if Youre a Walking Stereotype.\n\n And indeed, we often see people who look very stereotypical. I tried to think of more such scenarios and came to the conclusion that another environment, where this phenomenon can be found quite often, is a university campus. So often you walk around the campus and see students, who just look like a law student, a computer science nerd, a sportsman, etc. Sometimes I'm so curious that I almost want to ask them whether my assumption is correct.\n \n After having read the above paper, I wondered if some machine learning model might be able to quantify these latent assumptions and find out a stereotypical-looking student's profession or major. \n\nAlthough I only have a little more than basic knowledge in machine learning, especially in image classification using deep neural nets, I took it as a personal challenge to **build a classifier, that detects academics' major based on an image of their face**. \n\n# Disclaimer\nPlease don't take this article too serious. I'm not a machine learning expert or a professional scientist. There might be some mistakes in my methodology or implementation. However, I'd love to hear your thoughts and feedback.\n\n# Approach\nMy first (and final) approach was to **(1.) collect face pictures of students** or other academics, **(2.) label them** with a small, limited set of classes, corresponding to their major, and eventually **(3.) fit a convolutional neural net (CNN)** as a classifier. I thought of fields of study, whose students might potentially look a bit stereotypical and came up with four classes:\n\n1. computer science (~ cs)\n2. economics (~ econ)\n3. (German) linguistics (~ german)\n4. mechanical engineering (~ mechanical)\n\nPlease note that this is not meant to be offending by any means! (I'm a computer science nerd myself ).\n\n# Getting the data\nThe very first prerequisite is training data - as usual, when doing machine learning. And since I aimed at training a convolutional neural net (CNN), there should be a lot of data, preferably.\n\nWhile it would have been a funny approach to walk around my campus and ask students for their major and a picture of their face, I would probably not have ended up with a lot of data. Instead, I decided to **crawl pictures from university websites**. Almost every department at every university has a page called \"[Staff](http://dbis.ipd.kit.edu/english/722.php)\", \"People\", \"Researchers\" or the like on their websites. While these are not particularly lists of students, but of professors, research assistants and PhD candidates, I presumed that those pictures should still be sufficient as training data. \n\nI wrote a bunch of **crawler scripts** using Python and [Selenium WebDriver](https://www.seleniumhq.org/) to crawl **57** different websites, including the websites of various departments of the following universities:\n\n* Karlsruhe Institute of Technology\n* TU Munich\n* University of Munich\n* University of Wrzburg\n* University of Siegen\n* University of Bayreuth\n* University of Feiburg\n* University of Heidelberg\n* University of Erlangen\n* University of Bamberg\n* University of Mannheim\n\nAfter a bit of manual data cleaning (removing pictures without faces, rotating pictures, ...), I ended up with a total of **1369** labeled images from four different classes. While this is not very much data for training a CNN, I decided to give it a try anyway.\n\n## Examples\n### Images\n\n**An excerpt from the folder containing all raw images after crawling:**\n![Excerpt from all crawled raw images](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/academic_faces1.png)\n(If you are in one of these pictures and want to get removed, please contact me.)\n\n### Labels\n**An excerpt from `index.csv` containing labels and meta-data for every image:**\n```csv\nid,category,image_url,name\nc35464fd,mechanical,http://www.fast.kit.edu/lff/1011_1105.php,Prof. Dr. rer. nat. Frank Gauterin\na73d11a7,cs,http://h2t.anthropomatik.kit.edu/21_1784.php,Kevin Liang\n97e230ff,econ,http://marketing.iism.kit.edu/21_371.php,Dr. Hanna Schumacher\ncde71a5c,german,https://www.germanistik.uni-muenchen.de/personal/ndl/mitarbeiter/bach/index.html,Dr. Oliver Bach\n```\n\n# Preprocessing the data\nBefore the images could be used as training data for a learning algorithm, a bit of preprocessing needed to be applied. Mainly, I did two major steps of preprocessing.\n\n1. **Cropping** images to faces - As you can see, pictures are taken from different angles, some of them contain a lot of background, some are not centered, etc. To get better training data, the pictures have to be cropped to only the face and nothing else. \n2. **Scaling** - All pictures come in different resolutions, but eventually need to be of exactly the same size in order to be used as input to a neural network. \n\nTo achieve both of these preprocessing steps I used a great, little, open-source, OpenCV-based Python tool called [autocrop](https://github.com/leblancfg/autocrop) with the following command:\n\n`autocrop -i raw -o preprocessed -w 128 -H 128 > autocrop.log`.\n\nThis detects the face in every picture in `raw` folder, crops the picture to that face, re-scales the resulting image to 128 x 128 pixels and saves it to `preprocessed` folder. Of course, there are some pictures in which the algorithm can not detect a face. Those are logged to stdout and persisted to `autocrop.log`.\n\nIn addition, I wrote a script that parses `autocrop.log` to get the failed images and subsequently split the images into _train_ (70 %), _test_ (20 %) and _validation_ (10 %) and copy them to a folder structure that is compatible to the format required by [Keras ImageDataGenerator](https://keras.io/preprocessing/image/) to read training data.\n\n```\n- raw\n    - index.csv\n    - c35464fd.jpg\n    - a73d11a7.jpg\n    - ...\n- preprocessed \n    - train\n        - cs\n            - a73d11a7.jpg\n            - ...\n        - econ\n            - 97e230ff.jpg\n            - ...\n        - german\n            - cde71a5c.jpg\n            - ...\n        - mechanical\n            - c35464fd.jpg\n            - ...\n    - test\n        - cs\n            - ...\n        - ...\n    - validation\n        - cs\n            - ...\n        - ...\n```\n\n# Building a model\n## Approach 1: Simple, custom CNN\n\n**Code**\n* [custom_model.ipynb](https://gist.github.com/muety/78bf6d7929e4facd199ad0ffea0b3ad9)\n\n\nI decided to start simple and see if anything can be learned from the data at all. I defined the following simple CNN architecture in Keras: \n\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 62, 62, 32)        320       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 12, 12, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1152)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                73792     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 260       \n=================================================================\nTotal params: 92,868\nTrainable params: 92,868\nNon-trainable params: 0\n```\n\nI used Keras' [ImageDataGenerator](https://keras.io/preprocessing/image/) (great tool!) to read images into NumPy arrays, re-scale them to a shape of `(64, 63, 3)` (64 x 64 pixels, RGB) and perform some data augmentation using transformations like rotations, zooming, horizontal flipping, etc. to blow up my training data and hopefully build more robust, less overfitted models.\n\nI let the model train for **100 epochs**, using the **Adam optimizer** with default parameters and **categorical crossentropy loss**, a mini-batch size of **32** and **3x augmentation** (use transformations to blow up training data by a factor of three). \n\n### Results (57.1 % accuracy)\nThe maximum **validation accuracy of 0.66** was reached after 74 epochs. **Test accuracy** turned out to be **0.571**. Considering that a quite simple model was trained completely from scratch with less than 1000 training examples, I am quite impressed by that result. It means that on average the model predicts more than every second student's major correctly. The **a-priori probability** of a correct classification **is 0.25**, so the model has definitely learned at least something.\n\n## Approach 2: Fine-tuning VGGFace\n\n**Code**\n* [vggfaces_bottleneck_model.ipynb](https://gist.github.com/muety/a079dcb27d921d58323c9574152b2c2d)\n* [vggfaces_finetuned_model.ipynb](https://gist.github.com/muety/c3b9e9401f178807c91ad890a6c67e18)\n\n\nAs an alternative to a simple, custom-defined CNN model, that is trained from scratch, I wanted to follow the common approach of fine-tuning the weights of an existing, pre-trained model. The basic idea of such an approach is to not \"re-invent the wheel\", but take advantage of what was already learned before and only slightly adapt that \"knowledge\" (in form of weights) to a certain problem. Latent features in images, which a learning algorithm had already extracted from a giant set of training data before, can just be leveraged. [\"Image Classification using pre-trained models in Keras\"](https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/) gives an excellent overview of how **fine-tuning** works and how it is different from **transfer learning** and custom models. Expectations are that my given classification problem can be solved more accurately with less data. \n\n I decided to take a **VGG16** model architecture trained on [**VGGFace**](http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/) as a base (using the [keras-vggface](https://github.com/rcmalli/keras-vggface) implementation) and followed [this guide](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) to fine-tune it. VGGFace is a dataset published by the University of Oxford that contains more than 3.3 million face images. Accordingly, I expected it to have extracted very robust facial features and to be quite well-suited for face classification. \n\n### Step 1: Transfer-learning to initialize weights\nMy implementation consists of two steps, since [it is recommended](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) that\n\n> in order to perform fine-tuning, all layers should start with properly trained weights.\n\nIn this first step, transfer-learning is used to find proper weights for a set of a few newly added, custom, fully-connected classification layers. These are used as the initial weights in step 2 later on. To perform this initialization, a pre-trained VGGFace model, with the final classification layers cut off, is used to extract 128 _bottleneck features_ for every image. Subsequently, another tiny model, consisting of fully-connected layers, is trained on these features to perform the eventual classification. The weights are persisted to a file and loaded again in step 2.\n\nThe model architecture looks like this:\n\n```\n________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 128)               65664     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 516       \n=================================================================\nTotal params: 66,180\nTrainable params: 66,180\nNon-trainable params: 0\n```\n\n### Step 2: Fine-tuning \nIn this second step, a pre-trained VGGFace model (with the first n - 3 layers freezed) is used in combination with the pre-trained top layers from step 1 to fine-tune weights for our specific classification task. It takes mini-batches of (128, 128, 3)-shaped tensors (128 x 128 pixels, RGB) as input and predicts probabilities for each of our four target classes.\n\nThe architecture of the combined model looks like this:\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvggface_vgg16 (Model)        (None, 512)               14714688  \n_________________________________________________________________\ntop (Sequential)             (None, 4)                 66180     \n=================================================================\nTotal params: 14,780,868\nTrainable params: 2,425,988\nNon-trainable params: 12,354,880\n```\n\n`top` is the model described in step 1, `vggface_vgg16` is a VGG16 model and looks like this:\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         (None, 128, 128, 3)       0         \n_________________________________________________________________\nconv1_1 (Conv2D)             (None, 128, 128, 64)      1792      \n_________________________________________________________________\nconv1_2 (Conv2D)             (None, 128, 128, 64)      36928     \n_________________________________________________________________\npool1 (MaxPooling2D)         (None, 64, 64, 64)        0         \n_________________________________________________________________\nconv2_1 (Conv2D)             (None, 64, 64, 128)       73856     \n_________________________________________________________________\nconv2_2 (Conv2D)             (None, 64, 64, 128)       147584    \n_________________________________________________________________\npool2 (MaxPooling2D)         (None, 32, 32, 128)       0         \n_________________________________________________________________\nconv3_1 (Conv2D)             (None, 32, 32, 256)       295168    \n_________________________________________________________________\nconv3_2 (Conv2D)             (None, 32, 32, 256)       590080    \n_________________________________________________________________\nconv3_3 (Conv2D)             (None, 32, 32, 256)       590080    \n_________________________________________________________________\npool3 (MaxPooling2D)         (None, 16, 16, 256)       0         \n_________________________________________________________________\nconv4_1 (Conv2D)             (None, 16, 16, 512)       1180160   \n_________________________________________________________________\nconv4_2 (Conv2D)             (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nconv4_3 (Conv2D)             (None, 16, 16, 512)       2359808   \n_________________________________________________________________\npool4 (MaxPooling2D)         (None, 8, 8, 512)         0         \n_________________________________________________________________\nconv5_1 (Conv2D)             (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nconv5_2 (Conv2D)             (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nconv5_3 (Conv2D)             (None, 8, 8, 512)         2359808   \n_________________________________________________________________\npool5 (MaxPooling2D)         (None, 4, 4, 512)         0         \n_________________________________________________________________\nglobal_max_pooling2d_3 (Glob (None, 512)               0         \n=================================================================\nTotal params: 14,714,688\nTrainable params: 2,359,808\nNon-trainable params: 12,354,880\n```\n\n\nI was using Keras _ImageDataGenerator_ again for loading the data, augmenting (3x) and resizing it. As [recommended](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), _stochastic gradient descent_ is used with a small learning rate (10^-4) to carefully adapt weights. The model was trained for **100 epochs** on **batches of 32 images** and, again, used **categorical cross entropy** as a loss function. \n\n### Results (54.6 % accuracy)\nThe maximum **validation accuracy of 0.64** was reached after 38 epochs already. **Test accuracy** turned out to be **0.546**, which is a quite disappointing result, considering that even our simple, custom CNN-model achieved a higher accuracy. Maybe the model's complexity is too high for the small amount of training data?\n\n# Inspecting the model\nTo get better insights on how the model performs, I briefly inspected it with regards to several criteria. This is a short summary of my finding. \n\n## Code\n* [inspection.ipynb](https://gist.github.com/muety/404befcfb2eef4b59398f3c8590ce692) \n\n## Class distribution\nThe first thing I looked at was the class distribution. How are the four study major subjects represented in our data and what does the model predict?\n\n | cs | econ | german | mechanical\n- | - | - | - | -\n*real* | 0.2510 | 0.2808 | 0.2127 | 0.2553\n*pred* | 0.2595 | 0.2936 | 0.1361 | 0.3106\n\nApparently, the model neglects the class of _german linguists_ a bit. That is also the class for which we have the least training data. Probably I should collect more.\n\n## Examples of false classifications\nI wanted to get an idea of what the model does wrong and what it does right. Consequently, I took a look at the top (with respect to confidence) five **(1) false negatives**, **(2) false positives** and **(3) true positives**. \n\nHere is an excerpt for class _econ_:\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/academic_faces2.png)\n\nThe top row shows examples of economists, who the model didn't recognize as such.\nThe center row depicts examples of what the model \"thinks\" economists look like, but who are actually students / researchers with a different major.\nFinally, the bottom row shows examples of good matches, i.e. people for whom the model had a very high confidence for their actual class.\n\nAgain, if you are in one of these pictures and want to get removed, please contact me.\n\n## Confusion matrix\nTo see which profession the model is unsure about, I calculated the confusion matrix.\n\n```\narray([[12.76595745,  5.95744681,  0.        ,  6.38297872],\n       [ 3.40425532, 12.76595745,  3.82978723,  8.08510638],\n       [ 3.82978723,  5.53191489,  8.5106383 ,  3.40425532],\n       [ 5.95744681,  5.10638298,  1.27659574, 13.19148936]])\n```\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/academic_faces3.png)\n**Legend:**\n* 0 = cs, 1 = econ, 2 = german, 3 = mechanical\n* Brighter colors ~ higher value\n\nWhat we can read from the confusion matrix is that, for instance, the model tends to classify economists as mechanical engineers quite often. \n\n# Conclusion\nFirst of all, this is not a scientific study, but rather a small hobby project of mine. Also, it does not have a lot of real-world importance, since one might rarely want to classify students into four categories.\n\nAlthough the results are not spectacular, I am still quite happy about them and at least my model was able to do a lot better than random guessing. Given an **accuracy of 57 %** with four classes, you could definitely say that it is, to some extent, possible to learn a stereotypical-looking person's study major from only in image of their face. Of course, this only holds true within a bounded context and under a set of restrictions, but it is still an interesting insight to me. \n\nMoreover, I am quite sure that there is still a lot of room for improvements to the model, which could yield a better performance. Those might include:\n* More training data from a wider range of sources\n* More thorough preprocessing (e.g. filter out images of secretaries)\n* Different model architecture\n* Hyper-parameter tuning\n* Manual feature engineering\n* ...\n\nPlease let me know what you think of this project. I would love to get some feedback!","source":"_posts/detecting-academics-major-from-facial-images.md","raw":"---\ntitle: Detecting academics' major from facial images\ndate: 2019-01-02 11:02:21\ntags:\n---\n\n# The Idea\nA few months ago I read a paper with the title [\"Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images\"](https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual), which caused a lot of [controversy](https://news.ycombinator.com/item?id=15198997). While I don't want to comment on the methodology and quality of the paper (that was already done, e.g. in [an article by Jeremy Howard](https://www.fast.ai/2017/09/13/kosinski/)), I found it very interesting and inspiring. In a nutshell, the researchers collected face pictures from dating websites and built a machine learning model to classify people's sexual orientation and reached quite an impressive accuracy with their approach.\n\n[This guest post](https://scatter.wordpress.com/2017/09/10/guest-post-artificial-intelligence-discovers-gayface-sigh/) summarizes the results as: \n\n> AI Cant Tell if Youre Gay But it Can Tell if Youre a Walking Stereotype.\n\n And indeed, we often see people who look very stereotypical. I tried to think of more such scenarios and came to the conclusion that another environment, where this phenomenon can be found quite often, is a university campus. So often you walk around the campus and see students, who just look like a law student, a computer science nerd, a sportsman, etc. Sometimes I'm so curious that I almost want to ask them whether my assumption is correct.\n \n After having read the above paper, I wondered if some machine learning model might be able to quantify these latent assumptions and find out a stereotypical-looking student's profession or major. \n\nAlthough I only have a little more than basic knowledge in machine learning, especially in image classification using deep neural nets, I took it as a personal challenge to **build a classifier, that detects academics' major based on an image of their face**. \n\n# Disclaimer\nPlease don't take this article too serious. I'm not a machine learning expert or a professional scientist. There might be some mistakes in my methodology or implementation. However, I'd love to hear your thoughts and feedback.\n\n# Approach\nMy first (and final) approach was to **(1.) collect face pictures of students** or other academics, **(2.) label them** with a small, limited set of classes, corresponding to their major, and eventually **(3.) fit a convolutional neural net (CNN)** as a classifier. I thought of fields of study, whose students might potentially look a bit stereotypical and came up with four classes:\n\n1. computer science (~ cs)\n2. economics (~ econ)\n3. (German) linguistics (~ german)\n4. mechanical engineering (~ mechanical)\n\nPlease note that this is not meant to be offending by any means! (I'm a computer science nerd myself ).\n\n# Getting the data\nThe very first prerequisite is training data - as usual, when doing machine learning. And since I aimed at training a convolutional neural net (CNN), there should be a lot of data, preferably.\n\nWhile it would have been a funny approach to walk around my campus and ask students for their major and a picture of their face, I would probably not have ended up with a lot of data. Instead, I decided to **crawl pictures from university websites**. Almost every department at every university has a page called \"[Staff](http://dbis.ipd.kit.edu/english/722.php)\", \"People\", \"Researchers\" or the like on their websites. While these are not particularly lists of students, but of professors, research assistants and PhD candidates, I presumed that those pictures should still be sufficient as training data. \n\nI wrote a bunch of **crawler scripts** using Python and [Selenium WebDriver](https://www.seleniumhq.org/) to crawl **57** different websites, including the websites of various departments of the following universities:\n\n* Karlsruhe Institute of Technology\n* TU Munich\n* University of Munich\n* University of Wrzburg\n* University of Siegen\n* University of Bayreuth\n* University of Feiburg\n* University of Heidelberg\n* University of Erlangen\n* University of Bamberg\n* University of Mannheim\n\nAfter a bit of manual data cleaning (removing pictures without faces, rotating pictures, ...), I ended up with a total of **1369** labeled images from four different classes. While this is not very much data for training a CNN, I decided to give it a try anyway.\n\n## Examples\n### Images\n\n**An excerpt from the folder containing all raw images after crawling:**\n![Excerpt from all crawled raw images](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/academic_faces1.png)\n(If you are in one of these pictures and want to get removed, please contact me.)\n\n### Labels\n**An excerpt from `index.csv` containing labels and meta-data for every image:**\n```csv\nid,category,image_url,name\nc35464fd,mechanical,http://www.fast.kit.edu/lff/1011_1105.php,Prof. Dr. rer. nat. Frank Gauterin\na73d11a7,cs,http://h2t.anthropomatik.kit.edu/21_1784.php,Kevin Liang\n97e230ff,econ,http://marketing.iism.kit.edu/21_371.php,Dr. Hanna Schumacher\ncde71a5c,german,https://www.germanistik.uni-muenchen.de/personal/ndl/mitarbeiter/bach/index.html,Dr. Oliver Bach\n```\n\n# Preprocessing the data\nBefore the images could be used as training data for a learning algorithm, a bit of preprocessing needed to be applied. Mainly, I did two major steps of preprocessing.\n\n1. **Cropping** images to faces - As you can see, pictures are taken from different angles, some of them contain a lot of background, some are not centered, etc. To get better training data, the pictures have to be cropped to only the face and nothing else. \n2. **Scaling** - All pictures come in different resolutions, but eventually need to be of exactly the same size in order to be used as input to a neural network. \n\nTo achieve both of these preprocessing steps I used a great, little, open-source, OpenCV-based Python tool called [autocrop](https://github.com/leblancfg/autocrop) with the following command:\n\n`autocrop -i raw -o preprocessed -w 128 -H 128 > autocrop.log`.\n\nThis detects the face in every picture in `raw` folder, crops the picture to that face, re-scales the resulting image to 128 x 128 pixels and saves it to `preprocessed` folder. Of course, there are some pictures in which the algorithm can not detect a face. Those are logged to stdout and persisted to `autocrop.log`.\n\nIn addition, I wrote a script that parses `autocrop.log` to get the failed images and subsequently split the images into _train_ (70 %), _test_ (20 %) and _validation_ (10 %) and copy them to a folder structure that is compatible to the format required by [Keras ImageDataGenerator](https://keras.io/preprocessing/image/) to read training data.\n\n```\n- raw\n    - index.csv\n    - c35464fd.jpg\n    - a73d11a7.jpg\n    - ...\n- preprocessed \n    - train\n        - cs\n            - a73d11a7.jpg\n            - ...\n        - econ\n            - 97e230ff.jpg\n            - ...\n        - german\n            - cde71a5c.jpg\n            - ...\n        - mechanical\n            - c35464fd.jpg\n            - ...\n    - test\n        - cs\n            - ...\n        - ...\n    - validation\n        - cs\n            - ...\n        - ...\n```\n\n# Building a model\n## Approach 1: Simple, custom CNN\n\n**Code**\n* [custom_model.ipynb](https://gist.github.com/muety/78bf6d7929e4facd199ad0ffea0b3ad9)\n\n\nI decided to start simple and see if anything can be learned from the data at all. I defined the following simple CNN architecture in Keras: \n\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 62, 62, 32)        320       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 12, 12, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1152)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 64)                73792     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 260       \n=================================================================\nTotal params: 92,868\nTrainable params: 92,868\nNon-trainable params: 0\n```\n\nI used Keras' [ImageDataGenerator](https://keras.io/preprocessing/image/) (great tool!) to read images into NumPy arrays, re-scale them to a shape of `(64, 63, 3)` (64 x 64 pixels, RGB) and perform some data augmentation using transformations like rotations, zooming, horizontal flipping, etc. to blow up my training data and hopefully build more robust, less overfitted models.\n\nI let the model train for **100 epochs**, using the **Adam optimizer** with default parameters and **categorical crossentropy loss**, a mini-batch size of **32** and **3x augmentation** (use transformations to blow up training data by a factor of three). \n\n### Results (57.1 % accuracy)\nThe maximum **validation accuracy of 0.66** was reached after 74 epochs. **Test accuracy** turned out to be **0.571**. Considering that a quite simple model was trained completely from scratch with less than 1000 training examples, I am quite impressed by that result. It means that on average the model predicts more than every second student's major correctly. The **a-priori probability** of a correct classification **is 0.25**, so the model has definitely learned at least something.\n\n## Approach 2: Fine-tuning VGGFace\n\n**Code**\n* [vggfaces_bottleneck_model.ipynb](https://gist.github.com/muety/a079dcb27d921d58323c9574152b2c2d)\n* [vggfaces_finetuned_model.ipynb](https://gist.github.com/muety/c3b9e9401f178807c91ad890a6c67e18)\n\n\nAs an alternative to a simple, custom-defined CNN model, that is trained from scratch, I wanted to follow the common approach of fine-tuning the weights of an existing, pre-trained model. The basic idea of such an approach is to not \"re-invent the wheel\", but take advantage of what was already learned before and only slightly adapt that \"knowledge\" (in form of weights) to a certain problem. Latent features in images, which a learning algorithm had already extracted from a giant set of training data before, can just be leveraged. [\"Image Classification using pre-trained models in Keras\"](https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/) gives an excellent overview of how **fine-tuning** works and how it is different from **transfer learning** and custom models. Expectations are that my given classification problem can be solved more accurately with less data. \n\n I decided to take a **VGG16** model architecture trained on [**VGGFace**](http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/) as a base (using the [keras-vggface](https://github.com/rcmalli/keras-vggface) implementation) and followed [this guide](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) to fine-tune it. VGGFace is a dataset published by the University of Oxford that contains more than 3.3 million face images. Accordingly, I expected it to have extracted very robust facial features and to be quite well-suited for face classification. \n\n### Step 1: Transfer-learning to initialize weights\nMy implementation consists of two steps, since [it is recommended](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) that\n\n> in order to perform fine-tuning, all layers should start with properly trained weights.\n\nIn this first step, transfer-learning is used to find proper weights for a set of a few newly added, custom, fully-connected classification layers. These are used as the initial weights in step 2 later on. To perform this initialization, a pre-trained VGGFace model, with the final classification layers cut off, is used to extract 128 _bottleneck features_ for every image. Subsequently, another tiny model, consisting of fully-connected layers, is trained on these features to perform the eventual classification. The weights are persisted to a file and loaded again in step 2.\n\nThe model architecture looks like this:\n\n```\n________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 128)               65664     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4)                 516       \n=================================================================\nTotal params: 66,180\nTrainable params: 66,180\nNon-trainable params: 0\n```\n\n### Step 2: Fine-tuning \nIn this second step, a pre-trained VGGFace model (with the first n - 3 layers freezed) is used in combination with the pre-trained top layers from step 1 to fine-tune weights for our specific classification task. It takes mini-batches of (128, 128, 3)-shaped tensors (128 x 128 pixels, RGB) as input and predicts probabilities for each of our four target classes.\n\nThe architecture of the combined model looks like this:\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvggface_vgg16 (Model)        (None, 512)               14714688  \n_________________________________________________________________\ntop (Sequential)             (None, 4)                 66180     \n=================================================================\nTotal params: 14,780,868\nTrainable params: 2,425,988\nNon-trainable params: 12,354,880\n```\n\n`top` is the model described in step 1, `vggface_vgg16` is a VGG16 model and looks like this:\n```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         (None, 128, 128, 3)       0         \n_________________________________________________________________\nconv1_1 (Conv2D)             (None, 128, 128, 64)      1792      \n_________________________________________________________________\nconv1_2 (Conv2D)             (None, 128, 128, 64)      36928     \n_________________________________________________________________\npool1 (MaxPooling2D)         (None, 64, 64, 64)        0         \n_________________________________________________________________\nconv2_1 (Conv2D)             (None, 64, 64, 128)       73856     \n_________________________________________________________________\nconv2_2 (Conv2D)             (None, 64, 64, 128)       147584    \n_________________________________________________________________\npool2 (MaxPooling2D)         (None, 32, 32, 128)       0         \n_________________________________________________________________\nconv3_1 (Conv2D)             (None, 32, 32, 256)       295168    \n_________________________________________________________________\nconv3_2 (Conv2D)             (None, 32, 32, 256)       590080    \n_________________________________________________________________\nconv3_3 (Conv2D)             (None, 32, 32, 256)       590080    \n_________________________________________________________________\npool3 (MaxPooling2D)         (None, 16, 16, 256)       0         \n_________________________________________________________________\nconv4_1 (Conv2D)             (None, 16, 16, 512)       1180160   \n_________________________________________________________________\nconv4_2 (Conv2D)             (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nconv4_3 (Conv2D)             (None, 16, 16, 512)       2359808   \n_________________________________________________________________\npool4 (MaxPooling2D)         (None, 8, 8, 512)         0         \n_________________________________________________________________\nconv5_1 (Conv2D)             (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nconv5_2 (Conv2D)             (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nconv5_3 (Conv2D)             (None, 8, 8, 512)         2359808   \n_________________________________________________________________\npool5 (MaxPooling2D)         (None, 4, 4, 512)         0         \n_________________________________________________________________\nglobal_max_pooling2d_3 (Glob (None, 512)               0         \n=================================================================\nTotal params: 14,714,688\nTrainable params: 2,359,808\nNon-trainable params: 12,354,880\n```\n\n\nI was using Keras _ImageDataGenerator_ again for loading the data, augmenting (3x) and resizing it. As [recommended](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), _stochastic gradient descent_ is used with a small learning rate (10^-4) to carefully adapt weights. The model was trained for **100 epochs** on **batches of 32 images** and, again, used **categorical cross entropy** as a loss function. \n\n### Results (54.6 % accuracy)\nThe maximum **validation accuracy of 0.64** was reached after 38 epochs already. **Test accuracy** turned out to be **0.546**, which is a quite disappointing result, considering that even our simple, custom CNN-model achieved a higher accuracy. Maybe the model's complexity is too high for the small amount of training data?\n\n# Inspecting the model\nTo get better insights on how the model performs, I briefly inspected it with regards to several criteria. This is a short summary of my finding. \n\n## Code\n* [inspection.ipynb](https://gist.github.com/muety/404befcfb2eef4b59398f3c8590ce692) \n\n## Class distribution\nThe first thing I looked at was the class distribution. How are the four study major subjects represented in our data and what does the model predict?\n\n | cs | econ | german | mechanical\n- | - | - | - | -\n*real* | 0.2510 | 0.2808 | 0.2127 | 0.2553\n*pred* | 0.2595 | 0.2936 | 0.1361 | 0.3106\n\nApparently, the model neglects the class of _german linguists_ a bit. That is also the class for which we have the least training data. Probably I should collect more.\n\n## Examples of false classifications\nI wanted to get an idea of what the model does wrong and what it does right. Consequently, I took a look at the top (with respect to confidence) five **(1) false negatives**, **(2) false positives** and **(3) true positives**. \n\nHere is an excerpt for class _econ_:\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/academic_faces2.png)\n\nThe top row shows examples of economists, who the model didn't recognize as such.\nThe center row depicts examples of what the model \"thinks\" economists look like, but who are actually students / researchers with a different major.\nFinally, the bottom row shows examples of good matches, i.e. people for whom the model had a very high confidence for their actual class.\n\nAgain, if you are in one of these pictures and want to get removed, please contact me.\n\n## Confusion matrix\nTo see which profession the model is unsure about, I calculated the confusion matrix.\n\n```\narray([[12.76595745,  5.95744681,  0.        ,  6.38297872],\n       [ 3.40425532, 12.76595745,  3.82978723,  8.08510638],\n       [ 3.82978723,  5.53191489,  8.5106383 ,  3.40425532],\n       [ 5.95744681,  5.10638298,  1.27659574, 13.19148936]])\n```\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/academic_faces3.png)\n**Legend:**\n* 0 = cs, 1 = econ, 2 = german, 3 = mechanical\n* Brighter colors ~ higher value\n\nWhat we can read from the confusion matrix is that, for instance, the model tends to classify economists as mechanical engineers quite often. \n\n# Conclusion\nFirst of all, this is not a scientific study, but rather a small hobby project of mine. Also, it does not have a lot of real-world importance, since one might rarely want to classify students into four categories.\n\nAlthough the results are not spectacular, I am still quite happy about them and at least my model was able to do a lot better than random guessing. Given an **accuracy of 57 %** with four classes, you could definitely say that it is, to some extent, possible to learn a stereotypical-looking person's study major from only in image of their face. Of course, this only holds true within a bounded context and under a set of restrictions, but it is still an interesting insight to me. \n\nMoreover, I am quite sure that there is still a lot of room for improvements to the model, which could yield a better performance. Those might include:\n* More training data from a wider range of sources\n* More thorough preprocessing (e.g. filter out images of secretaries)\n* Different model architecture\n* Hyper-parameter tuning\n* Manual feature engineering\n* ...\n\nPlease let me know what you think of this project. I would love to get some feedback!","slug":"detecting-academics-major-from-facial-images","published":1,"updated":"2020-10-30T20:05:40.282Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckh0yeqmp0019o2e0gn9agplg","content":"<h1 id=\"The-Idea\"><a href=\"#The-Idea\" class=\"headerlink\" title=\"The Idea\"></a>The Idea</h1><p>A few months ago I read a paper with the title <a href=\"https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual\">Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images</a>, which caused a lot of <a href=\"https://news.ycombinator.com/item?id=15198997\">controversy</a>. While I dont want to comment on the methodology and quality of the paper (that was already done, e.g. in <a href=\"https://www.fast.ai/2017/09/13/kosinski/\">an article by Jeremy Howard</a>), I found it very interesting and inspiring. In a nutshell, the researchers collected face pictures from dating websites and built a machine learning model to classify peoples sexual orientation and reached quite an impressive accuracy with their approach.</p>\n<p><a href=\"https://scatter.wordpress.com/2017/09/10/guest-post-artificial-intelligence-discovers-gayface-sigh/\">This guest post</a> summarizes the results as: </p>\n<blockquote>\n<p>AI Cant Tell if Youre Gay But it Can Tell if Youre a Walking Stereotype.</p>\n</blockquote>\n<p> And indeed, we often see people who look very stereotypical. I tried to think of more such scenarios and came to the conclusion that another environment, where this phenomenon can be found quite often, is a university campus. So often you walk around the campus and see students, who just look like a law student, a computer science nerd, a sportsman, etc. Sometimes Im so curious that I almost want to ask them whether my assumption is correct.</p>\n<p> After having read the above paper, I wondered if some machine learning model might be able to quantify these latent assumptions and find out a stereotypical-looking students profession or major. </p>\n<p>Although I only have a little more than basic knowledge in machine learning, especially in image classification using deep neural nets, I took it as a personal challenge to <strong>build a classifier, that detects academics major based on an image of their face</strong>. </p>\n<h1 id=\"Disclaimer\"><a href=\"#Disclaimer\" class=\"headerlink\" title=\"Disclaimer\"></a>Disclaimer</h1><p>Please dont take this article too serious. Im not a machine learning expert or a professional scientist. There might be some mistakes in my methodology or implementation. However, Id love to hear your thoughts and feedback.</p>\n<h1 id=\"Approach\"><a href=\"#Approach\" class=\"headerlink\" title=\"Approach\"></a>Approach</h1><p>My first (and final) approach was to <strong>(1.) collect face pictures of students</strong> or other academics, <strong>(2.) label them</strong> with a small, limited set of classes, corresponding to their major, and eventually <strong>(3.) fit a convolutional neural net (CNN)</strong> as a classifier. I thought of fields of study, whose students might potentially look a bit stereotypical and came up with four classes:</p>\n<ol>\n<li>computer science (~ cs)</li>\n<li>economics (~ econ)</li>\n<li>(German) linguistics (~ german)</li>\n<li>mechanical engineering (~ mechanical)</li>\n</ol>\n<p>Please note that this is not meant to be offending by any means! (Im a computer science nerd myself ).</p>\n<h1 id=\"Getting-the-data\"><a href=\"#Getting-the-data\" class=\"headerlink\" title=\"Getting the data\"></a>Getting the data</h1><p>The very first prerequisite is training data - as usual, when doing machine learning. And since I aimed at training a convolutional neural net (CNN), there should be a lot of data, preferably.</p>\n<p>While it would have been a funny approach to walk around my campus and ask students for their major and a picture of their face, I would probably not have ended up with a lot of data. Instead, I decided to <strong>crawl pictures from university websites</strong>. Almost every department at every university has a page called <a href=\"http://dbis.ipd.kit.edu/english/722.php\">Staff</a>, People, Researchers or the like on their websites. While these are not particularly lists of students, but of professors, research assistants and PhD candidates, I presumed that those pictures should still be sufficient as training data. </p>\n<p>I wrote a bunch of <strong>crawler scripts</strong> using Python and <a href=\"https://www.seleniumhq.org/\">Selenium WebDriver</a> to crawl <strong>57</strong> different websites, including the websites of various departments of the following universities:</p>\n<ul>\n<li>Karlsruhe Institute of Technology</li>\n<li>TU Munich</li>\n<li>University of Munich</li>\n<li>University of Wrzburg</li>\n<li>University of Siegen</li>\n<li>University of Bayreuth</li>\n<li>University of Feiburg</li>\n<li>University of Heidelberg</li>\n<li>University of Erlangen</li>\n<li>University of Bamberg</li>\n<li>University of Mannheim</li>\n</ul>\n<p>After a bit of manual data cleaning (removing pictures without faces, rotating pictures, ), I ended up with a total of <strong>1369</strong> labeled images from four different classes. While this is not very much data for training a CNN, I decided to give it a try anyway.</p>\n<h2 id=\"Examples\"><a href=\"#Examples\" class=\"headerlink\" title=\"Examples\"></a>Examples</h2><h3 id=\"Images\"><a href=\"#Images\" class=\"headerlink\" title=\"Images\"></a>Images</h3><p><strong>An excerpt from the folder containing all raw images after crawling:</strong><br><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/academic_faces1.png\" alt=\"Excerpt from all crawled raw images\"><br>(If you are in one of these pictures and want to get removed, please contact me.)</p>\n<h3 id=\"Labels\"><a href=\"#Labels\" class=\"headerlink\" title=\"Labels\"></a>Labels</h3><p><strong>An excerpt from <code>index.csv</code> containing labels and meta-data for every image:</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">id,category,image_url,name</span><br><span class=\"line\">c35464fd,mechanical,http:&#x2F;&#x2F;www.fast.kit.edu&#x2F;lff&#x2F;1011_1105.php,Prof. Dr. rer. nat. Frank Gauterin</span><br><span class=\"line\">a73d11a7,cs,http:&#x2F;&#x2F;h2t.anthropomatik.kit.edu&#x2F;21_1784.php,Kevin Liang</span><br><span class=\"line\">97e230ff,econ,http:&#x2F;&#x2F;marketing.iism.kit.edu&#x2F;21_371.php,Dr. Hanna Schumacher</span><br><span class=\"line\">cde71a5c,german,https:&#x2F;&#x2F;www.germanistik.uni-muenchen.de&#x2F;personal&#x2F;ndl&#x2F;mitarbeiter&#x2F;bach&#x2F;index.html,Dr. Oliver Bach</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Preprocessing-the-data\"><a href=\"#Preprocessing-the-data\" class=\"headerlink\" title=\"Preprocessing the data\"></a>Preprocessing the data</h1><p>Before the images could be used as training data for a learning algorithm, a bit of preprocessing needed to be applied. Mainly, I did two major steps of preprocessing.</p>\n<ol>\n<li><strong>Cropping</strong> images to faces - As you can see, pictures are taken from different angles, some of them contain a lot of background, some are not centered, etc. To get better training data, the pictures have to be cropped to only the face and nothing else. </li>\n<li><strong>Scaling</strong> - All pictures come in different resolutions, but eventually need to be of exactly the same size in order to be used as input to a neural network. </li>\n</ol>\n<p>To achieve both of these preprocessing steps I used a great, little, open-source, OpenCV-based Python tool called <a href=\"https://github.com/leblancfg/autocrop\">autocrop</a> with the following command:</p>\n<p><code>autocrop -i raw -o preprocessed -w 128 -H 128 &gt; autocrop.log</code>.</p>\n<p>This detects the face in every picture in <code>raw</code> folder, crops the picture to that face, re-scales the resulting image to 128 x 128 pixels and saves it to <code>preprocessed</code> folder. Of course, there are some pictures in which the algorithm can not detect a face. Those are logged to stdout and persisted to <code>autocrop.log</code>.</p>\n<p>In addition, I wrote a script that parses <code>autocrop.log</code> to get the failed images and subsequently split the images into <em>train</em> (70 %), <em>test</em> (20 %) and <em>validation</em> (10 %) and copy them to a folder structure that is compatible to the format required by <a href=\"https://keras.io/preprocessing/image/\">Keras ImageDataGenerator</a> to read training data.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- raw</span><br><span class=\"line\">    - index.csv</span><br><span class=\"line\">    - c35464fd.jpg</span><br><span class=\"line\">    - a73d11a7.jpg</span><br><span class=\"line\">    - ...</span><br><span class=\"line\">- preprocessed </span><br><span class=\"line\">    - train</span><br><span class=\"line\">        - cs</span><br><span class=\"line\">            - a73d11a7.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - econ</span><br><span class=\"line\">            - 97e230ff.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - german</span><br><span class=\"line\">            - cde71a5c.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - mechanical</span><br><span class=\"line\">            - c35464fd.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">    - test</span><br><span class=\"line\">        - cs</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - ...</span><br><span class=\"line\">    - validation</span><br><span class=\"line\">        - cs</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - ...</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Building-a-model\"><a href=\"#Building-a-model\" class=\"headerlink\" title=\"Building a model\"></a>Building a model</h1><h2 id=\"Approach-1-Simple-custom-CNN\"><a href=\"#Approach-1-Simple-custom-CNN\" class=\"headerlink\" title=\"Approach 1: Simple, custom CNN\"></a>Approach 1: Simple, custom CNN</h2><p><strong>Code</strong></p>\n<ul>\n<li><a href=\"https://gist.github.com/muety/78bf6d7929e4facd199ad0ffea0b3ad9\">custom_model.ipynb</a></li>\n</ul>\n<p>I decided to start simple and see if anything can be learned from the data at all. I defined the following simple CNN architecture in Keras: </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">conv2d_1 (Conv2D)            (None, 62, 62, 32)        320       </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_3 (Conv2D)            (None, 12, 12, 32)        9248      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">flatten_1 (Flatten)          (None, 1152)              0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_1 (Dense)              (None, 64)                73792     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dropout_1 (Dropout)          (None, 64)                0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_2 (Dense)              (None, 4)                 260       </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">Total params: 92,868</span><br><span class=\"line\">Trainable params: 92,868</span><br><span class=\"line\">Non-trainable params: 0</span><br></pre></td></tr></table></figure>\n\n<p>I used Keras <a href=\"https://keras.io/preprocessing/image/\">ImageDataGenerator</a> (great tool!) to read images into NumPy arrays, re-scale them to a shape of <code>(64, 63, 3)</code> (64 x 64 pixels, RGB) and perform some data augmentation using transformations like rotations, zooming, horizontal flipping, etc. to blow up my training data and hopefully build more robust, less overfitted models.</p>\n<p>I let the model train for <strong>100 epochs</strong>, using the <strong>Adam optimizer</strong> with default parameters and <strong>categorical crossentropy loss</strong>, a mini-batch size of <strong>32</strong> and <strong>3x augmentation</strong> (use transformations to blow up training data by a factor of three). </p>\n<h3 id=\"Results-57-1-accuracy\"><a href=\"#Results-57-1-accuracy\" class=\"headerlink\" title=\"Results (57.1 % accuracy)\"></a>Results (57.1 % accuracy)</h3><p>The maximum <strong>validation accuracy of 0.66</strong> was reached after 74 epochs. <strong>Test accuracy</strong> turned out to be <strong>0.571</strong>. Considering that a quite simple model was trained completely from scratch with less than 1000 training examples, I am quite impressed by that result. It means that on average the model predicts more than every second students major correctly. The <strong>a-priori probability</strong> of a correct classification <strong>is 0.25</strong>, so the model has definitely learned at least something.</p>\n<h2 id=\"Approach-2-Fine-tuning-VGGFace\"><a href=\"#Approach-2-Fine-tuning-VGGFace\" class=\"headerlink\" title=\"Approach 2: Fine-tuning VGGFace\"></a>Approach 2: Fine-tuning VGGFace</h2><p><strong>Code</strong></p>\n<ul>\n<li><a href=\"https://gist.github.com/muety/a079dcb27d921d58323c9574152b2c2d\">vggfaces_bottleneck_model.ipynb</a></li>\n<li><a href=\"https://gist.github.com/muety/c3b9e9401f178807c91ad890a6c67e18\">vggfaces_finetuned_model.ipynb</a></li>\n</ul>\n<p>As an alternative to a simple, custom-defined CNN model, that is trained from scratch, I wanted to follow the common approach of fine-tuning the weights of an existing, pre-trained model. The basic idea of such an approach is to not re-invent the wheel, but take advantage of what was already learned before and only slightly adapt that knowledge (in form of weights) to a certain problem. Latent features in images, which a learning algorithm had already extracted from a giant set of training data before, can just be leveraged. <a href=\"https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/\">Image Classification using pre-trained models in Keras</a> gives an excellent overview of how <strong>fine-tuning</strong> works and how it is different from <strong>transfer learning</strong> and custom models. Expectations are that my given classification problem can be solved more accurately with less data. </p>\n<p> I decided to take a <strong>VGG16</strong> model architecture trained on <a href=\"http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/\"><strong>VGGFace</strong></a> as a base (using the <a href=\"https://github.com/rcmalli/keras-vggface\">keras-vggface</a> implementation) and followed <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\">this guide</a> to fine-tune it. VGGFace is a dataset published by the University of Oxford that contains more than 3.3 million face images. Accordingly, I expected it to have extracted very robust facial features and to be quite well-suited for face classification. </p>\n<h3 id=\"Step-1-Transfer-learning-to-initialize-weights\"><a href=\"#Step-1-Transfer-learning-to-initialize-weights\" class=\"headerlink\" title=\"Step 1: Transfer-learning to initialize weights\"></a>Step 1: Transfer-learning to initialize weights</h3><p>My implementation consists of two steps, since <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\">it is recommended</a> that</p>\n<blockquote>\n<p>in order to perform fine-tuning, all layers should start with properly trained weights.</p>\n</blockquote>\n<p>In this first step, transfer-learning is used to find proper weights for a set of a few newly added, custom, fully-connected classification layers. These are used as the initial weights in step 2 later on. To perform this initialization, a pre-trained VGGFace model, with the final classification layers cut off, is used to extract 128 <em>bottleneck features</em> for every image. Subsequently, another tiny model, consisting of fully-connected layers, is trained on these features to perform the eventual classification. The weights are persisted to a file and loaded again in step 2.</p>\n<p>The model architecture looks like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">dense_1 (Dense)              (None, 128)               65664     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dropout_1 (Dropout)          (None, 128)               0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_2 (Dense)              (None, 4)                 516       </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">Total params: 66,180</span><br><span class=\"line\">Trainable params: 66,180</span><br><span class=\"line\">Non-trainable params: 0</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Step-2-Fine-tuning\"><a href=\"#Step-2-Fine-tuning\" class=\"headerlink\" title=\"Step 2: Fine-tuning\"></a>Step 2: Fine-tuning</h3><p>In this second step, a pre-trained VGGFace model (with the first n - 3 layers freezed) is used in combination with the pre-trained top layers from step 1 to fine-tune weights for our specific classification task. It takes mini-batches of (128, 128, 3)-shaped tensors (128 x 128 pixels, RGB) as input and predicts probabilities for each of our four target classes.</p>\n<p>The architecture of the combined model looks like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">vggface_vgg16 (Model)        (None, 512)               14714688  </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">top (Sequential)             (None, 4)                 66180     </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">Total params: 14,780,868</span><br><span class=\"line\">Trainable params: 2,425,988</span><br><span class=\"line\">Non-trainable params: 12,354,880</span><br></pre></td></tr></table></figure>\n\n<p><code>top</code> is the model described in step 1, <code>vggface_vgg16</code> is a VGG16 model and looks like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">input_3 (InputLayer)         (None, 128, 128, 3)       0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv1_1 (Conv2D)             (None, 128, 128, 64)      1792      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv1_2 (Conv2D)             (None, 128, 128, 64)      36928     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool1 (MaxPooling2D)         (None, 64, 64, 64)        0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2_1 (Conv2D)             (None, 64, 64, 128)       73856     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2_2 (Conv2D)             (None, 64, 64, 128)       147584    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool2 (MaxPooling2D)         (None, 32, 32, 128)       0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv3_1 (Conv2D)             (None, 32, 32, 256)       295168    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv3_2 (Conv2D)             (None, 32, 32, 256)       590080    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv3_3 (Conv2D)             (None, 32, 32, 256)       590080    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool3 (MaxPooling2D)         (None, 16, 16, 256)       0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv4_1 (Conv2D)             (None, 16, 16, 512)       1180160   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv4_2 (Conv2D)             (None, 16, 16, 512)       2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv4_3 (Conv2D)             (None, 16, 16, 512)       2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool4 (MaxPooling2D)         (None, 8, 8, 512)         0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv5_1 (Conv2D)             (None, 8, 8, 512)         2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv5_2 (Conv2D)             (None, 8, 8, 512)         2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv5_3 (Conv2D)             (None, 8, 8, 512)         2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool5 (MaxPooling2D)         (None, 4, 4, 512)         0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">global_max_pooling2d_3 (Glob (None, 512)               0         </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">Total params: 14,714,688</span><br><span class=\"line\">Trainable params: 2,359,808</span><br><span class=\"line\">Non-trainable params: 12,354,880</span><br></pre></td></tr></table></figure>\n\n\n<p>I was using Keras <em>ImageDataGenerator</em> again for loading the data, augmenting (3x) and resizing it. As <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\">recommended</a>, <em>stochastic gradient descent</em> is used with a small learning rate (10^-4) to carefully adapt weights. The model was trained for <strong>100 epochs</strong> on <strong>batches of 32 images</strong> and, again, used <strong>categorical cross entropy</strong> as a loss function. </p>\n<h3 id=\"Results-54-6-accuracy\"><a href=\"#Results-54-6-accuracy\" class=\"headerlink\" title=\"Results (54.6 % accuracy)\"></a>Results (54.6 % accuracy)</h3><p>The maximum <strong>validation accuracy of 0.64</strong> was reached after 38 epochs already. <strong>Test accuracy</strong> turned out to be <strong>0.546</strong>, which is a quite disappointing result, considering that even our simple, custom CNN-model achieved a higher accuracy. Maybe the models complexity is too high for the small amount of training data?</p>\n<h1 id=\"Inspecting-the-model\"><a href=\"#Inspecting-the-model\" class=\"headerlink\" title=\"Inspecting the model\"></a>Inspecting the model</h1><p>To get better insights on how the model performs, I briefly inspected it with regards to several criteria. This is a short summary of my finding. </p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><ul>\n<li><a href=\"https://gist.github.com/muety/404befcfb2eef4b59398f3c8590ce692\">inspection.ipynb</a> </li>\n</ul>\n<h2 id=\"Class-distribution\"><a href=\"#Class-distribution\" class=\"headerlink\" title=\"Class distribution\"></a>Class distribution</h2><p>The first thing I looked at was the class distribution. How are the four study major subjects represented in our data and what does the model predict?</p>\n<p> | cs | econ | german | mechanical</p>\n<ul>\n<li>| - | - | - | -</li>\n</ul>\n<p><em>real</em> | 0.2510 | 0.2808 | 0.2127 | 0.2553<br><em>pred</em> | 0.2595 | 0.2936 | 0.1361 | 0.3106</p>\n<p>Apparently, the model neglects the class of <em>german linguists</em> a bit. That is also the class for which we have the least training data. Probably I should collect more.</p>\n<h2 id=\"Examples-of-false-classifications\"><a href=\"#Examples-of-false-classifications\" class=\"headerlink\" title=\"Examples of false classifications\"></a>Examples of false classifications</h2><p>I wanted to get an idea of what the model does wrong and what it does right. Consequently, I took a look at the top (with respect to confidence) five <strong>(1) false negatives</strong>, <strong>(2) false positives</strong> and <strong>(3) true positives</strong>. </p>\n<p>Here is an excerpt for class <em>econ</em>:</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/academic_faces2.png\"></p>\n<p>The top row shows examples of economists, who the model didnt recognize as such.<br>The center row depicts examples of what the model thinks economists look like, but who are actually students / researchers with a different major.<br>Finally, the bottom row shows examples of good matches, i.e. people for whom the model had a very high confidence for their actual class.</p>\n<p>Again, if you are in one of these pictures and want to get removed, please contact me.</p>\n<h2 id=\"Confusion-matrix\"><a href=\"#Confusion-matrix\" class=\"headerlink\" title=\"Confusion matrix\"></a>Confusion matrix</h2><p>To see which profession the model is unsure about, I calculated the confusion matrix.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array([[12.76595745,  5.95744681,  0.        ,  6.38297872],</span><br><span class=\"line\">       [ 3.40425532, 12.76595745,  3.82978723,  8.08510638],</span><br><span class=\"line\">       [ 3.82978723,  5.53191489,  8.5106383 ,  3.40425532],</span><br><span class=\"line\">       [ 5.95744681,  5.10638298,  1.27659574, 13.19148936]])</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/academic_faces3.png\"><br><strong>Legend:</strong></p>\n<ul>\n<li>0 = cs, 1 = econ, 2 = german, 3 = mechanical</li>\n<li>Brighter colors ~ higher value</li>\n</ul>\n<p>What we can read from the confusion matrix is that, for instance, the model tends to classify economists as mechanical engineers quite often. </p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>First of all, this is not a scientific study, but rather a small hobby project of mine. Also, it does not have a lot of real-world importance, since one might rarely want to classify students into four categories.</p>\n<p>Although the results are not spectacular, I am still quite happy about them and at least my model was able to do a lot better than random guessing. Given an <strong>accuracy of 57 %</strong> with four classes, you could definitely say that it is, to some extent, possible to learn a stereotypical-looking persons study major from only in image of their face. Of course, this only holds true within a bounded context and under a set of restrictions, but it is still an interesting insight to me. </p>\n<p>Moreover, I am quite sure that there is still a lot of room for improvements to the model, which could yield a better performance. Those might include:</p>\n<ul>\n<li>More training data from a wider range of sources</li>\n<li>More thorough preprocessing (e.g. filter out images of secretaries)</li>\n<li>Different model architecture</li>\n<li>Hyper-parameter tuning</li>\n<li>Manual feature engineering</li>\n<li></li>\n</ul>\n<p>Please let me know what you think of this project. I would love to get some feedback!</p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1 id=\"The-Idea\"><a href=\"#The-Idea\" class=\"headerlink\" title=\"The Idea\"></a>The Idea</h1><p>A few months ago I read a paper with the title <a href=\"https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual\">Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images</a>, which caused a lot of <a href=\"https://news.ycombinator.com/item?id=15198997\">controversy</a>. While I dont want to comment on the methodology and quality of the paper (that was already done, e.g. in <a href=\"https://www.fast.ai/2017/09/13/kosinski/\">an article by Jeremy Howard</a>), I found it very interesting and inspiring. In a nutshell, the researchers collected face pictures from dating websites and built a machine learning model to classify peoples sexual orientation and reached quite an impressive accuracy with their approach.</p>\n<p><a href=\"https://scatter.wordpress.com/2017/09/10/guest-post-artificial-intelligence-discovers-gayface-sigh/\">This guest post</a> summarizes the results as: </p>\n<blockquote>\n<p>AI Cant Tell if Youre Gay But it Can Tell if Youre a Walking Stereotype.</p>\n</blockquote>\n<p> And indeed, we often see people who look very stereotypical. I tried to think of more such scenarios and came to the conclusion that another environment, where this phenomenon can be found quite often, is a university campus. So often you walk around the campus and see students, who just look like a law student, a computer science nerd, a sportsman, etc. Sometimes Im so curious that I almost want to ask them whether my assumption is correct.</p>\n<p> After having read the above paper, I wondered if some machine learning model might be able to quantify these latent assumptions and find out a stereotypical-looking students profession or major. </p>\n<p>Although I only have a little more than basic knowledge in machine learning, especially in image classification using deep neural nets, I took it as a personal challenge to <strong>build a classifier, that detects academics major based on an image of their face</strong>. </p>\n<h1 id=\"Disclaimer\"><a href=\"#Disclaimer\" class=\"headerlink\" title=\"Disclaimer\"></a>Disclaimer</h1><p>Please dont take this article too serious. Im not a machine learning expert or a professional scientist. There might be some mistakes in my methodology or implementation. However, Id love to hear your thoughts and feedback.</p>\n<h1 id=\"Approach\"><a href=\"#Approach\" class=\"headerlink\" title=\"Approach\"></a>Approach</h1><p>My first (and final) approach was to <strong>(1.) collect face pictures of students</strong> or other academics, <strong>(2.) label them</strong> with a small, limited set of classes, corresponding to their major, and eventually <strong>(3.) fit a convolutional neural net (CNN)</strong> as a classifier. I thought of fields of study, whose students might potentially look a bit stereotypical and came up with four classes:</p>\n<ol>\n<li>computer science (~ cs)</li>\n<li>economics (~ econ)</li>\n<li>(German) linguistics (~ german)</li>\n<li>mechanical engineering (~ mechanical)</li>\n</ol>\n<p>Please note that this is not meant to be offending by any means! (Im a computer science nerd myself ).</p>\n<h1 id=\"Getting-the-data\"><a href=\"#Getting-the-data\" class=\"headerlink\" title=\"Getting the data\"></a>Getting the data</h1><p>The very first prerequisite is training data - as usual, when doing machine learning. And since I aimed at training a convolutional neural net (CNN), there should be a lot of data, preferably.</p>\n<p>While it would have been a funny approach to walk around my campus and ask students for their major and a picture of their face, I would probably not have ended up with a lot of data. Instead, I decided to <strong>crawl pictures from university websites</strong>. Almost every department at every university has a page called <a href=\"http://dbis.ipd.kit.edu/english/722.php\">Staff</a>, People, Researchers or the like on their websites. While these are not particularly lists of students, but of professors, research assistants and PhD candidates, I presumed that those pictures should still be sufficient as training data. </p>\n<p>I wrote a bunch of <strong>crawler scripts</strong> using Python and <a href=\"https://www.seleniumhq.org/\">Selenium WebDriver</a> to crawl <strong>57</strong> different websites, including the websites of various departments of the following universities:</p>\n<ul>\n<li>Karlsruhe Institute of Technology</li>\n<li>TU Munich</li>\n<li>University of Munich</li>\n<li>University of Wrzburg</li>\n<li>University of Siegen</li>\n<li>University of Bayreuth</li>\n<li>University of Feiburg</li>\n<li>University of Heidelberg</li>\n<li>University of Erlangen</li>\n<li>University of Bamberg</li>\n<li>University of Mannheim</li>\n</ul>\n<p>After a bit of manual data cleaning (removing pictures without faces, rotating pictures, ), I ended up with a total of <strong>1369</strong> labeled images from four different classes. While this is not very much data for training a CNN, I decided to give it a try anyway.</p>\n<h2 id=\"Examples\"><a href=\"#Examples\" class=\"headerlink\" title=\"Examples\"></a>Examples</h2><h3 id=\"Images\"><a href=\"#Images\" class=\"headerlink\" title=\"Images\"></a>Images</h3><p><strong>An excerpt from the folder containing all raw images after crawling:</strong><br><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/academic_faces1.png\" alt=\"Excerpt from all crawled raw images\"><br>(If you are in one of these pictures and want to get removed, please contact me.)</p>\n<h3 id=\"Labels\"><a href=\"#Labels\" class=\"headerlink\" title=\"Labels\"></a>Labels</h3><p><strong>An excerpt from <code>index.csv</code> containing labels and meta-data for every image:</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">id,category,image_url,name</span><br><span class=\"line\">c35464fd,mechanical,http:&#x2F;&#x2F;www.fast.kit.edu&#x2F;lff&#x2F;1011_1105.php,Prof. Dr. rer. nat. Frank Gauterin</span><br><span class=\"line\">a73d11a7,cs,http:&#x2F;&#x2F;h2t.anthropomatik.kit.edu&#x2F;21_1784.php,Kevin Liang</span><br><span class=\"line\">97e230ff,econ,http:&#x2F;&#x2F;marketing.iism.kit.edu&#x2F;21_371.php,Dr. Hanna Schumacher</span><br><span class=\"line\">cde71a5c,german,https:&#x2F;&#x2F;www.germanistik.uni-muenchen.de&#x2F;personal&#x2F;ndl&#x2F;mitarbeiter&#x2F;bach&#x2F;index.html,Dr. Oliver Bach</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Preprocessing-the-data\"><a href=\"#Preprocessing-the-data\" class=\"headerlink\" title=\"Preprocessing the data\"></a>Preprocessing the data</h1><p>Before the images could be used as training data for a learning algorithm, a bit of preprocessing needed to be applied. Mainly, I did two major steps of preprocessing.</p>\n<ol>\n<li><strong>Cropping</strong> images to faces - As you can see, pictures are taken from different angles, some of them contain a lot of background, some are not centered, etc. To get better training data, the pictures have to be cropped to only the face and nothing else. </li>\n<li><strong>Scaling</strong> - All pictures come in different resolutions, but eventually need to be of exactly the same size in order to be used as input to a neural network. </li>\n</ol>\n<p>To achieve both of these preprocessing steps I used a great, little, open-source, OpenCV-based Python tool called <a href=\"https://github.com/leblancfg/autocrop\">autocrop</a> with the following command:</p>\n<p><code>autocrop -i raw -o preprocessed -w 128 -H 128 &gt; autocrop.log</code>.</p>\n<p>This detects the face in every picture in <code>raw</code> folder, crops the picture to that face, re-scales the resulting image to 128 x 128 pixels and saves it to <code>preprocessed</code> folder. Of course, there are some pictures in which the algorithm can not detect a face. Those are logged to stdout and persisted to <code>autocrop.log</code>.</p>\n<p>In addition, I wrote a script that parses <code>autocrop.log</code> to get the failed images and subsequently split the images into <em>train</em> (70 %), <em>test</em> (20 %) and <em>validation</em> (10 %) and copy them to a folder structure that is compatible to the format required by <a href=\"https://keras.io/preprocessing/image/\">Keras ImageDataGenerator</a> to read training data.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- raw</span><br><span class=\"line\">    - index.csv</span><br><span class=\"line\">    - c35464fd.jpg</span><br><span class=\"line\">    - a73d11a7.jpg</span><br><span class=\"line\">    - ...</span><br><span class=\"line\">- preprocessed </span><br><span class=\"line\">    - train</span><br><span class=\"line\">        - cs</span><br><span class=\"line\">            - a73d11a7.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - econ</span><br><span class=\"line\">            - 97e230ff.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - german</span><br><span class=\"line\">            - cde71a5c.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - mechanical</span><br><span class=\"line\">            - c35464fd.jpg</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">    - test</span><br><span class=\"line\">        - cs</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - ...</span><br><span class=\"line\">    - validation</span><br><span class=\"line\">        - cs</span><br><span class=\"line\">            - ...</span><br><span class=\"line\">        - ...</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Building-a-model\"><a href=\"#Building-a-model\" class=\"headerlink\" title=\"Building a model\"></a>Building a model</h1><h2 id=\"Approach-1-Simple-custom-CNN\"><a href=\"#Approach-1-Simple-custom-CNN\" class=\"headerlink\" title=\"Approach 1: Simple, custom CNN\"></a>Approach 1: Simple, custom CNN</h2><p><strong>Code</strong></p>\n<ul>\n<li><a href=\"https://gist.github.com/muety/78bf6d7929e4facd199ad0ffea0b3ad9\">custom_model.ipynb</a></li>\n</ul>\n<p>I decided to start simple and see if anything can be learned from the data at all. I defined the following simple CNN architecture in Keras: </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">conv2d_1 (Conv2D)            (None, 62, 62, 32)        320       </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_3 (Conv2D)            (None, 12, 12, 32)        9248      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">flatten_1 (Flatten)          (None, 1152)              0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_1 (Dense)              (None, 64)                73792     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dropout_1 (Dropout)          (None, 64)                0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_2 (Dense)              (None, 4)                 260       </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">Total params: 92,868</span><br><span class=\"line\">Trainable params: 92,868</span><br><span class=\"line\">Non-trainable params: 0</span><br></pre></td></tr></table></figure>\n\n<p>I used Keras <a href=\"https://keras.io/preprocessing/image/\">ImageDataGenerator</a> (great tool!) to read images into NumPy arrays, re-scale them to a shape of <code>(64, 63, 3)</code> (64 x 64 pixels, RGB) and perform some data augmentation using transformations like rotations, zooming, horizontal flipping, etc. to blow up my training data and hopefully build more robust, less overfitted models.</p>\n<p>I let the model train for <strong>100 epochs</strong>, using the <strong>Adam optimizer</strong> with default parameters and <strong>categorical crossentropy loss</strong>, a mini-batch size of <strong>32</strong> and <strong>3x augmentation</strong> (use transformations to blow up training data by a factor of three). </p>\n<h3 id=\"Results-57-1-accuracy\"><a href=\"#Results-57-1-accuracy\" class=\"headerlink\" title=\"Results (57.1 % accuracy)\"></a>Results (57.1 % accuracy)</h3><p>The maximum <strong>validation accuracy of 0.66</strong> was reached after 74 epochs. <strong>Test accuracy</strong> turned out to be <strong>0.571</strong>. Considering that a quite simple model was trained completely from scratch with less than 1000 training examples, I am quite impressed by that result. It means that on average the model predicts more than every second students major correctly. The <strong>a-priori probability</strong> of a correct classification <strong>is 0.25</strong>, so the model has definitely learned at least something.</p>\n<h2 id=\"Approach-2-Fine-tuning-VGGFace\"><a href=\"#Approach-2-Fine-tuning-VGGFace\" class=\"headerlink\" title=\"Approach 2: Fine-tuning VGGFace\"></a>Approach 2: Fine-tuning VGGFace</h2><p><strong>Code</strong></p>\n<ul>\n<li><a href=\"https://gist.github.com/muety/a079dcb27d921d58323c9574152b2c2d\">vggfaces_bottleneck_model.ipynb</a></li>\n<li><a href=\"https://gist.github.com/muety/c3b9e9401f178807c91ad890a6c67e18\">vggfaces_finetuned_model.ipynb</a></li>\n</ul>\n<p>As an alternative to a simple, custom-defined CNN model, that is trained from scratch, I wanted to follow the common approach of fine-tuning the weights of an existing, pre-trained model. The basic idea of such an approach is to not re-invent the wheel, but take advantage of what was already learned before and only slightly adapt that knowledge (in form of weights) to a certain problem. Latent features in images, which a learning algorithm had already extracted from a giant set of training data before, can just be leveraged. <a href=\"https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/\">Image Classification using pre-trained models in Keras</a> gives an excellent overview of how <strong>fine-tuning</strong> works and how it is different from <strong>transfer learning</strong> and custom models. Expectations are that my given classification problem can be solved more accurately with less data. </p>\n<p> I decided to take a <strong>VGG16</strong> model architecture trained on <a href=\"http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/\"><strong>VGGFace</strong></a> as a base (using the <a href=\"https://github.com/rcmalli/keras-vggface\">keras-vggface</a> implementation) and followed <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\">this guide</a> to fine-tune it. VGGFace is a dataset published by the University of Oxford that contains more than 3.3 million face images. Accordingly, I expected it to have extracted very robust facial features and to be quite well-suited for face classification. </p>\n<h3 id=\"Step-1-Transfer-learning-to-initialize-weights\"><a href=\"#Step-1-Transfer-learning-to-initialize-weights\" class=\"headerlink\" title=\"Step 1: Transfer-learning to initialize weights\"></a>Step 1: Transfer-learning to initialize weights</h3><p>My implementation consists of two steps, since <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\">it is recommended</a> that</p>\n<blockquote>\n<p>in order to perform fine-tuning, all layers should start with properly trained weights.</p>\n</blockquote>\n<p>In this first step, transfer-learning is used to find proper weights for a set of a few newly added, custom, fully-connected classification layers. These are used as the initial weights in step 2 later on. To perform this initialization, a pre-trained VGGFace model, with the final classification layers cut off, is used to extract 128 <em>bottleneck features</em> for every image. Subsequently, another tiny model, consisting of fully-connected layers, is trained on these features to perform the eventual classification. The weights are persisted to a file and loaded again in step 2.</p>\n<p>The model architecture looks like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">dense_1 (Dense)              (None, 128)               65664     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dropout_1 (Dropout)          (None, 128)               0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_2 (Dense)              (None, 4)                 516       </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">Total params: 66,180</span><br><span class=\"line\">Trainable params: 66,180</span><br><span class=\"line\">Non-trainable params: 0</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Step-2-Fine-tuning\"><a href=\"#Step-2-Fine-tuning\" class=\"headerlink\" title=\"Step 2: Fine-tuning\"></a>Step 2: Fine-tuning</h3><p>In this second step, a pre-trained VGGFace model (with the first n - 3 layers freezed) is used in combination with the pre-trained top layers from step 1 to fine-tune weights for our specific classification task. It takes mini-batches of (128, 128, 3)-shaped tensors (128 x 128 pixels, RGB) as input and predicts probabilities for each of our four target classes.</p>\n<p>The architecture of the combined model looks like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">vggface_vgg16 (Model)        (None, 512)               14714688  </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">top (Sequential)             (None, 4)                 66180     </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">Total params: 14,780,868</span><br><span class=\"line\">Trainable params: 2,425,988</span><br><span class=\"line\">Non-trainable params: 12,354,880</span><br></pre></td></tr></table></figure>\n\n<p><code>top</code> is the model described in step 1, <code>vggface_vgg16</code> is a VGG16 model and looks like this:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (type)                 Output Shape              Param #   </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">input_3 (InputLayer)         (None, 128, 128, 3)       0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv1_1 (Conv2D)             (None, 128, 128, 64)      1792      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv1_2 (Conv2D)             (None, 128, 128, 64)      36928     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool1 (MaxPooling2D)         (None, 64, 64, 64)        0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2_1 (Conv2D)             (None, 64, 64, 128)       73856     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2_2 (Conv2D)             (None, 64, 64, 128)       147584    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool2 (MaxPooling2D)         (None, 32, 32, 128)       0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv3_1 (Conv2D)             (None, 32, 32, 256)       295168    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv3_2 (Conv2D)             (None, 32, 32, 256)       590080    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv3_3 (Conv2D)             (None, 32, 32, 256)       590080    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool3 (MaxPooling2D)         (None, 16, 16, 256)       0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv4_1 (Conv2D)             (None, 16, 16, 512)       1180160   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv4_2 (Conv2D)             (None, 16, 16, 512)       2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv4_3 (Conv2D)             (None, 16, 16, 512)       2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool4 (MaxPooling2D)         (None, 8, 8, 512)         0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv5_1 (Conv2D)             (None, 8, 8, 512)         2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv5_2 (Conv2D)             (None, 8, 8, 512)         2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv5_3 (Conv2D)             (None, 8, 8, 512)         2359808   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">pool5 (MaxPooling2D)         (None, 4, 4, 512)         0         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">global_max_pooling2d_3 (Glob (None, 512)               0         </span><br><span class=\"line\">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">Total params: 14,714,688</span><br><span class=\"line\">Trainable params: 2,359,808</span><br><span class=\"line\">Non-trainable params: 12,354,880</span><br></pre></td></tr></table></figure>\n\n\n<p>I was using Keras <em>ImageDataGenerator</em> again for loading the data, augmenting (3x) and resizing it. As <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\">recommended</a>, <em>stochastic gradient descent</em> is used with a small learning rate (10^-4) to carefully adapt weights. The model was trained for <strong>100 epochs</strong> on <strong>batches of 32 images</strong> and, again, used <strong>categorical cross entropy</strong> as a loss function. </p>\n<h3 id=\"Results-54-6-accuracy\"><a href=\"#Results-54-6-accuracy\" class=\"headerlink\" title=\"Results (54.6 % accuracy)\"></a>Results (54.6 % accuracy)</h3><p>The maximum <strong>validation accuracy of 0.64</strong> was reached after 38 epochs already. <strong>Test accuracy</strong> turned out to be <strong>0.546</strong>, which is a quite disappointing result, considering that even our simple, custom CNN-model achieved a higher accuracy. Maybe the models complexity is too high for the small amount of training data?</p>\n<h1 id=\"Inspecting-the-model\"><a href=\"#Inspecting-the-model\" class=\"headerlink\" title=\"Inspecting the model\"></a>Inspecting the model</h1><p>To get better insights on how the model performs, I briefly inspected it with regards to several criteria. This is a short summary of my finding. </p>\n<h2 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h2><ul>\n<li><a href=\"https://gist.github.com/muety/404befcfb2eef4b59398f3c8590ce692\">inspection.ipynb</a> </li>\n</ul>\n<h2 id=\"Class-distribution\"><a href=\"#Class-distribution\" class=\"headerlink\" title=\"Class distribution\"></a>Class distribution</h2><p>The first thing I looked at was the class distribution. How are the four study major subjects represented in our data and what does the model predict?</p>\n<p> | cs | econ | german | mechanical</p>\n<ul>\n<li>| - | - | - | -</li>\n</ul>\n<p><em>real</em> | 0.2510 | 0.2808 | 0.2127 | 0.2553<br><em>pred</em> | 0.2595 | 0.2936 | 0.1361 | 0.3106</p>\n<p>Apparently, the model neglects the class of <em>german linguists</em> a bit. That is also the class for which we have the least training data. Probably I should collect more.</p>\n<h2 id=\"Examples-of-false-classifications\"><a href=\"#Examples-of-false-classifications\" class=\"headerlink\" title=\"Examples of false classifications\"></a>Examples of false classifications</h2><p>I wanted to get an idea of what the model does wrong and what it does right. Consequently, I took a look at the top (with respect to confidence) five <strong>(1) false negatives</strong>, <strong>(2) false positives</strong> and <strong>(3) true positives</strong>. </p>\n<p>Here is an excerpt for class <em>econ</em>:</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/academic_faces2.png\"></p>\n<p>The top row shows examples of economists, who the model didnt recognize as such.<br>The center row depicts examples of what the model thinks economists look like, but who are actually students / researchers with a different major.<br>Finally, the bottom row shows examples of good matches, i.e. people for whom the model had a very high confidence for their actual class.</p>\n<p>Again, if you are in one of these pictures and want to get removed, please contact me.</p>\n<h2 id=\"Confusion-matrix\"><a href=\"#Confusion-matrix\" class=\"headerlink\" title=\"Confusion matrix\"></a>Confusion matrix</h2><p>To see which profession the model is unsure about, I calculated the confusion matrix.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">array([[12.76595745,  5.95744681,  0.        ,  6.38297872],</span><br><span class=\"line\">       [ 3.40425532, 12.76595745,  3.82978723,  8.08510638],</span><br><span class=\"line\">       [ 3.82978723,  5.53191489,  8.5106383 ,  3.40425532],</span><br><span class=\"line\">       [ 5.95744681,  5.10638298,  1.27659574, 13.19148936]])</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/academic_faces3.png\"><br><strong>Legend:</strong></p>\n<ul>\n<li>0 = cs, 1 = econ, 2 = german, 3 = mechanical</li>\n<li>Brighter colors ~ higher value</li>\n</ul>\n<p>What we can read from the confusion matrix is that, for instance, the model tends to classify economists as mechanical engineers quite often. </p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>First of all, this is not a scientific study, but rather a small hobby project of mine. Also, it does not have a lot of real-world importance, since one might rarely want to classify students into four categories.</p>\n<p>Although the results are not spectacular, I am still quite happy about them and at least my model was able to do a lot better than random guessing. Given an <strong>accuracy of 57 %</strong> with four classes, you could definitely say that it is, to some extent, possible to learn a stereotypical-looking persons study major from only in image of their face. Of course, this only holds true within a bounded context and under a set of restrictions, but it is still an interesting insight to me. </p>\n<p>Moreover, I am quite sure that there is still a lot of room for improvements to the model, which could yield a better performance. Those might include:</p>\n<ul>\n<li>More training data from a wider range of sources</li>\n<li>More thorough preprocessing (e.g. filter out images of secretaries)</li>\n<li>Different model architecture</li>\n<li>Hyper-parameter tuning</li>\n<li>Manual feature engineering</li>\n<li></li>\n</ul>\n<p>Please let me know what you think of this project. I would love to get some feedback!</p>\n"},{"title":"Mastering Software Versioning in JavaScript Projects","date":"2020-11-07T15:22:37.000Z","_content":"\n# Introduction\n\nA frequently overlooked aspect of software development is the proper versioning of code. Consistent and descriptive version numbers not only help developers keep track of their own work, but can also inform users of your software about what to expect from a new release. While versioning is especially important for libraries and frameworks which other projects depend on, benefits apply to standalone applications equally.\n\nIn this article, we introduce techniques, conventions, and tooling that helped us establish a robust way of versioning our JavaScript- and/or TypeScript-based software projects. \n\n# Concepts\n\n## Semantic Versioning\n\nOne of the most important aspects to think of when it comes to versioning is the version number itself. Before caring about tooling and others, you need to come up with syntax and semantics for it. \n\nA concept that is well established among open source software projects is [Semantic Versioning](https://semver.org/), or _SemVer_. When following this specification, a version number consists of three digits separated by dots, like `1.5.4` or, more formally `<MAJOR>.<MINOR>.<PATCH>`, where each individual part has a meaning:\n\n* `MAJOR`: Incrementing it indicates that there have been fundamental changes to the software and the new version is most likely not backward-compatible with the previous one.\n* `MINOR`: Essentially indicates that new features were added, but backward-compatibiltiy is still guaranteed.\n* `PATCH` or `BUG FIX`: Gives a hint that minor changes or bug fixes had been introduced recently.\n\nStrictly following these semantics helps to maintain a common understanding of what a certain version means and what to expect from a new release.\n\n## Conventional Commits\n\nThe second concept that we committed ourselves to follow is [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/#summary). Similar to semantic versioning, the conventional commits specification provides common syntax and semantics for information provided by a developer. However, in this case, the convention is not about the version number itself, but about the commit messages composed by developers when checking in new code into version control. The goal is to standardize their format and make them machine-readable to a certain extent.\n\nWhen following conventional commits, a commit message essentially has to be prefixed with one of a few keywords.\n\n* `fix:`  A commit message with this prefix indicates a bug fix\n* `feat:`  A commit message with this prefix indicates the introduction of a new feature or functionality\n* `refactor:`  A commit with, whose message is prefixed like this, contains code refactorings, i.e. internal, technical modifications of the implementation of certain logic\n* `chore:`  This prefix indicates minor, miscellaneous changes of any type, that do not necessarily affect the user immediately\n* `BREAKING CHANGE!:` A commit message with this prefix warns about comprehensive, fundamental changes and indicates that the newly released version is likely to be incompatible with the previous one\n\nThe conventional commits specification comprises a few more keywords and also allows developers to come up with custom ones. However, these are the most relevant ones.\n\n# Tooling\n\nWhen having paid attention, one might have recognized a few similarities in the semantics of conventional commits and semantic versioning. Commits with `fix`-changes correspond to the `PATCH` version, `feat` goes well with the `MINOR` version and `BREAKING CHANGE`es will inevitably result in a new `MAJOR` version. \n\nAs a consequence of following the above conventions, we enabled our project for an automated versioning workflow. \n\n## [standard-version](https://www.npmjs.com/package/standard-version) CLI\n\n[standard-version](https://www.npmjs.com/package/standard-version) is a JavaScript tool that utilizes conventional commits to automatically enforce semantic versioning. Moreover, it is capable of automatically generating a changelog in Markdown format, which developers can provide their users with. \n\nWhen running `standard-version`, the tool scans your commit history since when it was last executed, searches for fixes, feats, or breaking changes, and adapts the project's version accordingly.\n\nTo add it to an existing project, all you need to do is:\n\n1. Install it as a dependency\n```bash\n$ yarn add -D standard-version  # (or npm i --save-dev standard-version)\n```\n\n2. Optionally add it as an NPM script to your `package.json`\n```json\n{\n    \"name\": \"your-cool-project\",\n    \"version:\": \"0.0.1\",\n    ...\n    \"scripts:\" {\n        \"release\": \"standard-version\"\n        ...\n    }\n    ...\n}\n```\n\n# Release Workflow\n\nAfter the development team has committed to consequently follow the conventional commits specification and all tooling is set up, a typical workflow to release new versions of your software might look like so.\n\nOnce a new version is ready to be released, i.e. at the end of a sprint, a developer executes `yarn release` (or `npm run release`) to kick off `standard-version`. As a result ...\n\n1. ... the project's commit history is scanned to determine which part of the version number needs to be incremented\n1. ... the `version` property of the project's top-level `package.json` is set to the new version\n1. ... a `CHANGELOG.md` file is written, containing separate sections for features and bug fixes\n1. ... the changes are committed to Git\n1. ... the new commit is given a Git tag corresponding to the new version\n\nDepending on your setup, a push to the remote repository might kick off your CI/CD workflow, which may automatically build a new Docker image with the newly introduced tag and push it to a public or private registry. Using tools like [Watchtower](https://github.com/containrrr/watchtower), the new image might even be rolled out to production automatically.\n\nThe only manual steps required in this workflow were a single `yarn release` command and a Git push. Nothing more, nothing less.\n\n# Conclusion\n\nThe above workflow has proven to be a convenient and consistent way of managing and releasing new versions of our JavaScript- and TypeScript-based frontend-, backend- and library projects and is even more beneficial with proper CI/CD pipelines and tooling like [GitLab](https://gitlab.com), [Docker](https://docker.io), [Watchtower](https://github.com/containrrr/watchtower), [Portainer](https://portainer.io), and others. It might even be adapted to projects written in other programming languages. ","source":"_posts/mastering-software-versioning-in-javascript-projects.md","raw":"---\ntitle: Mastering Software Versioning in JavaScript Projects\ndate: 2020-11-07 16:22:37\ntags:\n---\n\n# Introduction\n\nA frequently overlooked aspect of software development is the proper versioning of code. Consistent and descriptive version numbers not only help developers keep track of their own work, but can also inform users of your software about what to expect from a new release. While versioning is especially important for libraries and frameworks which other projects depend on, benefits apply to standalone applications equally.\n\nIn this article, we introduce techniques, conventions, and tooling that helped us establish a robust way of versioning our JavaScript- and/or TypeScript-based software projects. \n\n# Concepts\n\n## Semantic Versioning\n\nOne of the most important aspects to think of when it comes to versioning is the version number itself. Before caring about tooling and others, you need to come up with syntax and semantics for it. \n\nA concept that is well established among open source software projects is [Semantic Versioning](https://semver.org/), or _SemVer_. When following this specification, a version number consists of three digits separated by dots, like `1.5.4` or, more formally `<MAJOR>.<MINOR>.<PATCH>`, where each individual part has a meaning:\n\n* `MAJOR`: Incrementing it indicates that there have been fundamental changes to the software and the new version is most likely not backward-compatible with the previous one.\n* `MINOR`: Essentially indicates that new features were added, but backward-compatibiltiy is still guaranteed.\n* `PATCH` or `BUG FIX`: Gives a hint that minor changes or bug fixes had been introduced recently.\n\nStrictly following these semantics helps to maintain a common understanding of what a certain version means and what to expect from a new release.\n\n## Conventional Commits\n\nThe second concept that we committed ourselves to follow is [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/#summary). Similar to semantic versioning, the conventional commits specification provides common syntax and semantics for information provided by a developer. However, in this case, the convention is not about the version number itself, but about the commit messages composed by developers when checking in new code into version control. The goal is to standardize their format and make them machine-readable to a certain extent.\n\nWhen following conventional commits, a commit message essentially has to be prefixed with one of a few keywords.\n\n* `fix:`  A commit message with this prefix indicates a bug fix\n* `feat:`  A commit message with this prefix indicates the introduction of a new feature or functionality\n* `refactor:`  A commit with, whose message is prefixed like this, contains code refactorings, i.e. internal, technical modifications of the implementation of certain logic\n* `chore:`  This prefix indicates minor, miscellaneous changes of any type, that do not necessarily affect the user immediately\n* `BREAKING CHANGE!:` A commit message with this prefix warns about comprehensive, fundamental changes and indicates that the newly released version is likely to be incompatible with the previous one\n\nThe conventional commits specification comprises a few more keywords and also allows developers to come up with custom ones. However, these are the most relevant ones.\n\n# Tooling\n\nWhen having paid attention, one might have recognized a few similarities in the semantics of conventional commits and semantic versioning. Commits with `fix`-changes correspond to the `PATCH` version, `feat` goes well with the `MINOR` version and `BREAKING CHANGE`es will inevitably result in a new `MAJOR` version. \n\nAs a consequence of following the above conventions, we enabled our project for an automated versioning workflow. \n\n## [standard-version](https://www.npmjs.com/package/standard-version) CLI\n\n[standard-version](https://www.npmjs.com/package/standard-version) is a JavaScript tool that utilizes conventional commits to automatically enforce semantic versioning. Moreover, it is capable of automatically generating a changelog in Markdown format, which developers can provide their users with. \n\nWhen running `standard-version`, the tool scans your commit history since when it was last executed, searches for fixes, feats, or breaking changes, and adapts the project's version accordingly.\n\nTo add it to an existing project, all you need to do is:\n\n1. Install it as a dependency\n```bash\n$ yarn add -D standard-version  # (or npm i --save-dev standard-version)\n```\n\n2. Optionally add it as an NPM script to your `package.json`\n```json\n{\n    \"name\": \"your-cool-project\",\n    \"version:\": \"0.0.1\",\n    ...\n    \"scripts:\" {\n        \"release\": \"standard-version\"\n        ...\n    }\n    ...\n}\n```\n\n# Release Workflow\n\nAfter the development team has committed to consequently follow the conventional commits specification and all tooling is set up, a typical workflow to release new versions of your software might look like so.\n\nOnce a new version is ready to be released, i.e. at the end of a sprint, a developer executes `yarn release` (or `npm run release`) to kick off `standard-version`. As a result ...\n\n1. ... the project's commit history is scanned to determine which part of the version number needs to be incremented\n1. ... the `version` property of the project's top-level `package.json` is set to the new version\n1. ... a `CHANGELOG.md` file is written, containing separate sections for features and bug fixes\n1. ... the changes are committed to Git\n1. ... the new commit is given a Git tag corresponding to the new version\n\nDepending on your setup, a push to the remote repository might kick off your CI/CD workflow, which may automatically build a new Docker image with the newly introduced tag and push it to a public or private registry. Using tools like [Watchtower](https://github.com/containrrr/watchtower), the new image might even be rolled out to production automatically.\n\nThe only manual steps required in this workflow were a single `yarn release` command and a Git push. Nothing more, nothing less.\n\n# Conclusion\n\nThe above workflow has proven to be a convenient and consistent way of managing and releasing new versions of our JavaScript- and TypeScript-based frontend-, backend- and library projects and is even more beneficial with proper CI/CD pipelines and tooling like [GitLab](https://gitlab.com), [Docker](https://docker.io), [Watchtower](https://github.com/containrrr/watchtower), [Portainer](https://portainer.io), and others. It might even be adapted to projects written in other programming languages. ","slug":"mastering-software-versioning-in-javascript-projects","published":1,"updated":"2020-11-07T16:52:29.982Z","_id":"ckh7u6y57000077e0hrxnem22","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>A frequently overlooked aspect of software development is the proper versioning of code. Consistent and descriptive version numbers not only help developers keep track of their own work, but can also inform users of your software about what to expect from a new release. While versioning is especially important for libraries and frameworks which other projects depend on, benefits apply to standalone applications equally.</p>\n<p>In this article, we introduce techniques, conventions, and tooling that helped us establish a robust way of versioning our JavaScript- and/or TypeScript-based software projects. </p>\n<h1 id=\"Concepts\"><a href=\"#Concepts\" class=\"headerlink\" title=\"Concepts\"></a>Concepts</h1><h2 id=\"Semantic-Versioning\"><a href=\"#Semantic-Versioning\" class=\"headerlink\" title=\"Semantic Versioning\"></a>Semantic Versioning</h2><p>One of the most important aspects to think of when it comes to versioning is the version number itself. Before caring about tooling and others, you need to come up with syntax and semantics for it. </p>\n<p>A concept that is well established among open source software projects is <a href=\"https://semver.org/\">Semantic Versioning</a>, or <em>SemVer</em>. When following this specification, a version number consists of three digits separated by dots, like <code>1.5.4</code> or, more formally <code>&lt;MAJOR&gt;.&lt;MINOR&gt;.&lt;PATCH&gt;</code>, where each individual part has a meaning:</p>\n<ul>\n<li><code>MAJOR</code>: Incrementing it indicates that there have been fundamental changes to the software and the new version is most likely not backward-compatible with the previous one.</li>\n<li><code>MINOR</code>: Essentially indicates that new features were added, but backward-compatibiltiy is still guaranteed.</li>\n<li><code>PATCH</code> or <code>BUG FIX</code>: Gives a hint that minor changes or bug fixes had been introduced recently.</li>\n</ul>\n<p>Strictly following these semantics helps to maintain a common understanding of what a certain version means and what to expect from a new release.</p>\n<h2 id=\"Conventional-Commits\"><a href=\"#Conventional-Commits\" class=\"headerlink\" title=\"Conventional Commits\"></a>Conventional Commits</h2><p>The second concept that we committed ourselves to follow is <a href=\"https://www.conventionalcommits.org/en/v1.0.0/#summary\">Conventional Commits</a>. Similar to semantic versioning, the conventional commits specification provides common syntax and semantics for information provided by a developer. However, in this case, the convention is not about the version number itself, but about the commit messages composed by developers when checking in new code into version control. The goal is to standardize their format and make them machine-readable to a certain extent.</p>\n<p>When following conventional commits, a commit message essentially has to be prefixed with one of a few keywords.</p>\n<ul>\n<li><code>fix:</code>  A commit message with this prefix indicates a bug fix</li>\n<li><code>feat:</code>  A commit message with this prefix indicates the introduction of a new feature or functionality</li>\n<li><code>refactor:</code>  A commit with, whose message is prefixed like this, contains code refactorings, i.e. internal, technical modifications of the implementation of certain logic</li>\n<li><code>chore:</code>  This prefix indicates minor, miscellaneous changes of any type, that do not necessarily affect the user immediately</li>\n<li><code>BREAKING CHANGE!:</code> A commit message with this prefix warns about comprehensive, fundamental changes and indicates that the newly released version is likely to be incompatible with the previous one</li>\n</ul>\n<p>The conventional commits specification comprises a few more keywords and also allows developers to come up with custom ones. However, these are the most relevant ones.</p>\n<h1 id=\"Tooling\"><a href=\"#Tooling\" class=\"headerlink\" title=\"Tooling\"></a>Tooling</h1><p>When having paid attention, one might have recognized a few similarities in the semantics of conventional commits and semantic versioning. Commits with <code>fix</code>-changes correspond to the <code>PATCH</code> version, <code>feat</code> goes well with the <code>MINOR</code> version and <code>BREAKING CHANGE</code>es will inevitably result in a new <code>MAJOR</code> version. </p>\n<p>As a consequence of following the above conventions, we enabled our project for an automated versioning workflow. </p>\n<h2 id=\"standard-version-CLI\"><a href=\"#standard-version-CLI\" class=\"headerlink\" title=\"standard-version CLI\"></a><a href=\"https://www.npmjs.com/package/standard-version\">standard-version</a> CLI</h2><p><a href=\"https://www.npmjs.com/package/standard-version\">standard-version</a> is a JavaScript tool that utilizes conventional commits to automatically enforce semantic versioning. Moreover, it is capable of automatically generating a changelog in Markdown format, which developers can provide their users with. </p>\n<p>When running <code>standard-version</code>, the tool scans your commit history since when it was last executed, searches for fixes, feats, or breaking changes, and adapts the projects version accordingly.</p>\n<p>To add it to an existing project, all you need to do is:</p>\n<ol>\n<li><p>Install it as a dependency</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ yarn add -D standard-version  <span class=\"comment\"># (or npm i --save-dev standard-version)</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Optionally add it as an NPM script to your <code>package.json</code></p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">&quot;name&quot;</span>: <span class=\"string\">&quot;your-cool-project&quot;</span>,</span><br><span class=\"line\">    <span class=\"attr\">&quot;version:&quot;</span>: <span class=\"string\">&quot;0.0.1&quot;</span>,</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"string\">&quot;scripts:&quot;</span> &#123;</span><br><span class=\"line\">        <span class=\"attr\">&quot;release&quot;</span>: <span class=\"string\">&quot;standard-version&quot;</span></span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<h1 id=\"Release-Workflow\"><a href=\"#Release-Workflow\" class=\"headerlink\" title=\"Release Workflow\"></a>Release Workflow</h1><p>After the development team has committed to consequently follow the conventional commits specification and all tooling is set up, a typical workflow to release new versions of your software might look like so.</p>\n<p>Once a new version is ready to be released, i.e. at the end of a sprint, a developer executes <code>yarn release</code> (or <code>npm run release</code>) to kick off <code>standard-version</code>. As a result </p>\n<ol>\n<li> the projects commit history is scanned to determine which part of the version number needs to be incremented</li>\n<li> the <code>version</code> property of the projects top-level <code>package.json</code> is set to the new version</li>\n<li> a <code>CHANGELOG.md</code> file is written, containing separate sections for features and bug fixes</li>\n<li> the changes are committed to Git</li>\n<li> the new commit is given a Git tag corresponding to the new version</li>\n</ol>\n<p>Depending on your setup, a push to the remote repository might kick off your CI/CD workflow, which may automatically build a new Docker image with the newly introduced tag and push it to a public or private registry. Using tools like <a href=\"https://github.com/containrrr/watchtower\">Watchtower</a>, the new image might even be rolled out to production automatically.</p>\n<p>The only manual steps required in this workflow were a single <code>yarn release</code> command and a Git push. Nothing more, nothing less.</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>The above workflow has proven to be a convenient and consistent way of managing and releasing new versions of our JavaScript- and TypeScript-based frontend-, backend- and library projects and is even more beneficial with proper CI/CD pipelines and tooling like <a href=\"https://gitlab.com/\">GitLab</a>, <a href=\"https://docker.io/\">Docker</a>, <a href=\"https://github.com/containrrr/watchtower\">Watchtower</a>, <a href=\"https://portainer.io/\">Portainer</a>, and others. It might even be adapted to projects written in other programming languages. </p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>A frequently overlooked aspect of software development is the proper versioning of code. Consistent and descriptive version numbers not only help developers keep track of their own work, but can also inform users of your software about what to expect from a new release. While versioning is especially important for libraries and frameworks which other projects depend on, benefits apply to standalone applications equally.</p>\n<p>In this article, we introduce techniques, conventions, and tooling that helped us establish a robust way of versioning our JavaScript- and/or TypeScript-based software projects. </p>\n<h1 id=\"Concepts\"><a href=\"#Concepts\" class=\"headerlink\" title=\"Concepts\"></a>Concepts</h1><h2 id=\"Semantic-Versioning\"><a href=\"#Semantic-Versioning\" class=\"headerlink\" title=\"Semantic Versioning\"></a>Semantic Versioning</h2><p>One of the most important aspects to think of when it comes to versioning is the version number itself. Before caring about tooling and others, you need to come up with syntax and semantics for it. </p>\n<p>A concept that is well established among open source software projects is <a href=\"https://semver.org/\">Semantic Versioning</a>, or <em>SemVer</em>. When following this specification, a version number consists of three digits separated by dots, like <code>1.5.4</code> or, more formally <code>&lt;MAJOR&gt;.&lt;MINOR&gt;.&lt;PATCH&gt;</code>, where each individual part has a meaning:</p>\n<ul>\n<li><code>MAJOR</code>: Incrementing it indicates that there have been fundamental changes to the software and the new version is most likely not backward-compatible with the previous one.</li>\n<li><code>MINOR</code>: Essentially indicates that new features were added, but backward-compatibiltiy is still guaranteed.</li>\n<li><code>PATCH</code> or <code>BUG FIX</code>: Gives a hint that minor changes or bug fixes had been introduced recently.</li>\n</ul>\n<p>Strictly following these semantics helps to maintain a common understanding of what a certain version means and what to expect from a new release.</p>\n<h2 id=\"Conventional-Commits\"><a href=\"#Conventional-Commits\" class=\"headerlink\" title=\"Conventional Commits\"></a>Conventional Commits</h2><p>The second concept that we committed ourselves to follow is <a href=\"https://www.conventionalcommits.org/en/v1.0.0/#summary\">Conventional Commits</a>. Similar to semantic versioning, the conventional commits specification provides common syntax and semantics for information provided by a developer. However, in this case, the convention is not about the version number itself, but about the commit messages composed by developers when checking in new code into version control. The goal is to standardize their format and make them machine-readable to a certain extent.</p>\n<p>When following conventional commits, a commit message essentially has to be prefixed with one of a few keywords.</p>\n<ul>\n<li><code>fix:</code>  A commit message with this prefix indicates a bug fix</li>\n<li><code>feat:</code>  A commit message with this prefix indicates the introduction of a new feature or functionality</li>\n<li><code>refactor:</code>  A commit with, whose message is prefixed like this, contains code refactorings, i.e. internal, technical modifications of the implementation of certain logic</li>\n<li><code>chore:</code>  This prefix indicates minor, miscellaneous changes of any type, that do not necessarily affect the user immediately</li>\n<li><code>BREAKING CHANGE!:</code> A commit message with this prefix warns about comprehensive, fundamental changes and indicates that the newly released version is likely to be incompatible with the previous one</li>\n</ul>\n<p>The conventional commits specification comprises a few more keywords and also allows developers to come up with custom ones. However, these are the most relevant ones.</p>\n<h1 id=\"Tooling\"><a href=\"#Tooling\" class=\"headerlink\" title=\"Tooling\"></a>Tooling</h1><p>When having paid attention, one might have recognized a few similarities in the semantics of conventional commits and semantic versioning. Commits with <code>fix</code>-changes correspond to the <code>PATCH</code> version, <code>feat</code> goes well with the <code>MINOR</code> version and <code>BREAKING CHANGE</code>es will inevitably result in a new <code>MAJOR</code> version. </p>\n<p>As a consequence of following the above conventions, we enabled our project for an automated versioning workflow. </p>\n<h2 id=\"standard-version-CLI\"><a href=\"#standard-version-CLI\" class=\"headerlink\" title=\"standard-version CLI\"></a><a href=\"https://www.npmjs.com/package/standard-version\">standard-version</a> CLI</h2><p><a href=\"https://www.npmjs.com/package/standard-version\">standard-version</a> is a JavaScript tool that utilizes conventional commits to automatically enforce semantic versioning. Moreover, it is capable of automatically generating a changelog in Markdown format, which developers can provide their users with. </p>\n<p>When running <code>standard-version</code>, the tool scans your commit history since when it was last executed, searches for fixes, feats, or breaking changes, and adapts the projects version accordingly.</p>\n<p>To add it to an existing project, all you need to do is:</p>\n<ol>\n<li><p>Install it as a dependency</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ yarn add -D standard-version  <span class=\"comment\"># (or npm i --save-dev standard-version)</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Optionally add it as an NPM script to your <code>package.json</code></p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">&quot;name&quot;</span>: <span class=\"string\">&quot;your-cool-project&quot;</span>,</span><br><span class=\"line\">    <span class=\"attr\">&quot;version:&quot;</span>: <span class=\"string\">&quot;0.0.1&quot;</span>,</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    <span class=\"string\">&quot;scripts:&quot;</span> &#123;</span><br><span class=\"line\">        <span class=\"attr\">&quot;release&quot;</span>: <span class=\"string\">&quot;standard-version&quot;</span></span><br><span class=\"line\">        ...</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<h1 id=\"Release-Workflow\"><a href=\"#Release-Workflow\" class=\"headerlink\" title=\"Release Workflow\"></a>Release Workflow</h1><p>After the development team has committed to consequently follow the conventional commits specification and all tooling is set up, a typical workflow to release new versions of your software might look like so.</p>\n<p>Once a new version is ready to be released, i.e. at the end of a sprint, a developer executes <code>yarn release</code> (or <code>npm run release</code>) to kick off <code>standard-version</code>. As a result </p>\n<ol>\n<li> the projects commit history is scanned to determine which part of the version number needs to be incremented</li>\n<li> the <code>version</code> property of the projects top-level <code>package.json</code> is set to the new version</li>\n<li> a <code>CHANGELOG.md</code> file is written, containing separate sections for features and bug fixes</li>\n<li> the changes are committed to Git</li>\n<li> the new commit is given a Git tag corresponding to the new version</li>\n</ol>\n<p>Depending on your setup, a push to the remote repository might kick off your CI/CD workflow, which may automatically build a new Docker image with the newly introduced tag and push it to a public or private registry. Using tools like <a href=\"https://github.com/containrrr/watchtower\">Watchtower</a>, the new image might even be rolled out to production automatically.</p>\n<p>The only manual steps required in this workflow were a single <code>yarn release</code> command and a Git push. Nothing more, nothing less.</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>The above workflow has proven to be a convenient and consistent way of managing and releasing new versions of our JavaScript- and TypeScript-based frontend-, backend- and library projects and is even more beneficial with proper CI/CD pipelines and tooling like <a href=\"https://gitlab.com/\">GitLab</a>, <a href=\"https://docker.io/\">Docker</a>, <a href=\"https://github.com/containrrr/watchtower\">Watchtower</a>, <a href=\"https://portainer.io/\">Portainer</a>, and others. It might even be adapted to projects written in other programming languages. </p>\n"},{"title":"Watching stock prices with Node-RED and Webhook2Telegram","date":"2020-11-30T20:08:47.000Z","description":"Brief description of my setup to watch stock watch stock prices using Node-RED and Telegram","_content":"\n# Motivation\nI hold a few stocks and I want to stay up-to-date about their quotations. However, I found it a bit tedious to actively log in to my portfolio every day to see what has changed. So I decided that I needed a notification system that **automatically informs me about the relative price changes** for all of my stocks once a day. Since I get all kinds of notifications  including security alerts from my server, updates in my GitHub feed, and more  via [Telegram](https://telegram.org), the choice to use that messenger for stock price notifications as well was quite obvious. Another obvious decision what have been to write a small Python script, that gets executed once a day via CRON. However, this time, **I didn't want to write any code**, but instead try the flow-based visual programming tool Node-RED.\n\n[Node-RED](https://nodered.org/) is a JavaScript-based platform to compose logical workflows through combining small, elementary building blocks together. Such building block, called nodes, include functionality to ingest data (e.g. via HTTP calls, MQTT subscriptions or reading a file), process it (e.g. string replacements, logical condition checks, aggregations, etc.) and output it in some way again (again, via HTTP, MQTT, files, etc.). Without writing any code, but only through configuring these elementary operations, whole programs can be built. While Node-RED is primarily used in IoT contexts, it basically serves any purpose. An even more comprehensive and \"mature\" alternative is, to some extent, [Apache NiFi](http://nifi.apache.org/). However, while Node-RED is perfect for tinkering and small projects, NiFi focuses on scalability and Big-Data-like workloads.\n\n# Flow\nThe resulting flow, that fulfills the above mentioned purpose looks like this.\n\n[![](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/nodered-flow.png)](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nodered-flow.png)\n\nThe flow's entry node is an `inject` node, that holds a JSON array of all my stocks' symbols (e.g. `QCOM`) and is automatically executed once every afternoon. The message is then split into multiple, individual messages, namely, one for every stock symbol. A `http request` node then calls the [Alphavantage API](https://www.alphavantage.co/) once for every message to fetch the intra-day price changes. Subsequently, the response is parsed, post-processed and formatted as Markdown. Eventually all individual messages are combined into one again before my [Webhook2Telegram](https://github.com/muety/webhook2telegram) bot is requested to send me the message as a last step.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nodered-flow2.png)","source":"_posts/watching-stock-prices-with-node-red-and-webhook2telegram.md","raw":"---\ntitle: Watching stock prices with Node-RED and Webhook2Telegram\ndate: 2020-11-30 21:08:47\ntags:\ndescription: Brief description of my setup to watch stock watch stock prices using Node-RED and Telegram\n---\n\n# Motivation\nI hold a few stocks and I want to stay up-to-date about their quotations. However, I found it a bit tedious to actively log in to my portfolio every day to see what has changed. So I decided that I needed a notification system that **automatically informs me about the relative price changes** for all of my stocks once a day. Since I get all kinds of notifications  including security alerts from my server, updates in my GitHub feed, and more  via [Telegram](https://telegram.org), the choice to use that messenger for stock price notifications as well was quite obvious. Another obvious decision what have been to write a small Python script, that gets executed once a day via CRON. However, this time, **I didn't want to write any code**, but instead try the flow-based visual programming tool Node-RED.\n\n[Node-RED](https://nodered.org/) is a JavaScript-based platform to compose logical workflows through combining small, elementary building blocks together. Such building block, called nodes, include functionality to ingest data (e.g. via HTTP calls, MQTT subscriptions or reading a file), process it (e.g. string replacements, logical condition checks, aggregations, etc.) and output it in some way again (again, via HTTP, MQTT, files, etc.). Without writing any code, but only through configuring these elementary operations, whole programs can be built. While Node-RED is primarily used in IoT contexts, it basically serves any purpose. An even more comprehensive and \"mature\" alternative is, to some extent, [Apache NiFi](http://nifi.apache.org/). However, while Node-RED is perfect for tinkering and small projects, NiFi focuses on scalability and Big-Data-like workloads.\n\n# Flow\nThe resulting flow, that fulfills the above mentioned purpose looks like this.\n\n[![](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/nodered-flow.png)](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nodered-flow.png)\n\nThe flow's entry node is an `inject` node, that holds a JSON array of all my stocks' symbols (e.g. `QCOM`) and is automatically executed once every afternoon. The message is then split into multiple, individual messages, namely, one for every stock symbol. A `http request` node then calls the [Alphavantage API](https://www.alphavantage.co/) once for every message to fetch the intra-day price changes. Subsequently, the response is parsed, post-processed and formatted as Markdown. Eventually all individual messages are combined into one again before my [Webhook2Telegram](https://github.com/muety/webhook2telegram) bot is requested to send me the message as a last step.\n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nodered-flow2.png)","slug":"watching-stock-prices-with-node-red-and-webhook2telegram","published":1,"updated":"2020-11-30T21:04:39.835Z","_id":"cki50jc590000m9e0635o0iwj","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h1><p>I hold a few stocks and I want to stay up-to-date about their quotations. However, I found it a bit tedious to actively log in to my portfolio every day to see what has changed. So I decided that I needed a notification system that <strong>automatically informs me about the relative price changes</strong> for all of my stocks once a day. Since I get all kinds of notifications  including security alerts from my server, updates in my GitHub feed, and more  via <a href=\"https://telegram.org/\">Telegram</a>, the choice to use that messenger for stock price notifications as well was quite obvious. Another obvious decision what have been to write a small Python script, that gets executed once a day via CRON. However, this time, <strong>I didnt want to write any code</strong>, but instead try the flow-based visual programming tool Node-RED.</p>\n<p><a href=\"https://nodered.org/\">Node-RED</a> is a JavaScript-based platform to compose logical workflows through combining small, elementary building blocks together. Such building block, called nodes, include functionality to ingest data (e.g. via HTTP calls, MQTT subscriptions or reading a file), process it (e.g. string replacements, logical condition checks, aggregations, etc.) and output it in some way again (again, via HTTP, MQTT, files, etc.). Without writing any code, but only through configuring these elementary operations, whole programs can be built. While Node-RED is primarily used in IoT contexts, it basically serves any purpose. An even more comprehensive and mature alternative is, to some extent, <a href=\"http://nifi.apache.org/\">Apache NiFi</a>. However, while Node-RED is perfect for tinkering and small projects, NiFi focuses on scalability and Big-Data-like workloads.</p>\n<h1 id=\"Flow\"><a href=\"#Flow\" class=\"headerlink\" title=\"Flow\"></a>Flow</h1><p>The resulting flow, that fulfills the above mentioned purpose looks like this.</p>\n<p><a href=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nodered-flow.png\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/nodered-flow.png\"></a></p>\n<p>The flows entry node is an <code>inject</code> node, that holds a JSON array of all my stocks symbols (e.g. <code>QCOM</code>) and is automatically executed once every afternoon. The message is then split into multiple, individual messages, namely, one for every stock symbol. A <code>http request</code> node then calls the <a href=\"https://www.alphavantage.co/\">Alphavantage API</a> once for every message to fetch the intra-day price changes. Subsequently, the response is parsed, post-processed and formatted as Markdown. Eventually all individual messages are combined into one again before my <a href=\"https://github.com/muety/webhook2telegram\">Webhook2Telegram</a> bot is requested to send me the message as a last step.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nodered-flow2.png\"></p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<h1 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h1><p>I hold a few stocks and I want to stay up-to-date about their quotations. However, I found it a bit tedious to actively log in to my portfolio every day to see what has changed. So I decided that I needed a notification system that <strong>automatically informs me about the relative price changes</strong> for all of my stocks once a day. Since I get all kinds of notifications  including security alerts from my server, updates in my GitHub feed, and more  via <a href=\"https://telegram.org/\">Telegram</a>, the choice to use that messenger for stock price notifications as well was quite obvious. Another obvious decision what have been to write a small Python script, that gets executed once a day via CRON. However, this time, <strong>I didnt want to write any code</strong>, but instead try the flow-based visual programming tool Node-RED.</p>\n<p><a href=\"https://nodered.org/\">Node-RED</a> is a JavaScript-based platform to compose logical workflows through combining small, elementary building blocks together. Such building block, called nodes, include functionality to ingest data (e.g. via HTTP calls, MQTT subscriptions or reading a file), process it (e.g. string replacements, logical condition checks, aggregations, etc.) and output it in some way again (again, via HTTP, MQTT, files, etc.). Without writing any code, but only through configuring these elementary operations, whole programs can be built. While Node-RED is primarily used in IoT contexts, it basically serves any purpose. An even more comprehensive and mature alternative is, to some extent, <a href=\"http://nifi.apache.org/\">Apache NiFi</a>. However, while Node-RED is perfect for tinkering and small projects, NiFi focuses on scalability and Big-Data-like workloads.</p>\n<h1 id=\"Flow\"><a href=\"#Flow\" class=\"headerlink\" title=\"Flow\"></a>Flow</h1><p>The resulting flow, that fulfills the above mentioned purpose looks like this.</p>\n<p><a href=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nodered-flow.png\"><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/nodered-flow.png\"></a></p>\n<p>The flows entry node is an <code>inject</code> node, that holds a JSON array of all my stocks symbols (e.g. <code>QCOM</code>) and is automatically executed once every afternoon. The message is then split into multiple, individual messages, namely, one for every stock symbol. A <code>http request</code> node then calls the <a href=\"https://www.alphavantage.co/\">Alphavantage API</a> once for every message to fetch the intra-day price changes. Subsequently, the response is parsed, post-processed and formatted as Markdown. Eventually all individual messages are combined into one again before my <a href=\"https://github.com/muety/webhook2telegram\">Webhook2Telegram</a> bot is requested to send me the message as a last step.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/nodered-flow2.png\"></p>\n"},{"title":"Donating for a Good Cause","date":"2020-12-12T13:37:38.000Z","_content":"\nChristmas is just around the corner and what feels better than making someone happy with a nice gift?  In this article, I want to state my views on donations and charity.\n\n# Status Quo\nDid you know that the world has more than 2200 billionaires with a net capital of around ten trillion US dollars [[1]](https://www.abendblatt.de/politik/ausland/article231104240/Welternaehrungsprogramm-bittet-Milliardaere-um-Hilfe.html), while the world's entire capital is estimated to 360 trillion dollars [[2]](https://www.zeit.de/2020/52/david-beasley-friedensnobelpreis-wfp-hungersnot-spenden-jeff-bezos/seite-2?utm_referrer=https%3A%2F%2Fwww.ecosia.org%2F)? At the same time, about 10 % of the world's population live in extreme poverty [[3]](https://www.worldbank.org/en/topic/poverty/overview), 30 million people are entirely dependent on the world food program [[4]](https://www.abendblatt.de/politik/ausland/article231104240/Welternaehrungsprogramm-bittet-Milliardaere-um-Hilfe.html) and countless children suffer from hunger.  \n\n![World Poverty](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/world_poverty.png)\n([worldpoverty.io](https://worldpoverty.io/map))\n\nObviously, one individual can not end the worlds's misery  okay, in terms of money, Jeff Bezos probably could, however, money is not the only factor. But if everyone would slightly change their mind to become just a tiny bit more generous, it would certainly not hurt at all. \n\n# Everyone's own decision\n\nOf course, the decision how to spend one's money is a very personal and individual one. However, in my opinion, people who are sufficiently wealthy  I leave the definition to you  should consider spending a small portion of their income to things they do not immediately get a benefit from. It might be anything from donating for a charity organization over supporting a homeless person on the street over giving a generous tip throughout to paying for inherently free software. \n\nI, personally, decided to dedicate around 10 % of my monthly net income to donations and sponsoring. Around one half goes to charitable purposes and the other half goes into the open source community. The latter might be a bit surprising, especially as software developers usually do not live in poverty and are not particularly in need. However, free software and free knowledge in general is very important to me, which I will explain in more detail below. \n\n# Donating for good\nIf you want to donate money for a good cause in general, think about what matters to you. Your donations can help in various areas, ranging from humanitarian aid over education over environmentalism and animal welfare over freedom of press and speech throughout to acute support in a crisis (like the CoViD pandemic) and many more. Admittedly, the process of finding suitable projects or organizations to donate to is quite tedious. However, the best place to start is indeed Google. If you care about maritime environments, for instance, search for something like _\"ocean clean up charity organization\"_ or so. Platforms like [betterplace.org](https://betterplace.org) may assist you.\nOf course, non-monetary support, like your personal involvement, is a great option, too.\n\n![Betterplace Platform](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/betterplace.png)\n([betterplace.org](https://betterplace.org))\n\n# Supporting the free internet\nWhile contributing to what is generally referred to as a good cause  like humanitarian and environmental projects as described above  is definitely of great importance for society, one further aspect should not be disregarded: the internet. It gives us the great privilege to essentially have access to mankind's entire knowledge right from your smartphone. If you are eager enough, you can basically learn anything you want, entirely for free. This is why a portion of my monthly donations goes to the [Wikimedia Foundation](https://wikimediafoundation.org/).\n\nBesides an incredible amount of knowledge, the internet also has to offer [countless](https://github.blog/2018-11-08-100M-repos/) free, open-source software projects, without which today's tech world would not be anywhere near what it is now. \n\nIn an [upcoming blog post](https://muetsch.io/consider-sponsoring-open-source.html) I will give a few good reasons why it is worth to monetarily support open source projects and which different options you have. Stay tuned. ","source":"_posts/donating-for-a-good-cause.md","raw":"---\ntitle: Donating for a Good Cause\ndate: 2020-12-12 14:37:38\ntags:\n---\n\nChristmas is just around the corner and what feels better than making someone happy with a nice gift?  In this article, I want to state my views on donations and charity.\n\n# Status Quo\nDid you know that the world has more than 2200 billionaires with a net capital of around ten trillion US dollars [[1]](https://www.abendblatt.de/politik/ausland/article231104240/Welternaehrungsprogramm-bittet-Milliardaere-um-Hilfe.html), while the world's entire capital is estimated to 360 trillion dollars [[2]](https://www.zeit.de/2020/52/david-beasley-friedensnobelpreis-wfp-hungersnot-spenden-jeff-bezos/seite-2?utm_referrer=https%3A%2F%2Fwww.ecosia.org%2F)? At the same time, about 10 % of the world's population live in extreme poverty [[3]](https://www.worldbank.org/en/topic/poverty/overview), 30 million people are entirely dependent on the world food program [[4]](https://www.abendblatt.de/politik/ausland/article231104240/Welternaehrungsprogramm-bittet-Milliardaere-um-Hilfe.html) and countless children suffer from hunger.  \n\n![World Poverty](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/world_poverty.png)\n([worldpoverty.io](https://worldpoverty.io/map))\n\nObviously, one individual can not end the worlds's misery  okay, in terms of money, Jeff Bezos probably could, however, money is not the only factor. But if everyone would slightly change their mind to become just a tiny bit more generous, it would certainly not hurt at all. \n\n# Everyone's own decision\n\nOf course, the decision how to spend one's money is a very personal and individual one. However, in my opinion, people who are sufficiently wealthy  I leave the definition to you  should consider spending a small portion of their income to things they do not immediately get a benefit from. It might be anything from donating for a charity organization over supporting a homeless person on the street over giving a generous tip throughout to paying for inherently free software. \n\nI, personally, decided to dedicate around 10 % of my monthly net income to donations and sponsoring. Around one half goes to charitable purposes and the other half goes into the open source community. The latter might be a bit surprising, especially as software developers usually do not live in poverty and are not particularly in need. However, free software and free knowledge in general is very important to me, which I will explain in more detail below. \n\n# Donating for good\nIf you want to donate money for a good cause in general, think about what matters to you. Your donations can help in various areas, ranging from humanitarian aid over education over environmentalism and animal welfare over freedom of press and speech throughout to acute support in a crisis (like the CoViD pandemic) and many more. Admittedly, the process of finding suitable projects or organizations to donate to is quite tedious. However, the best place to start is indeed Google. If you care about maritime environments, for instance, search for something like _\"ocean clean up charity organization\"_ or so. Platforms like [betterplace.org](https://betterplace.org) may assist you.\nOf course, non-monetary support, like your personal involvement, is a great option, too.\n\n![Betterplace Platform](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/betterplace.png)\n([betterplace.org](https://betterplace.org))\n\n# Supporting the free internet\nWhile contributing to what is generally referred to as a good cause  like humanitarian and environmental projects as described above  is definitely of great importance for society, one further aspect should not be disregarded: the internet. It gives us the great privilege to essentially have access to mankind's entire knowledge right from your smartphone. If you are eager enough, you can basically learn anything you want, entirely for free. This is why a portion of my monthly donations goes to the [Wikimedia Foundation](https://wikimediafoundation.org/).\n\nBesides an incredible amount of knowledge, the internet also has to offer [countless](https://github.blog/2018-11-08-100M-repos/) free, open-source software projects, without which today's tech world would not be anywhere near what it is now. \n\nIn an [upcoming blog post](https://muetsch.io/consider-sponsoring-open-source.html) I will give a few good reasons why it is worth to monetarily support open source projects and which different options you have. Stay tuned. ","slug":"donating-for-a-good-cause","published":1,"updated":"2020-12-12T20:08:54.206Z","_id":"ckim0i2ri0000ere09waj47al","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Christmas is just around the corner and what feels better than making someone happy with a nice gift?  In this article, I want to state my views on donations and charity.</p>\n<h1 id=\"Status-Quo\"><a href=\"#Status-Quo\" class=\"headerlink\" title=\"Status Quo\"></a>Status Quo</h1><p>Did you know that the world has more than 2200 billionaires with a net capital of around ten trillion US dollars <a href=\"https://www.abendblatt.de/politik/ausland/article231104240/Welternaehrungsprogramm-bittet-Milliardaere-um-Hilfe.html\">[1]</a>, while the worlds entire capital is estimated to 360 trillion dollars <a href=\"https://www.zeit.de/2020/52/david-beasley-friedensnobelpreis-wfp-hungersnot-spenden-jeff-bezos/seite-2?utm_referrer=https://www.ecosia.org/\">[2]</a>? At the same time, about 10 % of the worlds population live in extreme poverty <a href=\"https://www.worldbank.org/en/topic/poverty/overview\">[3]</a>, 30 million people are entirely dependent on the world food program <a href=\"https://www.abendblatt.de/politik/ausland/article231104240/Welternaehrungsprogramm-bittet-Milliardaere-um-Hilfe.html\">[4]</a> and countless children suffer from hunger.  </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/world_poverty.png\" alt=\"World Poverty\"><br>(<a href=\"https://worldpoverty.io/map\">worldpoverty.io</a>)</p>\n<p>Obviously, one individual can not end the worldss misery  okay, in terms of money, Jeff Bezos probably could, however, money is not the only factor. But if everyone would slightly change their mind to become just a tiny bit more generous, it would certainly not hurt at all. </p>\n<h1 id=\"Everyones-own-decision\"><a href=\"#Everyones-own-decision\" class=\"headerlink\" title=\"Everyones own decision\"></a>Everyones own decision</h1><p>Of course, the decision how to spend ones money is a very personal and individual one. However, in my opinion, people who are sufficiently wealthy  I leave the definition to you  should consider spending a small portion of their income to things they do not immediately get a benefit from. It might be anything from donating for a charity organization over supporting a homeless person on the street over giving a generous tip throughout to paying for inherently free software. </p>\n<p>I, personally, decided to dedicate around 10 % of my monthly net income to donations and sponsoring. Around one half goes to charitable purposes and the other half goes into the open source community. The latter might be a bit surprising, especially as software developers usually do not live in poverty and are not particularly in need. However, free software and free knowledge in general is very important to me, which I will explain in more detail below. </p>\n<h1 id=\"Donating-for-good\"><a href=\"#Donating-for-good\" class=\"headerlink\" title=\"Donating for good\"></a>Donating for good</h1><p>If you want to donate money for a good cause in general, think about what matters to you. Your donations can help in various areas, ranging from humanitarian aid over education over environmentalism and animal welfare over freedom of press and speech throughout to acute support in a crisis (like the CoViD pandemic) and many more. Admittedly, the process of finding suitable projects or organizations to donate to is quite tedious. However, the best place to start is indeed Google. If you care about maritime environments, for instance, search for something like <em>ocean clean up charity organization</em> or so. Platforms like <a href=\"https://betterplace.org/\">betterplace.org</a> may assist you.<br>Of course, non-monetary support, like your personal involvement, is a great option, too.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/betterplace.png\" alt=\"Betterplace Platform\"><br>(<a href=\"https://betterplace.org/\">betterplace.org</a>)</p>\n<h1 id=\"Supporting-the-free-internet\"><a href=\"#Supporting-the-free-internet\" class=\"headerlink\" title=\"Supporting the free internet\"></a>Supporting the free internet</h1><p>While contributing to what is generally referred to as a good cause  like humanitarian and environmental projects as described above  is definitely of great importance for society, one further aspect should not be disregarded: the internet. It gives us the great privilege to essentially have access to mankinds entire knowledge right from your smartphone. If you are eager enough, you can basically learn anything you want, entirely for free. This is why a portion of my monthly donations goes to the <a href=\"https://wikimediafoundation.org/\">Wikimedia Foundation</a>.</p>\n<p>Besides an incredible amount of knowledge, the internet also has to offer <a href=\"https://github.blog/2018-11-08-100M-repos/\">countless</a> free, open-source software projects, without which todays tech world would not be anywhere near what it is now. </p>\n<p>In an <a href=\"https://muetsch.io/consider-sponsoring-open-source.html\">upcoming blog post</a> I will give a few good reasons why it is worth to monetarily support open source projects and which different options you have. Stay tuned. </p>\n","site":{"data":{"badges":["https://img.shields.io/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://img.shields.io/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p>Christmas is just around the corner and what feels better than making someone happy with a nice gift?  In this article, I want to state my views on donations and charity.</p>\n<h1 id=\"Status-Quo\"><a href=\"#Status-Quo\" class=\"headerlink\" title=\"Status Quo\"></a>Status Quo</h1><p>Did you know that the world has more than 2200 billionaires with a net capital of around ten trillion US dollars <a href=\"https://www.abendblatt.de/politik/ausland/article231104240/Welternaehrungsprogramm-bittet-Milliardaere-um-Hilfe.html\">[1]</a>, while the worlds entire capital is estimated to 360 trillion dollars <a href=\"https://www.zeit.de/2020/52/david-beasley-friedensnobelpreis-wfp-hungersnot-spenden-jeff-bezos/seite-2?utm_referrer=https://www.ecosia.org/\">[2]</a>? At the same time, about 10 % of the worlds population live in extreme poverty <a href=\"https://www.worldbank.org/en/topic/poverty/overview\">[3]</a>, 30 million people are entirely dependent on the world food program <a href=\"https://www.abendblatt.de/politik/ausland/article231104240/Welternaehrungsprogramm-bittet-Milliardaere-um-Hilfe.html\">[4]</a> and countless children suffer from hunger.  </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/world_poverty.png\" alt=\"World Poverty\"><br>(<a href=\"https://worldpoverty.io/map\">worldpoverty.io</a>)</p>\n<p>Obviously, one individual can not end the worldss misery  okay, in terms of money, Jeff Bezos probably could, however, money is not the only factor. But if everyone would slightly change their mind to become just a tiny bit more generous, it would certainly not hurt at all. </p>\n<h1 id=\"Everyones-own-decision\"><a href=\"#Everyones-own-decision\" class=\"headerlink\" title=\"Everyones own decision\"></a>Everyones own decision</h1><p>Of course, the decision how to spend ones money is a very personal and individual one. However, in my opinion, people who are sufficiently wealthy  I leave the definition to you  should consider spending a small portion of their income to things they do not immediately get a benefit from. It might be anything from donating for a charity organization over supporting a homeless person on the street over giving a generous tip throughout to paying for inherently free software. </p>\n<p>I, personally, decided to dedicate around 10 % of my monthly net income to donations and sponsoring. Around one half goes to charitable purposes and the other half goes into the open source community. The latter might be a bit surprising, especially as software developers usually do not live in poverty and are not particularly in need. However, free software and free knowledge in general is very important to me, which I will explain in more detail below. </p>\n<h1 id=\"Donating-for-good\"><a href=\"#Donating-for-good\" class=\"headerlink\" title=\"Donating for good\"></a>Donating for good</h1><p>If you want to donate money for a good cause in general, think about what matters to you. Your donations can help in various areas, ranging from humanitarian aid over education over environmentalism and animal welfare over freedom of press and speech throughout to acute support in a crisis (like the CoViD pandemic) and many more. Admittedly, the process of finding suitable projects or organizations to donate to is quite tedious. However, the best place to start is indeed Google. If you care about maritime environments, for instance, search for something like <em>ocean clean up charity organization</em> or so. Platforms like <a href=\"https://betterplace.org/\">betterplace.org</a> may assist you.<br>Of course, non-monetary support, like your personal involvement, is a great option, too.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/betterplace.png\" alt=\"Betterplace Platform\"><br>(<a href=\"https://betterplace.org/\">betterplace.org</a>)</p>\n<h1 id=\"Supporting-the-free-internet\"><a href=\"#Supporting-the-free-internet\" class=\"headerlink\" title=\"Supporting the free internet\"></a>Supporting the free internet</h1><p>While contributing to what is generally referred to as a good cause  like humanitarian and environmental projects as described above  is definitely of great importance for society, one further aspect should not be disregarded: the internet. It gives us the great privilege to essentially have access to mankinds entire knowledge right from your smartphone. If you are eager enough, you can basically learn anything you want, entirely for free. This is why a portion of my monthly donations goes to the <a href=\"https://wikimediafoundation.org/\">Wikimedia Foundation</a>.</p>\n<p>Besides an incredible amount of knowledge, the internet also has to offer <a href=\"https://github.blog/2018-11-08-100M-repos/\">countless</a> free, open-source software projects, without which todays tech world would not be anywhere near what it is now. </p>\n<p>In an <a href=\"https://muetsch.io/consider-sponsoring-open-source.html\">upcoming blog post</a> I will give a few good reasons why it is worth to monetarily support open source projects and which different options you have. Stay tuned. </p>\n"},{"title":"Consider Sponsoring Open Source","date":"2020-12-12T18:36:00.000Z","_content":"\n![Title Image](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/foss_sponsoring.png)\n\n# Open Source powers the world\nWhat would the software world be without open source projects? Imagine there was no Linux, no Java or Python, no Android, no Firefox or Chromium, no Apache2 or nginx, no Kubernetes, no VSCode, no Angular, React or Vue, no Numpy, TensorFlow or PyTorch, ... The list of big open source projects, that power the vast majority of the entire internet, is almost infinitely long. Besides that, at least as important are the countless, small hobby projects driven my one or a few individuals. If you ever were looking for some script, tool, app or library, chances are high that you found exactly what you needed on GitHub.\n\nWhile most of the projects mentioned above are backed by large companies like Google or Microsoft or non-profit organizations like the [Open Source Initiative](https://opensource.org/) or the [Apache Foundation](https://apache.org/), individual developers with projects with only a few hundred stars build excellent software as well. But they do it in their spare time and without getting paid anything. No matter if they build and maintain their software out of passion, in order to solve a problem for themselves or just for learning purposes  in my opinion, the efforts of individual contributors should be appreciated even more.\n\nAccordingly, in this article, I want to **encourage people to consider sponsoring for free software projects** they like. It complements my previous article on _[Donating for a Good Cause](https://muetsch.io/donating-for-a-good-cause.html)_.\n\n# Supporting the small ones\n> While the big corporations have the necessary funding and resources, most open source projects are developed by individuals in their spare time. However, it does require ones efforts, time and probably includes some overhead costs too. Monetary supports surely help drive the project development. [[1]](https://itsfoss.com/open-source-funding-platforms/)\n\nWhen maintaining a GitHub project, your aim is not to make money from it  although some [lucky people](https://www.youtube.com/watch?v=OrxmtDw4pVI) can actually earn their living from their open source work. However, receiving small donations here and there, which express other people's appreciation for your efforts, will definitely boost your motiviation to keep on coding. Of course, monetary donations are only one of [many options](https://itsfoss.com/help-linux-grow/) to support a project, but, indeed, probably the easiest one.\n\n# Ways to give\n> If you really like a product, buy it.\n\nThere are many different options to support a project. \n\n1. **Star it .** This is obvious, doesn't cost you anything and the project's maintainer is likely going to be a bit proud for a moment.\n1. **Buy the _Pro_ version .** Apps or web services often have a free basic version and a paid version with some benefits. Buying the advanced version will not only give you more features or less ads, but is usually also quite cheap.\n1. **Give a one-time donation .** Most of the time, a small one-time donation of a few dollars is already enough to make the project's author happy and boost her motivation. And, of course, the more people do it, the better.\n1. **Set up recurring payments .** If you really like a project and strongly want it to keep going  especially if it has explicit maintenance costs like hosting, etc.  consider donating a small amount on a regular basis.\n1. **Get _sponsorware_ content .** [Sponsorware](https://github.com/sponsorware/docs) is a release strategy for open-source software that enables developers to be compensated for their open-source work with fewer downsides than traditional open-source funding models.\n\n# Sponsoring platforms\nDifferent sponsoring / donations platforms have established over the years, many of them with an explicit focus on software projects and some of them also being non-profit. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_sponsors.png)\n\n* **GitHub Sponsors:** First and foremost, there is GitHub's own sponsoring feature, that was [introduced in early 2020](https://github.blog/2020-03-24-getting-started-with-github-sponsors/). On one hand, it is a sponsoring platform for itself  mostly for recurring, monthly payments  where GitHub is in the role of managing the donations. Like with most other platforms, you do not directly sponsor a certain project, but a user. On the other hand, the new feature also allows project maintainers to link other, third-party donation platforms to their projects.\n* **Liberapay:** Founded in 2016, [Liberapay](https://liberapay.com/) is one of the major donation platforms in the open source world. It is [partially open-source](https://github.com/liberapay) as well, has [around 3000 active users](https://medium.com/liberapay-blog/the-fourth-year-of-liberapay-bbb8563cfac8) and is free of fees, except for the payment provider's (Stripe) fees. Liberapay is for recurring donations on a weekly basis.\n* **Opencollective:** Equally as wide-spread as Liberapay is [Opencollective](https://opencollective.com/), which has the same purpose.\n* **Ko-Fi:** [Ko-Fi](https://ko-fi.com/) is another platform that you will occasionally see linked to GitHub or GitLab projects. It is [100 % free](https://help.ko-fi.com/hc/en-us/articles/360002506494-Does-Ko-fi-take-a-fee-) and supports both one-time donations as well as subscriptions.\n* **Buymeacoffee:** Very similar to Ko-Fi, [Buy me a coffee](https://www.buymeacoffee.com/) is another donations platform with support for one-time donations and monthly subscriptions. You see it often used not only for software, but also on blogs or by content creators.\n* **Bountysource:** Lastly, there is [Bountysource](https://www.bountysource.com/), which is specifically for software, but works a bit different than the above platforms in a way that you place a bounty on a specific issue on a repo. This way, you can push features of a project that are especially important to you. When the issue is resolved, the reward does not necessarily go to the project owner, but to the person who implemented the respective feature or bug fix. \n\nThere are more platforms, but these are the ones that I consider most common or relevant.\n\nThe purpose of this article is to give a few good reasons why sponsoring software projects is desirable and important and provide a brief overview of different ways to sponsor software projects. I would be happy to see more people being a bit more generous when it comes to free software .","source":"_posts/consider-sponsoring-open-source.md","raw":"---\ntitle: Consider Sponsoring Open Source\ndate: 2020-12-12 19:36:00\ntags:\n---\n\n![Title Image](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/foss_sponsoring.png)\n\n# Open Source powers the world\nWhat would the software world be without open source projects? Imagine there was no Linux, no Java or Python, no Android, no Firefox or Chromium, no Apache2 or nginx, no Kubernetes, no VSCode, no Angular, React or Vue, no Numpy, TensorFlow or PyTorch, ... The list of big open source projects, that power the vast majority of the entire internet, is almost infinitely long. Besides that, at least as important are the countless, small hobby projects driven my one or a few individuals. If you ever were looking for some script, tool, app or library, chances are high that you found exactly what you needed on GitHub.\n\nWhile most of the projects mentioned above are backed by large companies like Google or Microsoft or non-profit organizations like the [Open Source Initiative](https://opensource.org/) or the [Apache Foundation](https://apache.org/), individual developers with projects with only a few hundred stars build excellent software as well. But they do it in their spare time and without getting paid anything. No matter if they build and maintain their software out of passion, in order to solve a problem for themselves or just for learning purposes  in my opinion, the efforts of individual contributors should be appreciated even more.\n\nAccordingly, in this article, I want to **encourage people to consider sponsoring for free software projects** they like. It complements my previous article on _[Donating for a Good Cause](https://muetsch.io/donating-for-a-good-cause.html)_.\n\n# Supporting the small ones\n> While the big corporations have the necessary funding and resources, most open source projects are developed by individuals in their spare time. However, it does require ones efforts, time and probably includes some overhead costs too. Monetary supports surely help drive the project development. [[1]](https://itsfoss.com/open-source-funding-platforms/)\n\nWhen maintaining a GitHub project, your aim is not to make money from it  although some [lucky people](https://www.youtube.com/watch?v=OrxmtDw4pVI) can actually earn their living from their open source work. However, receiving small donations here and there, which express other people's appreciation for your efforts, will definitely boost your motiviation to keep on coding. Of course, monetary donations are only one of [many options](https://itsfoss.com/help-linux-grow/) to support a project, but, indeed, probably the easiest one.\n\n# Ways to give\n> If you really like a product, buy it.\n\nThere are many different options to support a project. \n\n1. **Star it .** This is obvious, doesn't cost you anything and the project's maintainer is likely going to be a bit proud for a moment.\n1. **Buy the _Pro_ version .** Apps or web services often have a free basic version and a paid version with some benefits. Buying the advanced version will not only give you more features or less ads, but is usually also quite cheap.\n1. **Give a one-time donation .** Most of the time, a small one-time donation of a few dollars is already enough to make the project's author happy and boost her motivation. And, of course, the more people do it, the better.\n1. **Set up recurring payments .** If you really like a project and strongly want it to keep going  especially if it has explicit maintenance costs like hosting, etc.  consider donating a small amount on a regular basis.\n1. **Get _sponsorware_ content .** [Sponsorware](https://github.com/sponsorware/docs) is a release strategy for open-source software that enables developers to be compensated for their open-source work with fewer downsides than traditional open-source funding models.\n\n# Sponsoring platforms\nDifferent sponsoring / donations platforms have established over the years, many of them with an explicit focus on software projects and some of them also being non-profit. \n\n![](https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_sponsors.png)\n\n* **GitHub Sponsors:** First and foremost, there is GitHub's own sponsoring feature, that was [introduced in early 2020](https://github.blog/2020-03-24-getting-started-with-github-sponsors/). On one hand, it is a sponsoring platform for itself  mostly for recurring, monthly payments  where GitHub is in the role of managing the donations. Like with most other platforms, you do not directly sponsor a certain project, but a user. On the other hand, the new feature also allows project maintainers to link other, third-party donation platforms to their projects.\n* **Liberapay:** Founded in 2016, [Liberapay](https://liberapay.com/) is one of the major donation platforms in the open source world. It is [partially open-source](https://github.com/liberapay) as well, has [around 3000 active users](https://medium.com/liberapay-blog/the-fourth-year-of-liberapay-bbb8563cfac8) and is free of fees, except for the payment provider's (Stripe) fees. Liberapay is for recurring donations on a weekly basis.\n* **Opencollective:** Equally as wide-spread as Liberapay is [Opencollective](https://opencollective.com/), which has the same purpose.\n* **Ko-Fi:** [Ko-Fi](https://ko-fi.com/) is another platform that you will occasionally see linked to GitHub or GitLab projects. It is [100 % free](https://help.ko-fi.com/hc/en-us/articles/360002506494-Does-Ko-fi-take-a-fee-) and supports both one-time donations as well as subscriptions.\n* **Buymeacoffee:** Very similar to Ko-Fi, [Buy me a coffee](https://www.buymeacoffee.com/) is another donations platform with support for one-time donations and monthly subscriptions. You see it often used not only for software, but also on blogs or by content creators.\n* **Bountysource:** Lastly, there is [Bountysource](https://www.bountysource.com/), which is specifically for software, but works a bit different than the above platforms in a way that you place a bounty on a specific issue on a repo. This way, you can push features of a project that are especially important to you. When the issue is resolved, the reward does not necessarily go to the project owner, but to the person who implemented the respective feature or bug fix. \n\nThere are more platforms, but these are the ones that I consider most common or relevant.\n\nThe purpose of this article is to give a few good reasons why sponsoring software projects is desirable and important and provide a brief overview of different ways to sponsor software projects. I would be happy to see more people being a bit more generous when it comes to free software .","slug":"consider-sponsoring-open-source","published":1,"updated":"2021-01-04T17:15:54.796Z","_id":"ckim1qs860000vse0cura92bv","comments":1,"layout":"post","photos":[],"link":"","content":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/foss_sponsoring.png\" alt=\"Title Image\"></p>\n<h1 id=\"Open-Source-powers-the-world\"><a href=\"#Open-Source-powers-the-world\" class=\"headerlink\" title=\"Open Source powers the world\"></a>Open Source powers the world</h1><p>What would the software world be without open source projects? Imagine there was no Linux, no Java or Python, no Android, no Firefox or Chromium, no Apache2 or nginx, no Kubernetes, no VSCode, no Angular, React or Vue, no Numpy, TensorFlow or PyTorch,  The list of big open source projects, that power the vast majority of the entire internet, is almost infinitely long. Besides that, at least as important are the countless, small hobby projects driven my one or a few individuals. If you ever were looking for some script, tool, app or library, chances are high that you found exactly what you needed on GitHub.</p>\n<p>While most of the projects mentioned above are backed by large companies like Google or Microsoft or non-profit organizations like the <a href=\"https://opensource.org/\">Open Source Initiative</a> or the <a href=\"https://apache.org/\">Apache Foundation</a>, individual developers with projects with only a few hundred stars build excellent software as well. But they do it in their spare time and without getting paid anything. No matter if they build and maintain their software out of passion, in order to solve a problem for themselves or just for learning purposes  in my opinion, the efforts of individual contributors should be appreciated even more.</p>\n<p>Accordingly, in this article, I want to <strong>encourage people to consider sponsoring for free software projects</strong> they like. It complements my previous article on <em><a href=\"https://muetsch.io/donating-for-a-good-cause.html\">Donating for a Good Cause</a></em>.</p>\n<h1 id=\"Supporting-the-small-ones\"><a href=\"#Supporting-the-small-ones\" class=\"headerlink\" title=\"Supporting the small ones\"></a>Supporting the small ones</h1><blockquote>\n<p>While the big corporations have the necessary funding and resources, most open source projects are developed by individuals in their spare time. However, it does require ones efforts, time and probably includes some overhead costs too. Monetary supports surely help drive the project development. <a href=\"https://itsfoss.com/open-source-funding-platforms/\">[1]</a></p>\n</blockquote>\n<p>When maintaining a GitHub project, your aim is not to make money from it  although some <a href=\"https://www.youtube.com/watch?v=OrxmtDw4pVI\">lucky people</a> can actually earn their living from their open source work. However, receiving small donations here and there, which express other peoples appreciation for your efforts, will definitely boost your motiviation to keep on coding. Of course, monetary donations are only one of <a href=\"https://itsfoss.com/help-linux-grow/\">many options</a> to support a project, but, indeed, probably the easiest one.</p>\n<h1 id=\"Ways-to-give\"><a href=\"#Ways-to-give\" class=\"headerlink\" title=\"Ways to give\"></a>Ways to give</h1><blockquote>\n<p>If you really like a product, buy it.</p>\n</blockquote>\n<p>There are many different options to support a project. </p>\n<ol>\n<li><strong>Star it .</strong> This is obvious, doesnt cost you anything and the projects maintainer is likely going to be a bit proud for a moment.</li>\n<li><strong>Buy the <em>Pro</em> version .</strong> Apps or web services often have a free basic version and a paid version with some benefits. Buying the advanced version will not only give you more features or less ads, but is usually also quite cheap.</li>\n<li><strong>Give a one-time donation .</strong> Most of the time, a small one-time donation of a few dollars is already enough to make the projects author happy and boost her motivation. And, of course, the more people do it, the better.</li>\n<li><strong>Set up recurring payments .</strong> If you really like a project and strongly want it to keep going  especially if it has explicit maintenance costs like hosting, etc.  consider donating a small amount on a regular basis.</li>\n<li><strong>Get <em>sponsorware</em> content .</strong> <a href=\"https://github.com/sponsorware/docs\">Sponsorware</a> is a release strategy for open-source software that enables developers to be compensated for their open-source work with fewer downsides than traditional open-source funding models.</li>\n</ol>\n<h1 id=\"Sponsoring-platforms\"><a href=\"#Sponsoring-platforms\" class=\"headerlink\" title=\"Sponsoring platforms\"></a>Sponsoring platforms</h1><p>Different sponsoring / donations platforms have established over the years, many of them with an explicit focus on software projects and some of them also being non-profit. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_sponsors.png\"></p>\n<ul>\n<li><strong>GitHub Sponsors:</strong> First and foremost, there is GitHubs own sponsoring feature, that was <a href=\"https://github.blog/2020-03-24-getting-started-with-github-sponsors/\">introduced in early 2020</a>. On one hand, it is a sponsoring platform for itself  mostly for recurring, monthly payments  where GitHub is in the role of managing the donations. Like with most other platforms, you do not directly sponsor a certain project, but a user. On the other hand, the new feature also allows project maintainers to link other, third-party donation platforms to their projects.</li>\n<li><strong>Liberapay:</strong> Founded in 2016, <a href=\"https://liberapay.com/\">Liberapay</a> is one of the major donation platforms in the open source world. It is <a href=\"https://github.com/liberapay\">partially open-source</a> as well, has <a href=\"https://medium.com/liberapay-blog/the-fourth-year-of-liberapay-bbb8563cfac8\">around 3000 active users</a> and is free of fees, except for the payment providers (Stripe) fees. Liberapay is for recurring donations on a weekly basis.</li>\n<li><strong>Opencollective:</strong> Equally as wide-spread as Liberapay is <a href=\"https://opencollective.com/\">Opencollective</a>, which has the same purpose.</li>\n<li><strong>Ko-Fi:</strong> <a href=\"https://ko-fi.com/\">Ko-Fi</a> is another platform that you will occasionally see linked to GitHub or GitLab projects. It is <a href=\"https://help.ko-fi.com/hc/en-us/articles/360002506494-Does-Ko-fi-take-a-fee-\">100 % free</a> and supports both one-time donations as well as subscriptions.</li>\n<li><strong>Buymeacoffee:</strong> Very similar to Ko-Fi, <a href=\"https://www.buymeacoffee.com/\">Buy me a coffee</a> is another donations platform with support for one-time donations and monthly subscriptions. You see it often used not only for software, but also on blogs or by content creators.</li>\n<li><strong>Bountysource:</strong> Lastly, there is <a href=\"https://www.bountysource.com/\">Bountysource</a>, which is specifically for software, but works a bit different than the above platforms in a way that you place a bounty on a specific issue on a repo. This way, you can push features of a project that are especially important to you. When the issue is resolved, the reward does not necessarily go to the project owner, but to the person who implemented the respective feature or bug fix. </li>\n</ul>\n<p>There are more platforms, but these are the ones that I consider most common or relevant.</p>\n<p>The purpose of this article is to give a few good reasons why sponsoring software projects is desirable and important and provide a brief overview of different ways to sponsor software projects. I would be happy to see more people being a bit more generous when it comes to free software .</p>\n","site":{"data":{"badges":["https://badges.fw-web.space/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://badges.fw-web.space/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/foss_sponsoring.png\" alt=\"Title Image\"></p>\n<h1 id=\"Open-Source-powers-the-world\"><a href=\"#Open-Source-powers-the-world\" class=\"headerlink\" title=\"Open Source powers the world\"></a>Open Source powers the world</h1><p>What would the software world be without open source projects? Imagine there was no Linux, no Java or Python, no Android, no Firefox or Chromium, no Apache2 or nginx, no Kubernetes, no VSCode, no Angular, React or Vue, no Numpy, TensorFlow or PyTorch,  The list of big open source projects, that power the vast majority of the entire internet, is almost infinitely long. Besides that, at least as important are the countless, small hobby projects driven my one or a few individuals. If you ever were looking for some script, tool, app or library, chances are high that you found exactly what you needed on GitHub.</p>\n<p>While most of the projects mentioned above are backed by large companies like Google or Microsoft or non-profit organizations like the <a href=\"https://opensource.org/\">Open Source Initiative</a> or the <a href=\"https://apache.org/\">Apache Foundation</a>, individual developers with projects with only a few hundred stars build excellent software as well. But they do it in their spare time and without getting paid anything. No matter if they build and maintain their software out of passion, in order to solve a problem for themselves or just for learning purposes  in my opinion, the efforts of individual contributors should be appreciated even more.</p>\n<p>Accordingly, in this article, I want to <strong>encourage people to consider sponsoring for free software projects</strong> they like. It complements my previous article on <em><a href=\"https://muetsch.io/donating-for-a-good-cause.html\">Donating for a Good Cause</a></em>.</p>\n<h1 id=\"Supporting-the-small-ones\"><a href=\"#Supporting-the-small-ones\" class=\"headerlink\" title=\"Supporting the small ones\"></a>Supporting the small ones</h1><blockquote>\n<p>While the big corporations have the necessary funding and resources, most open source projects are developed by individuals in their spare time. However, it does require ones efforts, time and probably includes some overhead costs too. Monetary supports surely help drive the project development. <a href=\"https://itsfoss.com/open-source-funding-platforms/\">[1]</a></p>\n</blockquote>\n<p>When maintaining a GitHub project, your aim is not to make money from it  although some <a href=\"https://www.youtube.com/watch?v=OrxmtDw4pVI\">lucky people</a> can actually earn their living from their open source work. However, receiving small donations here and there, which express other peoples appreciation for your efforts, will definitely boost your motiviation to keep on coding. Of course, monetary donations are only one of <a href=\"https://itsfoss.com/help-linux-grow/\">many options</a> to support a project, but, indeed, probably the easiest one.</p>\n<h1 id=\"Ways-to-give\"><a href=\"#Ways-to-give\" class=\"headerlink\" title=\"Ways to give\"></a>Ways to give</h1><blockquote>\n<p>If you really like a product, buy it.</p>\n</blockquote>\n<p>There are many different options to support a project. </p>\n<ol>\n<li><strong>Star it .</strong> This is obvious, doesnt cost you anything and the projects maintainer is likely going to be a bit proud for a moment.</li>\n<li><strong>Buy the <em>Pro</em> version .</strong> Apps or web services often have a free basic version and a paid version with some benefits. Buying the advanced version will not only give you more features or less ads, but is usually also quite cheap.</li>\n<li><strong>Give a one-time donation .</strong> Most of the time, a small one-time donation of a few dollars is already enough to make the projects author happy and boost her motivation. And, of course, the more people do it, the better.</li>\n<li><strong>Set up recurring payments .</strong> If you really like a project and strongly want it to keep going  especially if it has explicit maintenance costs like hosting, etc.  consider donating a small amount on a regular basis.</li>\n<li><strong>Get <em>sponsorware</em> content .</strong> <a href=\"https://github.com/sponsorware/docs\">Sponsorware</a> is a release strategy for open-source software that enables developers to be compensated for their open-source work with fewer downsides than traditional open-source funding models.</li>\n</ol>\n<h1 id=\"Sponsoring-platforms\"><a href=\"#Sponsoring-platforms\" class=\"headerlink\" title=\"Sponsoring platforms\"></a>Sponsoring platforms</h1><p>Different sponsoring / donations platforms have established over the years, many of them with an explicit focus on software projects and some of them also being non-profit. </p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto?image=https://muetsch.io/images/gh_sponsors.png\"></p>\n<ul>\n<li><strong>GitHub Sponsors:</strong> First and foremost, there is GitHubs own sponsoring feature, that was <a href=\"https://github.blog/2020-03-24-getting-started-with-github-sponsors/\">introduced in early 2020</a>. On one hand, it is a sponsoring platform for itself  mostly for recurring, monthly payments  where GitHub is in the role of managing the donations. Like with most other platforms, you do not directly sponsor a certain project, but a user. On the other hand, the new feature also allows project maintainers to link other, third-party donation platforms to their projects.</li>\n<li><strong>Liberapay:</strong> Founded in 2016, <a href=\"https://liberapay.com/\">Liberapay</a> is one of the major donation platforms in the open source world. It is <a href=\"https://github.com/liberapay\">partially open-source</a> as well, has <a href=\"https://medium.com/liberapay-blog/the-fourth-year-of-liberapay-bbb8563cfac8\">around 3000 active users</a> and is free of fees, except for the payment providers (Stripe) fees. Liberapay is for recurring donations on a weekly basis.</li>\n<li><strong>Opencollective:</strong> Equally as wide-spread as Liberapay is <a href=\"https://opencollective.com/\">Opencollective</a>, which has the same purpose.</li>\n<li><strong>Ko-Fi:</strong> <a href=\"https://ko-fi.com/\">Ko-Fi</a> is another platform that you will occasionally see linked to GitHub or GitLab projects. It is <a href=\"https://help.ko-fi.com/hc/en-us/articles/360002506494-Does-Ko-fi-take-a-fee-\">100 % free</a> and supports both one-time donations as well as subscriptions.</li>\n<li><strong>Buymeacoffee:</strong> Very similar to Ko-Fi, <a href=\"https://www.buymeacoffee.com/\">Buy me a coffee</a> is another donations platform with support for one-time donations and monthly subscriptions. You see it often used not only for software, but also on blogs or by content creators.</li>\n<li><strong>Bountysource:</strong> Lastly, there is <a href=\"https://www.bountysource.com/\">Bountysource</a>, which is specifically for software, but works a bit different than the above platforms in a way that you place a bounty on a specific issue on a repo. This way, you can push features of a project that are especially important to you. When the issue is resolved, the reward does not necessarily go to the project owner, but to the person who implemented the respective feature or bug fix. </li>\n</ul>\n<p>There are more platforms, but these are the ones that I consider most common or relevant.</p>\n<p>The purpose of this article is to give a few good reasons why sponsoring software projects is desirable and important and provide a brief overview of different ways to sponsor software projects. I would be happy to see more people being a bit more generous when it comes to free software .</p>\n"},{"title":"My Experiences with the Oracle Java 11 Developer Certification","date":"2021-03-19T12:03:30.000Z","_content":"\n![](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert2.png)\n\nOn the occasion of the Java programming language's 25th anniversary, Oracle just recently announced a pretty nice discount on the _Java 11 SE Developer Certificaion_. For a limited period of time you can get the exam, which normally costs something around $ 250 and the accompanying course, which, I believe, is even more expensive, for a total of $ 25.\n\n At [Frachtwerk](https://frachtwerk.de), we decided to take the chance and give all backend developers the opportunity to get certified. Today, I had my exam and here are my thought on the journey to get there.\n\n## The Course\nThe course's target audience includes developers, who already have a decent amount of experience with programming in Java, but not necessarily are experts, yet. Motivation for taking the course could be to further improve your skills or to specifically prepare for the certification. A certification like this, in turn, can help developers set themselves apart from potential competitors on the job market.\n\nI, personally, have been developing web application in Java for a few years now and would consider my experience and language skills fairly advanced. However, I was surprised how much I could still learn, though. I even got to know feature of Java that I haven't had heard of before.\n\nContents of the course range from the most essential things like types, variables, loops and conditions over inheritance principles, streams, I/O and JDBC throughout to low-level multi-threading and concurrency concepts and details about the fairly new Java 9 module system.\n\n![](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert1.png)\n\nOverall, my experience with the course was very good! All content is of overly high quality, explanations are clear and understandable and the topics covered are not at all only superficial, but indeed quite advanced. At the end of each section there are quizzes for reviewing your knowledge. \n\nHere are a few examples of such questions:\n\n![](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert6.png)\n![](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert4.png)\n![](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert5.png)\n\nExperienced Java developers might easily skip the first ten chapters, however, the higher ones were actually very interesting to me.\n\nAmong others, I got to learn about how [type erasure](https://docs.oracle.com/javase/tutorial/java/generics/erasure.html) of generics works, what [heap pollution](https://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#heap_pollution) is, how the `volatile` type modifier functions and that [interfaces can have private, non-abstract methods](images/java_cert3.png).\n\nEspecially since it is free, I would definitely recommend the course, even to experienced Java developers. \n\n## The Exam\nAfter I had spent about 1.5 full work days for preparation, I eventually felt confident enough to register for the exam. It takes place online and you can choose from a variety of different dates and times. \n\nI was very satisfied with the learning path so far. However, there were a few points about the exam which I was not happy about. \n\nWhile the certification program itself is offered by Oracle, the exam takes place through an external provider called Pearson VUE. Being spezialized on online exams, they take various measures to prevent cheating. In principle, this is a positive thing. I could not take a certificate seriously if I knew that you could google during the exam or do partner work. However, in my opinion, things were a bit too strict here.\n\nFirst of all, you have to download and install a desktop application through which the exam itself is conducted. Unfortunately, it is only available for Windows and Mac and it turned out to be non-trivial to find a non-Linux computer with webcam and microphone at our office. The program runs full-screen and you can not exit to your desktop or other programs from it, which makes sense. Also, it automatically kills processes like `teamviewer.exe`, `anydesk.exe` and all kinds of software you might be able to use for cheating. \n\nBefore the exam starts, you are assigned an instructor, who, apparently, is an employee at Pearson VUE, sitting in a call center somewhere. He or she asks you to upload pictures of your surrounding environment, including your desk. Also, you need to provide your ID card. The guy asked me to remove nearly everything except my notebook from my desk. You are not allowed to have pen and paper on your desk, no smartphone, no smartwatch, no food or drinks, except water. \n\nSo far so good. However, at some point, things started to get a little ridiculous for my taste. I have an external monitor on my desk, which I was asked to unplug and show the unplugged power supply cable on webcam. During the exam, you are constantly being monitored through cam and microphone. If your face disappears on the webcam, you failed. If another person enters the room, you failed. A point at which I started to get annoyed was when the guy told me to not speak and not move my lips. I tend to speak questions and answers to myself while I analyze and think about them. Not allowed. I wanted to take a short break for a moment and look out of the window. Immediately, the guy reached out to me to remind me that if I looked away from my screen again, he would have to cancel my exam. Being monitored in such a strict way was distracting. \n\nEventually I passed the exam after 90 minutes and 50 multiple-choice coding questions and got my certificate. The only disappointing thing about it is the fact that my last name, `Mtsch` is actually written `M%C3%Btsch` on it. I would not have guessed that non-ASCII characters like German umlauts would still be a problem in 2021. But anyway, I am happy that I passed the exam, which, indeed, was way harder than I would have expected.\n\nAll in all, I am still happy with this experience and would recommend it to any Java developer, too. ","source":"_posts/my-experiences-with-the-oracle-java-11-developer-certification.md","raw":"---\ntitle: My Experiences with the Oracle Java 11 Developer Certification\ndate: 2021-03-19 13:03:30\ntags:\n---\n\n![](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert2.png)\n\nOn the occasion of the Java programming language's 25th anniversary, Oracle just recently announced a pretty nice discount on the _Java 11 SE Developer Certificaion_. For a limited period of time you can get the exam, which normally costs something around $ 250 and the accompanying course, which, I believe, is even more expensive, for a total of $ 25.\n\n At [Frachtwerk](https://frachtwerk.de), we decided to take the chance and give all backend developers the opportunity to get certified. Today, I had my exam and here are my thought on the journey to get there.\n\n## The Course\nThe course's target audience includes developers, who already have a decent amount of experience with programming in Java, but not necessarily are experts, yet. Motivation for taking the course could be to further improve your skills or to specifically prepare for the certification. A certification like this, in turn, can help developers set themselves apart from potential competitors on the job market.\n\nI, personally, have been developing web application in Java for a few years now and would consider my experience and language skills fairly advanced. However, I was surprised how much I could still learn, though. I even got to know feature of Java that I haven't had heard of before.\n\nContents of the course range from the most essential things like types, variables, loops and conditions over inheritance principles, streams, I/O and JDBC throughout to low-level multi-threading and concurrency concepts and details about the fairly new Java 9 module system.\n\n![](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert1.png)\n\nOverall, my experience with the course was very good! All content is of overly high quality, explanations are clear and understandable and the topics covered are not at all only superficial, but indeed quite advanced. At the end of each section there are quizzes for reviewing your knowledge. \n\nHere are a few examples of such questions:\n\n![](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert6.png)\n![](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert4.png)\n![](https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert5.png)\n\nExperienced Java developers might easily skip the first ten chapters, however, the higher ones were actually very interesting to me.\n\nAmong others, I got to learn about how [type erasure](https://docs.oracle.com/javase/tutorial/java/generics/erasure.html) of generics works, what [heap pollution](https://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#heap_pollution) is, how the `volatile` type modifier functions and that [interfaces can have private, non-abstract methods](images/java_cert3.png).\n\nEspecially since it is free, I would definitely recommend the course, even to experienced Java developers. \n\n## The Exam\nAfter I had spent about 1.5 full work days for preparation, I eventually felt confident enough to register for the exam. It takes place online and you can choose from a variety of different dates and times. \n\nI was very satisfied with the learning path so far. However, there were a few points about the exam which I was not happy about. \n\nWhile the certification program itself is offered by Oracle, the exam takes place through an external provider called Pearson VUE. Being spezialized on online exams, they take various measures to prevent cheating. In principle, this is a positive thing. I could not take a certificate seriously if I knew that you could google during the exam or do partner work. However, in my opinion, things were a bit too strict here.\n\nFirst of all, you have to download and install a desktop application through which the exam itself is conducted. Unfortunately, it is only available for Windows and Mac and it turned out to be non-trivial to find a non-Linux computer with webcam and microphone at our office. The program runs full-screen and you can not exit to your desktop or other programs from it, which makes sense. Also, it automatically kills processes like `teamviewer.exe`, `anydesk.exe` and all kinds of software you might be able to use for cheating. \n\nBefore the exam starts, you are assigned an instructor, who, apparently, is an employee at Pearson VUE, sitting in a call center somewhere. He or she asks you to upload pictures of your surrounding environment, including your desk. Also, you need to provide your ID card. The guy asked me to remove nearly everything except my notebook from my desk. You are not allowed to have pen and paper on your desk, no smartphone, no smartwatch, no food or drinks, except water. \n\nSo far so good. However, at some point, things started to get a little ridiculous for my taste. I have an external monitor on my desk, which I was asked to unplug and show the unplugged power supply cable on webcam. During the exam, you are constantly being monitored through cam and microphone. If your face disappears on the webcam, you failed. If another person enters the room, you failed. A point at which I started to get annoyed was when the guy told me to not speak and not move my lips. I tend to speak questions and answers to myself while I analyze and think about them. Not allowed. I wanted to take a short break for a moment and look out of the window. Immediately, the guy reached out to me to remind me that if I looked away from my screen again, he would have to cancel my exam. Being monitored in such a strict way was distracting. \n\nEventually I passed the exam after 90 minutes and 50 multiple-choice coding questions and got my certificate. The only disappointing thing about it is the fact that my last name, `Mtsch` is actually written `M%C3%Btsch` on it. I would not have guessed that non-ASCII characters like German umlauts would still be a problem in 2021. But anyway, I am happy that I passed the exam, which, indeed, was way harder than I would have expected.\n\nAll in all, I am still happy with this experience and would recommend it to any Java developer, too. ","slug":"my-experiences-with-the-oracle-java-11-developer-certification","published":1,"updated":"2021-03-19T14:44:53.203Z","_id":"ckmgeljc90000vwe0h2vs06f6","comments":1,"layout":"post","photos":[],"link":"","content":"<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert2.png\"></p>\n<p>On the occasion of the Java programming languages 25th anniversary, Oracle just recently announced a pretty nice discount on the <em>Java 11 SE Developer Certificaion</em>. For a limited period of time you can get the exam, which normally costs something around $ 250 and the accompanying course, which, I believe, is even more expensive, for a total of $ 25.</p>\n<p> At <a href=\"https://frachtwerk.de/\">Frachtwerk</a>, we decided to take the chance and give all backend developers the opportunity to get certified. Today, I had my exam and here are my thought on the journey to get there.</p>\n<h2 id=\"The-Course\"><a href=\"#The-Course\" class=\"headerlink\" title=\"The Course\"></a>The Course</h2><p>The courses target audience includes developers, who already have a decent amount of experience with programming in Java, but not necessarily are experts, yet. Motivation for taking the course could be to further improve your skills or to specifically prepare for the certification. A certification like this, in turn, can help developers set themselves apart from potential competitors on the job market.</p>\n<p>I, personally, have been developing web application in Java for a few years now and would consider my experience and language skills fairly advanced. However, I was surprised how much I could still learn, though. I even got to know feature of Java that I havent had heard of before.</p>\n<p>Contents of the course range from the most essential things like types, variables, loops and conditions over inheritance principles, streams, I/O and JDBC throughout to low-level multi-threading and concurrency concepts and details about the fairly new Java 9 module system.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert1.png\"></p>\n<p>Overall, my experience with the course was very good! All content is of overly high quality, explanations are clear and understandable and the topics covered are not at all only superficial, but indeed quite advanced. At the end of each section there are quizzes for reviewing your knowledge. </p>\n<p>Here are a few examples of such questions:</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert6.png\"><br><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert4.png\"><br><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert5.png\"></p>\n<p>Experienced Java developers might easily skip the first ten chapters, however, the higher ones were actually very interesting to me.</p>\n<p>Among others, I got to learn about how <a href=\"https://docs.oracle.com/javase/tutorial/java/generics/erasure.html\">type erasure</a> of generics works, what <a href=\"https://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#heap_pollution\">heap pollution</a> is, how the <code>volatile</code> type modifier functions and that <a href=\"images/java_cert3.png\">interfaces can have private, non-abstract methods</a>.</p>\n<p>Especially since it is free, I would definitely recommend the course, even to experienced Java developers. </p>\n<h2 id=\"The-Exam\"><a href=\"#The-Exam\" class=\"headerlink\" title=\"The Exam\"></a>The Exam</h2><p>After I had spent about 1.5 full work days for preparation, I eventually felt confident enough to register for the exam. It takes place online and you can choose from a variety of different dates and times. </p>\n<p>I was very satisfied with the learning path so far. However, there were a few points about the exam which I was not happy about. </p>\n<p>While the certification program itself is offered by Oracle, the exam takes place through an external provider called Pearson VUE. Being spezialized on online exams, they take various measures to prevent cheating. In principle, this is a positive thing. I could not take a certificate seriously if I knew that you could google during the exam or do partner work. However, in my opinion, things were a bit too strict here.</p>\n<p>First of all, you have to download and install a desktop application through which the exam itself is conducted. Unfortunately, it is only available for Windows and Mac and it turned out to be non-trivial to find a non-Linux computer with webcam and microphone at our office. The program runs full-screen and you can not exit to your desktop or other programs from it, which makes sense. Also, it automatically kills processes like <code>teamviewer.exe</code>, <code>anydesk.exe</code> and all kinds of software you might be able to use for cheating. </p>\n<p>Before the exam starts, you are assigned an instructor, who, apparently, is an employee at Pearson VUE, sitting in a call center somewhere. He or she asks you to upload pictures of your surrounding environment, including your desk. Also, you need to provide your ID card. The guy asked me to remove nearly everything except my notebook from my desk. You are not allowed to have pen and paper on your desk, no smartphone, no smartwatch, no food or drinks, except water. </p>\n<p>So far so good. However, at some point, things started to get a little ridiculous for my taste. I have an external monitor on my desk, which I was asked to unplug and show the unplugged power supply cable on webcam. During the exam, you are constantly being monitored through cam and microphone. If your face disappears on the webcam, you failed. If another person enters the room, you failed. A point at which I started to get annoyed was when the guy told me to not speak and not move my lips. I tend to speak questions and answers to myself while I analyze and think about them. Not allowed. I wanted to take a short break for a moment and look out of the window. Immediately, the guy reached out to me to remind me that if I looked away from my screen again, he would have to cancel my exam. Being monitored in such a strict way was distracting. </p>\n<p>Eventually I passed the exam after 90 minutes and 50 multiple-choice coding questions and got my certificate. The only disappointing thing about it is the fact that my last name, <code>Mtsch</code> is actually written <code>M%C3%Btsch</code> on it. I would not have guessed that non-ASCII characters like German umlauts would still be a problem in 2021. But anyway, I am happy that I passed the exam, which, indeed, was way harder than I would have expected.</p>\n<p>All in all, I am still happy with this experience and would recommend it to any Java developer, too. </p>\n","site":{"data":{"badges":["https://badges.fw-web.space/badge/HTTP%2F2.0-enabled-2BBC8A?style=flat-square","https://badges.fw-web.space/badge/IPv6-enabled-2BBC8A?style=flat-square"],"projects":[{"name":" Wakapi","url":"https://github.com/muety/wakapi","desc":"A minimalistic, self-hosted WakaTime-compatible backend for coding statistics written in Go."},{"name":" KITSquid","url":"https://kitsquid.de","desc":"KITSquid is an independent, alternative course catalog for KIT students, including course recommendations and other community features. Open-source and written in Go."},{"name":" Wafflr","url":"https://wafflr.de","desc":"Wafflr is a modern web app for cafs and ice-cream shops. Customers can use it to quickly and easily place orders, while employees get a live overview of all pending orders. Made with GraphQL, Go and Vue."},{"name":" Anchr for Android","url":"https://github.com/muety/anchr-android","desc":"An Android app, build with Dart and Flutter, to manage links and bookmarks using Anchr.io on your mobile device."},{"name":" QuizNerd for Android","url":"https://play.google.com/store/apps/details?id=com.github.muety.quiznerd","desc":"A multi-player trivia Android game made for developers and IT enthusiasts."},{"name":" MiniNote","url":"https://github.com/muety/mininote","desc":"A simple, persistent, self-hosted Markdown note-taking app built with VueJS."},{"name":" Webhook2Telegram","url":"https://github.com/muety/webhook2telegram","desc":"A Telegram bot which translates push messages sent as simple HTTP calls into Telegram messages you can subscribe to. Written in Go."},{"name":" Anchr.io","url":"https://github.com/muety/anchr","desc":"A toolbox service for maintaining bookmark collections, shortening links and uploading and sharing images encryptedly."},{"name":" Telegram ExpenseBot","url":"https://muetsch.io/telegram-expensebot-doodlerbot.html","desc":"A Telegram bot to manage your daily expenses and keep track of your financial situation."}]}},"excerpt":"","more":"<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert2.png\"></p>\n<p>On the occasion of the Java programming languages 25th anniversary, Oracle just recently announced a pretty nice discount on the <em>Java 11 SE Developer Certificaion</em>. For a limited period of time you can get the exam, which normally costs something around $ 250 and the accompanying course, which, I believe, is even more expensive, for a total of $ 25.</p>\n<p> At <a href=\"https://frachtwerk.de/\">Frachtwerk</a>, we decided to take the chance and give all backend developers the opportunity to get certified. Today, I had my exam and here are my thought on the journey to get there.</p>\n<h2 id=\"The-Course\"><a href=\"#The-Course\" class=\"headerlink\" title=\"The Course\"></a>The Course</h2><p>The courses target audience includes developers, who already have a decent amount of experience with programming in Java, but not necessarily are experts, yet. Motivation for taking the course could be to further improve your skills or to specifically prepare for the certification. A certification like this, in turn, can help developers set themselves apart from potential competitors on the job market.</p>\n<p>I, personally, have been developing web application in Java for a few years now and would consider my experience and language skills fairly advanced. However, I was surprised how much I could still learn, though. I even got to know feature of Java that I havent had heard of before.</p>\n<p>Contents of the course range from the most essential things like types, variables, loops and conditions over inheritance principles, streams, I/O and JDBC throughout to low-level multi-threading and concurrency concepts and details about the fairly new Java 9 module system.</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert1.png\"></p>\n<p>Overall, my experience with the course was very good! All content is of overly high quality, explanations are clear and understandable and the topics covered are not at all only superficial, but indeed quite advanced. At the end of each section there are quizzes for reviewing your knowledge. </p>\n<p>Here are a few examples of such questions:</p>\n<p><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert6.png\"><br><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert4.png\"><br><img src=\"https://apps.muetsch.io/images/o:auto/rs,s:640?image=https://muetsch.io/images/java_cert5.png\"></p>\n<p>Experienced Java developers might easily skip the first ten chapters, however, the higher ones were actually very interesting to me.</p>\n<p>Among others, I got to learn about how <a href=\"https://docs.oracle.com/javase/tutorial/java/generics/erasure.html\">type erasure</a> of generics works, what <a href=\"https://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#heap_pollution\">heap pollution</a> is, how the <code>volatile</code> type modifier functions and that <a href=\"images/java_cert3.png\">interfaces can have private, non-abstract methods</a>.</p>\n<p>Especially since it is free, I would definitely recommend the course, even to experienced Java developers. </p>\n<h2 id=\"The-Exam\"><a href=\"#The-Exam\" class=\"headerlink\" title=\"The Exam\"></a>The Exam</h2><p>After I had spent about 1.5 full work days for preparation, I eventually felt confident enough to register for the exam. It takes place online and you can choose from a variety of different dates and times. </p>\n<p>I was very satisfied with the learning path so far. However, there were a few points about the exam which I was not happy about. </p>\n<p>While the certification program itself is offered by Oracle, the exam takes place through an external provider called Pearson VUE. Being spezialized on online exams, they take various measures to prevent cheating. In principle, this is a positive thing. I could not take a certificate seriously if I knew that you could google during the exam or do partner work. However, in my opinion, things were a bit too strict here.</p>\n<p>First of all, you have to download and install a desktop application through which the exam itself is conducted. Unfortunately, it is only available for Windows and Mac and it turned out to be non-trivial to find a non-Linux computer with webcam and microphone at our office. The program runs full-screen and you can not exit to your desktop or other programs from it, which makes sense. Also, it automatically kills processes like <code>teamviewer.exe</code>, <code>anydesk.exe</code> and all kinds of software you might be able to use for cheating. </p>\n<p>Before the exam starts, you are assigned an instructor, who, apparently, is an employee at Pearson VUE, sitting in a call center somewhere. He or she asks you to upload pictures of your surrounding environment, including your desk. Also, you need to provide your ID card. The guy asked me to remove nearly everything except my notebook from my desk. You are not allowed to have pen and paper on your desk, no smartphone, no smartwatch, no food or drinks, except water. </p>\n<p>So far so good. However, at some point, things started to get a little ridiculous for my taste. I have an external monitor on my desk, which I was asked to unplug and show the unplugged power supply cable on webcam. During the exam, you are constantly being monitored through cam and microphone. If your face disappears on the webcam, you failed. If another person enters the room, you failed. A point at which I started to get annoyed was when the guy told me to not speak and not move my lips. I tend to speak questions and answers to myself while I analyze and think about them. Not allowed. I wanted to take a short break for a moment and look out of the window. Immediately, the guy reached out to me to remind me that if I looked away from my screen again, he would have to cancel my exam. Being monitored in such a strict way was distracting. </p>\n<p>Eventually I passed the exam after 90 minutes and 50 multiple-choice coding questions and got my certificate. The only disappointing thing about it is the fact that my last name, <code>Mtsch</code> is actually written <code>M%C3%Btsch</code> on it. I would not have guessed that non-ASCII characters like German umlauts would still be a problem in 2021. But anyway, I am happy that I passed the exam, which, indeed, was way harder than I would have expected.</p>\n<p>All in all, I am still happy with this experience and would recommend it to any Java developer, too. </p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}